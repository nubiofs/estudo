author@id@title@summary@published_year
Hiroki Hashiguchi@http://arxiv.org/abs/1201.0472v4@"Holonomic gradient method for the distribution function of the largest
  root of a Wishart matrix"@"We apply the holonomic gradient method introduced by Nakayama et al.(2011) to
the evaluation of the exact distribution function of the largest root of a
Wishart matrix, which involves a hypergeometric function 1F1 of a matrix
argument. Numerical evaluation of the hypergeometric function has been one of
the longstanding problems in multivariate distribution theory. The holonomic
gradient method offers a totally new approach, which is complementary to the
infinite series expansion around the origin in terms of zonal polynomials. It
allows us to move away from the origin by the use of partial differential
equations satisfied by the hypergeometric function. From numerical viewpoint we
show that the method works well up to dimension 10. From theoretical viewpoint
the method offers many challenging problems both to statistics and D-module
theory."@2012
Yasuhide Numata@http://arxiv.org/abs/1201.0472v4@"Holonomic gradient method for the distribution function of the largest
  root of a Wishart matrix"@"We apply the holonomic gradient method introduced by Nakayama et al.(2011) to
the evaluation of the exact distribution function of the largest root of a
Wishart matrix, which involves a hypergeometric function 1F1 of a matrix
argument. Numerical evaluation of the hypergeometric function has been one of
the longstanding problems in multivariate distribution theory. The holonomic
gradient method offers a totally new approach, which is complementary to the
infinite series expansion around the origin in terms of zonal polynomials. It
allows us to move away from the origin by the use of partial differential
equations satisfied by the hypergeometric function. From numerical viewpoint we
show that the method works well up to dimension 10. From theoretical viewpoint
the method offers many challenging problems both to statistics and D-module
theory."@2012
Nobuki Takayama@http://arxiv.org/abs/1201.0472v4@"Holonomic gradient method for the distribution function of the largest
  root of a Wishart matrix"@"We apply the holonomic gradient method introduced by Nakayama et al.(2011) to
the evaluation of the exact distribution function of the largest root of a
Wishart matrix, which involves a hypergeometric function 1F1 of a matrix
argument. Numerical evaluation of the hypergeometric function has been one of
the longstanding problems in multivariate distribution theory. The holonomic
gradient method offers a totally new approach, which is complementary to the
infinite series expansion around the origin in terms of zonal polynomials. It
allows us to move away from the origin by the use of partial differential
equations satisfied by the hypergeometric function. From numerical viewpoint we
show that the method works well up to dimension 10. From theoretical viewpoint
the method offers many challenging problems both to statistics and D-module
theory."@2012
Akimichi Takemura@http://arxiv.org/abs/1201.0472v4@"Holonomic gradient method for the distribution function of the largest
  root of a Wishart matrix"@"We apply the holonomic gradient method introduced by Nakayama et al.(2011) to
the evaluation of the exact distribution function of the largest root of a
Wishart matrix, which involves a hypergeometric function 1F1 of a matrix
argument. Numerical evaluation of the hypergeometric function has been one of
the longstanding problems in multivariate distribution theory. The holonomic
gradient method offers a totally new approach, which is complementary to the
infinite series expansion around the origin in terms of zonal polynomials. It
allows us to move away from the origin by the use of partial differential
equations satisfied by the hypergeometric function. From numerical viewpoint we
show that the method works well up to dimension 10. From theoretical viewpoint
the method offers many challenging problems both to statistics and D-module
theory."@2012
Gérard Biau@http://arxiv.org/abs/1201.0586v2@An Affine Invariant $k$-Nearest Neighbor Regression Estimate@"We design a data-dependent metric in $\mathbb R^d$ and use it to define the
$k$-nearest neighbors of a given point. Our metric is invariant under all
affine transformations. We show that, with this metric, the standard
$k$-nearest neighbor regression estimate is asymptotically consistent under the
usual conditions on $k$, and minimal requirements on the input data."@2012
Luc Devroye@http://arxiv.org/abs/1201.0586v2@An Affine Invariant $k$-Nearest Neighbor Regression Estimate@"We design a data-dependent metric in $\mathbb R^d$ and use it to define the
$k$-nearest neighbors of a given point. Our metric is invariant under all
affine transformations. We show that, with this metric, the standard
$k$-nearest neighbor regression estimate is asymptotically consistent under the
usual conditions on $k$, and minimal requirements on the input data."@2012
Vida Dujmovic@http://arxiv.org/abs/1201.0586v2@An Affine Invariant $k$-Nearest Neighbor Regression Estimate@"We design a data-dependent metric in $\mathbb R^d$ and use it to define the
$k$-nearest neighbors of a given point. Our metric is invariant under all
affine transformations. We show that, with this metric, the standard
$k$-nearest neighbor regression estimate is asymptotically consistent under the
usual conditions on $k$, and minimal requirements on the input data."@2012
Adam Krzyzak@http://arxiv.org/abs/1201.0586v2@An Affine Invariant $k$-Nearest Neighbor Regression Estimate@"We design a data-dependent metric in $\mathbb R^d$ and use it to define the
$k$-nearest neighbors of a given point. Our metric is invariant under all
affine transformations. We show that, with this metric, the standard
$k$-nearest neighbor regression estimate is asymptotically consistent under the
usual conditions on $k$, and minimal requirements on the input data."@2012
Luo Xiao@http://arxiv.org/abs/1201.0708v3@Local Asymptotics of P-splines@"This report studies local asymptotics of P-splines with $p$th degree
B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$
restricted is extended to the general case. Asymptotically, penalized splines
are kernel estimators with equivalent kernels depending on $m$, but not on $p$.
A central limit theorem provides simple expressions for the asymptotic mean and
variance. Provided it is fast enough, the divergence rate of the number of
knots does not affect the asymptotic distribution. The optimal convergence rate
of the penalty parameter is given."@2012
Yingxing Li@http://arxiv.org/abs/1201.0708v3@Local Asymptotics of P-splines@"This report studies local asymptotics of P-splines with $p$th degree
B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$
restricted is extended to the general case. Asymptotically, penalized splines
are kernel estimators with equivalent kernels depending on $m$, but not on $p$.
A central limit theorem provides simple expressions for the asymptotic mean and
variance. Provided it is fast enough, the divergence rate of the number of
knots does not affect the asymptotic distribution. The optimal convergence rate
of the penalty parameter is given."@2012
Tatiyana V. Apanasovich@http://arxiv.org/abs/1201.0708v3@Local Asymptotics of P-splines@"This report studies local asymptotics of P-splines with $p$th degree
B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$
restricted is extended to the general case. Asymptotically, penalized splines
are kernel estimators with equivalent kernels depending on $m$, but not on $p$.
A central limit theorem provides simple expressions for the asymptotic mean and
variance. Provided it is fast enough, the divergence rate of the number of
knots does not affect the asymptotic distribution. The optimal convergence rate
of the penalty parameter is given."@2012
David Ruppert@http://arxiv.org/abs/1201.0708v3@Local Asymptotics of P-splines@"This report studies local asymptotics of P-splines with $p$th degree
B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$
restricted is extended to the general case. Asymptotically, penalized splines
are kernel estimators with equivalent kernels depending on $m$, but not on $p$.
A central limit theorem provides simple expressions for the asymptotic mean and
variance. Provided it is fast enough, the divergence rate of the number of
knots does not affect the asymptotic distribution. The optimal convergence rate
of the penalty parameter is given."@2012
Ole E. Barndorff-Nielsen@http://arxiv.org/abs/1201.0868v1@Multipower variation for Brownian semistationary processes@"In this paper we study the asymptotic behaviour of power and multipower
variations of processes $Y$:\[Y_t=\int_{-\in
fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where
$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma >0$ is a random
process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process
in the nature of a drift term. Processes of this type serve, in particular, to
model data of velocity increments of a fluid in a turbulence regime with spot
intermittency $\sigma$. The purpose of this paper is to determine the
probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis
for studying properties of the intermittency process $\sigma$. Notably the
processes $Y$ are in general not of the semimartingale kind and the established
theory of multipower variation for semimartingales does not suffice for
deriving the limit properties. As a key tool for the results, a general central
limit theorem for triangular Gaussian schemes is formulated and proved.
Examples and an application to the realised variance ratio are given."@2012
José Manuel Corcuera@http://arxiv.org/abs/1201.0868v1@Multipower variation for Brownian semistationary processes@"In this paper we study the asymptotic behaviour of power and multipower
variations of processes $Y$:\[Y_t=\int_{-\in
fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where
$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma >0$ is a random
process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process
in the nature of a drift term. Processes of this type serve, in particular, to
model data of velocity increments of a fluid in a turbulence regime with spot
intermittency $\sigma$. The purpose of this paper is to determine the
probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis
for studying properties of the intermittency process $\sigma$. Notably the
processes $Y$ are in general not of the semimartingale kind and the established
theory of multipower variation for semimartingales does not suffice for
deriving the limit properties. As a key tool for the results, a general central
limit theorem for triangular Gaussian schemes is formulated and proved.
Examples and an application to the realised variance ratio are given."@2012
Mark Podolskij@http://arxiv.org/abs/1201.0868v1@Multipower variation for Brownian semistationary processes@"In this paper we study the asymptotic behaviour of power and multipower
variations of processes $Y$:\[Y_t=\int_{-\in
fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where
$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma >0$ is a random
process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process
in the nature of a drift term. Processes of this type serve, in particular, to
model data of velocity increments of a fluid in a turbulence regime with spot
intermittency $\sigma$. The purpose of this paper is to determine the
probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis
for studying properties of the intermittency process $\sigma$. Notably the
processes $Y$ are in general not of the semimartingale kind and the established
theory of multipower variation for semimartingales does not suffice for
deriving the limit properties. As a key tool for the results, a general central
limit theorem for triangular Gaussian schemes is formulated and proved.
Examples and an application to the realised variance ratio are given."@2012
Yuqiang Li@http://arxiv.org/abs/1201.0872v1@Approximations of fractional Brownian motion@"Approximations of fractional Brownian motion using Poisson processes whose
parameter sets have the same dimensions as the approximated processes have been
studied in the literature. In this paper, a special approximation to the
one-parameter fractional Brownian motion is constructed using a two-parameter
Poisson process. The proof involves the tightness and identification of
finite-dimensional distributions."@2012
Hongshuai Dai@http://arxiv.org/abs/1201.0872v1@Approximations of fractional Brownian motion@"Approximations of fractional Brownian motion using Poisson processes whose
parameter sets have the same dimensions as the approximated processes have been
studied in the literature. In this paper, a special approximation to the
one-parameter fractional Brownian motion is constructed using a two-parameter
Poisson process. The proof involves the tightness and identification of
finite-dimensional distributions."@2012
Nelson Antunes@http://arxiv.org/abs/1201.1076v1@Probabilistic sampling of finite renewal processes@"Consider a finite renewal process in the sense that interrenewal times are
positive i.i.d. variables and the total number of renewals is a random
variable, independent of interrenewal times. A finite point process can be
obtained by probabilistic sampling of the finite renewal process, where each
renewal is sampled with a fixed probability and independently of other
renewals. The problem addressed in this work concerns statistical inference of
the original distributions of the total number of renewals and interrenewal
times from a sample of i.i.d. finite point processes obtained by sampling
finite renewal processes. This problem is motivated by traffic measurements in
the Internet in order to characterize flows of packets (which can be seen as
finite renewal processes) and where the use of packet sampling is becoming
prevalent due to increasing link speeds and limited storage and processing
capacities."@2012
Vladas Pipiras@http://arxiv.org/abs/1201.1076v1@Probabilistic sampling of finite renewal processes@"Consider a finite renewal process in the sense that interrenewal times are
positive i.i.d. variables and the total number of renewals is a random
variable, independent of interrenewal times. A finite point process can be
obtained by probabilistic sampling of the finite renewal process, where each
renewal is sampled with a fixed probability and independently of other
renewals. The problem addressed in this work concerns statistical inference of
the original distributions of the total number of renewals and interrenewal
times from a sample of i.i.d. finite point processes obtained by sampling
finite renewal processes. This problem is motivated by traffic measurements in
the Internet in order to characterize flows of packets (which can be seen as
finite renewal processes) and where the use of packet sampling is becoming
prevalent due to increasing link speeds and limited storage and processing
capacities."@2012
István Berkes@http://arxiv.org/abs/1201.1124v1@Asymptotics of trimmed CUSUM statistics@"There is a wide literature on change point tests, but the case of variables
with infinite variances is essentially unexplored. In this paper we address
this problem by studying the asymptotic behavior of trimmed CUSUM statistics.
We show that in a location model with i.i.d. errors in the domain of attraction
of a stable law of parameter $0<\alpha <2$, the appropriately trimmed CUSUM
process converges weakly to a Brownian bridge. Thus, after moderate trimming,
the classical method for detecting change points remains valid also for
populations with infinite variance. We note that according to the classical
theory, the partial sums of trimmed variables are generally not asymptotically
normal and using random centering in the test statistics is crucial in the
infinite variance case. We also show that the partial sums of truncated and
trimmed random variables have different asymptotic behavior. Finally, we
discuss resampling procedures which enable one to determine critical values in
the case of small and moderate sample sizes."@2012
Lajos Horváth@http://arxiv.org/abs/1201.1124v1@Asymptotics of trimmed CUSUM statistics@"There is a wide literature on change point tests, but the case of variables
with infinite variances is essentially unexplored. In this paper we address
this problem by studying the asymptotic behavior of trimmed CUSUM statistics.
We show that in a location model with i.i.d. errors in the domain of attraction
of a stable law of parameter $0<\alpha <2$, the appropriately trimmed CUSUM
process converges weakly to a Brownian bridge. Thus, after moderate trimming,
the classical method for detecting change points remains valid also for
populations with infinite variance. We note that according to the classical
theory, the partial sums of trimmed variables are generally not asymptotically
normal and using random centering in the test statistics is crucial in the
infinite variance case. We also show that the partial sums of truncated and
trimmed random variables have different asymptotic behavior. Finally, we
discuss resampling procedures which enable one to determine critical values in
the case of small and moderate sample sizes."@2012
Johannes Schauer@http://arxiv.org/abs/1201.1124v1@Asymptotics of trimmed CUSUM statistics@"There is a wide literature on change point tests, but the case of variables
with infinite variances is essentially unexplored. In this paper we address
this problem by studying the asymptotic behavior of trimmed CUSUM statistics.
We show that in a location model with i.i.d. errors in the domain of attraction
of a stable law of parameter $0<\alpha <2$, the appropriately trimmed CUSUM
process converges weakly to a Brownian bridge. Thus, after moderate trimming,
the classical method for detecting change points remains valid also for
populations with infinite variance. We note that according to the classical
theory, the partial sums of trimmed variables are generally not asymptotically
normal and using random centering in the test statistics is crucial in the
infinite variance case. We also show that the partial sums of truncated and
trimmed random variables have different asymptotic behavior. Finally, we
discuss resampling procedures which enable one to determine critical values in
the case of small and moderate sample sizes."@2012
Marten Wegkamp@http://arxiv.org/abs/1201.1140v1@Support vector machines with a reject option@"This paper studies $\ell_1$ regularization with high-dimensional features for
support vector machines with a built-in reject option (meaning that the
decision of classifying an observation can be withheld at a cost lower than
that of misclassification). The procedure can be conveniently implemented as a
linear program and computed using standard software. We prove that the
minimizer of the penalized population risk favors sparse solutions and show
that the behavior of the empirical risk minimizer mimics that of the population
risk minimizer. We also introduce a notion of classification complexity and
prove that our minimizers adapt to the unknown complexity. Using a novel oracle
inequality for the excess risk, we identify situations where fast rates of
convergence occur."@2012
Ming Yuan@http://arxiv.org/abs/1201.1140v1@Support vector machines with a reject option@"This paper studies $\ell_1$ regularization with high-dimensional features for
support vector machines with a built-in reject option (meaning that the
decision of classifying an observation can be withheld at a cost lower than
that of misclassification). The procedure can be conveniently implemented as a
linear program and computed using standard software. We prove that the
minimizer of the penalized population risk favors sparse solutions and show
that the behavior of the empirical risk minimizer mimics that of the population
risk minimizer. We also introduce a notion of classification complexity and
prove that our minimizers adapt to the unknown complexity. Using a novel oracle
inequality for the excess risk, we identify situations where fast rates of
convergence occur."@2012
Yonggang Hu@http://arxiv.org/abs/1201.1146v1@Tensor-based projection depth@"The conventional definition of a depth function is vector-based. In this
paper, a novel projection depth (PD) technique directly based on tensors, such
as matrices, is instead proposed. Tensor projection depth (TPD) is still an
ideal depth function and its computation can be achieved through the iteration
of PD. Furthermore, we also discuss the cases for sparse samples and higher
order tensors. Experimental results in data classification with the two
projection depths show that TPD performs much better than PD for data with a
natural tensor form, and even when the data have a natural vector form, TPD
appears to perform no worse than PD."@2012
Yong Wang@http://arxiv.org/abs/1201.1146v1@Tensor-based projection depth@"The conventional definition of a depth function is vector-based. In this
paper, a novel projection depth (PD) technique directly based on tensors, such
as matrices, is instead proposed. Tensor projection depth (TPD) is still an
ideal depth function and its computation can be achieved through the iteration
of PD. Furthermore, we also discuss the cases for sparse samples and higher
order tensors. Experimental results in data classification with the two
projection depths show that TPD performs much better than PD for data with a
natural tensor form, and even when the data have a natural vector form, TPD
appears to perform no worse than PD."@2012
Yi Wu@http://arxiv.org/abs/1201.1146v1@Tensor-based projection depth@"The conventional definition of a depth function is vector-based. In this
paper, a novel projection depth (PD) technique directly based on tensors, such
as matrices, is instead proposed. Tensor projection depth (TPD) is still an
ideal depth function and its computation can be achieved through the iteration
of PD. Furthermore, we also discuss the cases for sparse samples and higher
order tensors. Experimental results in data classification with the two
projection depths show that TPD performs much better than PD for data with a
natural tensor form, and even when the data have a natural vector form, TPD
appears to perform no worse than PD."@2012
Jianhua Hu@http://arxiv.org/abs/1201.1155v1@"Estimation for an additive growth curve model with orthogonal design
  matrices"@"An additive growth curve model with orthogonal design matrices is proposed in
which observations may have different profile forms. The proposed model allows
us to fit data and then estimate parameters in a more parsimonious way than the
traditional growth curve model. Two-stage generalized least-squares estimators
for the regression coefficients are derived where a quadratic estimator for the
covariance of observations is taken as the first-stage estimator. Consistency,
asymptotic normality and asymptotic independence of these estimators are
investigated. Simulation studies and a numerical example are given to
illustrate the efficiency and parsimony of the proposed model for model
specifications in the sense of minimizing Akaike's information criterion (AIC)."@2012
Guohua Yan@http://arxiv.org/abs/1201.1155v1@"Estimation for an additive growth curve model with orthogonal design
  matrices"@"An additive growth curve model with orthogonal design matrices is proposed in
which observations may have different profile forms. The proposed model allows
us to fit data and then estimate parameters in a more parsimonious way than the
traditional growth curve model. Two-stage generalized least-squares estimators
for the regression coefficients are derived where a quadratic estimator for the
covariance of observations is taken as the first-stage estimator. Consistency,
asymptotic normality and asymptotic independence of these estimators are
investigated. Simulation studies and a numerical example are given to
illustrate the efficiency and parsimony of the proposed model for model
specifications in the sense of minimizing Akaike's information criterion (AIC)."@2012
Jinhong You@http://arxiv.org/abs/1201.1155v1@"Estimation for an additive growth curve model with orthogonal design
  matrices"@"An additive growth curve model with orthogonal design matrices is proposed in
which observations may have different profile forms. The proposed model allows
us to fit data and then estimate parameters in a more parsimonious way than the
traditional growth curve model. Two-stage generalized least-squares estimators
for the regression coefficients are derived where a quadratic estimator for the
covariance of observations is taken as the first-stage estimator. Consistency,
asymptotic normality and asymptotic independence of these estimators are
investigated. Simulation studies and a numerical example are given to
illustrate the efficiency and parsimony of the proposed model for model
specifications in the sense of minimizing Akaike's information criterion (AIC)."@2012
Abhishek Bhattacharya@http://arxiv.org/abs/1201.1166v1@Resampling in Time Series Models@"This project revolves around studying estimators for parameters in different
Time Series models and studying their assymptotic properties. We introduce
various bootstrap techniques for the estimators obtained. Our special emphasis
is on Weighted Bootstrap. We establish the consistency of this scheme in a AR
model and its variations. Numerical calculations lend further support to our
consistency results. Next we analyze ARCH models, and study various estimators
used for different error distributions. We also present resampling techniques
for estimating the distribution of the estimators. Finally by simulating data,
we analyze the numerical properties of the estimators."@2012
Arup Bose@http://arxiv.org/abs/1201.1166v1@Resampling in Time Series Models@"This project revolves around studying estimators for parameters in different
Time Series models and studying their assymptotic properties. We introduce
various bootstrap techniques for the estimators obtained. Our special emphasis
is on Weighted Bootstrap. We establish the consistency of this scheme in a AR
model and its variations. Numerical calculations lend further support to our
consistency results. Next we analyze ARCH models, and study various estimators
used for different error distributions. We also present resampling techniques
for estimating the distribution of the estimators. Finally by simulating data,
we analyze the numerical properties of the estimators."@2012
Subhajit Dutta@http://arxiv.org/abs/1201.1171v1@Some intriguing properties of Tukey's half-space depth@"For multivariate data, Tukey's half-space depth is one of the most popular
depth functions available in the literature. It is conceptually simple and
satisfies several desirable properties of depth functions. The Tukey median,
the multivariate median associated with the half-space depth, is also a
well-known measure of center for multivariate data with several interesting
properties. In this article, we derive and investigate some interesting
properties of half-space depth and its associated multivariate median. These
properties, some of which are counterintuitive, have important statistical
consequences in multivariate analysis. We also investigate a natural extension
of Tukey's half-space depth and the related median for probability
distributions on any Banach space (which may be finite- or
infinite-dimensional) and prove some results that demonstrate anomalous
behavior of half-space depth in infinite-dimensional spaces."@2012
Anil K. Ghosh@http://arxiv.org/abs/1201.1171v1@Some intriguing properties of Tukey's half-space depth@"For multivariate data, Tukey's half-space depth is one of the most popular
depth functions available in the literature. It is conceptually simple and
satisfies several desirable properties of depth functions. The Tukey median,
the multivariate median associated with the half-space depth, is also a
well-known measure of center for multivariate data with several interesting
properties. In this article, we derive and investigate some interesting
properties of half-space depth and its associated multivariate median. These
properties, some of which are counterintuitive, have important statistical
consequences in multivariate analysis. We also investigate a natural extension
of Tukey's half-space depth and the related median for probability
distributions on any Banach space (which may be finite- or
infinite-dimensional) and prove some results that demonstrate anomalous
behavior of half-space depth in infinite-dimensional spaces."@2012
Probal Chaudhuri@http://arxiv.org/abs/1201.1171v1@Some intriguing properties of Tukey's half-space depth@"For multivariate data, Tukey's half-space depth is one of the most popular
depth functions available in the literature. It is conceptually simple and
satisfies several desirable properties of depth functions. The Tukey median,
the multivariate median associated with the half-space depth, is also a
well-known measure of center for multivariate data with several interesting
properties. In this article, we derive and investigate some interesting
properties of half-space depth and its associated multivariate median. These
properties, some of which are counterintuitive, have important statistical
consequences in multivariate analysis. We also investigate a natural extension
of Tukey's half-space depth and the related median for probability
distributions on any Banach space (which may be finite- or
infinite-dimensional) and prove some results that demonstrate anomalous
behavior of half-space depth in infinite-dimensional spaces."@2012
Efstathios Paparoditis@http://arxiv.org/abs/1201.2617v1@"Short-Term Load Forecasting: The Similar Shape Functional Time Series
  Predictor"@"We introduce a novel functional time series methodology for short-term load
forecasting. The prediction is performed by means of a weighted average of past
daily load segments, the shape of which is similar to the expected shape of the
load segment to be predicted. The past load segments are identified from the
available history of the observed load segments by means of their closeness to
a so-called reference load segment, the later being selected in a manner that
captures the expected qualitative and quantitative characteristics of the load
segment to be predicted. Weak consistency of the suggested functional similar
shape predictor is established. As an illustration, we apply the suggested
functional time series forecasting methodology to historical daily load data in
Cyprus and compare its performance to that of a recently proposed alternative
functional time series methodology for short-term load forecasting."@2012
Theofanis Sapatinas@http://arxiv.org/abs/1201.2617v1@"Short-Term Load Forecasting: The Similar Shape Functional Time Series
  Predictor"@"We introduce a novel functional time series methodology for short-term load
forecasting. The prediction is performed by means of a weighted average of past
daily load segments, the shape of which is similar to the expected shape of the
load segment to be predicted. The past load segments are identified from the
available history of the observed load segments by means of their closeness to
a so-called reference load segment, the later being selected in a manner that
captures the expected qualitative and quantitative characteristics of the load
segment to be predicted. Weak consistency of the suggested functional similar
shape predictor is established. As an illustration, we apply the suggested
functional time series forecasting methodology to historical daily load data in
Cyprus and compare its performance to that of a recently proposed alternative
functional time series methodology for short-term load forecasting."@2012
Ryan Martin@http://arxiv.org/abs/1201.3102v7@"A note on Bayesian convergence rates under local prior support
  conditions"@"Bounds on Bayesian posterior convergence rates, assuming the prior satisfies
both local and global support conditions, are now readily available. In this
paper we explore, in the context of density estimation, Bayesian convergence
rates assuming only local prior support conditions. Our results give optimal
rates under minimal conditions using very simple arguments."@2012
Liang Hong@http://arxiv.org/abs/1201.3102v7@"A note on Bayesian convergence rates under local prior support
  conditions"@"Bounds on Bayesian posterior convergence rates, assuming the prior satisfies
both local and global support conditions, are now readily available. In this
paper we explore, in the context of density estimation, Bayesian convergence
rates assuming only local prior support conditions. Our results give optimal
rates under minimal conditions using very simple arguments."@2012
Stephen G. Walker@http://arxiv.org/abs/1201.3102v7@"A note on Bayesian convergence rates under local prior support
  conditions"@"Bounds on Bayesian posterior convergence rates, assuming the prior satisfies
both local and global support conditions, are now readily available. In this
paper we explore, in the context of density estimation, Bayesian convergence
rates assuming only local prior support conditions. Our results give optimal
rates under minimal conditions using very simple arguments."@2012
Tamio Koyama@http://arxiv.org/abs/1201.3239v5@"Holonomic Gradient Descent for the Fisher-Bingham Distribution on the
  $d$-dimensional Sphere"@"We propose an accelerated version of the holonomic gradient descent and apply
it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham
distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an
integrable connection) and a series expansion associated with the normalizing
constant with an error estimation. These enable us to solve some MLE problems
up to dimension $d=7$ with a specified accuracy."@2012
Hiromasa Nakayama@http://arxiv.org/abs/1201.3239v5@"Holonomic Gradient Descent for the Fisher-Bingham Distribution on the
  $d$-dimensional Sphere"@"We propose an accelerated version of the holonomic gradient descent and apply
it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham
distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an
integrable connection) and a series expansion associated with the normalizing
constant with an error estimation. These enable us to solve some MLE problems
up to dimension $d=7$ with a specified accuracy."@2012
Kenta Nishiyama@http://arxiv.org/abs/1201.3239v5@"Holonomic Gradient Descent for the Fisher-Bingham Distribution on the
  $d$-dimensional Sphere"@"We propose an accelerated version of the holonomic gradient descent and apply
it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham
distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an
integrable connection) and a series expansion associated with the normalizing
constant with an error estimation. These enable us to solve some MLE problems
up to dimension $d=7$ with a specified accuracy."@2012
Nobuki Takayama@http://arxiv.org/abs/1201.3239v5@"Holonomic Gradient Descent for the Fisher-Bingham Distribution on the
  $d$-dimensional Sphere"@"We propose an accelerated version of the holonomic gradient descent and apply
it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham
distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an
integrable connection) and a series expansion associated with the normalizing
constant with an error estimation. These enable us to solve some MLE problems
up to dimension $d=7$ with a specified accuracy."@2012
Sébastien Loustau@http://arxiv.org/abs/1201.3283v3@Minimax fast rates for discriminant analysis with errors in variables@"The effect of measurement errors in discriminant analysis is investigated.
Given observations $Z=X+\epsilon$, where $\epsilon$ denotes a random noise, the
goal is to predict the density of $X$ among two possible candidates $f$ and
$g$. We suppose that we have at our disposal two learning samples. The aim is
to approach the best possible decision rule $G^\star$ defined as a minimizer of
the Bayes risk. In the free-noise case $(\epsilon=0)$, minimax fast rates of
convergence are well-known under the margin assumption in discriminant analysis
(see \cite{mammen}) or in the more general classification framework (see
\cite{tsybakov2004,AT}). In this paper we intend to establish similar results
in the noisy case, i.e. when dealing with errors in variables. We prove minimax
lower bounds for this problem and explain how can these rates be attained,
using in particular an Empirical Risk Minimizer (ERM) method based on
deconvolution kernel estimators."@2012
Clément Marteau@http://arxiv.org/abs/1201.3283v3@Minimax fast rates for discriminant analysis with errors in variables@"The effect of measurement errors in discriminant analysis is investigated.
Given observations $Z=X+\epsilon$, where $\epsilon$ denotes a random noise, the
goal is to predict the density of $X$ among two possible candidates $f$ and
$g$. We suppose that we have at our disposal two learning samples. The aim is
to approach the best possible decision rule $G^\star$ defined as a minimizer of
the Bayes risk. In the free-noise case $(\epsilon=0)$, minimax fast rates of
convergence are well-known under the margin assumption in discriminant analysis
(see \cite{mammen}) or in the more general classification framework (see
\cite{tsybakov2004,AT}). In this paper we intend to establish similar results
in the noisy case, i.e. when dealing with errors in variables. We prove minimax
lower bounds for this problem and explain how can these rates be attained,
using in particular an Empirical Risk Minimizer (ERM) method based on
deconvolution kernel estimators."@2012
E. Rufeil Fiori@http://arxiv.org/abs/1201.4507v2@A Shannon-Tsallis transformation@"We determine a general link between two different solutions of the MaxEnt
variational problem, namely, the ones that correspond to using either Shannon's
or Tsallis' entropies in the concomitant variational problem. It is shown that
the two variations lead to equivalent solutions that take different appearances
but contain the same information. These solutions are linked by our
transformation."@2012
A. Plastino@http://arxiv.org/abs/1201.4507v2@A Shannon-Tsallis transformation@"We determine a general link between two different solutions of the MaxEnt
variational problem, namely, the ones that correspond to using either Shannon's
or Tsallis' entropies in the concomitant variational problem. It is shown that
the two variations lead to equivalent solutions that take different appearances
but contain the same information. These solutions are linked by our
transformation."@2012
Davit Varron@http://arxiv.org/abs/1201.5507v1@Uniform in bandwidth exact rates for a class of kernel estimators@"Given an i.i.d sample $(Y_i,Z_i)$, taking values in $\RRR^{d'}\times \RRR^d$,
we consider a collection Nadarya-Watson kernel estimators of the conditional
expectations $\EEE(<c_g(z),g(Y)>+d_g(z)\mid Z=z)$, where $z$ belongs to a
compact set $H\subset \RRR^d$, $g$ a Borel function on $\RRR^{d'}$ and
$c_g(\cdot),d_g(\cdot)$ are continuous functions on $\RRR^d$. Given two
bandwidth sequences $h_n<\wth_n$ fulfilling mild conditions, we obtain an exact
and explicit almost sure limit bounds for the deviations of these estimators
around their expectations, uniformly in $g\in\GG,\;z\in H$ and $h_n\le h\le
\wth_n$ under mild conditions on the density $f_Z$, the class $\GG$, the kernel
$K$ and the functions $c_g(\cdot),d_g(\cdot)$. We apply this result to prove
that smoothed empirical likelihood can be used to build confidence intervals
for conditional probabilities $\PPP(Y\in C\mid Z=z)$, that hold uniformly in
$z\in H,\; C\in \CC,\; h\in [h_n,\wth_n]$. Here $\CC$ is a Vapnik-Chervonenkis
class of sets."@2012
Ingrid Van Keilegom@http://arxiv.org/abs/1201.5507v1@Uniform in bandwidth exact rates for a class of kernel estimators@"Given an i.i.d sample $(Y_i,Z_i)$, taking values in $\RRR^{d'}\times \RRR^d$,
we consider a collection Nadarya-Watson kernel estimators of the conditional
expectations $\EEE(<c_g(z),g(Y)>+d_g(z)\mid Z=z)$, where $z$ belongs to a
compact set $H\subset \RRR^d$, $g$ a Borel function on $\RRR^{d'}$ and
$c_g(\cdot),d_g(\cdot)$ are continuous functions on $\RRR^d$. Given two
bandwidth sequences $h_n<\wth_n$ fulfilling mild conditions, we obtain an exact
and explicit almost sure limit bounds for the deviations of these estimators
around their expectations, uniformly in $g\in\GG,\;z\in H$ and $h_n\le h\le
\wth_n$ under mild conditions on the density $f_Z$, the class $\GG$, the kernel
$K$ and the functions $c_g(\cdot),d_g(\cdot)$. We apply this result to prove
that smoothed empirical likelihood can be used to build confidence intervals
for conditional probabilities $\PPP(Y\in C\mid Z=z)$, that hold uniformly in
$z\in H,\; C\in \CC,\; h\in [h_n,\wth_n]$. Here $\CC$ is a Vapnik-Chervonenkis
class of sets."@2012
Davit Varron@http://arxiv.org/abs/1201.5528v1@"Non standard functional limit laws for the increments of the compound
  empirical distribution function"@"Let $(Y_i,Z_i)_{i\geq 1}$ be a sequence of independent, identically
distributed (i.i.d.) random vectors taking values in $\RRR^k\times\RRR^d$, for
some integers $k$ and $d$. Given $z\in \RRR^d$, we provide a nonstandard
functional limit law for the sequence of functional increments of the compound
empirical process, namely $$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot):=
\frac{1}{nh_n}\sliin 1_{[0,\cdot)}\poo \frac{Z_i-z}{{h_n}^{1/d}}\pff Y_i.$$
Provided that $nh_n\sim c\log n $ as $\nif$, we obtain, under some natural
conditions on the conditional exponential moments of $Y\mid Z=z$, that
$$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot)\leadsto \Gam\text{almost surely},$$
where $\leadsto$ denotes the clustering process under the sup norm on $\Idd$.
Here, $\Gam$ is a compact set that is related to the large deviations of
certain compound Poisson processes."@2012
Myriam Maumy@http://arxiv.org/abs/1201.5528v1@"Non standard functional limit laws for the increments of the compound
  empirical distribution function"@"Let $(Y_i,Z_i)_{i\geq 1}$ be a sequence of independent, identically
distributed (i.i.d.) random vectors taking values in $\RRR^k\times\RRR^d$, for
some integers $k$ and $d$. Given $z\in \RRR^d$, we provide a nonstandard
functional limit law for the sequence of functional increments of the compound
empirical process, namely $$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot):=
\frac{1}{nh_n}\sliin 1_{[0,\cdot)}\poo \frac{Z_i-z}{{h_n}^{1/d}}\pff Y_i.$$
Provided that $nh_n\sim c\log n $ as $\nif$, we obtain, under some natural
conditions on the conditional exponential moments of $Y\mid Z=z$, that
$$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot)\leadsto \Gam\text{almost surely},$$
where $\leadsto$ denotes the clustering process under the sup norm on $\Idd$.
Here, $\Gam$ is a compact set that is related to the large deviations of
certain compound Poisson processes."@2012
Francisco J. Caro-Lopera@http://arxiv.org/abs/1201.5705v1@Matrix Kummer-Pearson VII Relation and its Application in Affine Shape@"A case of the matrix Kummer relation of Herz (1955) based on the Pearson VII
type matrix model is derived in this paper. As a consequence, the polynomial
Pearson VII configuration density is obtained and this set the corresponding
exact inference as a solvable aspect in shape theory."@2012
Jose A. Diaz-Garcia@http://arxiv.org/abs/1201.5705v1@Matrix Kummer-Pearson VII Relation and its Application in Affine Shape@"A case of the matrix Kummer relation of Herz (1955) based on the Pearson VII
type matrix model is derived in this paper. As a consequence, the polynomial
Pearson VII configuration density is obtained and this set the corresponding
exact inference as a solvable aspect in shape theory."@2012
F. Bartolucci@http://arxiv.org/abs/1201.5990v1@"A note on the application of the Oakes' identity to obtain the observed
  information matrix of hidden Markov models"@"We derive the observed information matrix of hidden Markov models by the
application of the Oakes (1999)'s identity. The method only requires the first
derivative of the forward-backward recursions of Baum and Welch (1970), instead
of the second derivative of the forward recursion, which is required within the
approach of Lystig and Hughes (2002). The method is illustrated by an example
based on the analysis of a longitudinal dataset which is well known in
sociology."@2012
A. Farcomeni@http://arxiv.org/abs/1201.5990v1@"A note on the application of the Oakes' identity to obtain the observed
  information matrix of hidden Markov models"@"We derive the observed information matrix of hidden Markov models by the
application of the Oakes (1999)'s identity. The method only requires the first
derivative of the forward-backward recursions of Baum and Welch (1970), instead
of the second derivative of the forward recursion, which is required within the
approach of Lystig and Hughes (2002). The method is illustrated by an example
based on the analysis of a longitudinal dataset which is well known in
sociology."@2012
F. Pennoni@http://arxiv.org/abs/1201.5990v1@"A note on the application of the Oakes' identity to obtain the observed
  information matrix of hidden Markov models"@"We derive the observed information matrix of hidden Markov models by the
application of the Oakes (1999)'s identity. The method only requires the first
derivative of the forward-backward recursions of Baum and Welch (1970), instead
of the second derivative of the forward recursion, which is required within the
approach of Lystig and Hughes (2002). The method is illustrated by an example
based on the analysis of a longitudinal dataset which is well known in
sociology."@2012
Jens-Peter Kreiss@http://arxiv.org/abs/1201.6211v1@On the range of validity of the autoregressive sieve bootstrap@"We explore the limits of the autoregressive (AR) sieve bootstrap, and show
that its applicability extends well beyond the realm of linear time series as
has been previously thought. In particular, for appropriate statistics, the
AR-sieve bootstrap is valid for stationary processes possessing a general
Wold-type autoregressive representation with respect to a white noise; in
essence, this includes all stationary, purely nondeterministic processes, whose
spectral density is everywhere positive. Our main theorem provides a simple and
effective tool in assessing whether the AR-sieve bootstrap is asymptotically
valid in any given situation. In effect, the large-sample distribution of the
statistic in question must only depend on the first and second order moments of
the process; prominent examples include the sample mean and the spectral
density. As a counterexample, we show how the AR-sieve bootstrap is not always
valid for the sample autocovariance even when the underlying process is linear."@2012
Efstathios Paparoditis@http://arxiv.org/abs/1201.6211v1@On the range of validity of the autoregressive sieve bootstrap@"We explore the limits of the autoregressive (AR) sieve bootstrap, and show
that its applicability extends well beyond the realm of linear time series as
has been previously thought. In particular, for appropriate statistics, the
AR-sieve bootstrap is valid for stationary processes possessing a general
Wold-type autoregressive representation with respect to a white noise; in
essence, this includes all stationary, purely nondeterministic processes, whose
spectral density is everywhere positive. Our main theorem provides a simple and
effective tool in assessing whether the AR-sieve bootstrap is asymptotically
valid in any given situation. In effect, the large-sample distribution of the
statistic in question must only depend on the first and second order moments of
the process; prominent examples include the sample mean and the spectral
density. As a counterexample, we show how the AR-sieve bootstrap is not always
valid for the sample autocovariance even when the underlying process is linear."@2012
Dimitris N. Politis@http://arxiv.org/abs/1201.6211v1@On the range of validity of the autoregressive sieve bootstrap@"We explore the limits of the autoregressive (AR) sieve bootstrap, and show
that its applicability extends well beyond the realm of linear time series as
has been previously thought. In particular, for appropriate statistics, the
AR-sieve bootstrap is valid for stationary processes possessing a general
Wold-type autoregressive representation with respect to a white noise; in
essence, this includes all stationary, purely nondeterministic processes, whose
spectral density is everywhere positive. Our main theorem provides a simple and
effective tool in assessing whether the AR-sieve bootstrap is asymptotically
valid in any given situation. In effect, the large-sample distribution of the
statistic in question must only depend on the first and second order moments of
the process; prominent examples include the sample mean and the spectral
density. As a counterexample, we show how the AR-sieve bootstrap is not always
valid for the sample autocovariance even when the underlying process is linear."@2012
Ke Zhu@http://arxiv.org/abs/1201.6216v1@"Global self-weighted and local quasi-maximum exponential likelihood
  estimators for ARMA--GARCH/IGARCH models"@"This paper investigates the asymptotic theory of the quasi-maximum
exponential likelihood estimators (QMELE) for ARMA--GARCH models. Under only a
fractional moment condition, the strong consistency and the asymptotic
normality of the global self-weighted QMELE are obtained. Based on this
self-weighted QMELE, the local QMELE is showed to be asymptotically normal for
the ARMA model with GARCH (finite variance) and IGARCH errors. A formal
comparison of two estimators is given for some cases. A simulation study is
carried out to assess the performance of these estimators, and a real example
on the world crude oil price is given."@2012
Shiqing Ling@http://arxiv.org/abs/1201.6216v1@"Global self-weighted and local quasi-maximum exponential likelihood
  estimators for ARMA--GARCH/IGARCH models"@"This paper investigates the asymptotic theory of the quasi-maximum
exponential likelihood estimators (QMELE) for ARMA--GARCH models. Under only a
fractional moment condition, the strong consistency and the asymptotic
normality of the global self-weighted QMELE are obtained. Based on this
self-weighted QMELE, the local QMELE is showed to be asymptotically normal for
the ARMA model with GARCH (finite variance) and IGARCH errors. A formal
comparison of two estimators is given for some cases. A simulation study is
carried out to assess the performance of these estimators, and a real example
on the world crude oil price is given."@2012
Valentin Konakov@http://arxiv.org/abs/1201.6307v2@Statistical convergence of Markov experiments to diffusion limits@"Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov
chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency
Markov chain runs in the background on a very fine time grid but that it is
only observed on a coarser grid. This asymptotics reflects a set up occurring
in the high frequency statistical analysis for financial data where diffusion
approximations are used only for coarser time scales. In this paper, we show
that under appropriate conditions the L$_1$-distance between the joint
distribution of the Markov chain and the distribution of the discretized
diffusion limit converges to zero. The result implies that the LeCam deficiency
distance between the statistical Markov experiment and its diffusion limit
converges to zero. This result can be applied to Euler approximations for the
joint distribution of diffusions observed at points
$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by
generating Euler approximations at the points $\Delta k^{-1},2\Delta
k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions
the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$."@2012
Enno Mammen@http://arxiv.org/abs/1201.6307v2@Statistical convergence of Markov experiments to diffusion limits@"Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov
chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency
Markov chain runs in the background on a very fine time grid but that it is
only observed on a coarser grid. This asymptotics reflects a set up occurring
in the high frequency statistical analysis for financial data where diffusion
approximations are used only for coarser time scales. In this paper, we show
that under appropriate conditions the L$_1$-distance between the joint
distribution of the Markov chain and the distribution of the discretized
diffusion limit converges to zero. The result implies that the LeCam deficiency
distance between the statistical Markov experiment and its diffusion limit
converges to zero. This result can be applied to Euler approximations for the
joint distribution of diffusions observed at points
$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by
generating Euler approximations at the points $\Delta k^{-1},2\Delta
k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions
the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$."@2012
Jeannette Woerner@http://arxiv.org/abs/1201.6307v2@Statistical convergence of Markov experiments to diffusion limits@"Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov
chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency
Markov chain runs in the background on a very fine time grid but that it is
only observed on a coarser grid. This asymptotics reflects a set up occurring
in the high frequency statistical analysis for financial data where diffusion
approximations are used only for coarser time scales. In this paper, we show
that under appropriate conditions the L$_1$-distance between the joint
distribution of the Markov chain and the distribution of the discretized
diffusion limit converges to zero. The result implies that the LeCam deficiency
distance between the statistical Markov experiment and its diffusion limit
converges to zero. This result can be applied to Euler approximations for the
joint distribution of diffusions observed at points
$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by
generating Euler approximations at the points $\Delta k^{-1},2\Delta
k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions
the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$."@2012
Dominique Bontemps@http://arxiv.org/abs/1202.0258v4@About adaptive coding on countable alphabets@"This paper sheds light on universal coding with respect to classes of
memoryless sources over a countable alphabet defined by an envelope function
with finite and non-decreasing hazard rate. We prove that the auto-censuring AC
code introduced by Bontemps (2011) is adaptive with respect to the collection
of such classes. The analysis builds on the tight characterization of universal
redundancy rate in terms of metric entropy % of small source classes by Opper
and Haussler (1997) and on a careful analysis of the performance of the
AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of
samples from discrete distributions with finite and non-decreasing hazard rate."@2012
Stéphane Boucheron@http://arxiv.org/abs/1202.0258v4@About adaptive coding on countable alphabets@"This paper sheds light on universal coding with respect to classes of
memoryless sources over a countable alphabet defined by an envelope function
with finite and non-decreasing hazard rate. We prove that the auto-censuring AC
code introduced by Bontemps (2011) is adaptive with respect to the collection
of such classes. The analysis builds on the tight characterization of universal
redundancy rate in terms of metric entropy % of small source classes by Opper
and Haussler (1997) and on a careful analysis of the performance of the
AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of
samples from discrete distributions with finite and non-decreasing hazard rate."@2012
Elisabeth Gassiat@http://arxiv.org/abs/1202.0258v4@About adaptive coding on countable alphabets@"This paper sheds light on universal coding with respect to classes of
memoryless sources over a countable alphabet defined by an envelope function
with finite and non-decreasing hazard rate. We prove that the auto-censuring AC
code introduced by Bontemps (2011) is adaptive with respect to the collection
of such classes. The analysis builds on the tight characterization of universal
redundancy rate in terms of metric entropy % of small source classes by Opper
and Haussler (1997) and on a careful analysis of the performance of the
AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of
samples from discrete distributions with finite and non-decreasing hazard rate."@2012
Wei Liu@http://arxiv.org/abs/1202.0391v1@Parametric or nonparametric? A parametricness index for model selection@"In model selection literature, two classes of criteria perform well
asymptotically in different situations: Bayesian information criterion (BIC)
(as a representative) is consistent in selection when the true model is finite
dimensional (parametric scenario); Akaike's information criterion (AIC)
performs well in an asymptotic efficiency when the true model is infinite
dimensional (nonparametric scenario). But there is little work that addresses
if it is possible and how to detect the situation that a specific model
selection problem is in. In this work, we differentiate the two scenarios
theoretically under some conditions. We develop a measure, parametricness index
(PI), to assess whether a model selected by a potentially consistent procedure
can be practically treated as the true model, which also hints on AIC or BIC is
better suited for the data for the goal of estimating the regression function.
A consequence is that by switching between AIC and BIC based on the PI, the
resulting regression estimator is simultaneously asymptotically efficient for
both parametric and nonparametric scenarios. In addition, we systematically
investigate the behaviors of PI in simulation and real data and show its
usefulness."@2012
Yuhong Yang@http://arxiv.org/abs/1202.0391v1@Parametric or nonparametric? A parametricness index for model selection@"In model selection literature, two classes of criteria perform well
asymptotically in different situations: Bayesian information criterion (BIC)
(as a representative) is consistent in selection when the true model is finite
dimensional (parametric scenario); Akaike's information criterion (AIC)
performs well in an asymptotic efficiency when the true model is infinite
dimensional (nonparametric scenario). But there is little work that addresses
if it is possible and how to detect the situation that a specific model
selection problem is in. In this work, we differentiate the two scenarios
theoretically under some conditions. We develop a measure, parametricness index
(PI), to assess whether a model selected by a potentially consistent procedure
can be practically treated as the true model, which also hints on AIC or BIC is
better suited for the data for the goal of estimating the regression function.
A consequence is that by switching between AIC and BIC based on the PI, the
resulting regression estimator is simultaneously asymptotically efficient for
both parametric and nonparametric scenarios. In addition, we systematically
investigate the behaviors of PI in simulation and real data and show its
usefulness."@2012
Matieyendou Lamboni@http://arxiv.org/abs/1202.0943v2@"Derivative-based global sensitivity measures: general links with Sobol'
  indices and numerical tests"@"The estimation of variance-based importance measures (called Sobol' indices)
of the input variables of a numerical model can require a large number of model
evaluations. It turns to be unacceptable for high-dimensional model involving a
large number of input variables (typically more than ten). Recently, Sobol and
Kucherenko have proposed the Derivative-based Global Sensitivity Measures
(DGSM), defined as the integral of the squared derivatives of the model output,
showing that it can help to solve the problem of dimensionality in some cases.
We provide a general inequality link between DGSM and total Sobol' indices for
input variables belonging to the class of Boltzmann probability measures, thus
extending the previous results of Sobol and Kucherenko for uniform and normal
measures. The special case of log-concave measures is also described. This link
provides a DGSM-based maximal bound for the total Sobol indices. Numerical
tests show the performance of the bound and its usefulness in practice."@2012
Bertrand Iooss@http://arxiv.org/abs/1202.0943v2@"Derivative-based global sensitivity measures: general links with Sobol'
  indices and numerical tests"@"The estimation of variance-based importance measures (called Sobol' indices)
of the input variables of a numerical model can require a large number of model
evaluations. It turns to be unacceptable for high-dimensional model involving a
large number of input variables (typically more than ten). Recently, Sobol and
Kucherenko have proposed the Derivative-based Global Sensitivity Measures
(DGSM), defined as the integral of the squared derivatives of the model output,
showing that it can help to solve the problem of dimensionality in some cases.
We provide a general inequality link between DGSM and total Sobol' indices for
input variables belonging to the class of Boltzmann probability measures, thus
extending the previous results of Sobol and Kucherenko for uniform and normal
measures. The special case of log-concave measures is also described. This link
provides a DGSM-based maximal bound for the total Sobol indices. Numerical
tests show the performance of the bound and its usefulness in practice."@2012
Anne-Laure Popelin@http://arxiv.org/abs/1202.0943v2@"Derivative-based global sensitivity measures: general links with Sobol'
  indices and numerical tests"@"The estimation of variance-based importance measures (called Sobol' indices)
of the input variables of a numerical model can require a large number of model
evaluations. It turns to be unacceptable for high-dimensional model involving a
large number of input variables (typically more than ten). Recently, Sobol and
Kucherenko have proposed the Derivative-based Global Sensitivity Measures
(DGSM), defined as the integral of the squared derivatives of the model output,
showing that it can help to solve the problem of dimensionality in some cases.
We provide a general inequality link between DGSM and total Sobol' indices for
input variables belonging to the class of Boltzmann probability measures, thus
extending the previous results of Sobol and Kucherenko for uniform and normal
measures. The special case of log-concave measures is also described. This link
provides a DGSM-based maximal bound for the total Sobol indices. Numerical
tests show the performance of the bound and its usefulness in practice."@2012
Fabrice Gamboa@http://arxiv.org/abs/1202.0943v2@"Derivative-based global sensitivity measures: general links with Sobol'
  indices and numerical tests"@"The estimation of variance-based importance measures (called Sobol' indices)
of the input variables of a numerical model can require a large number of model
evaluations. It turns to be unacceptable for high-dimensional model involving a
large number of input variables (typically more than ten). Recently, Sobol and
Kucherenko have proposed the Derivative-based Global Sensitivity Measures
(DGSM), defined as the integral of the squared derivatives of the model output,
showing that it can help to solve the problem of dimensionality in some cases.
We provide a general inequality link between DGSM and total Sobol' indices for
input variables belonging to the class of Boltzmann probability measures, thus
extending the previous results of Sobol and Kucherenko for uniform and normal
measures. The special case of log-concave measures is also described. This link
provides a DGSM-based maximal bound for the total Sobol indices. Numerical
tests show the performance of the bound and its usefulness in practice."@2012
Ting Yan@http://arxiv.org/abs/1202.1058v3@"Approximating the inverse of a balanced symmetric matrix with positive
  elements"@"For an $n\times n$ balanced symmetric matrix $T=(t_{i,j})$ with positive
elements satisfying $t_{i,i}= \sum_{j\neq i} t_{i,j}$ and certain bounding
conditions, we propose to use the matrix $S=(s_{i,j})$ to approximate its
inverse, where $s_{i,j}=\delta_{i,j}/t_{i,i}-1/t_{..}$, $\delta_{i,j}$ is the
Kronecker delta function, and $t_{..}=\sum_{i,j=1 }^{n}(1-\delta_{i,j})
t_{i,j}$. An explicit bound on the approximation error is obtained, showing
that the inverse is well approximated to order $1/(n-1)^2$ uniformly."@2012
Xu Jinfeng@http://arxiv.org/abs/1202.1058v3@"Approximating the inverse of a balanced symmetric matrix with positive
  elements"@"For an $n\times n$ balanced symmetric matrix $T=(t_{i,j})$ with positive
elements satisfying $t_{i,i}= \sum_{j\neq i} t_{i,j}$ and certain bounding
conditions, we propose to use the matrix $S=(s_{i,j})$ to approximate its
inverse, where $s_{i,j}=\delta_{i,j}/t_{i,i}-1/t_{..}$, $\delta_{i,j}$ is the
Kronecker delta function, and $t_{..}=\sum_{i,j=1 }^{n}(1-\delta_{i,j})
t_{i,j}$. An explicit bound on the approximation error is obtained, showing
that the inverse is well approximated to order $1/(n-1)^2$ uniformly."@2012
Filippo Palombi@http://arxiv.org/abs/1202.1838v2@"Numerical reconstruction of the covariance matrix of a spherically
  truncated multinormal distribution"@"In this paper we relate the matrix $S_B$ of the second moments of a
spherically truncated normal multivariate to its full covariance matrix
$\Sigma$ and present an algorithm to invert the relation and reconstruct
$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by
the truncation, its eigenvalues are non-uniformly damped. We show that the
eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts
via a fixed point iteration, whose convergence we prove analytically. The
procedure requires the computation of multidimensional Gaussian integrals over
a Euclidean ball, for which we extend a numerical technique, originally
proposed by Ruben in 1962, based on a series expansion in chi-square
distributions. In order to study the feasibility of our approach, we examine
the convergence rate of some iterative schemes on suitably chosen ensembles of
Wishart matrices. We finally discuss the practical difficulties arising in
sample space and outline a regularization of the problem based on perturbation
theory."@2012
Simona Toti@http://arxiv.org/abs/1202.1838v2@"Numerical reconstruction of the covariance matrix of a spherically
  truncated multinormal distribution"@"In this paper we relate the matrix $S_B$ of the second moments of a
spherically truncated normal multivariate to its full covariance matrix
$\Sigma$ and present an algorithm to invert the relation and reconstruct
$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by
the truncation, its eigenvalues are non-uniformly damped. We show that the
eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts
via a fixed point iteration, whose convergence we prove analytically. The
procedure requires the computation of multidimensional Gaussian integrals over
a Euclidean ball, for which we extend a numerical technique, originally
proposed by Ruben in 1962, based on a series expansion in chi-square
distributions. In order to study the feasibility of our approach, we examine
the convergence rate of some iterative schemes on suitably chosen ensembles of
Wishart matrices. We finally discuss the practical difficulties arising in
sample space and outline a regularization of the problem based on perturbation
theory."@2012
Romina Filippini@http://arxiv.org/abs/1202.1838v2@"Numerical reconstruction of the covariance matrix of a spherically
  truncated multinormal distribution"@"In this paper we relate the matrix $S_B$ of the second moments of a
spherically truncated normal multivariate to its full covariance matrix
$\Sigma$ and present an algorithm to invert the relation and reconstruct
$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by
the truncation, its eigenvalues are non-uniformly damped. We show that the
eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts
via a fixed point iteration, whose convergence we prove analytically. The
procedure requires the computation of multidimensional Gaussian integrals over
a Euclidean ball, for which we extend a numerical technique, originally
proposed by Ruben in 1962, based on a series expansion in chi-square
distributions. In order to study the feasibility of our approach, we examine
the convergence rate of some iterative schemes on suitably chosen ensembles of
Wishart matrices. We finally discuss the practical difficulties arising in
sample space and outline a regularization of the problem based on perturbation
theory."@2012
Elena Di Bernadino@http://arxiv.org/abs/1202.2035v1@"Estimating level sets of a distribution function using a plug-in method:
  a multidimensional extension"@"This paper deals with the problem of estimating the level sets $L(c)= \{F(x)
\geq c \}$, with $c \in (0,1)$, of an unknown distribution function $F$ on
\mathbb{R}^d_+$. A plug-in approach is followed. That is, given a consistent
estimator $F_n$ of $F$, we estimate $L(c)$ by $L_n(c)= \{F_n(x) \geq c \}$. We
state consistency results with respect to the Hausdorff distance and the volume
of the symmetric difference. These results can be considered as generalizations
of results previously obtained, in a bivariate framework, in Di Bernardino et
al. (2011). Finally we investigate the effects of scaling data on our
consistency results."@2012
Thomas Laloë@http://arxiv.org/abs/1202.2035v1@"Estimating level sets of a distribution function using a plug-in method:
  a multidimensional extension"@"This paper deals with the problem of estimating the level sets $L(c)= \{F(x)
\geq c \}$, with $c \in (0,1)$, of an unknown distribution function $F$ on
\mathbb{R}^d_+$. A plug-in approach is followed. That is, given a consistent
estimator $F_n$ of $F$, we estimate $L(c)$ by $L_n(c)= \{F_n(x) \geq c \}$. We
state consistency results with respect to the Hausdorff distance and the volume
of the symmetric difference. These results can be considered as generalizations
of results previously obtained, in a bivariate framework, in Di Bernardino et
al. (2011). Finally we investigate the effects of scaling data on our
consistency results."@2012
Romain Azaïs@http://arxiv.org/abs/1202.2211v2@"Nonparametric estimation of the jump rate for non-homogeneous marked
  renewal processes"@"This paper is devoted to the nonparametric estimation of the jump rate and
the cumulative rate for a general class of non-homogeneous marked renewal
processes, defined on a separable metric space. In our framework, the
estimation needs only one observation of the process within a long time. Our
approach is based on a generalization of the multiplicative intensity model,
introduced by Aalen in the seventies. We provide consistent estimators of these
two functions, under some assumptions related to the ergodicity of an embedded
chain and the characteristics of the process. The paper is illustrated by a
numerical example."@2012
François Dufour@http://arxiv.org/abs/1202.2211v2@"Nonparametric estimation of the jump rate for non-homogeneous marked
  renewal processes"@"This paper is devoted to the nonparametric estimation of the jump rate and
the cumulative rate for a general class of non-homogeneous marked renewal
processes, defined on a separable metric space. In our framework, the
estimation needs only one observation of the process within a long time. Our
approach is based on a generalization of the multiplicative intensity model,
introduced by Aalen in the seventies. We provide consistent estimators of these
two functions, under some assumptions related to the ergodicity of an embedded
chain and the characteristics of the process. The paper is illustrated by a
numerical example."@2012
Anne Gégout-Petit@http://arxiv.org/abs/1202.2211v2@"Nonparametric estimation of the jump rate for non-homogeneous marked
  renewal processes"@"This paper is devoted to the nonparametric estimation of the jump rate and
the cumulative rate for a general class of non-homogeneous marked renewal
processes, defined on a separable metric space. In our framework, the
estimation needs only one observation of the process within a long time. Our
approach is based on a generalization of the multiplicative intensity model,
introduced by Aalen in the seventies. We provide consistent estimators of these
two functions, under some assumptions related to the ergodicity of an embedded
chain and the characteristics of the process. The paper is illustrated by a
numerical example."@2012
Romain Azaïs@http://arxiv.org/abs/1202.2212v2@"Nonparametric estimation of the conditional distribution of the
  inter-jumping times for piecewise-deterministic Markov processes"@"This paper presents a nonparametric method for estimating the conditional
density associated to the jump rate of a piecewise-deterministic Markov
process. In our framework, the estimation needs only one observation of the
process within a long time interval. Our method relies on a generalization of
Aalen's multiplicative intensity model. We prove the uniform consistency of our
estimator, under some reasonable assumptions related to the primitive
characteristics of the process. A simulation example illustrates the behavior
of our estimator."@2012
François Dufour@http://arxiv.org/abs/1202.2212v2@"Nonparametric estimation of the conditional distribution of the
  inter-jumping times for piecewise-deterministic Markov processes"@"This paper presents a nonparametric method for estimating the conditional
density associated to the jump rate of a piecewise-deterministic Markov
process. In our framework, the estimation needs only one observation of the
process within a long time interval. Our method relies on a generalization of
Aalen's multiplicative intensity model. We prove the uniform consistency of our
estimator, under some reasonable assumptions related to the primitive
characteristics of the process. A simulation example illustrates the behavior
of our estimator."@2012
Anne Gégout-Petit@http://arxiv.org/abs/1202.2212v2@"Nonparametric estimation of the conditional distribution of the
  inter-jumping times for piecewise-deterministic Markov processes"@"This paper presents a nonparametric method for estimating the conditional
density associated to the jump rate of a piecewise-deterministic Markov
process. In our framework, the estimation needs only one observation of the
process within a long time interval. Our method relies on a generalization of
Aalen's multiplicative intensity model. We prove the uniform consistency of our
estimator, under some reasonable assumptions related to the primitive
characteristics of the process. A simulation example illustrates the behavior
of our estimator."@2012
Peter S. Chami@http://arxiv.org/abs/1202.2395v1@"A two parameter ratio-product-ratio estimator using auxiliary
  information"@"We propose a two parameter ratio-product-ratio estimator for a finite
population mean in a simple random sample without replacement following the
methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai
(1985) and Singh and Ruiz Espejo (2003).
  The bias and mean square error of our proposed estimator are obtained to the
first degree of approximation. We derive conditions for the parameters under
which the proposed estimator has smaller mean square error than the sample
mean, ratio and product estimators.
  We carry out an application showing that the proposed estimator outperforms
the traditional estimators using groundwater data taken from a geological site
in the state of Florida."@2012
Bernd Sing@http://arxiv.org/abs/1202.2395v1@"A two parameter ratio-product-ratio estimator using auxiliary
  information"@"We propose a two parameter ratio-product-ratio estimator for a finite
population mean in a simple random sample without replacement following the
methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai
(1985) and Singh and Ruiz Espejo (2003).
  The bias and mean square error of our proposed estimator are obtained to the
first degree of approximation. We derive conditions for the parameters under
which the proposed estimator has smaller mean square error than the sample
mean, ratio and product estimators.
  We carry out an application showing that the proposed estimator outperforms
the traditional estimators using groundwater data taken from a geological site
in the state of Florida."@2012
Doneal Thomas@http://arxiv.org/abs/1202.2395v1@"A two parameter ratio-product-ratio estimator using auxiliary
  information"@"We propose a two parameter ratio-product-ratio estimator for a finite
population mean in a simple random sample without replacement following the
methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai
(1985) and Singh and Ruiz Espejo (2003).
  The bias and mean square error of our proposed estimator are obtained to the
first degree of approximation. We derive conditions for the parameters under
which the proposed estimator has smaller mean square error than the sample
mean, ratio and product estimators.
  We carry out an application showing that the proposed estimator outperforms
the traditional estimators using groundwater data taken from a geological site
in the state of Florida."@2012
Ting Yan@http://arxiv.org/abs/1202.3307v3@"A central limit theorem in the $β$-model for undirected random
  graphs with a diverging number of vertices"@"Chatterjee, Diaconis and Sly (2011) recently established the consistency of
the maximum likelihood estimate in the $\beta$-model when the number of
vertices goes to infinity. By approximating the inverse of the Fisher
information matrix, we obtain its asymptotic normality under mild conditions.
Simulation studies and a data example illustrate the theoretical results."@2012
Jinfeng Xu@http://arxiv.org/abs/1202.3307v3@"A central limit theorem in the $β$-model for undirected random
  graphs with a diverging number of vertices"@"Chatterjee, Diaconis and Sly (2011) recently established the consistency of
the maximum likelihood estimate in the $\beta$-model when the number of
vertices goes to infinity. By approximating the inverse of the Fisher
information matrix, we obtain its asymptotic normality under mild conditions.
Simulation studies and a data example illustrate the theoretical results."@2012
Elisabeth Gassiat@http://arxiv.org/abs/1202.3482v2@The local geometry of finite mixtures@"We establish that for q>=1, the class of convex combinations of q translates
of a smooth probability density has local doubling dimension proportional to q.
The key difficulty in the proof is to control the local geometric structure of
mixture classes. Our local geometry theorem yields a bound on the (bracketing)
metric entropy of a class of normalized densities, from which a local entropy
bound is deduced by a general slicing procedure."@2012
Ramon Van Handel@http://arxiv.org/abs/1202.3482v2@The local geometry of finite mixtures@"We establish that for q>=1, the class of convex combinations of q translates
of a smooth probability density has local doubling dimension proportional to q.
The key difficulty in the proof is to control the local geometric structure of
mixture classes. Our local geometry theorem yields a bound on the (bracketing)
metric entropy of a class of normalized densities, from which a local entropy
bound is deduced by a general slicing procedure."@2012
Takuma Yoshida@http://arxiv.org/abs/1202.3483v1@Semiparametric Penalized Spline Regression@"In this paper, we propose a new semiparametric regression estimator by using
a hybrid technique of a parametric approach and a nonparametric penalized
spline method. The overall shape of the true regression function is captured by
the parametric part, while its residual is consistently estimated by the
nonparametric part. Asymptotic theory for the proposed semiparametric estimator
is developed, showing that its behavior is dependent on the asymptotics for the
nonparametric penalized spline estimator as well as on the discrepancy between
the true regression function and the parametric part. As a naturally associated
application of asymptotics, some criteria for the selection of parametric
models are addressed. Numerical experiments show that the proposed estimator
performs better than the existing kernel-based semiparametric estimator and the
fully nonparametric estimator, and that the proposed criteria work well for
choosing a reasonable parametric model."@2012
Kanta Naito@http://arxiv.org/abs/1202.3483v1@Semiparametric Penalized Spline Regression@"In this paper, we propose a new semiparametric regression estimator by using
a hybrid technique of a parametric approach and a nonparametric penalized
spline method. The overall shape of the true regression function is captured by
the parametric part, while its residual is consistently estimated by the
nonparametric part. Asymptotic theory for the proposed semiparametric estimator
is developed, showing that its behavior is dependent on the asymptotics for the
nonparametric penalized spline estimator as well as on the discrepancy between
the true regression function and the parametric part. As a naturally associated
application of asymptotics, some criteria for the selection of parametric
models are addressed. Numerical experiments show that the proposed estimator
performs better than the existing kernel-based semiparametric estimator and the
fully nonparametric estimator, and that the proposed criteria work well for
choosing a reasonable parametric model."@2012
Antar Bandyopadhyay@http://arxiv.org/abs/1202.3570v2@Variance Estimation for Tree Order Restricted Models@"In this article we discuss estimation of the common variance of several
normal populations with tree order restricted means. We discuss the asymptotic
properties of the maximum likelihood estimator of the variance as the number of
populations tends to infinity. We consider several cases of various orders of
the sample sizes and show that the maximum likelihood estimator of the variance
may or may not be consistent or be asymptotically normal."@2012
Sanjay Chaudhuri@http://arxiv.org/abs/1202.3570v2@Variance Estimation for Tree Order Restricted Models@"In this article we discuss estimation of the common variance of several
normal populations with tree order restricted means. We discuss the asymptotic
properties of the maximum likelihood estimator of the variance as the number of
populations tends to infinity. We consider several cases of various orders of
the sample sizes and show that the maximum likelihood estimator of the variance
may or may not be consistent or be asymptotically normal."@2012
Sylvain Arlot@http://arxiv.org/abs/1202.3878v2@A kernel multiple change-point algorithm via model selection@"We tackle the change-point problem with data belonging to a general set. We
build a penalty for choosing the number of change-points in the kernel-based
method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one
proposed by Lebarbier (2005) for one-dimensional signals. We prove a
non-asymptotic oracle inequality for the proposed method, thanks to a new
concentration result for some function of Hilbert-space valued random
variables. Experiments on synthetic data illustrate the accuracy of our method,
showing that it can detect changes in the whole distribution of data, even when
the mean and variance are constant."@2012
Alain Celisse@http://arxiv.org/abs/1202.3878v2@A kernel multiple change-point algorithm via model selection@"We tackle the change-point problem with data belonging to a general set. We
build a penalty for choosing the number of change-points in the kernel-based
method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one
proposed by Lebarbier (2005) for one-dimensional signals. We prove a
non-asymptotic oracle inequality for the proposed method, thanks to a new
concentration result for some function of Hilbert-space valued random
variables. Experiments on synthetic data illustrate the accuracy of our method,
showing that it can detect changes in the whole distribution of data, even when
the mean and variance are constant."@2012
Zaid Harchaoui@http://arxiv.org/abs/1202.3878v2@A kernel multiple change-point algorithm via model selection@"We tackle the change-point problem with data belonging to a general set. We
build a penalty for choosing the number of change-points in the kernel-based
method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one
proposed by Lebarbier (2005) for one-dimensional signals. We prove a
non-asymptotic oracle inequality for the proposed method, thanks to a new
concentration result for some function of Hilbert-space valued random
variables. Experiments on synthetic data illustrate the accuracy of our method,
showing that it can detect changes in the whole distribution of data, even when
the mean and variance are constant."@2012
Francesco Bartolucci@http://arxiv.org/abs/1202.4074v1@"Bayesian inference through encompassing priors and importance sampling
  for a class of marginal models for categorical data"@"We develop a Bayesian approach for selecting the model which is the most
supported by the data within a class of marginal models for categorical
variables formulated through equality and/or inequality constraints on
generalised logits (local, global, continuation or reverse continuation),
generalised log-odds ratios and similar higher-order interactions. For each
constrained model, the prior distribution of the model parameters is formulated
following the encompassing prior approach. Then, model selection is performed
by using Bayes factors which are estimated by an importance sampling method.
The approach is illustrated through three applications involving some datasets,
which also include explanatory variables. In connection with one of these
examples, a sensitivity analysis to the prior specification is also considered."@2012
Luisa Scaccia@http://arxiv.org/abs/1202.4074v1@"Bayesian inference through encompassing priors and importance sampling
  for a class of marginal models for categorical data"@"We develop a Bayesian approach for selecting the model which is the most
supported by the data within a class of marginal models for categorical
variables formulated through equality and/or inequality constraints on
generalised logits (local, global, continuation or reverse continuation),
generalised log-odds ratios and similar higher-order interactions. For each
constrained model, the prior distribution of the model parameters is formulated
following the encompassing prior approach. Then, model selection is performed
by using Bayes factors which are estimated by an importance sampling method.
The approach is illustrated through three applications involving some datasets,
which also include explanatory variables. In connection with one of these
examples, a sensitivity analysis to the prior specification is also considered."@2012
Alessio Farcomeni@http://arxiv.org/abs/1202.4074v1@"Bayesian inference through encompassing priors and importance sampling
  for a class of marginal models for categorical data"@"We develop a Bayesian approach for selecting the model which is the most
supported by the data within a class of marginal models for categorical
variables formulated through equality and/or inequality constraints on
generalised logits (local, global, continuation or reverse continuation),
generalised log-odds ratios and similar higher-order interactions. For each
constrained model, the prior distribution of the model parameters is formulated
following the encompassing prior approach. Then, model selection is performed
by using Bayes factors which are estimated by an importance sampling method.
The approach is illustrated through three applications involving some datasets,
which also include explanatory variables. In connection with one of these
examples, a sensitivity analysis to the prior specification is also considered."@2012
Pierre Alquier@http://arxiv.org/abs/1202.4283v1@Fast rates in learning with dependent observations@"In this paper we tackle the problem of fast rates in time series forecasting
from a statistical learning perspective. In a serie of papers (e.g. Meir 2000,
Modha and Masry 1998, Alquier and Wintenberger 2012) it is shown that the main
tools used in learning theory with iid observations can be extended to the
prediction of time series. The main message of these papers is that, given a
family of predictors, we are able to build a new predictor that predicts the
series as well as the best predictor in the family, up to a remainder of order
$1/\sqrt{n}$. It is known that this rate cannot be improved in general. In this
paper, we show that in the particular case of the least square loss, and under
a strong assumption on the time series (phi-mixing) the remainder is actually
of order $1/n$. Thus, the optimal rate for iid variables, see e.g. Tsybakov
2003, and individual sequences, see \cite{lugosi} is, for the first time,
achieved for uniformly mixing processes. We also show that our method is
optimal for aggregating sparse linear combinations of predictors."@2012
Olivier Wintenberger@http://arxiv.org/abs/1202.4283v1@Fast rates in learning with dependent observations@"In this paper we tackle the problem of fast rates in time series forecasting
from a statistical learning perspective. In a serie of papers (e.g. Meir 2000,
Modha and Masry 1998, Alquier and Wintenberger 2012) it is shown that the main
tools used in learning theory with iid observations can be extended to the
prediction of time series. The main message of these papers is that, given a
family of predictors, we are able to build a new predictor that predicts the
series as well as the best predictor in the family, up to a remainder of order
$1/\sqrt{n}$. It is known that this rate cannot be improved in general. In this
paper, we show that in the particular case of the least square loss, and under
a strong assumption on the time series (phi-mixing) the remainder is actually
of order $1/n$. Thus, the optimal rate for iid variables, see e.g. Tsybakov
2003, and individual sequences, see \cite{lugosi} is, for the first time,
achieved for uniformly mixing processes. We also show that our method is
optimal for aggregating sparse linear combinations of predictors."@2012
Pierre Alquier@http://arxiv.org/abs/1202.4294v2@"Prediction of quantiles by statistical learning and application to GDP
  forecasting"@"In this paper, we tackle the problem of prediction and confidence intervals
for time series using a statistical learning approach and quantile loss
functions. In a first time, we show that the Gibbs estimator (also known as
Exponentially Weighted aggregate) is able to predict as well as the best
predictor in a given family for a wide set of loss functions. In particular,
using the quantile loss function of Koenker and Bassett (1978), this allows to
build confidence intervals. We apply these results to the problem of prediction
and confidence regions for the French Gross Domestic Product (GDP) growth, with
promising results."@2012
Xiaoyin Li@http://arxiv.org/abs/1202.4294v2@"Prediction of quantiles by statistical learning and application to GDP
  forecasting"@"In this paper, we tackle the problem of prediction and confidence intervals
for time series using a statistical learning approach and quantile loss
functions. In a first time, we show that the Gibbs estimator (also known as
Exponentially Weighted aggregate) is able to predict as well as the best
predictor in a given family for a wide set of loss functions. In particular,
using the quantile loss function of Koenker and Bassett (1978), this allows to
build confidence intervals. We apply these results to the problem of prediction
and confidence regions for the French Gross Domestic Product (GDP) growth, with
promising results."@2012
Clément Dombry@http://arxiv.org/abs/1202.4737v2@Conditional simulation of extremal Gaussian processes@"Recently the regular conditional distributions of max-infinitely divisible
processes were derived by \citet{Dombry2011} and although these conditional
distributions have complicated closed forms, \citet{Dombry2011b} introduce an
algorithm to get conditional realizations of Brown-Resnick processes. In this
paper we derive the regular conditional distributions of the max-stable process
introduced by \citet{Schlather2002} and adapt the framework of
\citet{Dombry2011b} to this specific process. We test the methods on simulated
data and give an application to extreme temperatures in Switzerland. Results
show that the proposed sampling scheme provide accurate conditional simulations
and can handle real-sized problems."@2012
Mathieu Ribatet@http://arxiv.org/abs/1202.4737v2@Conditional simulation of extremal Gaussian processes@"Recently the regular conditional distributions of max-infinitely divisible
processes were derived by \citet{Dombry2011} and although these conditional
distributions have complicated closed forms, \citet{Dombry2011b} introduce an
algorithm to get conditional realizations of Brown-Resnick processes. In this
paper we derive the regular conditional distributions of the max-stable process
introduced by \citet{Schlather2002} and adapt the framework of
\citet{Dombry2011b} to this specific process. We test the methods on simulated
data and give an application to extreme temperatures in Switzerland. Results
show that the proposed sampling scheme provide accurate conditional simulations
and can handle real-sized problems."@2012
Willem Kruijer@http://arxiv.org/abs/1202.4863v1@"Bayesian semi-parametric estimation of the long-memory parameter under
  FEXP-priors"@"For a Gaussian time series with long-memory behavior, we use the FEXP-model
for semi-parametric estimation of the long-memory parameter $d$. The true
spectral density $f_o$ is assumed to have long-memory parameter $d_o$ and a
FEXP-expansion of Sobolev-regularity $\be > 1$. We prove that when $k$ follows
a Poisson or geometric prior, or a sieve prior increasing at rate
$n^{\frac{1}{1+2\be}}$, $d$ converges to $d_o$ at a suboptimal rate. When the
sieve prior increases at rate $n^{\frac{1}{2\be}}$ however, the minimax rate is
almost obtained. Our results can be seen as a Bayesian equivalent of the result
which Moulines and Soulier obtained for some frequentist estimators."@2012
Judith Rousseau@http://arxiv.org/abs/1202.4863v1@"Bayesian semi-parametric estimation of the long-memory parameter under
  FEXP-priors"@"For a Gaussian time series with long-memory behavior, we use the FEXP-model
for semi-parametric estimation of the long-memory parameter $d$. The true
spectral density $f_o$ is assumed to have long-memory parameter $d_o$ and a
FEXP-expansion of Sobolev-regularity $\be > 1$. We prove that when $k$ follows
a Poisson or geometric prior, or a sieve prior increasing at rate
$n^{\frac{1}{1+2\be}}$, $d$ converges to $d_o$ at a suboptimal rate. When the
sieve prior increases at rate $n^{\frac{1}{2\be}}$ however, the minimax rate is
almost obtained. Our results can be seen as a Bayesian equivalent of the result
which Moulines and Soulier obtained for some frequentist estimators."@2012
Peter J. Bickel@http://arxiv.org/abs/1202.5101v1@The method of moments and degree distributions for network models@"Probability models on graphs are becoming increasingly important in many
applications, but statistical tools for fitting such models are not yet well
developed. Here we propose a general method of moments approach that can be
used to fit a large class of probability models through empirical counts of
certain patterns in a graph. We establish some general asymptotic properties of
empirical graph moments and prove consistency of the estimates as the graph
size grows for all ranges of the average degree including $\Omega(1)$.
Additional results are obtained for the important special case of degree
distributions."@2012
Aiyou Chen@http://arxiv.org/abs/1202.5101v1@The method of moments and degree distributions for network models@"Probability models on graphs are becoming increasingly important in many
applications, but statistical tools for fitting such models are not yet well
developed. Here we propose a general method of moments approach that can be
used to fit a large class of probability models through empirical counts of
certain patterns in a graph. We establish some general asymptotic properties of
empirical graph moments and prove consistency of the estimates as the graph
size grows for all ranges of the average degree including $\Omega(1)$.
Additional results are obtained for the important special case of degree
distributions."@2012
Elizaveta Levina@http://arxiv.org/abs/1202.5101v1@The method of moments and degree distributions for network models@"Probability models on graphs are becoming increasingly important in many
applications, but statistical tools for fitting such models are not yet well
developed. Here we propose a general method of moments approach that can be
used to fit a large class of probability models through empirical counts of
certain patterns in a graph. We establish some general asymptotic properties of
empirical graph moments and prove consistency of the estimates as the graph
size grows for all ranges of the average degree including $\Omega(1)$.
Additional results are obtained for the important special case of degree
distributions."@2012
Peter Bühlmann@http://arxiv.org/abs/1202.5118v1@Introduction to the Lehmann special section@"The current Special Issue of The Annals of Statistics contains three invited
articles. Javier Rojo discusses Erich's scientific achievements and provides
complete lists of his scientific writings and his former Ph.D. students. Willem
van Zwet describes aspects of Erich's life and work, enriched with personal and
interesting anecdotes of Erich's long and productive scientific journey.
Finally, Peter Bickel, Aiyou Chen and Elizaveta Levina present a research paper
on network models: they dedicate their contribution to Erich, emphasizing that
their new nonparametric method and issues about optimality have been very much
influenced by Erich's thinking."@2012
Tony Cai@http://arxiv.org/abs/1202.5118v1@Introduction to the Lehmann special section@"The current Special Issue of The Annals of Statistics contains three invited
articles. Javier Rojo discusses Erich's scientific achievements and provides
complete lists of his scientific writings and his former Ph.D. students. Willem
van Zwet describes aspects of Erich's life and work, enriched with personal and
interesting anecdotes of Erich's long and productive scientific journey.
Finally, Peter Bickel, Aiyou Chen and Elizaveta Levina present a research paper
on network models: they dedicate their contribution to Erich, emphasizing that
their new nonparametric method and issues about optimality have been very much
influenced by Erich's thinking."@2012
T. Tony Cai@http://arxiv.org/abs/1202.5134v1@"Optimal estimation of the mean function based on discretely sampled
  functional data: Phase transition"@"The problem of estimating the mean of random functions based on discretely
sampled data arises naturally in functional data analysis. In this paper, we
study optimal estimation of the mean function under both common and independent
designs. Minimax rates of convergence are established and easily implementable
rate-optimal estimators are introduced. The analysis reveals interesting and
different phase transition phenomena in the two cases. Under the common design,
the sampling frequency solely determines the optimal rate of convergence when
it is relatively small and the sampling frequency has no effect on the optimal
rate when it is large. On the other hand, under the independent design, the
optimal rate of convergence is determined jointly by the sampling frequency and
the number of curves when the sampling frequency is relatively small. When it
is large, the sampling frequency has no effect on the optimal rate. Another
interesting contrast between the two settings is that smoothing is necessary
under the independent design, while, somewhat surprisingly, it is not essential
under the common design."@2012
Ming Yuan@http://arxiv.org/abs/1202.5134v1@"Optimal estimation of the mean function based on discretely sampled
  functional data: Phase transition"@"The problem of estimating the mean of random functions based on discretely
sampled data arises naturally in functional data analysis. In this paper, we
study optimal estimation of the mean function under both common and independent
designs. Minimax rates of convergence are established and easily implementable
rate-optimal estimators are introduced. The analysis reveals interesting and
different phase transition phenomena in the two cases. Under the common design,
the sampling frequency solely determines the optimal rate of convergence when
it is relatively small and the sampling frequency has no effect on the optimal
rate when it is large. On the other hand, under the independent design, the
optimal rate of convergence is determined jointly by the sampling frequency and
the number of curves when the sampling frequency is relatively small. When it
is large, the sampling frequency has no effect on the optimal rate. Another
interesting contrast between the two settings is that smoothing is necessary
under the independent design, while, somewhat surprisingly, it is not essential
under the common design."@2012
Tze Leung Lai@http://arxiv.org/abs/1202.5140v1@Evaluating probability forecasts@"Probability forecasts of events are routinely used in climate predictions, in
forecasting default probabilities on bank loans or in estimating the
probability of a patient's positive response to treatment. Scoring rules have
long been used to assess the efficacy of the forecast probabilities after
observing the occurrence, or nonoccurrence, of the predicted events. We develop
herein a statistical theory for scoring rules and propose an alternative
approach to the evaluation of probability forecasts. This approach uses loss
functions relating the predicted to the actual probabilities of the events and
applies martingale theory to exploit the temporal structure between the
forecast and the subsequent occurrence or nonoccurrence of the event."@2012
Shulamith T. Gross@http://arxiv.org/abs/1202.5140v1@Evaluating probability forecasts@"Probability forecasts of events are routinely used in climate predictions, in
forecasting default probabilities on bank loans or in estimating the
probability of a patient's positive response to treatment. Scoring rules have
long been used to assess the efficacy of the forecast probabilities after
observing the occurrence, or nonoccurrence, of the predicted events. We develop
herein a statistical theory for scoring rules and propose an alternative
approach to the evaluation of probability forecasts. This approach uses loss
functions relating the predicted to the actual probabilities of the events and
applies martingale theory to exploit the temporal structure between the
forecast and the subsequent occurrence or nonoccurrence of the event."@2012
David Bo Shen@http://arxiv.org/abs/1202.5140v1@Evaluating probability forecasts@"Probability forecasts of events are routinely used in climate predictions, in
forecasting default probabilities on bank loans or in estimating the
probability of a patient's positive response to treatment. Scoring rules have
long been used to assess the efficacy of the forecast probabilities after
observing the occurrence, or nonoccurrence, of the predicted events. We develop
herein a statistical theory for scoring rules and propose an alternative
approach to the evaluation of probability forecasts. This approach uses loss
functions relating the predicted to the actual probabilities of the events and
applies martingale theory to exploit the temporal structure between the
forecast and the subsequent occurrence or nonoccurrence of the event."@2012
Marc Hoffmann@http://arxiv.org/abs/1202.5145v1@On adaptive inference and confidence bands@"The problem of existence of adaptive confidence bands for an unknown density
$f$ that belongs to a nested scale of H\""{o}lder classes over $\mathbb{R}$ or
$[0,1]$ is considered. Whereas honest adaptive inference in this problem is
impossible already for a pair of H\""{o}lder balls $\Sigma(r),\Sigma(s),r\ne s$,
of fixed radius, a nonparametric distinguishability condition is introduced
under which adaptive confidence bands can be shown to exist. It is further
shown that this condition is necessary and sufficient for the existence of
honest asymptotic confidence bands, and that it is strictly weaker than similar
analytic conditions recently employed in Gin\'{e} and Nickl [Ann. Statist. 38
(2010) 1122--1170]. The exceptional sets for which honest inference is not
possible have vanishingly small probability under natural priors on H\""{o}lder
balls $\Sigma(s)$. If no upper bound for the radius of the H\""{o}lder balls is
known, a price for adaptation has to be paid, and near-optimal adaptation is
possible for standard procedures. The implications of these findings for a
general theory of adaptive inference are discussed."@2012
Richard Nickl@http://arxiv.org/abs/1202.5145v1@On adaptive inference and confidence bands@"The problem of existence of adaptive confidence bands for an unknown density
$f$ that belongs to a nested scale of H\""{o}lder classes over $\mathbb{R}$ or
$[0,1]$ is considered. Whereas honest adaptive inference in this problem is
impossible already for a pair of H\""{o}lder balls $\Sigma(r),\Sigma(s),r\ne s$,
of fixed radius, a nonparametric distinguishability condition is introduced
under which adaptive confidence bands can be shown to exist. It is further
shown that this condition is necessary and sufficient for the existence of
honest asymptotic confidence bands, and that it is strictly weaker than similar
analytic conditions recently employed in Gin\'{e} and Nickl [Ann. Statist. 38
(2010) 1122--1170]. The exceptional sets for which honest inference is not
possible have vanishingly small probability under natural priors on H\""{o}lder
balls $\Sigma(s)$. If no upper bound for the radius of the H\""{o}lder balls is
known, a price for adaptation has to be paid, and near-optimal adaptation is
possible for standard procedures. The implications of these findings for a
general theory of adaptive inference are discussed."@2012
Alois Kneip@http://arxiv.org/abs/1202.5151v1@"Factor models and variable selection in high-dimensional regression
  analysis"@"The paper considers linear regression problems where the number of predictor
variables is possibly larger than the sample size. The basic motivation of the
study is to combine the points of view of model selection and functional
regression by using a factor approach: it is assumed that the predictor vector
can be decomposed into a sum of two uncorrelated random components reflecting
common factors and specific variabilities of the explanatory variables. It is
shown that the traditional assumption of a sparse vector of parameters is
restrictive in this context. Common factors may possess a significant influence
on the response variable which cannot be captured by the specific effects of a
small number of individual variables. We therefore propose to include principal
components as additional explanatory variables in an augmented regression
model. We give finite sample inequalities for estimates of these components. It
is then shown that model selection procedures can be used to estimate the
parameters of the augmented model, and we derive theoretical properties of the
estimators. Finite sample performance is illustrated by a simulation study."@2012
Pascal Sarda@http://arxiv.org/abs/1202.5151v1@"Factor models and variable selection in high-dimensional regression
  analysis"@"The paper considers linear regression problems where the number of predictor
variables is possibly larger than the sample size. The basic motivation of the
study is to combine the points of view of model selection and functional
regression by using a factor approach: it is assumed that the predictor vector
can be decomposed into a sum of two uncorrelated random components reflecting
common factors and specific variabilities of the explanatory variables. It is
shown that the traditional assumption of a sparse vector of parameters is
restrictive in this context. Common factors may possess a significant influence
on the response variable which cannot be captured by the specific effects of a
small number of individual variables. We therefore propose to include principal
components as additional explanatory variables in an augmented regression
model. We give finite sample inequalities for estimates of these components. It
is then shown that model selection procedures can be used to estimate the
parameters of the augmented model, and we derive theoretical properties of the
estimators. Finite sample performance is illustrated by a simulation study."@2012
Pauliina Ilmonen@http://arxiv.org/abs/1202.5159v1@"Semiparametrically efficient inference based on signed ranks in
  symmetric independent component models"@"We consider semiparametric location-scatter models for which the $p$-variate
observation is obtained as $X=\Lambda Z+\mu$, where $\mu$ is a $p$-vector,
$\Lambda$ is a full-rank $p\times p$ matrix and the (unobserved) random
$p$-vector $Z$ has marginals that are centered and mutually independent but are
otherwise unspecified. As in blind source separation and independent component
analysis (ICA), the parameter of interest throughout the paper is $\Lambda$. On
the basis of $n$ i.i.d. copies of $X$, we develop, under a symmetry assumption
on $Z$, signed-rank one-sample testing and estimation procedures for $\Lambda$.
We exploit the uniform local and asymptotic normality (ULAN) of the model to
define signed-rank procedures that are semiparametrically efficient under
correctly specified densities. Yet, as is usual in rank-based inference, the
proposed procedures remain valid (correct asymptotic size under the null, for
hypothesis testing, and root-$n$ consistency, for point estimation) under a
very broad range of densities. We derive the asymptotic properties of the
proposed procedures and investigate their finite-sample behavior through
simulations."@2012
Davy Paindaveine@http://arxiv.org/abs/1202.5159v1@"Semiparametrically efficient inference based on signed ranks in
  symmetric independent component models"@"We consider semiparametric location-scatter models for which the $p$-variate
observation is obtained as $X=\Lambda Z+\mu$, where $\mu$ is a $p$-vector,
$\Lambda$ is a full-rank $p\times p$ matrix and the (unobserved) random
$p$-vector $Z$ has marginals that are centered and mutually independent but are
otherwise unspecified. As in blind source separation and independent component
analysis (ICA), the parameter of interest throughout the paper is $\Lambda$. On
the basis of $n$ i.i.d. copies of $X$, we develop, under a symmetry assumption
on $Z$, signed-rank one-sample testing and estimation procedures for $\Lambda$.
We exploit the uniform local and asymptotic normality (ULAN) of the model to
define signed-rank procedures that are semiparametrically efficient under
correctly specified densities. Yet, as is usual in rank-based inference, the
proposed procedures remain valid (correct asymptotic size under the null, for
hypothesis testing, and root-$n$ consistency, for point estimation) under a
very broad range of densities. We derive the asymptotic properties of the
proposed procedures and investigate their finite-sample behavior through
simulations."@2012
Eugenia Buta@http://arxiv.org/abs/1202.5160v1@"Computational approaches for empirical Bayes methods and Bayesian
  sensitivity analysis"@"We consider situations in Bayesian analysis where we have a family of priors
$\nu_h$ on the parameter $\theta$, where $h$ varies continuously over a space
$\mathcal{H}$, and we deal with two related problems. The first involves
sensitivity analysis and is stated as follows. Suppose we fix a function $f$ of
$\theta$. How do we efficiently estimate the posterior expectation of
$f(\theta)$ simultaneously for all $h$ in $\mathcal{H}$? The second problem is
how do we identify subsets of $\mathcal{H}$ which give rise to reasonable
choices of $\nu_h$? We assume that we are able to generate Markov chain samples
from the posterior for a finite number of the priors, and we develop a
methodology, based on a combination of importance sampling and the use of
control variates, for dealing with these two problems. The methodology applies
very generally, and we show how it applies in particular to a commonly used
model for variable selection in Bayesian linear regression, and give an
illustration on the US crime data of Vandaele."@2012
Hani Doss@http://arxiv.org/abs/1202.5160v1@"Computational approaches for empirical Bayes methods and Bayesian
  sensitivity analysis"@"We consider situations in Bayesian analysis where we have a family of priors
$\nu_h$ on the parameter $\theta$, where $h$ varies continuously over a space
$\mathcal{H}$, and we deal with two related problems. The first involves
sensitivity analysis and is stated as follows. Suppose we fix a function $f$ of
$\theta$. How do we efficiently estimate the posterior expectation of
$f(\theta)$ simultaneously for all $h$ in $\mathcal{H}$? The second problem is
how do we identify subsets of $\mathcal{H}$ which give rise to reasonable
choices of $\nu_h$? We assume that we are able to generate Markov chain samples
from the posterior for a finite number of the priors, and we develop a
methodology, based on a combination of importance sampling and the use of
control variates, for dealing with these two problems. The methodology applies
very generally, and we show how it applies in particular to a commonly used
model for variable selection in Bayesian linear regression, and give an
illustration on the US crime data of Vandaele."@2012
Peter Hall@http://arxiv.org/abs/1202.5183v1@"Asymptotic normality and valid inference for Gaussian variational
  approximation"@"We derive the precise asymptotic distributional behavior of Gaussian
variational approximate estimators of the parameters in a single-predictor
Poisson mixed model. These results are the deepest yet obtained concerning the
statistical properties of a variational approximation method. Moreover, they
give rise to asymptotically valid statistical inference. A simulation study
demonstrates that Gaussian variational approximate confidence intervals possess
good to excellent coverage properties, and have a similar precision to their
exact likelihood counterparts."@2012
Tung Pham@http://arxiv.org/abs/1202.5183v1@"Asymptotic normality and valid inference for Gaussian variational
  approximation"@"We derive the precise asymptotic distributional behavior of Gaussian
variational approximate estimators of the parameters in a single-predictor
Poisson mixed model. These results are the deepest yet obtained concerning the
statistical properties of a variational approximation method. Moreover, they
give rise to asymptotically valid statistical inference. A simulation study
demonstrates that Gaussian variational approximate confidence intervals possess
good to excellent coverage properties, and have a similar precision to their
exact likelihood counterparts."@2012
M. P. Wand@http://arxiv.org/abs/1202.5183v1@"Asymptotic normality and valid inference for Gaussian variational
  approximation"@"We derive the precise asymptotic distributional behavior of Gaussian
variational approximate estimators of the parameters in a single-predictor
Poisson mixed model. These results are the deepest yet obtained concerning the
statistical properties of a variational approximation method. Moreover, they
give rise to asymptotically valid statistical inference. A simulation study
demonstrates that Gaussian variational approximate confidence intervals possess
good to excellent coverage properties, and have a similar precision to their
exact likelihood counterparts."@2012
S. S. J. Wang@http://arxiv.org/abs/1202.5183v1@"Asymptotic normality and valid inference for Gaussian variational
  approximation"@"We derive the precise asymptotic distributional behavior of Gaussian
variational approximate estimators of the parameters in a single-predictor
Poisson mixed model. These results are the deepest yet obtained concerning the
statistical properties of a variational approximation method. Moreover, they
give rise to asymptotically valid statistical inference. A simulation study
demonstrates that Gaussian variational approximate confidence intervals possess
good to excellent coverage properties, and have a similar precision to their
exact likelihood counterparts."@2012
Kshitij Khare@http://arxiv.org/abs/1202.5205v1@"A spectral analytic comparison of trace-class data augmentation
  algorithms and their sandwich variants"@"The data augmentation (DA) algorithm is a widely used Markov chain Monte
Carlo algorithm that is easy to implement but often suffers from slow
convergence. The sandwich algorithm is an alternative that can converge much
faster while requiring roughly the same computational effort per iteration.
Theoretically, the sandwich algorithm always converges at least as fast as the
corresponding DA algorithm in the sense that $\Vert {K^*}\Vert \le \Vert
{K}\Vert$, where $K$ and $K^*$ are the Markov operators associated with the DA
and sandwich algorithms, respectively, and $\Vert\cdot\Vert$ denotes operator
norm. In this paper, a substantial refinement of this operator norm inequality
is developed. In particular, under regularity conditions implying that $K$ is a
trace-class operator, it is shown that $K^*$ is also a positive, trace-class
operator, and that the spectrum of $K^*$ dominates that of $K$ in the sense
that the ordered elements of the former are all less than or equal to the
corresponding elements of the latter. Furthermore, if the sandwich algorithm is
constructed using a group action, as described by Liu and Wu [J. Amer. Statist.
Assoc. 94 (1999) 1264--1274] and Hobert and Marchev [Ann. Statist. 36 (2008)
532--554], then there is strict inequality between at least one pair of
eigenvalues. These results are applied to a new DA algorithm for Bayesian
quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul.
81 (2011) 1565--1578]."@2012
James P. Hobert@http://arxiv.org/abs/1202.5205v1@"A spectral analytic comparison of trace-class data augmentation
  algorithms and their sandwich variants"@"The data augmentation (DA) algorithm is a widely used Markov chain Monte
Carlo algorithm that is easy to implement but often suffers from slow
convergence. The sandwich algorithm is an alternative that can converge much
faster while requiring roughly the same computational effort per iteration.
Theoretically, the sandwich algorithm always converges at least as fast as the
corresponding DA algorithm in the sense that $\Vert {K^*}\Vert \le \Vert
{K}\Vert$, where $K$ and $K^*$ are the Markov operators associated with the DA
and sandwich algorithms, respectively, and $\Vert\cdot\Vert$ denotes operator
norm. In this paper, a substantial refinement of this operator norm inequality
is developed. In particular, under regularity conditions implying that $K$ is a
trace-class operator, it is shown that $K^*$ is also a positive, trace-class
operator, and that the spectrum of $K^*$ dominates that of $K$ in the sense
that the ordered elements of the former are all less than or equal to the
corresponding elements of the latter. Furthermore, if the sandwich algorithm is
constructed using a group action, as described by Liu and Wu [J. Amer. Statist.
Assoc. 94 (1999) 1264--1274] and Hobert and Marchev [Ann. Statist. 36 (2008)
532--554], then there is strict inequality between at least one pair of
eigenvalues. These results are applied to a new DA algorithm for Bayesian
quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul.
81 (2011) 1565--1578]."@2012
Bernard Bercu@http://arxiv.org/abs/1202.5432v1@"On the asymptotic behavior of the Nadaraya-Watson estimator associated
  with the recursive SIR method"@"We investigate the asymptotic behavior of the Nadaraya-Watson estimator for
the estimation of the regression function in a semiparametric regression model.
On the one hand, we make use of the recursive version of the sliced inverse
regression method for the estimation of the unknown parameter of the model. On
the other hand, we implement a recursive Nadaraya-Watson procedure for the
estimation of the regression function which takes into account the previous
estimation of the parameter of the semiparametric regression model. We
establish the almost sure convergence as well as the asymptotic normality for
our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation
procedure on simulated data."@2012
Thi Mong Ngoc Nguyen@http://arxiv.org/abs/1202.5432v1@"On the asymptotic behavior of the Nadaraya-Watson estimator associated
  with the recursive SIR method"@"We investigate the asymptotic behavior of the Nadaraya-Watson estimator for
the estimation of the regression function in a semiparametric regression model.
On the one hand, we make use of the recursive version of the sliced inverse
regression method for the estimation of the unknown parameter of the model. On
the other hand, we implement a recursive Nadaraya-Watson procedure for the
estimation of the regression function which takes into account the previous
estimation of the parameter of the semiparametric regression model. We
establish the almost sure convergence as well as the asymptotic normality for
our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation
procedure on simulated data."@2012
Jerome Saracco@http://arxiv.org/abs/1202.5432v1@"On the asymptotic behavior of the Nadaraya-Watson estimator associated
  with the recursive SIR method"@"We investigate the asymptotic behavior of the Nadaraya-Watson estimator for
the estimation of the regression function in a semiparametric regression model.
On the one hand, we make use of the recursive version of the sliced inverse
regression method for the estimation of the unknown parameter of the model. On
the other hand, we implement a recursive Nadaraya-Watson procedure for the
estimation of the regression function which takes into account the previous
estimation of the parameter of the semiparametric regression model. We
establish the almost sure convergence as well as the asymptotic normality for
our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation
procedure on simulated data."@2012
Ery Arias-Castro@http://arxiv.org/abs/1202.5536v3@Detecting positive correlations in a multivariate sample@"We consider the problem of testing whether a correlation matrix of a
multivariate normal population is the identity matrix. We focus on sparse
classes of alternatives where only a few entries are nonzero and, in fact,
positive. We derive a general lower bound applicable to various classes and
study the performance of some near-optimal tests. We pay special attention to
computational feasibility and construct near-optimal tests that can be computed
efficiently. Finally, we apply our results to prove new lower bounds for the
clique number of high-dimensional random geometric graphs."@2012
Sébastien Bubeck@http://arxiv.org/abs/1202.5536v3@Detecting positive correlations in a multivariate sample@"We consider the problem of testing whether a correlation matrix of a
multivariate normal population is the identity matrix. We focus on sparse
classes of alternatives where only a few entries are nonzero and, in fact,
positive. We derive a general lower bound applicable to various classes and
study the performance of some near-optimal tests. We pay special attention to
computational feasibility and construct near-optimal tests that can be computed
efficiently. Finally, we apply our results to prove new lower bounds for the
clique number of high-dimensional random geometric graphs."@2012
Gábor Lugosi@http://arxiv.org/abs/1202.5536v3@Detecting positive correlations in a multivariate sample@"We consider the problem of testing whether a correlation matrix of a
multivariate normal population is the identity matrix. We focus on sparse
classes of alternatives where only a few entries are nonzero and, in fact,
positive. We derive a general lower bound applicable to various classes and
study the performance of some near-optimal tests. We pay special attention to
computational feasibility and construct near-optimal tests that can be computed
efficiently. Finally, we apply our results to prove new lower bounds for the
clique number of high-dimensional random geometric graphs."@2012
Fabien Navarro@http://arxiv.org/abs/1202.6316v3@"Block thresholding for wavelet-based estimation of function derivatives
  from a heteroscedastic multichannel convolution model"@"We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where
for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution
product of an unknown function $f$ and a known blurring function $g_v$
corrupted by Gaussian noise. Under an ordinary smoothness assumption on
$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak
sense) of $f$ from the observations. We propose an adaptive estimator based on
wavelet block thresholding, namely the ""BlockJS estimator"". Taking the mean
integrated squared error (MISE), our main theoretical result investigates the
minimax rates over Besov smoothness spaces, and shows that our block estimator
can achieve the optimal minimax rate, or is at least nearly-minimax in the
least favorable situation. We also report a comprehensive suite of numerical
simulations to support our theoretical findings. The practical performance of
our block estimator compares very favorably to existing methods of the
literature on a large set of test functions."@2012
Christophe Chesneau@http://arxiv.org/abs/1202.6316v3@"Block thresholding for wavelet-based estimation of function derivatives
  from a heteroscedastic multichannel convolution model"@"We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where
for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution
product of an unknown function $f$ and a known blurring function $g_v$
corrupted by Gaussian noise. Under an ordinary smoothness assumption on
$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak
sense) of $f$ from the observations. We propose an adaptive estimator based on
wavelet block thresholding, namely the ""BlockJS estimator"". Taking the mean
integrated squared error (MISE), our main theoretical result investigates the
minimax rates over Besov smoothness spaces, and shows that our block estimator
can achieve the optimal minimax rate, or is at least nearly-minimax in the
least favorable situation. We also report a comprehensive suite of numerical
simulations to support our theoretical findings. The practical performance of
our block estimator compares very favorably to existing methods of the
literature on a large set of test functions."@2012
Jalal Fadili@http://arxiv.org/abs/1202.6316v3@"Block thresholding for wavelet-based estimation of function derivatives
  from a heteroscedastic multichannel convolution model"@"We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where
for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution
product of an unknown function $f$ and a known blurring function $g_v$
corrupted by Gaussian noise. Under an ordinary smoothness assumption on
$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak
sense) of $f$ from the observations. We propose an adaptive estimator based on
wavelet block thresholding, namely the ""BlockJS estimator"". Taking the mean
integrated squared error (MISE), our main theoretical result investigates the
minimax rates over Besov smoothness spaces, and shows that our block estimator
can achieve the optimal minimax rate, or is at least nearly-minimax in the
least favorable situation. We also report a comprehensive suite of numerical
simulations to support our theoretical findings. The practical performance of
our block estimator compares very favorably to existing methods of the
literature on a large set of test functions."@2012
Taoufik Sassi@http://arxiv.org/abs/1202.6316v3@"Block thresholding for wavelet-based estimation of function derivatives
  from a heteroscedastic multichannel convolution model"@"We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where
for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution
product of an unknown function $f$ and a known blurring function $g_v$
corrupted by Gaussian noise. Under an ordinary smoothness assumption on
$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak
sense) of $f$ from the observations. We propose an adaptive estimator based on
wavelet block thresholding, namely the ""BlockJS estimator"". Taking the mean
integrated squared error (MISE), our main theoretical result investigates the
minimax rates over Besov smoothness spaces, and shows that our block estimator
can achieve the optimal minimax rate, or is at least nearly-minimax in the
least favorable situation. We also report a comprehensive suite of numerical
simulations to support our theoretical findings. The practical performance of
our block estimator compares very favorably to existing methods of the
literature on a large set of test functions."@2012
Abram M. Kagan@http://arxiv.org/abs/1202.6427v3@"Monotonicity in the Sample Size of the Length of Classical Confidence
  Intervals"@"It is proved that the average length of standard confidence intervals for
parameters of gamma and normal distributions monotonically decrease with the
sample size. The proofs are based on fine properties of the classical gamma
function."@2012
Yaakov Malinovsky@http://arxiv.org/abs/1202.6427v3@"Monotonicity in the Sample Size of the Length of Classical Confidence
  Intervals"@"It is proved that the average length of standard confidence intervals for
parameters of gamma and normal distributions monotonically decrease with the
sample size. The proofs are based on fine properties of the classical gamma
function."@2012
Jean-Baptiste Durand@http://arxiv.org/abs/1202.6545v1@"Localizing the Latent Structure Canonical Uncertainty: Entropy Profiles
  for Hidden Markov Models"@"This report addresses state inference for hidden Markov models. These models
rely on unobserved states, which often have a meaningful interpretation. This
makes it necessary to develop diagnostic tools for quantification of state
uncertainty. The entropy of the state sequence that explains an observed
sequence for a given hidden Markov chain model can be considered as the
canonical measure of state sequence uncertainty. This canonical measure of
state sequence uncertainty is not reflected by the classic multivariate state
profiles computed by the smoothing algorithm, which summarizes the possible
state sequences. Here, we introduce a new type of profiles which have the
following properties: (i) these profiles of conditional entropies are a
decomposition of the canonical measure of state sequence uncertainty along the
sequence and makes it possible to localize this uncertainty, (ii) these
profiles are univariate and thus remain easily interpretable on tree
structures. We show how to extend the smoothing algorithms for hidden Markov
chain and tree models to compute these entropy profiles efficiently."@2012
Y. Guédon@http://arxiv.org/abs/1202.6545v1@"Localizing the Latent Structure Canonical Uncertainty: Entropy Profiles
  for Hidden Markov Models"@"This report addresses state inference for hidden Markov models. These models
rely on unobserved states, which often have a meaningful interpretation. This
makes it necessary to develop diagnostic tools for quantification of state
uncertainty. The entropy of the state sequence that explains an observed
sequence for a given hidden Markov chain model can be considered as the
canonical measure of state sequence uncertainty. This canonical measure of
state sequence uncertainty is not reflected by the classic multivariate state
profiles computed by the smoothing algorithm, which summarizes the possible
state sequences. Here, we introduce a new type of profiles which have the
following properties: (i) these profiles of conditional entropies are a
decomposition of the canonical measure of state sequence uncertainty along the
sequence and makes it possible to localize this uncertainty, (ii) these
profiles are univariate and thus remain easily interpretable on tree
structures. We show how to extend the smoothing algorithms for hidden Markov
chain and tree models to compute these entropy profiles efficiently."@2012
Akram Kohansal@http://arxiv.org/abs/1203.0094v1@"Parameter Estimation of Type-II Hybrid Censored Weighted Exponential
  Distribution"@"A hybrid censoring scheme is a mixture of Type-I and Type-II censoring
schemes. We study the estimation of parameters of weighted exponential
distribution based on Type-II hybrid censored data. By applying EM algorithm,
maximum likelihood estimators are evaluated. Also using Fisher infirmation
matrix asymptotic confidence intervals are provided. By applying Markov Chain
Monte Carlo techniques Bayes estimators, and corresponding highest posterior
density confidence intervals of parameters are obtained. Monte Carlo
simulations to compare the performances of the different methods is performed
and one data set is analyzed for illustrative purposes."@2012
Saeid Rezakhah@http://arxiv.org/abs/1203.0094v1@"Parameter Estimation of Type-II Hybrid Censored Weighted Exponential
  Distribution"@"A hybrid censoring scheme is a mixture of Type-I and Type-II censoring
schemes. We study the estimation of parameters of weighted exponential
distribution based on Type-II hybrid censored data. By applying EM algorithm,
maximum likelihood estimators are evaluated. Also using Fisher infirmation
matrix asymptotic confidence intervals are provided. By applying Markov Chain
Monte Carlo techniques Bayes estimators, and corresponding highest posterior
density confidence intervals of parameters are obtained. Monte Carlo
simulations to compare the performances of the different methods is performed
and one data set is analyzed for illustrative purposes."@2012
Fatemeh Azizzadeh@http://arxiv.org/abs/1203.0097v1@"The CUSUM test for detecting structural changes in strong mixing
  processes"@"Strong mixing property holds for a broad class of linear and nonlinear time
series models such as ARMA and GARCH models. In this article we study
correlation structure of strong mixing sequences, and some asymptotic
properties are presented. We also present a new method for detecting change
point in correlation structure of strong mixing sequences, and present a
nonparametric CUSUM test statistic for this. Asymptotic consistency of this
test statistics is shown. This method is applied to simulated data of some
linear and nonlinear models and power of the test is evaluated. For linear
models, it is shown that this method have a better performance in compare to
Berkes et al.(2009)."@2012
Saeid Rezakhah@http://arxiv.org/abs/1203.0097v1@"The CUSUM test for detecting structural changes in strong mixing
  processes"@"Strong mixing property holds for a broad class of linear and nonlinear time
series models such as ARMA and GARCH models. In this article we study
correlation structure of strong mixing sequences, and some asymptotic
properties are presented. We also present a new method for detecting change
point in correlation structure of strong mixing sequences, and present a
nonparametric CUSUM test statistic for this. Asymptotic consistency of this
test statistics is shown. This method is applied to simulated data of some
linear and nonlinear models and power of the test is evaluated. For linear
models, it is shown that this method have a better performance in compare to
Berkes et al.(2009)."@2012
Rolando Biscay@http://arxiv.org/abs/1203.0107v1@Adaptive Covariance Estimation with model selection@"We provide in this paper a fully adaptive penalized procedure to select a
covariance among a collection of models observing i.i.d replications of the
process at fixed observation points. For this we generalize previous results of
Bigot and al. and propose to use a data driven penalty to obtain an oracle
inequality for the estimator. We prove that this method is an extension to the
matricial regression model of the work by Baraud."@2012
Hélène Lescornel@http://arxiv.org/abs/1203.0107v1@Adaptive Covariance Estimation with model selection@"We provide in this paper a fully adaptive penalized procedure to select a
covariance among a collection of models observing i.i.d replications of the
process at fixed observation points. For this we generalize previous results of
Bigot and al. and propose to use a data driven penalty to obtain an oracle
inequality for the estimator. We prove that this method is an extension to the
matricial regression model of the work by Baraud."@2012
Jean-Michel Loubes@http://arxiv.org/abs/1203.0107v1@Adaptive Covariance Estimation with model selection@"We provide in this paper a fully adaptive penalized procedure to select a
covariance among a collection of models observing i.i.d replications of the
process at fixed observation points. For this we generalize previous results of
Bigot and al. and propose to use a data driven penalty to obtain an oracle
inequality for the estimator. We prove that this method is an extension to the
matricial regression model of the work by Baraud."@2012
Eckhard Schlemm@http://arxiv.org/abs/1203.0131v1@"Multivariate CARMA processes, continuous-time state space models and
  complete regularity of the innovations of the sampled processes"@"The class of multivariate L\'{e}vy-driven autoregressive moving average
(MCARMA) processes, the continuous-time analogs of the classical vector ARMA
processes, is shown to be equivalent to the class of continuous-time state
space models. The linear innovations of the weak ARMA process arising from
sampling an MCARMA process at an equidistant grid are proved to be
exponentially completely regular ($\beta$-mixing) under a mild continuity
assumption on the driving L\'{e}vy process. It is verified that this continuity
assumption is satisfied in most practically relevant situations, including the
case where the driving L\'{e}vy process has a non-singular Gaussian component,
is compound Poisson with an absolutely continuous jump size distribution or has
an infinite L\'{e}vy measure admitting a density around zero."@2012
Robert Stelzer@http://arxiv.org/abs/1203.0131v1@"Multivariate CARMA processes, continuous-time state space models and
  complete regularity of the innovations of the sampled processes"@"The class of multivariate L\'{e}vy-driven autoregressive moving average
(MCARMA) processes, the continuous-time analogs of the classical vector ARMA
processes, is shown to be equivalent to the class of continuous-time state
space models. The linear innovations of the weak ARMA process arising from
sampling an MCARMA process at an equidistant grid are proved to be
exponentially completely regular ($\beta$-mixing) under a mild continuity
assumption on the driving L\'{e}vy process. It is verified that this continuity
assumption is satisfied in most practically relevant situations, including the
case where the driving L\'{e}vy process has a non-singular Gaussian component,
is compound Poisson with an absolutely continuous jump size distribution or has
an infinite L\'{e}vy measure admitting a density around zero."@2012
Boris Buchmann@http://arxiv.org/abs/1203.0186v1@Limit experiments of GARCH@"GARCH is one of the most prominent nonlinear time series models, both widely
applied and thoroughly studied. Recently, it has been shown that the COGARCH
model (which was introduced a few years ago by Kl\""{u}ppelberg, Lindner and
Maller) and Nelson's diffusion limit are the only functional continuous-time
limits of GARCH in distribution. In contrast to Nelson's diffusion limit,
COGARCH reproduces most of the stylized facts of financial time series. Since
it has been proven that Nelson's diffusion is not asymptotically equivalent to
GARCH in deficiency, in the present paper, we investigate the relation between
GARCH and COGARCH in Le Cam's framework of statistical equivalence. We show
that GARCH converges generically to COGARCH, even in deficiency, provided that
the volatility processes are observed. Hence, from a theoretical point of view,
COGARCH can indeed be considered as a continuous-time equivalent to GARCH.
Otherwise, when the observations are incomplete, GARCH still has a limiting
experiment, which we call MCOGARCH, which is not equivalent, but nevertheless
quite similar, to COGARCH. In the COGARCH model, the jump times can be more
random than for the MCOGARCH, a fact practitioners may see as an advantage of
COGARCH."@2012
Gernot Müller@http://arxiv.org/abs/1203.0186v1@Limit experiments of GARCH@"GARCH is one of the most prominent nonlinear time series models, both widely
applied and thoroughly studied. Recently, it has been shown that the COGARCH
model (which was introduced a few years ago by Kl\""{u}ppelberg, Lindner and
Maller) and Nelson's diffusion limit are the only functional continuous-time
limits of GARCH in distribution. In contrast to Nelson's diffusion limit,
COGARCH reproduces most of the stylized facts of financial time series. Since
it has been proven that Nelson's diffusion is not asymptotically equivalent to
GARCH in deficiency, in the present paper, we investigate the relation between
GARCH and COGARCH in Le Cam's framework of statistical equivalence. We show
that GARCH converges generically to COGARCH, even in deficiency, provided that
the volatility processes are observed. Hence, from a theoretical point of view,
COGARCH can indeed be considered as a continuous-time equivalent to GARCH.
Otherwise, when the observations are incomplete, GARCH still has a limiting
experiment, which we call MCOGARCH, which is not equivalent, but nevertheless
quite similar, to COGARCH. In the COGARCH model, the jump times can be more
random than for the MCOGARCH, a fact practitioners may see as an advantage of
COGARCH."@2012
A. E. Koudou@http://arxiv.org/abs/1203.0381v1@Independence properties of the Matsumoto--Yor type@"We define Letac-Wesolowski-Matsumoto-Yor (LWMY) functions as decreasing
functions from $(0,\infty)$ onto $(0,\infty)$ with the following property:
there exist independent, positive random variables $X$ and $Y$ such that the
variables $f(X+Y)$ and $f(X)-f(X+Y)$ are independent. We prove that, under
additional assumptions, there are essentially four such functions. The first
one is $f(x)=1/x$. In this case, referred to in the literature as the
Matsumoto-Yor property, the law of $X$ is generalized inverse Gaussian while
$Y$ is gamma distributed. In the three other cases, the associated densities
are provided. As a consequence, we obtain a new relation of convolution
involving gamma distributions and Kummer distributions of type 2."@2012
P. Vallois@http://arxiv.org/abs/1203.0381v1@Independence properties of the Matsumoto--Yor type@"We define Letac-Wesolowski-Matsumoto-Yor (LWMY) functions as decreasing
functions from $(0,\infty)$ onto $(0,\infty)$ with the following property:
there exist independent, positive random variables $X$ and $Y$ such that the
variables $f(X+Y)$ and $f(X)-f(X+Y)$ are independent. We prove that, under
additional assumptions, there are essentially four such functions. The first
one is $f(x)=1/x$. In this case, referred to in the literature as the
Matsumoto-Yor property, the law of $X$ is generalized inverse Gaussian while
$Y$ is gamma distributed. In the three other cases, the associated densities
are provided. As a consequence, we obtain a new relation of convolution
involving gamma distributions and Kummer distributions of type 2."@2012
Jan Beran@http://arxiv.org/abs/1203.0392v1@"On asymptotically optimal wavelet estimation of trend functions under
  long-range dependence"@"We consider data-adaptive wavelet estimation of a trend function in a time
series model with strongly dependent Gaussian residuals. Asymptotic expressions
for the optimal mean integrated squared error and corresponding optimal
smoothing and resolution parameters are derived. Due to adaptation to the
properties of the underlying trend function, the approach shows very good
performance for smooth trend functions while remaining competitive with minimax
wavelet estimation for functions with discontinuities. Simulations illustrate
the asymptotic results and finite-sample behavior."@2012
Yevgen Shumeyko@http://arxiv.org/abs/1203.0392v1@"On asymptotically optimal wavelet estimation of trend functions under
  long-range dependence"@"We consider data-adaptive wavelet estimation of a trend function in a time
series model with strongly dependent Gaussian residuals. Asymptotic expressions
for the optimal mean integrated squared error and corresponding optimal
smoothing and resolution parameters are derived. Due to adaptation to the
properties of the underlying trend function, the approach shows very good
performance for smooth trend functions while remaining competitive with minimax
wavelet estimation for functions with discontinuities. Simulations illustrate
the asymptotic results and finite-sample behavior."@2012
Young K. Lee@http://arxiv.org/abs/1203.0403v1@Projection-type estimation for varying coefficient regression models@"In this paper we introduce new estimators of the coefficient functions in the
varying coefficient regression model. The proposed estimators are obtained by
projecting the vector of the full-dimensional kernel-weighted local polynomial
estimators of the coefficient functions onto a Hilbert space with a suitable
norm. We provide a backfitting algorithm to compute the estimators. We show
that the algorithm converges at a geometric rate under weak conditions. We
derive the asymptotic distributions of the estimators and show that the
estimators have the oracle properties. This is done for the general order of
local polynomial fitting and for the estimation of the derivatives of the
coefficient functions, as well as the coefficient functions themselves. The
estimators turn out to have several theoretical and numerical advantages over
the marginal integration estimators studied by Yang, Park, Xue and H\""{a}rdle
[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]."@2012
Enno Mammen@http://arxiv.org/abs/1203.0403v1@Projection-type estimation for varying coefficient regression models@"In this paper we introduce new estimators of the coefficient functions in the
varying coefficient regression model. The proposed estimators are obtained by
projecting the vector of the full-dimensional kernel-weighted local polynomial
estimators of the coefficient functions onto a Hilbert space with a suitable
norm. We provide a backfitting algorithm to compute the estimators. We show
that the algorithm converges at a geometric rate under weak conditions. We
derive the asymptotic distributions of the estimators and show that the
estimators have the oracle properties. This is done for the general order of
local polynomial fitting and for the estimation of the derivatives of the
coefficient functions, as well as the coefficient functions themselves. The
estimators turn out to have several theoretical and numerical advantages over
the marginal integration estimators studied by Yang, Park, Xue and H\""{a}rdle
[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]."@2012
Byeong U. Park@http://arxiv.org/abs/1203.0403v1@Projection-type estimation for varying coefficient regression models@"In this paper we introduce new estimators of the coefficient functions in the
varying coefficient regression model. The proposed estimators are obtained by
projecting the vector of the full-dimensional kernel-weighted local polynomial
estimators of the coefficient functions onto a Hilbert space with a suitable
norm. We provide a backfitting algorithm to compute the estimators. We show
that the algorithm converges at a geometric rate under weak conditions. We
derive the asymptotic distributions of the estimators and show that the
estimators have the oracle properties. This is done for the general order of
local polynomial fitting and for the estimation of the derivatives of the
coefficient functions, as well as the coefficient functions themselves. The
estimators turn out to have several theoretical and numerical advantages over
the marginal integration estimators studied by Yang, Park, Xue and H\""{a}rdle
[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]."@2012
Ping Wu@http://arxiv.org/abs/1203.0431v1@Efficient estimation of moments in linear mixed models@"In the linear random effects model, when distributional assumptions such as
normality of the error variables cannot be justified, moments may serve as
alternatives to describe relevant distributions in neighborhoods of their
means. Generally, estimators may be obtained as solutions of estimating
equations. It turns out that there may be several equations, each of them
leading to consistent estimators, in which case finding the efficient estimator
becomes a crucial problem. In this paper, we systematically study estimation of
moments of the errors and random effects in linear mixed models."@2012
Winfried Stute@http://arxiv.org/abs/1203.0431v1@Efficient estimation of moments in linear mixed models@"In the linear random effects model, when distributional assumptions such as
normality of the error variables cannot be justified, moments may serve as
alternatives to describe relevant distributions in neighborhoods of their
means. Generally, estimators may be obtained as solutions of estimating
equations. It turns out that there may be several equations, each of them
leading to consistent estimators, in which case finding the efficient estimator
becomes a crucial problem. In this paper, we systematically study estimation of
moments of the errors and random effects in linear mixed models."@2012
Li-Xing Zhu@http://arxiv.org/abs/1203.0431v1@Efficient estimation of moments in linear mixed models@"In the linear random effects model, when distributional assumptions such as
normality of the error variables cannot be justified, moments may serve as
alternatives to describe relevant distributions in neighborhoods of their
means. Generally, estimators may be obtained as solutions of estimating
equations. It turns out that there may be several equations, each of them
leading to consistent estimators, in which case finding the efficient estimator
becomes a crucial problem. In this paper, we systematically study estimation of
moments of the errors and random effects in linear mixed models."@2012
Jana Jurečková@http://arxiv.org/abs/1203.0450v1@Nonparametric multivariate rank tests and their unbiasedness@"Although unbiasedness is a basic property of a good test, many tests on
vector parameters or scalar parameters against two-sided alternatives are not
finite-sample unbiased. This was already noticed by Sugiura [Ann. Inst.
Statist. Math. 17 (1965) 261--263]; he found an alternative against which the
Wilcoxon test is not unbiased. The problem is even more serious in multivariate
models. When testing the hypothesis against an alternative which fits well with
the experiment, it should be verified whether the power of the test under this
alternative cannot be smaller than the significance level. Surprisingly, this
serious problem is not frequently considered in the literature. The present
paper considers the two-sample multivariate testing problem. We construct
several rank tests which are finite-sample unbiased against a broad class of
location/scale alternatives and are finite-sample distribution-free under the
hypothesis and alternatives. Each of them is locally most powerful against a
specific alternative of the Lehmann type. Their powers against some
alternatives are numerically compared with each other and with other rank and
classical tests. The question of affine invariance of two-sample multivariate
tests is also discussed."@2012
Jan Kalina@http://arxiv.org/abs/1203.0450v1@Nonparametric multivariate rank tests and their unbiasedness@"Although unbiasedness is a basic property of a good test, many tests on
vector parameters or scalar parameters against two-sided alternatives are not
finite-sample unbiased. This was already noticed by Sugiura [Ann. Inst.
Statist. Math. 17 (1965) 261--263]; he found an alternative against which the
Wilcoxon test is not unbiased. The problem is even more serious in multivariate
models. When testing the hypothesis against an alternative which fits well with
the experiment, it should be verified whether the power of the test under this
alternative cannot be smaller than the significance level. Surprisingly, this
serious problem is not frequently considered in the literature. The present
paper considers the two-sample multivariate testing problem. We construct
several rank tests which are finite-sample unbiased against a broad class of
location/scale alternatives and are finite-sample distribution-free under the
hypothesis and alternatives. Each of them is locally most powerful against a
specific alternative of the Lehmann type. Their powers against some
alternatives are numerically compared with each other and with other rank and
classical tests. The question of affine invariance of two-sample multivariate
tests is also discussed."@2012
Pierre Del Moral@http://arxiv.org/abs/1203.0464v1@On adaptive resampling strategies for sequential Monte Carlo methods@"Sequential Monte Carlo (SMC) methods are a class of techniques to sample
approximately from any sequence of probability distributions using a
combination of importance sampling and resampling steps. This paper is
concerned with the convergence analysis of a class of SMC methods where the
times at which resampling occurs are computed online using criteria such as the
effective sample size. This is a popular approach amongst practitioners but
there are very few convergence results available for these methods. By
combining semigroup techniques with an original coupling argument, we obtain
functional central limit theorems and uniform exponential concentration
estimates for these algorithms."@2012
Arnaud Doucet@http://arxiv.org/abs/1203.0464v1@On adaptive resampling strategies for sequential Monte Carlo methods@"Sequential Monte Carlo (SMC) methods are a class of techniques to sample
approximately from any sequence of probability distributions using a
combination of importance sampling and resampling steps. This paper is
concerned with the convergence analysis of a class of SMC methods where the
times at which resampling occurs are computed online using criteria such as the
effective sample size. This is a popular approach amongst practitioners but
there are very few convergence results available for these methods. By
combining semigroup techniques with an original coupling argument, we obtain
functional central limit theorems and uniform exponential concentration
estimates for these algorithms."@2012
Ajay Jasra@http://arxiv.org/abs/1203.0464v1@On adaptive resampling strategies for sequential Monte Carlo methods@"Sequential Monte Carlo (SMC) methods are a class of techniques to sample
approximately from any sequence of probability distributions using a
combination of importance sampling and resampling steps. This paper is
concerned with the convergence analysis of a class of SMC methods where the
times at which resampling occurs are computed online using criteria such as the
effective sample size. This is a popular approach amongst practitioners but
there are very few convergence results available for these methods. By
combining semigroup techniques with an original coupling argument, we obtain
functional central limit theorems and uniform exponential concentration
estimates for these algorithms."@2012
Fadoua Balabdaoui@http://arxiv.org/abs/1203.0828v2@Chernoff's density is log-concave@"We show that the density of $Z=\mathop {\operatorname {argmax}}\{W(t)-t^2\}$,
sometimes known as Chernoff's density, is log-concave. We conjecture that
Chernoff's density is strongly log-concave or ""super-Gaussian"", and provide
evidence in support of the conjecture."@2012
Jon A. Wellner@http://arxiv.org/abs/1203.0828v2@Chernoff's density is log-concave@"We show that the density of $Z=\mathop {\operatorname {argmax}}\{W(t)-t^2\}$,
sometimes known as Chernoff's density, is log-concave. We conjecture that
Chernoff's density is strongly log-concave or ""super-Gaussian"", and provide
evidence in support of the conjecture."@2012
Aharon Birnbaum@http://arxiv.org/abs/1203.0967v1@Minimax bounds for sparse PCA with noisy high-dimensional data@"We study the problem of estimating the leading eigenvectors of a
high-dimensional population covariance matrix based on independent Gaussian
observations. We establish a lower bound on the minimax risk of estimators
under the $l_2$ loss, in the joint limit as dimension and sample size increase
to infinity, under various models of sparsity for the population eigenvectors.
The lower bound on the risk points to the existence of different regimes of
sparsity of the eigenvectors. We also propose a new method for estimating the
eigenvectors by a two-stage coordinate selection scheme."@2012
Iain M. Johnstone@http://arxiv.org/abs/1203.0967v1@Minimax bounds for sparse PCA with noisy high-dimensional data@"We study the problem of estimating the leading eigenvectors of a
high-dimensional population covariance matrix based on independent Gaussian
observations. We establish a lower bound on the minimax risk of estimators
under the $l_2$ loss, in the joint limit as dimension and sample size increase
to infinity, under various models of sparsity for the population eigenvectors.
The lower bound on the risk points to the existence of different regimes of
sparsity of the eigenvectors. We also propose a new method for estimating the
eigenvectors by a two-stage coordinate selection scheme."@2012
Boaz Nadler@http://arxiv.org/abs/1203.0967v1@Minimax bounds for sparse PCA with noisy high-dimensional data@"We study the problem of estimating the leading eigenvectors of a
high-dimensional population covariance matrix based on independent Gaussian
observations. We establish a lower bound on the minimax risk of estimators
under the $l_2$ loss, in the joint limit as dimension and sample size increase
to infinity, under various models of sparsity for the population eigenvectors.
The lower bound on the risk points to the existence of different regimes of
sparsity of the eigenvectors. We also propose a new method for estimating the
eigenvectors by a two-stage coordinate selection scheme."@2012
Debashis Paul@http://arxiv.org/abs/1203.0967v1@Minimax bounds for sparse PCA with noisy high-dimensional data@"We study the problem of estimating the leading eigenvectors of a
high-dimensional population covariance matrix based on independent Gaussian
observations. We establish a lower bound on the minimax risk of estimators
under the $l_2$ loss, in the joint limit as dimension and sample size increase
to infinity, under various models of sparsity for the population eigenvectors.
The lower bound on the risk points to the existence of different regimes of
sparsity of the eigenvectors. We also propose a new method for estimating the
eigenvectors by a two-stage coordinate selection scheme."@2012
Davide Farchione@http://arxiv.org/abs/1203.1093v2@Confidence intervals in regression centred on the SCAD estimator@"Consider a linear regression model. Fan and Li (2001) describe the smoothly
clipped absolute deviation (SCAD) point estimator of the regression parameter
vector. To gain insight into the properties of this estimator, they consider an
orthonormal design matrix and focus on the estimation of a specified component
of this vector. They show that the SCAD point estimator has three attractive
properties. We answer the question: To what extent can an interval estimator,
centred on the SCAD estimator, have similar attractive properties?"@2012
Paul Kabaila@http://arxiv.org/abs/1203.1093v2@Confidence intervals in regression centred on the SCAD estimator@"Consider a linear regression model. Fan and Li (2001) describe the smoothly
clipped absolute deviation (SCAD) point estimator of the regression parameter
vector. To gain insight into the properties of this estimator, they consider an
orthonormal design matrix and focus on the estimation of a specified component
of this vector. They show that the SCAD point estimator has three attractive
properties. We answer the question: To what extent can an interval estimator,
centred on the SCAD estimator, have similar attractive properties?"@2012
Eric Beutner@http://arxiv.org/abs/1203.1112v4@"Continuous mapping approach to the asymptotics of $U$- and
  $V$-statistics"@"We derive a new representation for $U$- and $V$-statistics. Using this
representation, the asymptotic distribution of $U$- and $V$-statistics can be
derived by a direct application of the Continuous Mapping theorem. That novel
approach not only encompasses most of the results on the asymptotic
distribution known in literature, but also allows for the first time a unifying
treatment of non-degenerate and degenerate $U$- and $V$-statistics. Moreover,
it yields a new and powerful tool to derive the asymptotic distribution of very
general $U$- and $V$-statistics based on long-memory sequences. This will be
exemplified by several astonishing examples. In particular, we shall present
examples where weak convergence of $U$- or $V$-statistics occurs at the rate
$a_n^3$ and $a_n^4$, respectively, when $a_n$ is the rate of weak convergence
of the empirical process. We also introduce the notion of asymptotic (non-)
degeneracy which often appears in the presence of long-memory sequences."@2012
Henryk Zähle@http://arxiv.org/abs/1203.1112v4@"Continuous mapping approach to the asymptotics of $U$- and
  $V$-statistics"@"We derive a new representation for $U$- and $V$-statistics. Using this
representation, the asymptotic distribution of $U$- and $V$-statistics can be
derived by a direct application of the Continuous Mapping theorem. That novel
approach not only encompasses most of the results on the asymptotic
distribution known in literature, but also allows for the first time a unifying
treatment of non-degenerate and degenerate $U$- and $V$-statistics. Moreover,
it yields a new and powerful tool to derive the asymptotic distribution of very
general $U$- and $V$-statistics based on long-memory sequences. This will be
exemplified by several astonishing examples. In particular, we shall present
examples where weak convergence of $U$- or $V$-statistics occurs at the rate
$a_n^3$ and $a_n^4$, respectively, when $a_n$ is the rate of weak convergence
of the empirical process. We also introduce the notion of asymptotic (non-)
degeneracy which often appears in the presence of long-memory sequences."@2012
Jean-David Fermanian@http://arxiv.org/abs/1203.1243v3@An asymptotic total variation test for copulas@"We propose a new goodness-of-fit test for copulas, based on empirical copula
processes and their nonparametric bootstrap counterparts. The standard
Kolmogorov-Smirnov type test for copulas that takes the supremum of the
empirical copula process indexed by half spaces is extended by test statistics
based on the supremum of the empirical copula process indexed by partitions of
Ln rectangles with Ln slowly tending to infinity. Although the underlying
empirical process does not converge, it is proved that the p-values of our new
test statistic can be consistently estimated by the bootstrap. Simulations
confirm that the power of the new procedure is higher than the power of the
standard Kolmogorov-Smirnov test for copulas."@2012
Dragan Radulovic@http://arxiv.org/abs/1203.1243v3@An asymptotic total variation test for copulas@"We propose a new goodness-of-fit test for copulas, based on empirical copula
processes and their nonparametric bootstrap counterparts. The standard
Kolmogorov-Smirnov type test for copulas that takes the supremum of the
empirical copula process indexed by half spaces is extended by test statistics
based on the supremum of the empirical copula process indexed by partitions of
Ln rectangles with Ln slowly tending to infinity. Although the underlying
empirical process does not converge, it is proved that the p-values of our new
test statistic can be consistently estimated by the bootstrap. Simulations
confirm that the power of the new procedure is higher than the power of the
standard Kolmogorov-Smirnov test for copulas."@2012
Marten Wegkamp@http://arxiv.org/abs/1203.1243v3@An asymptotic total variation test for copulas@"We propose a new goodness-of-fit test for copulas, based on empirical copula
processes and their nonparametric bootstrap counterparts. The standard
Kolmogorov-Smirnov type test for copulas that takes the supremum of the
empirical copula process indexed by half spaces is extended by test statistics
based on the supremum of the empirical copula process indexed by partitions of
Ln rectangles with Ln slowly tending to infinity. Although the underlying
empirical process does not converge, it is proved that the p-values of our new
test statistic can be consistently estimated by the bootstrap. Simulations
confirm that the power of the new procedure is higher than the power of the
standard Kolmogorov-Smirnov test for copulas."@2012
Jinyuan Chang@http://arxiv.org/abs/1203.2004v1@On the approximate maximum likelihood estimation for diffusion processes@"The transition density of a diffusion process does not admit an explicit
expression in general, which prevents the full maximum likelihood estimation
(MLE) based on discretely observed sample paths. A\""{\i}t-Sahalia [J. Finance
54 (1999) 1361--1395; Econometrica 70 (2002) 223--262] proposed asymptotic
expansions to the transition densities of diffusion processes, which lead to an
approximate maximum likelihood estimation (AMLE) for parameters. Built on
A\""{\i}t-Sahalia's [Econometrica 70 (2002) 223--262; Ann. Statist. 36 (2008)
906--937] proposal and analysis on the AMLE, we establish the consistency and
convergence rate of the AMLE, which reveal the roles played by the number of
terms used in the asymptotic density expansions and the sampling interval
between successive observations. We find conditions under which the AMLE has
the same asymptotic distribution as that of the full MLE. A first order
approximation to the Fisher information matrix is proposed."@2012
Song Xi Chen@http://arxiv.org/abs/1203.2004v1@On the approximate maximum likelihood estimation for diffusion processes@"The transition density of a diffusion process does not admit an explicit
expression in general, which prevents the full maximum likelihood estimation
(MLE) based on discretely observed sample paths. A\""{\i}t-Sahalia [J. Finance
54 (1999) 1361--1395; Econometrica 70 (2002) 223--262] proposed asymptotic
expansions to the transition densities of diffusion processes, which lead to an
approximate maximum likelihood estimation (AMLE) for parameters. Built on
A\""{\i}t-Sahalia's [Econometrica 70 (2002) 223--262; Ann. Statist. 36 (2008)
906--937] proposal and analysis on the AMLE, we establish the consistency and
convergence rate of the AMLE, which reveal the roles played by the number of
terms used in the asymptotic density expansions and the sampling interval
between successive observations. We find conditions under which the AMLE has
the same asymptotic distribution as that of the full MLE. A first order
approximation to the Fisher information matrix is proposed."@2012
Claire Lacour@http://arxiv.org/abs/1203.2008v2@Goodness-of-fit test for noisy directional data@"We consider spherical data $X_i$ noised by a random rotation
$\varepsilon_i\in$ SO(3) so that only the sample $Z_i=\varepsilon_iX_i$,
$i=1,\dots, N$ is observed. We define a nonparametric test procedure to
distinguish $H_0:$ ''the density $f$ of $X_i$ is the uniform density $f_0$ on
the sphere'' and $H_1:$ ''$\|f-f_0\|_2^2\geq \C\psi_N$ and $f$ is in a Sobolev
space with smoothness $s$''. For a noise density $f_\varepsilon$ with
smoothness index $\nu$, we show that an adaptive procedure (i.e. $s$ is not
assumed to be known) cannot have a faster rate of separation than
$\psi_N^{ad}(s)=(N/\sqrt{\log\log(N)})^{-2s/(2s+2\nu+1)}$ and we provide a
procedure which reaches this rate. We also deal with the case of super smooth
noise. We illustrate the theory by implementing our test procedure for various
kinds of noise on SO(3) and by comparing it to other procedures. Applications
to real data in astrophysics and paleomagnetism are provided."@2012
Thanh Mai Pham Ngoc@http://arxiv.org/abs/1203.2008v2@Goodness-of-fit test for noisy directional data@"We consider spherical data $X_i$ noised by a random rotation
$\varepsilon_i\in$ SO(3) so that only the sample $Z_i=\varepsilon_iX_i$,
$i=1,\dots, N$ is observed. We define a nonparametric test procedure to
distinguish $H_0:$ ''the density $f$ of $X_i$ is the uniform density $f_0$ on
the sphere'' and $H_1:$ ''$\|f-f_0\|_2^2\geq \C\psi_N$ and $f$ is in a Sobolev
space with smoothness $s$''. For a noise density $f_\varepsilon$ with
smoothness index $\nu$, we show that an adaptive procedure (i.e. $s$ is not
assumed to be known) cannot have a faster rate of separation than
$\psi_N^{ad}(s)=(N/\sqrt{\log\log(N)})^{-2s/(2s+2\nu+1)}$ and we provide a
procedure which reaches this rate. We also deal with the case of super smooth
noise. We illustrate the theory by implementing our test procedure for various
kinds of noise on SO(3) and by comparing it to other procedures. Applications
to real data in astrophysics and paleomagnetism are provided."@2012
Juan Lucas Bali@http://arxiv.org/abs/1203.2027v1@Robust functional principal components: A projection-pursuit approach@"In many situations, data are recorded over a period of time and may be
regarded as realizations of a stochastic process. In this paper, robust
estimators for the principal components are considered by adapting the
projection pursuit approach to the functional data setting. Our approach
combines robust projection-pursuit with different smoothing methods.
Consistency of the estimators are shown under mild assumptions. The performance
of the classical and robust procedures are compared in a simulation study under
different contamination schemes."@2012
Graciela Boente@http://arxiv.org/abs/1203.2027v1@Robust functional principal components: A projection-pursuit approach@"In many situations, data are recorded over a period of time and may be
regarded as realizations of a stochastic process. In this paper, robust
estimators for the principal components are considered by adapting the
projection pursuit approach to the functional data setting. Our approach
combines robust projection-pursuit with different smoothing methods.
Consistency of the estimators are shown under mild assumptions. The performance
of the classical and robust procedures are compared in a simulation study under
different contamination schemes."@2012
David E. Tyler@http://arxiv.org/abs/1203.2027v1@Robust functional principal components: A projection-pursuit approach@"In many situations, data are recorded over a period of time and may be
regarded as realizations of a stochastic process. In this paper, robust
estimators for the principal components are considered by adapting the
projection pursuit approach to the functional data setting. Our approach
combines robust projection-pursuit with different smoothing methods.
Consistency of the estimators are shown under mild assumptions. The performance
of the classical and robust procedures are compared in a simulation study under
different contamination schemes."@2012
Jane-Ling Wang@http://arxiv.org/abs/1203.2027v1@Robust functional principal components: A projection-pursuit approach@"In many situations, data are recorded over a period of time and may be
regarded as realizations of a stochastic process. In this paper, robust
estimators for the principal components are considered by adapting the
projection pursuit approach to the functional data setting. Our approach
combines robust projection-pursuit with different smoothing methods.
Consistency of the estimators are shown under mild assumptions. The performance
of the classical and robust procedures are compared in a simulation study under
different contamination schemes."@2012
Evarist Giné@http://arxiv.org/abs/1203.2043v1@"Rates of contraction for posterior distributions in
  $\bolds{L^r}$-metrics, $\bolds{1\le r\le\infty}$"@"The frequentist behavior of nonparametric Bayes estimates, more specifically,
rates of contraction of the posterior distributions to shrinking $L^r$-norm
neighborhoods, $1\le r\le\infty$, of the unknown parameter, are studied. A
theorem for nonparametric density estimation is proved under general
approximation-theoretic assumptions on the prior. The result is applied to a
variety of common examples, including Gaussian process, wavelet series, normal
mixture and histogram priors. The rates of contraction are minimax-optimal for
$1\le r\le2$, but deteriorate as $r$ increases beyond 2. In the case of
Gaussian nonparametric regression a Gaussian prior is devised for which the
posterior contracts at the optimal rate in all $L^r$-norms, $1\le r\le\infty$."@2012
Richard Nickl@http://arxiv.org/abs/1203.2043v1@"Rates of contraction for posterior distributions in
  $\bolds{L^r}$-metrics, $\bolds{1\le r\le\infty}$"@"The frequentist behavior of nonparametric Bayes estimates, more specifically,
rates of contraction of the posterior distributions to shrinking $L^r$-norm
neighborhoods, $1\le r\le\infty$, of the unknown parameter, are studied. A
theorem for nonparametric density estimation is proved under general
approximation-theoretic assumptions on the prior. The result is applied to a
variety of common examples, including Gaussian process, wavelet series, normal
mixture and histogram priors. The rates of contraction are minimax-optimal for
$1\le r\le2$, but deteriorate as $r$ increases beyond 2. In the case of
Gaussian nonparametric regression a Gaussian prior is devised for which the
posterior contracts at the optimal rate in all $L^r$-norms, $1\le r\le\infty$."@2012
Alexander Aue@http://arxiv.org/abs/1203.2087v1@On image segmentation using information theoretic criteria@"Image segmentation is a long-studied and important problem in image
processing. Different solutions have been proposed, many of which follow the
information theoretic paradigm. While these information theoretic segmentation
methods often produce excellent empirical results, their theoretical properties
are still largely unknown. The main goal of this paper is to conduct a rigorous
theoretical study into the statistical consistency properties of such methods.
To be more specific, this paper investigates if these methods can accurately
recover the true number of segments together with their true boundaries in the
image as the number of pixels tends to infinity. Our theoretical results show
that both the Bayesian information criterion (BIC) and the minimum description
length (MDL) principle can be applied to derive statistically consistent
segmentation methods, while the same is not true for the Akaike information
criterion (AIC). Numerical experiments were conducted to illustrate and support
our theoretical findings."@2012
Thomas C. M. Lee@http://arxiv.org/abs/1203.2087v1@On image segmentation using information theoretic criteria@"Image segmentation is a long-studied and important problem in image
processing. Different solutions have been proposed, many of which follow the
information theoretic paradigm. While these information theoretic segmentation
methods often produce excellent empirical results, their theoretical properties
are still largely unknown. The main goal of this paper is to conduct a rigorous
theoretical study into the statistical consistency properties of such methods.
To be more specific, this paper investigates if these methods can accurately
recover the true number of segments together with their true boundaries in the
image as the number of pixels tends to infinity. Our theoretical results show
that both the Bayesian information criterion (BIC) and the minimum description
length (MDL) principle can be applied to derive statistically consistent
segmentation methods, while the same is not true for the Akaike information
criterion (AIC). Numerical experiments were conducted to illustrate and support
our theoretical findings."@2012
Ben Haaland@http://arxiv.org/abs/1203.2433v1@Accurate emulators for large-scale computer experiments@"Large-scale computer experiments are becoming increasingly important in
science. A multi-step procedure is introduced to statisticians for modeling
such experiments, which builds an accurate interpolator in multiple steps. In
practice, the procedure shows substantial improvements in overall accuracy, but
its theoretical properties are not well established. We introduce the terms
nominal and numeric error and decompose the overall error of an interpolator
into nominal and numeric portions. Bounds on the numeric and nominal error are
developed to show theoretically that substantial gains in overall accuracy can
be attained with the multi-step approach."@2012
Peter Z. G. Qian@http://arxiv.org/abs/1203.2433v1@Accurate emulators for large-scale computer experiments@"Large-scale computer experiments are becoming increasingly important in
science. A multi-step procedure is introduced to statisticians for modeling
such experiments, which builds an accurate interpolator in multiple steps. In
practice, the procedure shows substantial improvements in overall accuracy, but
its theoretical properties are not well established. We introduce the terms
nominal and numeric error and decompose the overall error of an interpolator
into nominal and numeric portions. Bounds on the numeric and nominal error are
developed to show theoretically that substantial gains in overall accuracy can
be attained with the multi-step approach."@2012
Ying Ding@http://arxiv.org/abs/1203.2470v1@"A sieve M-theorem for bundled parameters in semiparametric models, with
  application to the efficient estimation in a linear model for censored data"@"In many semiparametric models that are parameterized by two types of
parameters---a Euclidean parameter of interest and an infinite-dimensional
nuisance parameter---the two parameters are bundled together, that is, the
nuisance parameter is an unknown function that contains the parameter of
interest as part of its argument. For example, in a linear regression model for
censored survival data, the unspecified error distribution function involves
the regression coefficients. Motivated by developing an efficient estimating
method for the regression parameters, we propose a general sieve M-theorem for
bundled parameters and apply the theorem to deriving the asymptotic theory for
the sieve maximum likelihood estimation in the linear regression model for
censored survival data. The numerical implementation of the proposed estimating
method can be achieved through the conventional gradient-based search
algorithms such as the Newton--Raphson algorithm. We show that the proposed
estimator is consistent and asymptotically normal and achieves the
semiparametric efficiency bound. Simulation studies demonstrate that the
proposed method performs well in practical settings and yields more efficient
estimates than existing estimating equation based methods. Illustration with a
real data example is also provided."@2012
Bin Nan@http://arxiv.org/abs/1203.2470v1@"A sieve M-theorem for bundled parameters in semiparametric models, with
  application to the efficient estimation in a linear model for censored data"@"In many semiparametric models that are parameterized by two types of
parameters---a Euclidean parameter of interest and an infinite-dimensional
nuisance parameter---the two parameters are bundled together, that is, the
nuisance parameter is an unknown function that contains the parameter of
interest as part of its argument. For example, in a linear regression model for
censored survival data, the unspecified error distribution function involves
the regression coefficients. Motivated by developing an efficient estimating
method for the regression parameters, we propose a general sieve M-theorem for
bundled parameters and apply the theorem to deriving the asymptotic theory for
the sieve maximum likelihood estimation in the linear regression model for
censored survival data. The numerical implementation of the proposed estimating
method can be achieved through the conventional gradient-based search
algorithms such as the Newton--Raphson algorithm. We show that the proposed
estimator is consistent and asymptotically normal and achieves the
semiparametric efficiency bound. Simulation studies demonstrate that the
proposed method performs well in practical settings and yields more efficient
estimates than existing estimating equation based methods. Illustration with a
real data example is also provided."@2012
Richard A. Davis@http://arxiv.org/abs/1203.2496v1@Unit roots in moving averages beyond first order@"The asymptotic theory of various estimators based on Gaussian likelihood has
been developed for the unit root and near unit root cases of a first-order
moving average model. Previous studies of the MA(1) unit root problem rely on
the special autocovariance structure of the MA(1) process, in which case, the
eigenvalues and eigenvectors of the covariance matrix of the data vector have
known analytical forms. In this paper, we take a different approach to first
consider the joint likelihood by including an augmented initial value as a
parameter and then recover the exact likelihood by integrating out the initial
value. This approach by-passes the difficulty of computing an explicit
decomposition of the covariance matrix and can be used to study unit root
behavior in moving averages beyond first order. The asymptotics of the
generalized likelihood ratio (GLR) statistic for testing unit roots are also
studied. The GLR test has operating characteristics that are competitive with
the locally best invariant unbiased (LBIU) test of Tanaka for some local
alternatives and dominates for all other alternatives."@2012
Li Song@http://arxiv.org/abs/1203.2496v1@Unit roots in moving averages beyond first order@"The asymptotic theory of various estimators based on Gaussian likelihood has
been developed for the unit root and near unit root cases of a first-order
moving average model. Previous studies of the MA(1) unit root problem rely on
the special autocovariance structure of the MA(1) process, in which case, the
eigenvalues and eigenvectors of the covariance matrix of the data vector have
known analytical forms. In this paper, we take a different approach to first
consider the joint likelihood by including an augmented initial value as a
parameter and then recover the exact likelihood by integrating out the initial
value. This approach by-passes the difficulty of computing an explicit
decomposition of the covariance matrix and can be used to study unit root
behavior in moving averages beyond first order. The asymptotics of the
generalized likelihood ratio (GLR) statistic for testing unit roots are also
studied. The GLR test has operating characteristics that are competitive with
the locally best invariant unbiased (LBIU) test of Tanaka for some local
alternatives and dominates for all other alternatives."@2012
Javier Hualde@http://arxiv.org/abs/1203.2750v1@"Gaussian pseudo-maximum likelihood estimation of fractional time series
  models"@"We consider the estimation of parametric fractional time series models in
which not only is the memory parameter unknown, but one may not know whether it
lies in the stationary/invertible region or the nonstationary or noninvertible
regions. In these circumstances, a proof of consistency (which is a
prerequisite for proving asymptotic normality) can be difficult owing to
nonuniform convergence of the objective function over a large admissible
parameter space. In particular, this is the case for the conditional sum of
squares estimate, which can be expected to be asymptotically efficient under
Gaussianity. Without the latter assumption, we establish consistency and
asymptotic normality for this estimate in case of a quite general univariate
model. For a multivariate model, we establish asymptotic normality of a
one-step estimate based on an initial $\sqrt{n}$-consistent estimate."@2012
Peter M. Robinson@http://arxiv.org/abs/1203.2750v1@"Gaussian pseudo-maximum likelihood estimation of fractional time series
  models"@"We consider the estimation of parametric fractional time series models in
which not only is the memory parameter unknown, but one may not know whether it
lies in the stationary/invertible region or the nonstationary or noninvertible
regions. In these circumstances, a proof of consistency (which is a
prerequisite for proving asymptotic normality) can be difficult owing to
nonuniform convergence of the objective function over a large admissible
parameter space. In particular, this is the case for the conditional sum of
squares estimate, which can be expected to be asymptotically efficient under
Gaussianity. Without the latter assumption, we establish consistency and
asymptotic normality for this estimate in case of a quite general univariate
model. For a multivariate model, we establish asymptotic normality of a
one-step estimate based on an initial $\sqrt{n}$-consistent estimate."@2012
Bing Li@http://arxiv.org/abs/1203.2790v1@"Principal support vector machines for linear and nonlinear sufficient
  dimension reduction"@"We introduce a principal support vector machine (PSVM) approach that can be
used for both linear and nonlinear sufficient dimension reduction. The basic
idea is to divide the response variables into slices and use a modified form of
support vector machine to find the optimal hyperplanes that separate them.
These optimal hyperplanes are then aligned by the principal components of their
normal vectors. It is proved that the aligned normal vectors provide an
unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the
sufficient dimension reduction space. The method is then generalized to
nonlinear sufficient dimension reduction using the reproducing kernel Hilbert
space. In that context, the aligned normal vectors become functions and it is
proved that they are unbiased in the sense that they are functions of the true
nonlinear sufficient predictors. We compare PSVM with other sufficient
dimension reduction methods by simulation and in real data analysis, and
through both comparisons firmly establish its practical advantages."@2012
Andreas Artemiou@http://arxiv.org/abs/1203.2790v1@"Principal support vector machines for linear and nonlinear sufficient
  dimension reduction"@"We introduce a principal support vector machine (PSVM) approach that can be
used for both linear and nonlinear sufficient dimension reduction. The basic
idea is to divide the response variables into slices and use a modified form of
support vector machine to find the optimal hyperplanes that separate them.
These optimal hyperplanes are then aligned by the principal components of their
normal vectors. It is proved that the aligned normal vectors provide an
unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the
sufficient dimension reduction space. The method is then generalized to
nonlinear sufficient dimension reduction using the reproducing kernel Hilbert
space. In that context, the aligned normal vectors become functions and it is
proved that they are unbiased in the sense that they are functions of the true
nonlinear sufficient predictors. We compare PSVM with other sufficient
dimension reduction methods by simulation and in real data analysis, and
through both comparisons firmly establish its practical advantages."@2012
Lexin Li@http://arxiv.org/abs/1203.2790v1@"Principal support vector machines for linear and nonlinear sufficient
  dimension reduction"@"We introduce a principal support vector machine (PSVM) approach that can be
used for both linear and nonlinear sufficient dimension reduction. The basic
idea is to divide the response variables into slices and use a modified form of
support vector machine to find the optimal hyperplanes that separate them.
These optimal hyperplanes are then aligned by the principal components of their
normal vectors. It is proved that the aligned normal vectors provide an
unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the
sufficient dimension reduction space. The method is then generalized to
nonlinear sufficient dimension reduction using the reproducing kernel Hilbert
space. In that context, the aligned normal vectors become functions and it is
proved that they are unbiased in the sense that they are functions of the true
nonlinear sufficient predictors. We compare PSVM with other sufficient
dimension reduction methods by simulation and in real data analysis, and
through both comparisons firmly establish its practical advantages."@2012
Sébastien Da Veiga@http://arxiv.org/abs/1203.2899v1@Efficient Estimation of Sensitivity Indices@"In this paper we address the problem of efficient estimation of Sobol
sensitivy indices. First, we focus on general functional integrals of
conditional moments of the form $\E(\psi(\E(\varphi(Y)|X)))$ where $(X,Y)$ is a
random vector with joint density $f$ and $\psi$ and $\varphi$ are functions
that are differentiable enough. In particular, we show that asymptotical
efficient estimation of this functional boils down to the estimation of crossed
quadratic functionals. An efficient estimate of first-order sensitivity indices
is then derived as a special case. We investigate its properties on several
analytical functions and illustrate its interest on a reservoir engineering
case."@2012
Fabrice Gamboa@http://arxiv.org/abs/1203.2899v1@Efficient Estimation of Sensitivity Indices@"In this paper we address the problem of efficient estimation of Sobol
sensitivy indices. First, we focus on general functional integrals of
conditional moments of the form $\E(\psi(\E(\varphi(Y)|X)))$ where $(X,Y)$ is a
random vector with joint density $f$ and $\psi$ and $\varphi$ are functions
that are differentiable enough. In particular, we show that asymptotical
efficient estimation of this functional boils down to the estimation of crossed
quadratic functionals. An efficient estimate of first-order sensitivity indices
is then derived as a special case. We investigate its properties on several
analytical functions and illustrate its interest on a reservoir engineering
case."@2012
Joseph S. Koopmeiners@http://arxiv.org/abs/1203.3014v1@"Asymptotic properties of the sequential empirical ROC, PPV and NPV
  curves under case-control sampling"@"The receiver operating characteristic (ROC) curve, the positive predictive
value (PPV) curve and the negative predictive value (NPV) curve are three
measures of performance for a continuous diagnostic biomarker. The ROC, PPV and
NPV curves are often estimated empirically to avoid assumptions about the
distributional form of the biomarkers. Recently, there has been a push to
incorporate group sequential methods into the design of diagnostic biomarker
studies. A thorough understanding of the asymptotic properties of the
sequential empirical ROC, PPV and NPV curves will provide more flexibility when
designing group sequential diagnostic biomarker studies. In this paper, we
derive asymptotic theory for the sequential empirical ROC, PPV and NPV curves
under case-control sampling using sequential empirical process theory. We show
that the sequential empirical ROC, PPV and NPV curves converge to the sum of
independent Kiefer processes and show how these results can be used to derive
asymptotic results for summaries of the sequential empirical ROC, PPV and NPV
curves."@2012
Ziding Feng@http://arxiv.org/abs/1203.3014v1@"Asymptotic properties of the sequential empirical ROC, PPV and NPV
  curves under case-control sampling"@"The receiver operating characteristic (ROC) curve, the positive predictive
value (PPV) curve and the negative predictive value (NPV) curve are three
measures of performance for a continuous diagnostic biomarker. The ROC, PPV and
NPV curves are often estimated empirically to avoid assumptions about the
distributional form of the biomarkers. Recently, there has been a push to
incorporate group sequential methods into the design of diagnostic biomarker
studies. A thorough understanding of the asymptotic properties of the
sequential empirical ROC, PPV and NPV curves will provide more flexibility when
designing group sequential diagnostic biomarker studies. In this paper, we
derive asymptotic theory for the sequential empirical ROC, PPV and NPV curves
under case-control sampling using sequential empirical process theory. We show
that the sequential empirical ROC, PPV and NPV curves converge to the sum of
independent Kiefer processes and show how these results can be used to derive
asymptotic results for summaries of the sequential empirical ROC, PPV and NPV
curves."@2012
G. Fort@http://arxiv.org/abs/1203.3036v1@"Convergence of adaptive and interacting Markov chain Monte Carlo
  algorithms"@"Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been
recently introduced in the literature. These novel simulation algorithms are
designed to increase the simulation efficiency to sample complex distributions.
Motivated by some recently introduced algorithms (such as the adaptive
Metropolis algorithm and the interacting tempering algorithm), we develop a
general methodological and theoretical framework to establish both the
convergence of the marginal distribution and a strong law of large numbers.
This framework weakens the conditions introduced in the pioneering paper by
Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the
case when the target distribution $\pi$ is sampled by using Markov transition
kernels with a stationary distribution that differs from $\pi$."@2012
E. Moulines@http://arxiv.org/abs/1203.3036v1@"Convergence of adaptive and interacting Markov chain Monte Carlo
  algorithms"@"Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been
recently introduced in the literature. These novel simulation algorithms are
designed to increase the simulation efficiency to sample complex distributions.
Motivated by some recently introduced algorithms (such as the adaptive
Metropolis algorithm and the interacting tempering algorithm), we develop a
general methodological and theoretical framework to establish both the
convergence of the marginal distribution and a strong law of large numbers.
This framework weakens the conditions introduced in the pioneering paper by
Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the
case when the target distribution $\pi$ is sampled by using Markov transition
kernels with a stationary distribution that differs from $\pi$."@2012
P. Priouret@http://arxiv.org/abs/1203.3036v1@"Convergence of adaptive and interacting Markov chain Monte Carlo
  algorithms"@"Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been
recently introduced in the literature. These novel simulation algorithms are
designed to increase the simulation efficiency to sample complex distributions.
Motivated by some recently introduced algorithms (such as the adaptive
Metropolis algorithm and the interacting tempering algorithm), we develop a
general methodological and theoretical framework to establish both the
convergence of the marginal distribution and a strong law of large numbers.
This framework weakens the conditions introduced in the pioneering paper by
Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the
case when the target distribution $\pi$ is sampled by using Markov transition
kernels with a stationary distribution that differs from $\pi$."@2012
Armin Schwartzman@http://arxiv.org/abs/1203.3063v1@Multiple testing of local maxima for detection of peaks in 1D@"A topological multiple testing scheme for one-dimensional domains is proposed
where, rather than testing every spatial or temporal location for the presence
of a signal, tests are performed only at the local maxima of the smoothed
observed sequence. Assuming unimodal true peaks with finite support and
Gaussian stationary ergodic noise, it is shown that the algorithm with
Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control
of the family wise error rate and false discovery rate, and is power
consistent, as the search space and the signal strength get large, where the
search space may grow exponentially faster than the signal strength.
Simulations show that error levels are maintained for nonasymptotic conditions,
and that power is maximized when the smoothing kernel is close in shape and
bandwidth to the signal peaks, akin to the matched filter theorem in signal
processing. The methods are illustrated in an analysis of electrical recordings
of neuronal cell activity."@2012
Yulia Gavrilov@http://arxiv.org/abs/1203.3063v1@Multiple testing of local maxima for detection of peaks in 1D@"A topological multiple testing scheme for one-dimensional domains is proposed
where, rather than testing every spatial or temporal location for the presence
of a signal, tests are performed only at the local maxima of the smoothed
observed sequence. Assuming unimodal true peaks with finite support and
Gaussian stationary ergodic noise, it is shown that the algorithm with
Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control
of the family wise error rate and false discovery rate, and is power
consistent, as the search space and the signal strength get large, where the
search space may grow exponentially faster than the signal strength.
Simulations show that error levels are maintained for nonasymptotic conditions,
and that power is maximized when the smoothing kernel is close in shape and
bandwidth to the signal peaks, akin to the matched filter theorem in signal
processing. The methods are illustrated in an analysis of electrical recordings
of neuronal cell activity."@2012
Robert J. Adler@http://arxiv.org/abs/1203.3063v1@Multiple testing of local maxima for detection of peaks in 1D@"A topological multiple testing scheme for one-dimensional domains is proposed
where, rather than testing every spatial or temporal location for the presence
of a signal, tests are performed only at the local maxima of the smoothed
observed sequence. Assuming unimodal true peaks with finite support and
Gaussian stationary ergodic noise, it is shown that the algorithm with
Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control
of the family wise error rate and false discovery rate, and is power
consistent, as the search space and the signal strength get large, where the
search space may grow exponentially faster than the signal strength.
Simulations show that error levels are maintained for nonasymptotic conditions,
and that power is maximized when the smoothing kernel is close in shape and
bandwidth to the signal peaks, akin to the matched filter theorem in signal
processing. The methods are illustrated in an analysis of electrical recordings
of neuronal cell activity."@2012
John Kolassa@http://arxiv.org/abs/1203.3106v1@"Saddlepoint approximations for likelihood ratio like statistics with
  applications to permutation tests"@"We obtain two theorems extending the use of a saddlepoint approximation to
multiparameter problems for likelihood ratio-like statistics which allow their
use in permutation and rank tests and could be used in bootstrap
approximations. In the first, we show that in some cases when no density
exists, the integral of the formal saddlepoint density over the set
corresponding to large values of the likelihood ratio-like statistic
approximates the true probability with relative error of order $1/n$. In the
second, we give multivariate generalizations of the Lugannani--Rice and
Barndorff-Nielsen or $r^*$ formulas for the approximations. These theorems are
applied to obtain permutation tests based on the likelihood ratio-like
statistics for the $k$ sample and the multivariate two-sample cases. Numerical
examples are given to illustrate the high degree of accuracy, and these
statistics are compared to the classical statistics in both cases."@2012
John Robinson@http://arxiv.org/abs/1203.3106v1@"Saddlepoint approximations for likelihood ratio like statistics with
  applications to permutation tests"@"We obtain two theorems extending the use of a saddlepoint approximation to
multiparameter problems for likelihood ratio-like statistics which allow their
use in permutation and rank tests and could be used in bootstrap
approximations. In the first, we show that in some cases when no density
exists, the integral of the formal saddlepoint density over the set
corresponding to large values of the likelihood ratio-like statistic
approximates the true probability with relative error of order $1/n$. In the
second, we give multivariate generalizations of the Lugannani--Rice and
Barndorff-Nielsen or $r^*$ formulas for the approximations. These theorems are
applied to obtain permutation tests based on the likelihood ratio-like
statistics for the $k$ sample and the multivariate two-sample cases. Numerical
examples are given to illustrate the high degree of accuracy, and these
statistics are compared to the classical statistics in both cases."@2012
Xiangrong Yin@http://arxiv.org/abs/1203.3313v1@"Sufficient dimension reduction based on an ensemble of minimum average
  variance estimators"@"We introduce a class of dimension reduction estimators based on an ensemble
of the minimum average variance estimates of functions that characterize the
central subspace, such as the characteristic functions, the Box--Cox
transformations and wavelet basis. The ensemble estimators exhaustively
estimate the central subspace without imposing restrictive conditions on the
predictors, and have the same convergence rate as the minimum average variance
estimates. They are flexible and easy to implement, and allow repeated use of
the available sample, which enhances accuracy. They are applicable to both
univariate and multivariate responses in a unified form. We establish the
consistency and convergence rate of these estimators, and the consistency of a
cross validation criterion for order determination. We compare the ensemble
estimators with other estimators in a wide variety of models, and establish
their competent performance."@2012
Bing Li@http://arxiv.org/abs/1203.3313v1@"Sufficient dimension reduction based on an ensemble of minimum average
  variance estimators"@"We introduce a class of dimension reduction estimators based on an ensemble
of the minimum average variance estimates of functions that characterize the
central subspace, such as the characteristic functions, the Box--Cox
transformations and wavelet basis. The ensemble estimators exhaustively
estimate the central subspace without imposing restrictive conditions on the
predictors, and have the same convergence rate as the minimum average variance
estimates. They are flexible and easy to implement, and allow repeated use of
the available sample, which enhances accuracy. They are applicable to both
univariate and multivariate responses in a unified form. We establish the
consistency and convergence rate of these estimators, and the consistency of a
cross validation criterion for order determination. We compare the ensemble
estimators with other estimators in a wide variety of models, and establish
their competent performance."@2012
L. R. Haff@http://arxiv.org/abs/1203.3342v1@Minimax estimation for mixtures of Wishart distributions@"The space of positive definite symmetric matrices has been studied
extensively as a means of understanding dependence in multivariate data along
with the accompanying problems in statistical inference. Many books and papers
have been written on this subject, and more recently there has been
considerable interest in high-dimensional random matrices with particular
emphasis on the distribution of certain eigenvalues. With the availability of
modern data acquisition capabilities, smoothing or nonparametric techniques are
required that go beyond those applicable only to data arising in Euclidean
spaces. Accordingly, we present a Fourier method of minimax Wishart mixture
density estimation on the space of positive definite symmetric matrices."@2012
P. T. Kim@http://arxiv.org/abs/1203.3342v1@Minimax estimation for mixtures of Wishart distributions@"The space of positive definite symmetric matrices has been studied
extensively as a means of understanding dependence in multivariate data along
with the accompanying problems in statistical inference. Many books and papers
have been written on this subject, and more recently there has been
considerable interest in high-dimensional random matrices with particular
emphasis on the distribution of certain eigenvalues. With the availability of
modern data acquisition capabilities, smoothing or nonparametric techniques are
required that go beyond those applicable only to data arising in Euclidean
spaces. Accordingly, we present a Fourier method of minimax Wishart mixture
density estimation on the space of positive definite symmetric matrices."@2012
J. -Y. Koo@http://arxiv.org/abs/1203.3342v1@Minimax estimation for mixtures of Wishart distributions@"The space of positive definite symmetric matrices has been studied
extensively as a means of understanding dependence in multivariate data along
with the accompanying problems in statistical inference. Many books and papers
have been written on this subject, and more recently there has been
considerable interest in high-dimensional random matrices with particular
emphasis on the distribution of certain eigenvalues. With the availability of
modern data acquisition capabilities, smoothing or nonparametric techniques are
required that go beyond those applicable only to data arising in Euclidean
spaces. Accordingly, we present a Fourier method of minimax Wishart mixture
density estimation on the space of positive definite symmetric matrices."@2012
D. St. P. Richards@http://arxiv.org/abs/1203.3342v1@Minimax estimation for mixtures of Wishart distributions@"The space of positive definite symmetric matrices has been studied
extensively as a means of understanding dependence in multivariate data along
with the accompanying problems in statistical inference. Many books and papers
have been written on this subject, and more recently there has been
considerable interest in high-dimensional random matrices with particular
emphasis on the distribution of certain eigenvalues. With the availability of
modern data acquisition capabilities, smoothing or nonparametric techniques are
required that go beyond those applicable only to data arising in Euclidean
spaces. Accordingly, we present a Fourier method of minimax Wishart mixture
density estimation on the space of positive definite symmetric matrices."@2012
Magalie Fromont@http://arxiv.org/abs/1203.3572v2@"The two-sample problem for Poisson processes: adaptive tests with a
  non-asymptotic wild bootstrap approach"@"Considering two independent Poisson processes, we address the question of
testing equality of their respective intensities. We first propose single tests
whose test statistics are U-statistics based on general kernel functions. The
corresponding critical values are constructed from a non-asymptotic wild
bootstrap approach, leading to level \alpha tests. Various choices for the
kernel functions are possible, including projection, approximation or
reproducing kernels. In this last case, we obtain a parametric rate of testing
for a weak metric defined in the RKHS associated with the considered
reproducing kernel. Then we introduce, in the other cases, an aggregation
procedure, which allows us to import ideas coming from model selection,
thresholding and/or approximation kernels adaptive estimation. The resulting
multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic
oracle type conditions for the classical L2-norm. From these conditions, we
deduce that they are adaptive in the minimax sense over a large variety of
classes of alternatives based on classical and weak Besov bodies in the
univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the
multivariate case."@2012
Béatrice Laurent@http://arxiv.org/abs/1203.3572v2@"The two-sample problem for Poisson processes: adaptive tests with a
  non-asymptotic wild bootstrap approach"@"Considering two independent Poisson processes, we address the question of
testing equality of their respective intensities. We first propose single tests
whose test statistics are U-statistics based on general kernel functions. The
corresponding critical values are constructed from a non-asymptotic wild
bootstrap approach, leading to level \alpha tests. Various choices for the
kernel functions are possible, including projection, approximation or
reproducing kernels. In this last case, we obtain a parametric rate of testing
for a weak metric defined in the RKHS associated with the considered
reproducing kernel. Then we introduce, in the other cases, an aggregation
procedure, which allows us to import ideas coming from model selection,
thresholding and/or approximation kernels adaptive estimation. The resulting
multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic
oracle type conditions for the classical L2-norm. From these conditions, we
deduce that they are adaptive in the minimax sense over a large variety of
classes of alternatives based on classical and weak Besov bodies in the
univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the
multivariate case."@2012
Patricia Reynaud-Bouret@http://arxiv.org/abs/1203.3572v2@"The two-sample problem for Poisson processes: adaptive tests with a
  non-asymptotic wild bootstrap approach"@"Considering two independent Poisson processes, we address the question of
testing equality of their respective intensities. We first propose single tests
whose test statistics are U-statistics based on general kernel functions. The
corresponding critical values are constructed from a non-asymptotic wild
bootstrap approach, leading to level \alpha tests. Various choices for the
kernel functions are possible, including projection, approximation or
reproducing kernels. In this last case, we obtain a parametric rate of testing
for a weak metric defined in the RKHS associated with the considered
reproducing kernel. Then we introduce, in the other cases, an aggregation
procedure, which allows us to import ideas coming from model selection,
thresholding and/or approximation kernels adaptive estimation. The resulting
multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic
oracle type conditions for the classical L2-norm. From these conditions, we
deduce that they are adaptive in the minimax sense over a large variety of
classes of alternatives based on classical and weak Besov bodies in the
univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the
multivariate case."@2012
Sándor Baran@http://arxiv.org/abs/1203.4346v1@Testing stability in a spatial unilateral autoregressive model@"Least squares estimator of the stability parameter $\varrho := |\alpha| +
|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha
X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.
Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable
case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1
X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares
estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is
not asymptotically normal in the unstable, i.e., in the unit root case."@2012
Gyula Pap@http://arxiv.org/abs/1203.4346v1@Testing stability in a spatial unilateral autoregressive model@"Least squares estimator of the stability parameter $\varrho := |\alpha| +
|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha
X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.
Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable
case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1
X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares
estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is
not asymptotically normal in the unstable, i.e., in the unit root case."@2012
Kinga Sikolya@http://arxiv.org/abs/1203.4346v1@Testing stability in a spatial unilateral autoregressive model@"Least squares estimator of the stability parameter $\varrho := |\alpha| +
|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha
X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.
Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable
case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1
X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares
estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is
not asymptotically normal in the unstable, i.e., in the unit root case."@2012
Lee Dicker@http://arxiv.org/abs/1203.4576v2@"Parallelism, Uniqueness, and Large-Sample Asymptotics for the Dantzig
  Selector"@"The Dantzig selector (Candes and Tao, 2007) is a popular l1-regularization
method for variable selection and estimation in linear regression. We present a
very weak geometric condition on the observed predictors which is related to
parallelism and, when satisfied, ensures the uniqueness of Dantzig selector
estimators. The condition holds with probability 1, if the predictors are drawn
from a continuous distribution. We discuss the necessity of this condition for
uniqueness and also provide a closely related condition which ensures
uniqueness of lasso estimators (Tibshirani, 1996). Large sample asymptotics for
the Dantzig selector, i.e. almost sure convergence and the asymptotic
distribution, follow directly from our uniqueness results and a continuity
argument. The limiting distribution of the Dantzig selector is generally
non-normal. Though our asymptotic results require that the number of predictors
is fixed (similar to (Knight and Fu, 2000)), our uniqueness results are valid
for an arbitrary number of predictors and observations."@2012
Xihong Lin@http://arxiv.org/abs/1203.4576v2@"Parallelism, Uniqueness, and Large-Sample Asymptotics for the Dantzig
  Selector"@"The Dantzig selector (Candes and Tao, 2007) is a popular l1-regularization
method for variable selection and estimation in linear regression. We present a
very weak geometric condition on the observed predictors which is related to
parallelism and, when satisfied, ensures the uniqueness of Dantzig selector
estimators. The condition holds with probability 1, if the predictors are drawn
from a continuous distribution. We discuss the necessity of this condition for
uniqueness and also provide a closely related condition which ensures
uniqueness of lasso estimators (Tibshirani, 1996). Large sample asymptotics for
the Dantzig selector, i.e. almost sure convergence and the asymptotic
distribution, follow directly from our uniqueness results and a continuity
argument. The limiting distribution of the Dantzig selector is generally
non-normal. Though our asymptotic results require that the number of predictors
is fixed (similar to (Knight and Fu, 2000)), our uniqueness results are valid
for an arbitrary number of predictors and observations."@2012
Tristan Launay@http://arxiv.org/abs/1203.4753v2@"Consistency of the posterior distribution and MLE for piecewise linear
  regression"@"We prove the weak consistency of the posterior distribution and that of the
Bayes estimator for a two-phase piecewise linear regression mdoel where the
break-point is unknown. The non-differentiability of the likelihood of the
model with regard to the break- point parameter induces technical difficulties
that we overcome by creating a regularised version of the problem at hand. We
first recover the strong consistency of the quantities of interest for the
regularised version, using results about the MLE, and we then prove that the
regularised version and the original version of the problem share the same
asymptotic properties."@2012
Anne Philippe@http://arxiv.org/abs/1203.4753v2@"Consistency of the posterior distribution and MLE for piecewise linear
  regression"@"We prove the weak consistency of the posterior distribution and that of the
Bayes estimator for a two-phase piecewise linear regression mdoel where the
break-point is unknown. The non-differentiability of the likelihood of the
model with regard to the break- point parameter induces technical difficulties
that we overcome by creating a regularised version of the problem at hand. We
first recover the strong consistency of the quantities of interest for the
regularised version, using results about the MLE, and we then prove that the
regularised version and the original version of the problem share the same
asymptotic properties."@2012
Sophie Lamarche@http://arxiv.org/abs/1203.4753v2@"Consistency of the posterior distribution and MLE for piecewise linear
  regression"@"We prove the weak consistency of the posterior distribution and that of the
Bayes estimator for a two-phase piecewise linear regression mdoel where the
break-point is unknown. The non-differentiability of the likelihood of the
model with regard to the break- point parameter induces technical difficulties
that we overcome by creating a regularised version of the problem at hand. We
first recover the strong consistency of the quantities of interest for the
regularised version, using results about the MLE, and we then prove that the
regularised version and the original version of the problem share the same
asymptotic properties."@2012
Herold Dehling@http://arxiv.org/abs/1203.4871v5@Testing for Changes in Kendall's Tau@"For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect
whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =
1,...,n$. We propose a nonparametric change-point test statistic based on
Kendall's tau and derive its asymptotic distribution under the null hypothesis
of no change by means a new U-statistic invariance principle for dependent
processes. The asymptotic distribution depends on the long run variance of
Kendall's tau, for which we propose an estimator and show its consistency.
Furthermore, assuming a single change-point, we show that the location of the
change-point is consistently estimated. Kendall's tau possesses a high
efficiency at the normal distribution, as compared to the normal maximum
likelihood estimator, Pearson's moment correlation coefficient. Contrary to
Pearson's correlation coefficient, it has excellent robustness properties and
shows no loss in efficiency at heavy-tailed distributions. We assume the data
$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an
absolutely regular process. The P-near epoch dependence condition constitutes a
generalization of the usually considered $L_p$-near epoch dependence, $p \ge
1$, that does not require the existence of any moments. It is therefore very
well suited for our objective to efficiently detect changes in correlation for
arbitrarily heavy-tailed data."@2012
Daniel Vogel@http://arxiv.org/abs/1203.4871v5@Testing for Changes in Kendall's Tau@"For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect
whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =
1,...,n$. We propose a nonparametric change-point test statistic based on
Kendall's tau and derive its asymptotic distribution under the null hypothesis
of no change by means a new U-statistic invariance principle for dependent
processes. The asymptotic distribution depends on the long run variance of
Kendall's tau, for which we propose an estimator and show its consistency.
Furthermore, assuming a single change-point, we show that the location of the
change-point is consistently estimated. Kendall's tau possesses a high
efficiency at the normal distribution, as compared to the normal maximum
likelihood estimator, Pearson's moment correlation coefficient. Contrary to
Pearson's correlation coefficient, it has excellent robustness properties and
shows no loss in efficiency at heavy-tailed distributions. We assume the data
$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an
absolutely regular process. The P-near epoch dependence condition constitutes a
generalization of the usually considered $L_p$-near epoch dependence, $p \ge
1$, that does not require the existence of any moments. It is therefore very
well suited for our objective to efficiently detect changes in correlation for
arbitrarily heavy-tailed data."@2012
Martin Wendler@http://arxiv.org/abs/1203.4871v5@Testing for Changes in Kendall's Tau@"For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect
whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =
1,...,n$. We propose a nonparametric change-point test statistic based on
Kendall's tau and derive its asymptotic distribution under the null hypothesis
of no change by means a new U-statistic invariance principle for dependent
processes. The asymptotic distribution depends on the long run variance of
Kendall's tau, for which we propose an estimator and show its consistency.
Furthermore, assuming a single change-point, we show that the location of the
change-point is consistently estimated. Kendall's tau possesses a high
efficiency at the normal distribution, as compared to the normal maximum
likelihood estimator, Pearson's moment correlation coefficient. Contrary to
Pearson's correlation coefficient, it has excellent robustness properties and
shows no loss in efficiency at heavy-tailed distributions. We assume the data
$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an
absolutely regular process. The P-near epoch dependence condition constitutes a
generalization of the usually considered $L_p$-near epoch dependence, $p \ge
1$, that does not require the existence of any moments. It is therefore very
well suited for our objective to efficiently detect changes in correlation for
arbitrarily heavy-tailed data."@2012
Dominik Wied@http://arxiv.org/abs/1203.4871v5@Testing for Changes in Kendall's Tau@"For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect
whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =
1,...,n$. We propose a nonparametric change-point test statistic based on
Kendall's tau and derive its asymptotic distribution under the null hypothesis
of no change by means a new U-statistic invariance principle for dependent
processes. The asymptotic distribution depends on the long run variance of
Kendall's tau, for which we propose an estimator and show its consistency.
Furthermore, assuming a single change-point, we show that the location of the
change-point is consistently estimated. Kendall's tau possesses a high
efficiency at the normal distribution, as compared to the normal maximum
likelihood estimator, Pearson's moment correlation coefficient. Contrary to
Pearson's correlation coefficient, it has excellent robustness properties and
shows no loss in efficiency at heavy-tailed distributions. We assume the data
$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an
absolutely regular process. The P-near epoch dependence condition constitutes a
generalization of the usually considered $L_p$-near epoch dependence, $p \ge
1$, that does not require the existence of any moments. It is therefore very
well suited for our objective to efficiently detect changes in correlation for
arbitrarily heavy-tailed data."@2012
Y. Ritov@http://arxiv.org/abs/1203.5471v4@"The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be
  CODA?"@"We consider the Bayesian analysis of a few complex, high-dimensional models
and show that intuitive priors, which are not tailored to the fine details of
the model and the estimated parameters, produce estimators which perform poorly
in situations in which good, simple frequentist estimators exist. The models we
consider are: stratified sampling, the partial linear model, linear and
quadratic functionals of white noise and estimation with stopping times. We
present a strong version of Doob's consistency theorem which demonstrates that
the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the
Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets
of prior probability 1. We also demonstrate that it is, at least, in principle,
possible to construct Bayes priors giving both global and local minimax rates,
using a suitable combination of loss functions. We argue that there is no
contradiction in these apparently conflicting findings."@2012
P. J. Bickel@http://arxiv.org/abs/1203.5471v4@"The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be
  CODA?"@"We consider the Bayesian analysis of a few complex, high-dimensional models
and show that intuitive priors, which are not tailored to the fine details of
the model and the estimated parameters, produce estimators which perform poorly
in situations in which good, simple frequentist estimators exist. The models we
consider are: stratified sampling, the partial linear model, linear and
quadratic functionals of white noise and estimation with stopping times. We
present a strong version of Doob's consistency theorem which demonstrates that
the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the
Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets
of prior probability 1. We also demonstrate that it is, at least, in principle,
possible to construct Bayes priors giving both global and local minimax rates,
using a suitable combination of loss functions. We argue that there is no
contradiction in these apparently conflicting findings."@2012
A. C. Gamst@http://arxiv.org/abs/1203.5471v4@"The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be
  CODA?"@"We consider the Bayesian analysis of a few complex, high-dimensional models
and show that intuitive priors, which are not tailored to the fine details of
the model and the estimated parameters, produce estimators which perform poorly
in situations in which good, simple frequentist estimators exist. The models we
consider are: stratified sampling, the partial linear model, linear and
quadratic functionals of white noise and estimation with stopping times. We
present a strong version of Doob's consistency theorem which demonstrates that
the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the
Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets
of prior probability 1. We also demonstrate that it is, at least, in principle,
possible to construct Bayes priors giving both global and local minimax rates,
using a suitable combination of loss functions. We argue that there is no
contradiction in these apparently conflicting findings."@2012
B. J. K. Kleijn@http://arxiv.org/abs/1203.5471v4@"The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be
  CODA?"@"We consider the Bayesian analysis of a few complex, high-dimensional models
and show that intuitive priors, which are not tailored to the fine details of
the model and the estimated parameters, produce estimators which perform poorly
in situations in which good, simple frequentist estimators exist. The models we
consider are: stratified sampling, the partial linear model, linear and
quadratic functionals of white noise and estimation with stopping times. We
present a strong version of Doob's consistency theorem which demonstrates that
the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the
Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets
of prior probability 1. We also demonstrate that it is, at least, in principle,
possible to construct Bayes priors giving both global and local minimax rates,
using a suitable combination of loss functions. We argue that there is no
contradiction in these apparently conflicting findings."@2012
Jose A. Diaz-Garcia@http://arxiv.org/abs/1203.5587v1@"Response surface methodology: Asymptotic normality of the optimal
  solution"@"Sensitivity analysis of the optimal solution in response surface methodology
is studied and an explicit form of the effect of perturbation of the regression
coefficients on the optimal solution is obtained. The characterisation of the
critical point of the convex program corresponding to the optimum of a response
surface model is also studied. The asymptotic normality of the optimal solution
follows by standard methods."@2012
Jose E. Rodriguez@http://arxiv.org/abs/1203.5587v1@"Response surface methodology: Asymptotic normality of the optimal
  solution"@"Sensitivity analysis of the optimal solution in response surface methodology
is studied and an explicit form of the effect of perturbation of the regression
coefficients on the optimal solution is obtained. The characterisation of the
critical point of the convex program corresponding to the optimum of a response
surface model is also studied. The asymptotic normality of the optimal solution
follows by standard methods."@2012
Rogelio Ramos-Quiroga@http://arxiv.org/abs/1203.5587v1@"Response surface methodology: Asymptotic normality of the optimal
  solution"@"Sensitivity analysis of the optimal solution in response surface methodology
is studied and an explicit form of the effect of perturbation of the regression
coefficients on the optimal solution is obtained. The characterisation of the
critical point of the convex program corresponding to the optimum of a response
surface model is also studied. The asymptotic normality of the optimal solution
follows by standard methods."@2012
Sergios Agapiou@http://arxiv.org/abs/1203.5753v5@"Posterior Contraction Rates for the Bayesian Approach to Linear
  Ill-Posed Inverse Problems"@"We consider a Bayesian nonparametric approach to a family of linear inverse
problems in a separable Hilbert space setting with Gaussian noise. We assume
Gaussian priors, which are conjugate to the model, and present a method of
identifying the posterior using its precision operator. Working with the
unbounded precision operator enables us to use partial differential equations
(PDE) methodology to obtain rates of contraction of the posterior distribution
to a Dirac measure centered on the true solution. Our methods assume a
relatively weak relation between the prior covariance, noise covariance and
forward operator, allowing for a wide range of applications."@2012
Stig Larsson@http://arxiv.org/abs/1203.5753v5@"Posterior Contraction Rates for the Bayesian Approach to Linear
  Ill-Posed Inverse Problems"@"We consider a Bayesian nonparametric approach to a family of linear inverse
problems in a separable Hilbert space setting with Gaussian noise. We assume
Gaussian priors, which are conjugate to the model, and present a method of
identifying the posterior using its precision operator. Working with the
unbounded precision operator enables us to use partial differential equations
(PDE) methodology to obtain rates of contraction of the posterior distribution
to a Dirac measure centered on the true solution. Our methods assume a
relatively weak relation between the prior covariance, noise covariance and
forward operator, allowing for a wide range of applications."@2012
Andrew M. Stuart@http://arxiv.org/abs/1203.5753v5@"Posterior Contraction Rates for the Bayesian Approach to Linear
  Ill-Posed Inverse Problems"@"We consider a Bayesian nonparametric approach to a family of linear inverse
problems in a separable Hilbert space setting with Gaussian noise. We assume
Gaussian priors, which are conjugate to the model, and present a method of
identifying the posterior using its precision operator. Working with the
unbounded precision operator enables us to use partial differential equations
(PDE) methodology to obtain rates of contraction of the posterior distribution
to a Dirac measure centered on the true solution. Our methods assume a
relatively weak relation between the prior covariance, noise covariance and
forward operator, allowing for a wide range of applications."@2012
Shuyuan He@http://arxiv.org/abs/1203.5955v1@Empirical Likelihood for Right Censored Lifetime Data@"This paper considers the empirical likelihood (EL) construction of confidence
intervals for a linear functional based on right censored lifetime data. Many
of the results in literature show that log EL has a limiting scaled chi-square
distribution, where the scale parameter is a function of the unknown asymptotic
variance. The scale parameter has to be estimated for the construction.
Additional estimation would reduce the coverage accuracy for the parameter.
This diminishes a main advantage of the EL method for censored data. By
utilizing certain influence functions in an estimating equation, it is shown
that under very general conditions, log EL converges weakly to a standard
chi-square distribution and thereby eliminates the need for estimating the
scale parameter. Moreover, a special way of employing influence functions eases
the otherwise very demanding computations of the EL method. Our approach yields
smaller asymptotic variance of the influence function than those comparable
ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not
surprising that confidence intervals using influence functions give a better
coverage accuracy as demonstrated by simulations."@2012
Wei Liang@http://arxiv.org/abs/1203.5955v1@Empirical Likelihood for Right Censored Lifetime Data@"This paper considers the empirical likelihood (EL) construction of confidence
intervals for a linear functional based on right censored lifetime data. Many
of the results in literature show that log EL has a limiting scaled chi-square
distribution, where the scale parameter is a function of the unknown asymptotic
variance. The scale parameter has to be estimated for the construction.
Additional estimation would reduce the coverage accuracy for the parameter.
This diminishes a main advantage of the EL method for censored data. By
utilizing certain influence functions in an estimating equation, it is shown
that under very general conditions, log EL converges weakly to a standard
chi-square distribution and thereby eliminates the need for estimating the
scale parameter. Moreover, a special way of employing influence functions eases
the otherwise very demanding computations of the EL method. Our approach yields
smaller asymptotic variance of the influence function than those comparable
ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not
surprising that confidence intervals using influence functions give a better
coverage accuracy as demonstrated by simulations."@2012
Junshan Shen@http://arxiv.org/abs/1203.5955v1@Empirical Likelihood for Right Censored Lifetime Data@"This paper considers the empirical likelihood (EL) construction of confidence
intervals for a linear functional based on right censored lifetime data. Many
of the results in literature show that log EL has a limiting scaled chi-square
distribution, where the scale parameter is a function of the unknown asymptotic
variance. The scale parameter has to be estimated for the construction.
Additional estimation would reduce the coverage accuracy for the parameter.
This diminishes a main advantage of the EL method for censored data. By
utilizing certain influence functions in an estimating equation, it is shown
that under very general conditions, log EL converges weakly to a standard
chi-square distribution and thereby eliminates the need for estimating the
scale parameter. Moreover, a special way of employing influence functions eases
the otherwise very demanding computations of the EL method. Our approach yields
smaller asymptotic variance of the influence function than those comparable
ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not
surprising that confidence intervals using influence functions give a better
coverage accuracy as demonstrated by simulations."@2012
Grace Yang@http://arxiv.org/abs/1203.5955v1@Empirical Likelihood for Right Censored Lifetime Data@"This paper considers the empirical likelihood (EL) construction of confidence
intervals for a linear functional based on right censored lifetime data. Many
of the results in literature show that log EL has a limiting scaled chi-square
distribution, where the scale parameter is a function of the unknown asymptotic
variance. The scale parameter has to be estimated for the construction.
Additional estimation would reduce the coverage accuracy for the parameter.
This diminishes a main advantage of the EL method for censored data. By
utilizing certain influence functions in an estimating equation, it is shown
that under very general conditions, log EL converges weakly to a standard
chi-square distribution and thereby eliminates the need for estimating the
scale parameter. Moreover, a special way of employing influence functions eases
the otherwise very demanding computations of the EL method. Our approach yields
smaller asymptotic variance of the influence function than those comparable
ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not
surprising that confidence intervals using influence functions give a better
coverage accuracy as demonstrated by simulations."@2012
Dominik Janzing@http://arxiv.org/abs/1203.6502v2@Quantifying causal influences@"Many methods for causal inference generate directed acyclic graphs (DAGs)
that formalize causal relations between $n$ variables. Given the joint
distribution on all these variables, the DAG contains all information about how
intervening on one variable changes the distribution of the other $n-1$
variables. However, quantifying the causal influence of one variable on another
one remains a nontrivial question. Here we propose a set of natural, intuitive
postulates that a measure of causal strength should satisfy. We then introduce
a communication scenario, where edges in a DAG play the role of channels that
can be locally corrupted by interventions. Causal strength is then the relative
entropy distance between the old and the new distribution. Many other measures
of causal strength have been proposed, including average causal effect,
transfer entropy, directed information, and information flow. We explain how
they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,
we investigate the behavior of our measure on time-series, supporting our
claims with experiments on simulated data."@2012
David Balduzzi@http://arxiv.org/abs/1203.6502v2@Quantifying causal influences@"Many methods for causal inference generate directed acyclic graphs (DAGs)
that formalize causal relations between $n$ variables. Given the joint
distribution on all these variables, the DAG contains all information about how
intervening on one variable changes the distribution of the other $n-1$
variables. However, quantifying the causal influence of one variable on another
one remains a nontrivial question. Here we propose a set of natural, intuitive
postulates that a measure of causal strength should satisfy. We then introduce
a communication scenario, where edges in a DAG play the role of channels that
can be locally corrupted by interventions. Causal strength is then the relative
entropy distance between the old and the new distribution. Many other measures
of causal strength have been proposed, including average causal effect,
transfer entropy, directed information, and information flow. We explain how
they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,
we investigate the behavior of our measure on time-series, supporting our
claims with experiments on simulated data."@2012
Moritz Grosse-Wentrup@http://arxiv.org/abs/1203.6502v2@Quantifying causal influences@"Many methods for causal inference generate directed acyclic graphs (DAGs)
that formalize causal relations between $n$ variables. Given the joint
distribution on all these variables, the DAG contains all information about how
intervening on one variable changes the distribution of the other $n-1$
variables. However, quantifying the causal influence of one variable on another
one remains a nontrivial question. Here we propose a set of natural, intuitive
postulates that a measure of causal strength should satisfy. We then introduce
a communication scenario, where edges in a DAG play the role of channels that
can be locally corrupted by interventions. Causal strength is then the relative
entropy distance between the old and the new distribution. Many other measures
of causal strength have been proposed, including average causal effect,
transfer entropy, directed information, and information flow. We explain how
they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,
we investigate the behavior of our measure on time-series, supporting our
claims with experiments on simulated data."@2012
Bernhard Schölkopf@http://arxiv.org/abs/1203.6502v2@Quantifying causal influences@"Many methods for causal inference generate directed acyclic graphs (DAGs)
that formalize causal relations between $n$ variables. Given the joint
distribution on all these variables, the DAG contains all information about how
intervening on one variable changes the distribution of the other $n-1$
variables. However, quantifying the causal influence of one variable on another
one remains a nontrivial question. Here we propose a set of natural, intuitive
postulates that a measure of causal strength should satisfy. We then introduce
a communication scenario, where edges in a DAG play the role of channels that
can be locally corrupted by interventions. Causal strength is then the relative
entropy distance between the old and the new distribution. Many other measures
of causal strength have been proposed, including average causal effect,
transfer entropy, directed information, and information flow. We explain how
they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,
we investigate the behavior of our measure on time-series, supporting our
claims with experiments on simulated data."@2012
Ilia Negri@http://arxiv.org/abs/1203.6547v1@"On Goodness-of-fit Testing for Ergodic Diffusion Process with Shift
  Parameter"@"A problem of goodness-of-fit test for ergodic diffusion processes is
presented. In the null hypothesis the drift of the diffusion is supposed to be
in a parametric form with unknown shift parameter. Two Cramer-Von Mises type
test statistics are studied. The first one is based on local time estimator of
the invariant density, the second one is based on the empirical distribution
function. The unknown parameter is estimated via the maximum likelihood
estimator. It is shown that both the limit distributions of the two test
statistics do not depend on the unknown parameter, so the distributions of the
tests are asymptotically parameter free. Some considerations on the consistency
of the proposed tests and some simulation studies are also given."@2012
Li Zhou@http://arxiv.org/abs/1203.6547v1@"On Goodness-of-fit Testing for Ergodic Diffusion Process with Shift
  Parameter"@"A problem of goodness-of-fit test for ergodic diffusion processes is
presented. In the null hypothesis the drift of the diffusion is supposed to be
in a parametric form with unknown shift parameter. Two Cramer-Von Mises type
test statistics are studied. The first one is based on local time estimator of
the invariant density, the second one is based on the empirical distribution
function. The unknown parameter is estimated via the maximum likelihood
estimator. It is shown that both the limit distributions of the two test
statistics do not depend on the unknown parameter, so the distributions of the
tests are asymptotically parameter free. Some considerations on the consistency
of the proposed tests and some simulation studies are also given."@2012
Randal Douc@http://arxiv.org/abs/1203.6898v2@"Long-term stability of sequential Monte Carlo methods under verifiable
  conditions"@"This paper discusses particle filtering in general hidden Markov models
(HMMs) and presents novel theoretical results on the long-term stability of
bootstrap-type particle filters. More specifically, we establish that the
asymptotic variance of the Monte Carlo estimates produced by the bootstrap
filter is uniformly bounded in time. On the contrary to most previous results
of this type, which in general presuppose that the state space of the hidden
state process is compact (an assumption that is rarely satisfied in practice),
our very mild assumptions are satisfied for a large class of HMMs with possibly
noncompact state space. In addition, we derive a similar time uniform bound on
the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for
misspecified models; that is, we do not at all assume that the data entering
into the particle filter originate from the model governing the dynamics of the
particles or not even from an HMM."@2012
Eric Moulines@http://arxiv.org/abs/1203.6898v2@"Long-term stability of sequential Monte Carlo methods under verifiable
  conditions"@"This paper discusses particle filtering in general hidden Markov models
(HMMs) and presents novel theoretical results on the long-term stability of
bootstrap-type particle filters. More specifically, we establish that the
asymptotic variance of the Monte Carlo estimates produced by the bootstrap
filter is uniformly bounded in time. On the contrary to most previous results
of this type, which in general presuppose that the state space of the hidden
state process is compact (an assumption that is rarely satisfied in practice),
our very mild assumptions are satisfied for a large class of HMMs with possibly
noncompact state space. In addition, we derive a similar time uniform bound on
the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for
misspecified models; that is, we do not at all assume that the data entering
into the particle filter originate from the model governing the dynamics of the
particles or not even from an HMM."@2012
Jimmy Olsson@http://arxiv.org/abs/1203.6898v2@"Long-term stability of sequential Monte Carlo methods under verifiable
  conditions"@"This paper discusses particle filtering in general hidden Markov models
(HMMs) and presents novel theoretical results on the long-term stability of
bootstrap-type particle filters. More specifically, we establish that the
asymptotic variance of the Monte Carlo estimates produced by the bootstrap
filter is uniformly bounded in time. On the contrary to most previous results
of this type, which in general presuppose that the state space of the hidden
state process is compact (an assumption that is rarely satisfied in practice),
our very mild assumptions are satisfied for a large class of HMMs with possibly
noncompact state space. In addition, we derive a similar time uniform bound on
the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for
misspecified models; that is, we do not at all assume that the data entering
into the particle filter originate from the model governing the dynamics of the
particles or not even from an HMM."@2012
Pongpol Ruankong@http://arxiv.org/abs/1204.0405v1@Shuffles of copulas and a new measure of dependence@"Using a characterization of Mutual Complete Dependence copulas, we show that,
with respect to the Sobolev norm, the MCD copulas can be approximated
arbitrarily closed by shuffles of Min. This result is then used to obtain a
characterization of generalized shuffles of copulas introduced by Durante,
Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by
Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the
corresponding shuffles of the two continuous random variables, we define a new
norm which is invariant under shuffling. This norm gives rise to a new measure
of dependence which shares many properties with the maximal correlation
coefficient, the only measure of dependence that satisfies all of R\'enyi's
postulates."@2012
Tippawan Santiwipanont@http://arxiv.org/abs/1204.0405v1@Shuffles of copulas and a new measure of dependence@"Using a characterization of Mutual Complete Dependence copulas, we show that,
with respect to the Sobolev norm, the MCD copulas can be approximated
arbitrarily closed by shuffles of Min. This result is then used to obtain a
characterization of generalized shuffles of copulas introduced by Durante,
Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by
Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the
corresponding shuffles of the two continuous random variables, we define a new
norm which is invariant under shuffling. This norm gives rise to a new measure
of dependence which shares many properties with the maximal correlation
coefficient, the only measure of dependence that satisfies all of R\'enyi's
postulates."@2012
Songkiat Sumetkijakan@http://arxiv.org/abs/1204.0405v1@Shuffles of copulas and a new measure of dependence@"Using a characterization of Mutual Complete Dependence copulas, we show that,
with respect to the Sobolev norm, the MCD copulas can be approximated
arbitrarily closed by shuffles of Min. This result is then used to obtain a
characterization of generalized shuffles of copulas introduced by Durante,
Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by
Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the
corresponding shuffles of the two continuous random variables, we define a new
norm which is invariant under shuffling. This norm gives rise to a new measure
of dependence which shares many properties with the maximal correlation
coefficient, the only measure of dependence that satisfies all of R\'enyi's
postulates."@2012
Xiaofeng Shao@http://arxiv.org/abs/1204.1035v1@"Fixed-b Subsampling and Block Bootstrap: Improved Confidence Sets Based
  on P-value Calibration"@"Subsampling and block-based bootstrap methods have been used in a wide range
of inference problems for time series. To accommodate the dependence, these
resampling methods involve a bandwidth parameter, such as subsampling window
width and block size in the block-based bootstrap. In empirical work, using
different bandwidth parameters could lead to different inference results, but
the traditional first order asymptotic theory does not capture the choice of
the bandwidth. In this article, we propose to adopt the fixed-b approach, as
advocated by Kiefer and Vogelsang (2005) in the
heteroscedasticity-autocorrelation robust testing context, to account for the
influence of the bandwidth on the inference. Under the fixed-b asymptotic
framework, we derive the asymptotic null distribution of the p-values for
subsampling and the moving block bootstrap, and further propose a calibration
of the traditional small-b based confidence intervals (regions, bands) and
tests. Our treatment is fairly general as it includes both finite dimensional
parameters and infinite dimensional parameters, such as marginal distribution
function and normalized spectral distribution function. Simulation results show
that the fixed-b approach is more accurate than the traditional small-b
approach in terms of approximating the finite sample distribution, and that the
calibrated confidence sets tend to have smaller coverage errors than the
uncalibrated counterparts."@2012
Dimitris N. Politis@http://arxiv.org/abs/1204.1035v1@"Fixed-b Subsampling and Block Bootstrap: Improved Confidence Sets Based
  on P-value Calibration"@"Subsampling and block-based bootstrap methods have been used in a wide range
of inference problems for time series. To accommodate the dependence, these
resampling methods involve a bandwidth parameter, such as subsampling window
width and block size in the block-based bootstrap. In empirical work, using
different bandwidth parameters could lead to different inference results, but
the traditional first order asymptotic theory does not capture the choice of
the bandwidth. In this article, we propose to adopt the fixed-b approach, as
advocated by Kiefer and Vogelsang (2005) in the
heteroscedasticity-autocorrelation robust testing context, to account for the
influence of the bandwidth on the inference. Under the fixed-b asymptotic
framework, we derive the asymptotic null distribution of the p-values for
subsampling and the moving block bootstrap, and further propose a calibration
of the traditional small-b based confidence intervals (regions, bands) and
tests. Our treatment is fairly general as it includes both finite dimensional
parameters and infinite dimensional parameters, such as marginal distribution
function and normalized spectral distribution function. Simulation results show
that the fixed-b approach is more accurate than the traditional small-b
approach in terms of approximating the finite sample distribution, and that the
calibrated confidence sets tend to have smaller coverage errors than the
uncalibrated counterparts."@2012
Jan Johannes@http://arxiv.org/abs/1204.1226v1@Adaptive Gaussian inverse regression with partially unknown operator@"This work deals with the ill-posed inverse problem of reconstructing a
function $f$ given implicitly as the solution of $g = Af$, where $A$ is a
compact linear operator with unknown singular values and known eigenfunctions.
We observe the function $g$ and the singular values of the operator subject to
Gaussian white noise with respective noise levels $\varepsilon$ and $\sigma$.
  We develop a minimax theory in terms of both noise levels and propose an
orthogonal series estimator attaining the minimax rates. This estimator
requires the optimal choice of a dimension parameter depending on certain
characteristics of $f$ and $A$. This work addresses the fully data-driven
choice of the dimension parameter combining model selection with Lepski's
method. We show that the fully data-driven estimator preserves minimax
optimality over a wide range of classes for $f$ and $A$ and noise levels
$\varepsilon$ and $\sigma$. The results are illustrated considering Sobolev
spaces and mildly and severely ill-posed inverse problems."@2012
Maik Schwarz@http://arxiv.org/abs/1204.1226v1@Adaptive Gaussian inverse regression with partially unknown operator@"This work deals with the ill-posed inverse problem of reconstructing a
function $f$ given implicitly as the solution of $g = Af$, where $A$ is a
compact linear operator with unknown singular values and known eigenfunctions.
We observe the function $g$ and the singular values of the operator subject to
Gaussian white noise with respective noise levels $\varepsilon$ and $\sigma$.
  We develop a minimax theory in terms of both noise levels and propose an
orthogonal series estimator attaining the minimax rates. This estimator
requires the optimal choice of a dimension parameter depending on certain
characteristics of $f$ and $A$. This work addresses the fully data-driven
choice of the dimension parameter combining model selection with Lepski's
method. We show that the fully data-driven estimator preserves minimax
optimality over a wide range of classes for $f$ and $A$ and noise levels
$\varepsilon$ and $\sigma$. The results are illustrated considering Sobolev
spaces and mildly and severely ill-posed inverse problems."@2012
Song Yu-Ping@http://arxiv.org/abs/1204.1454v1@"Local linear estimator for stochastic differential equations driven by
  $α$-stable Lévy motions"@"We study the local linear estimator for the drift coefficient of stochastic
differential equations driven by $\alpha$-stable L\'{e}vy motions observed at
discrete instants letting $T \rightarrow \infty$. Under regular conditions, we
derive the weak consistency and central limit theorem of the estimator. Compare
with Nadaraya-Watson estimator, the local linear estimator has a bias reduction
whether kernel function is symmetric or not under different schemes."@2012
Lin Zheng-Yan@http://arxiv.org/abs/1204.1454v1@"Local linear estimator for stochastic differential equations driven by
  $α$-stable Lévy motions"@"We study the local linear estimator for the drift coefficient of stochastic
differential equations driven by $\alpha$-stable L\'{e}vy motions observed at
discrete instants letting $T \rightarrow \infty$. Under regular conditions, we
derive the weak consistency and central limit theorem of the estimator. Compare
with Nadaraya-Watson estimator, the local linear estimator has a bias reduction
whether kernel function is symmetric or not under different schemes."@2012
Sonia Petrone@http://arxiv.org/abs/1204.1470v1@Bayes and empirical Bayes: do they merge?@"Bayesian inference is attractive for its coherence and good frequentist
properties. However, it is a common experience that eliciting a honest prior
may be difficult and, in practice, people often take an {\em empirical Bayes}
approach, plugging empirical estimates of the prior hyperparameters into the
posterior distribution. Even if not rigorously justified, the underlying idea
is that, when the sample size is large, empirical Bayes leads to ""similar""
inferential answers. Yet, precise mathematical results seem to be missing. In
this work, we give a more rigorous justification in terms of merging of Bayes
and empirical Bayes posterior distributions. We consider two notions of
merging: Bayesian weak merging and frequentist merging in total variation.
Since weak merging is related to consistency, we provide sufficient conditions
for consistency of empirical Bayes posteriors. Also, we show that, under
regularity conditions, the empirical Bayes procedure asymptotically selects the
value of the hyperparameter for which the prior mostly favors the ""truth"".
Examples include empirical Bayes density estimation with Dirichlet process
mixtures."@2012
Judith Rousseau@http://arxiv.org/abs/1204.1470v1@Bayes and empirical Bayes: do they merge?@"Bayesian inference is attractive for its coherence and good frequentist
properties. However, it is a common experience that eliciting a honest prior
may be difficult and, in practice, people often take an {\em empirical Bayes}
approach, plugging empirical estimates of the prior hyperparameters into the
posterior distribution. Even if not rigorously justified, the underlying idea
is that, when the sample size is large, empirical Bayes leads to ""similar""
inferential answers. Yet, precise mathematical results seem to be missing. In
this work, we give a more rigorous justification in terms of merging of Bayes
and empirical Bayes posterior distributions. We consider two notions of
merging: Bayesian weak merging and frequentist merging in total variation.
Since weak merging is related to consistency, we provide sufficient conditions
for consistency of empirical Bayes posteriors. Also, we show that, under
regularity conditions, the empirical Bayes procedure asymptotically selects the
value of the hyperparameter for which the prior mostly favors the ""truth"".
Examples include empirical Bayes density estimation with Dirichlet process
mixtures."@2012
Catia Scricciolo@http://arxiv.org/abs/1204.1470v1@Bayes and empirical Bayes: do they merge?@"Bayesian inference is attractive for its coherence and good frequentist
properties. However, it is a common experience that eliciting a honest prior
may be difficult and, in practice, people often take an {\em empirical Bayes}
approach, plugging empirical estimates of the prior hyperparameters into the
posterior distribution. Even if not rigorously justified, the underlying idea
is that, when the sample size is large, empirical Bayes leads to ""similar""
inferential answers. Yet, precise mathematical results seem to be missing. In
this work, we give a more rigorous justification in terms of merging of Bayes
and empirical Bayes posterior distributions. We consider two notions of
merging: Bayesian weak merging and frequentist merging in total variation.
Since weak merging is related to consistency, we provide sufficient conditions
for consistency of empirical Bayes posteriors. Also, we show that, under
regularity conditions, the empirical Bayes procedure asymptotically selects the
value of the hyperparameter for which the prior mostly favors the ""truth"".
Examples include empirical Bayes density estimation with Dirichlet process
mixtures."@2012
Mohamed Hebiri@http://arxiv.org/abs/1204.1605v2@How Correlations Influence Lasso Prediction@"We study how correlations in the design matrix influence Lasso prediction.
First, we argue that the higher the correlations are, the smaller the optimal
tuning parameter is. This implies in particular that the standard tuning
parameters, that do not depend on the design matrix, are not favorable.
Furthermore, we argue that Lasso prediction works well for any degree of
correlations if suitable tuning parameters are chosen. We study these two
subjects theoretically as well as with simulations."@2012
Johannes C. Lederer@http://arxiv.org/abs/1204.1605v2@How Correlations Influence Lasso Prediction@"We study how correlations in the design matrix influence Lasso prediction.
First, we argue that the higher the correlations are, the smaller the optimal
tuning parameter is. This implies in particular that the standard tuning
parameters, that do not depend on the design matrix, are not favorable.
Furthermore, we argue that Lasso prediction works well for any degree of
correlations if suitable tuning parameters are chosen. We study these two
subjects theoretically as well as with simulations."@2012
João Renato Sebastião@http://arxiv.org/abs/1204.1905v1@Estimating the Upcrossings Index@"For stationary sequences, under general local and asymptotic dependence
restrictions, any limiting point process for time normalized upcrossings of
high levels is a compound Poisson process, i.e., there is a clustering of high
upcrossings, where the underlying Poisson points represent cluster positions,
and the multiplicities correspond to cluster sizes. For such classes of
stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq
1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq
1,$ for suitable high levels. In this paper we consider the problem of
estimating the upcrossings index $\eta$ for a class of stationary sequences
satisfying a mild oscillation restriction. For the proposed estimator,
properties such as consistency and asymptotic normality are studied. Finally,
the performance of the estimator is assessed through simulation studies for
autoregressive processes and case studies in the fields of environment and
finance."@2012
Ana Paula Martins@http://arxiv.org/abs/1204.1905v1@Estimating the Upcrossings Index@"For stationary sequences, under general local and asymptotic dependence
restrictions, any limiting point process for time normalized upcrossings of
high levels is a compound Poisson process, i.e., there is a clustering of high
upcrossings, where the underlying Poisson points represent cluster positions,
and the multiplicities correspond to cluster sizes. For such classes of
stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq
1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq
1,$ for suitable high levels. In this paper we consider the problem of
estimating the upcrossings index $\eta$ for a class of stationary sequences
satisfying a mild oscillation restriction. For the proposed estimator,
properties such as consistency and asymptotic normality are studied. Finally,
the performance of the estimator is assessed through simulation studies for
autoregressive processes and case studies in the fields of environment and
finance."@2012
Helena Ferreira@http://arxiv.org/abs/1204.1905v1@Estimating the Upcrossings Index@"For stationary sequences, under general local and asymptotic dependence
restrictions, any limiting point process for time normalized upcrossings of
high levels is a compound Poisson process, i.e., there is a clustering of high
upcrossings, where the underlying Poisson points represent cluster positions,
and the multiplicities correspond to cluster sizes. For such classes of
stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq
1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq
1,$ for suitable high levels. In this paper we consider the problem of
estimating the upcrossings index $\eta$ for a class of stationary sequences
satisfying a mild oscillation restriction. For the proposed estimator,
properties such as consistency and asymptotic normality are studied. Finally,
the performance of the estimator is assessed through simulation studies for
autoregressive processes and case studies in the fields of environment and
finance."@2012
Luísa Pereira@http://arxiv.org/abs/1204.1905v1@Estimating the Upcrossings Index@"For stationary sequences, under general local and asymptotic dependence
restrictions, any limiting point process for time normalized upcrossings of
high levels is a compound Poisson process, i.e., there is a clustering of high
upcrossings, where the underlying Poisson points represent cluster positions,
and the multiplicities correspond to cluster sizes. For such classes of
stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq
1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq
1,$ for suitable high levels. In this paper we consider the problem of
estimating the upcrossings index $\eta$ for a class of stationary sequences
satisfying a mild oscillation restriction. For the proposed estimator,
properties such as consistency and asymptotic normality are studied. Finally,
the performance of the estimator is assessed through simulation studies for
autoregressive processes and case studies in the fields of environment and
finance."@2012
Julyan Arbel@http://arxiv.org/abs/1204.2392v2@Bayesian optimal adaptive estimation using a sieve prior@"We derive rates of contraction of posterior distributions on nonparametric
models resulting from sieve priors. The aim of the paper is to provide general
conditions to get posterior rates when the parameter space has a general
structure, and rate adaptation when the parameter space is, e.g., a Sobolev
class. The conditions employed, although standard in the literature, are
combined in a different way. The results are applied to density, regression,
nonlinear autoregression and Gaussian white noise models. In the latter we have
also considered a loss function which is different from the usual l2 norm,
namely the pointwise loss. In this case it is possible to prove that the
adaptive Bayesian approach for the l2 loss is strongly suboptimal and we
provide a lower bound on the rate."@2012
Ghislaine Gayraud@http://arxiv.org/abs/1204.2392v2@Bayesian optimal adaptive estimation using a sieve prior@"We derive rates of contraction of posterior distributions on nonparametric
models resulting from sieve priors. The aim of the paper is to provide general
conditions to get posterior rates when the parameter space has a general
structure, and rate adaptation when the parameter space is, e.g., a Sobolev
class. The conditions employed, although standard in the literature, are
combined in a different way. The results are applied to density, regression,
nonlinear autoregression and Gaussian white noise models. In the latter we have
also considered a loss function which is different from the usual l2 norm,
namely the pointwise loss. In this case it is possible to prove that the
adaptive Bayesian approach for the l2 loss is strongly suboptimal and we
provide a lower bound on the rate."@2012
Judith Rousseau@http://arxiv.org/abs/1204.2392v2@Bayesian optimal adaptive estimation using a sieve prior@"We derive rates of contraction of posterior distributions on nonparametric
models resulting from sieve priors. The aim of the paper is to provide general
conditions to get posterior rates when the parameter space has a general
structure, and rate adaptation when the parameter space is, e.g., a Sobolev
class. The conditions employed, although standard in the literature, are
combined in a different way. The results are applied to density, regression,
nonlinear autoregression and Gaussian white noise models. In the latter we have
also considered a loss function which is different from the usual l2 norm,
namely the pointwise loss. In this case it is possible to prove that the
adaptive Bayesian approach for the l2 loss is strongly suboptimal and we
provide a lower bound on the rate."@2012
Bin Nan@http://arxiv.org/abs/1204.2579v1@A general semiparametric Z-estimation approach for case-cohort studies@"Case-cohort design, an outcome-dependent sampling design for censored
survival data, is increasingly used in biomedical research. The development of
asymptotic theory for a case-cohort design in the current literature primarily
relies on counting process stochastic integrals. Such an approach, however, is
rather limited and lacks theoretical justification for outcome-dependent
weighted methods due to non-predictability. Instead of stochastic integrals, we
derive asymptotic properties for case-cohort studies based on a general
Z-estimation theory for semiparametric models with bundled parameters using
modern empirical processes. Both the Cox model and the additive hazards model
with time-dependent covariates are considered."@2012
Jon A. Wellner@http://arxiv.org/abs/1204.2579v1@A general semiparametric Z-estimation approach for case-cohort studies@"Case-cohort design, an outcome-dependent sampling design for censored
survival data, is increasingly used in biomedical research. The development of
asymptotic theory for a case-cohort design in the current literature primarily
relies on counting process stochastic integrals. Such an approach, however, is
rather limited and lacks theoretical justification for outcome-dependent
weighted methods due to non-predictability. Instead of stochastic integrals, we
derive asymptotic properties for case-cohort studies based on a general
Z-estimation theory for semiparametric models with bundled parameters using
modern empirical processes. Both the Cox model and the additive hazards model
with time-dependent covariates are considered."@2012
Thibault Espinasse@http://arxiv.org/abs/1204.2763v1@A Cramér-Rao inequality for non differentiable models@"We compute a variance lower bound for unbiased estimators in specified
statistical models. The construction of the bound is related to the original
Cram\'er-Rao bound, although it does not require the differentiability of the
model. Moreover, we show our efficiency bound to be always greater than the
Cram\'er-Rao bound in smooth models, thus providing a sharper result."@2012
Paul Rochet@http://arxiv.org/abs/1204.2763v1@A Cramér-Rao inequality for non differentiable models@"We compute a variance lower bound for unbiased estimators in specified
statistical models. The construction of the bound is related to the original
Cram\'er-Rao bound, although it does not require the differentiability of the
model. Moreover, we show our efficiency bound to be always greater than the
Cram\'er-Rao bound in smooth models, thus providing a sharper result."@2012
S. Delattre@http://arxiv.org/abs/1204.2964v1@"Blockwise SVD with error in the operator and application to blind
  deconvolution"@"We consider linear inverse problems in a nonparametric statistical framework.
Both the signal and the operator are unknown and subject to error measurements.
We establish minimax rates of convergence under squared error loss when the
operator admits a blockwise singular value decomposition (blockwise SVD) and
the smoothness of the signal is measured in a Sobolev sense. We construct a
nonlinear procedure adapting simultaneously to the unknown smoothness of both
the signal and the operator and achieving the optimal rate of convergence to
within logarithmic terms. When the noise level in the operator is dominant, by
taking full advantage of the blockwise SVD property, we demonstrate that the
block SVD procedure overperforms classical methods based on Galerkin projection
or nonlinear wavelet thresholding. We subsequently apply our abstract framework
to the specific case of blind deconvolution on the torus and on the sphere."@2012
M. Hoffmann@http://arxiv.org/abs/1204.2964v1@"Blockwise SVD with error in the operator and application to blind
  deconvolution"@"We consider linear inverse problems in a nonparametric statistical framework.
Both the signal and the operator are unknown and subject to error measurements.
We establish minimax rates of convergence under squared error loss when the
operator admits a blockwise singular value decomposition (blockwise SVD) and
the smoothness of the signal is measured in a Sobolev sense. We construct a
nonlinear procedure adapting simultaneously to the unknown smoothness of both
the signal and the operator and achieving the optimal rate of convergence to
within logarithmic terms. When the noise level in the operator is dominant, by
taking full advantage of the blockwise SVD property, we demonstrate that the
block SVD procedure overperforms classical methods based on Galerkin projection
or nonlinear wavelet thresholding. We subsequently apply our abstract framework
to the specific case of blind deconvolution on the torus and on the sphere."@2012
D. Picard@http://arxiv.org/abs/1204.2964v1@"Blockwise SVD with error in the operator and application to blind
  deconvolution"@"We consider linear inverse problems in a nonparametric statistical framework.
Both the signal and the operator are unknown and subject to error measurements.
We establish minimax rates of convergence under squared error loss when the
operator admits a blockwise singular value decomposition (blockwise SVD) and
the smoothness of the signal is measured in a Sobolev sense. We construct a
nonlinear procedure adapting simultaneously to the unknown smoothness of both
the signal and the operator and achieving the optimal rate of convergence to
within logarithmic terms. When the noise level in the operator is dominant, by
taking full advantage of the blockwise SVD property, we demonstrate that the
block SVD procedure overperforms classical methods based on Galerkin projection
or nonlinear wavelet thresholding. We subsequently apply our abstract framework
to the specific case of blind deconvolution on the torus and on the sphere."@2012
T. Vareschi@http://arxiv.org/abs/1204.2964v1@"Blockwise SVD with error in the operator and application to blind
  deconvolution"@"We consider linear inverse problems in a nonparametric statistical framework.
Both the signal and the operator are unknown and subject to error measurements.
We establish minimax rates of convergence under squared error loss when the
operator admits a blockwise singular value decomposition (blockwise SVD) and
the smoothness of the signal is measured in a Sobolev sense. We construct a
nonlinear procedure adapting simultaneously to the unknown smoothness of both
the signal and the operator and achieving the optimal rate of convergence to
within logarithmic terms. When the noise level in the operator is dominant, by
taking full advantage of the blockwise SVD property, we demonstrate that the
block SVD procedure overperforms classical methods based on Galerkin projection
or nonlinear wavelet thresholding. We subsequently apply our abstract framework
to the specific case of blind deconvolution on the torus and on the sphere."@2012
Evgueni Haroutunian@http://arxiv.org/abs/1204.2975v1@"Multiple Objects: Error Exponents in Hypotheses Testing and
  Identification"@"We servey a series of investigations of optimal testing of multiple
hypotheses conserning various multiobject models. These studies are a bright
instance of application of methods and technics developed in Shannon
information theory to solution of typical statistical problems."@2012
Parandzem Hakobyan@http://arxiv.org/abs/1204.2975v1@"Multiple Objects: Error Exponents in Hypotheses Testing and
  Identification"@"We servey a series of investigations of optimal testing of multiple
hypotheses conserning various multiobject models. These studies are a bright
instance of application of methods and technics developed in Shannon
information theory to solution of typical statistical problems."@2012
Davy Paindaveine@http://arxiv.org/abs/1204.2996v2@Nonparametrically consistent depth-based classifiers@"We introduce a class of depth-based classification procedures that are of a
nearest-neighbor nature. Depth, after symmetrization, indeed provides the
center-outward ordering that is necessary and sufficient to define nearest
neighbors. Like all their depth-based competitors, the resulting classifiers
are affine-invariant, hence in particular are insensitive to unit changes.
Unlike the former, however, the latter achieve Bayes consistency under
virtually any absolutely continuous distributions - a concept we call
nonparametric consistency, to stress the difference with the stronger universal
consistency of the standard $k$NN classifiers. We investigate the finite-sample
performances of the proposed classifiers through simulations and show that they
outperform affine-invariant nearest-neighbor classifiers obtained through an
obvious standardization construction. We illustrate the practical value of our
classifiers on two real data examples. Finally, we shortly discuss the possible
uses of our depth-based neighbors in other inference problems."@2012
Germain Van Bever@http://arxiv.org/abs/1204.2996v2@Nonparametrically consistent depth-based classifiers@"We introduce a class of depth-based classification procedures that are of a
nearest-neighbor nature. Depth, after symmetrization, indeed provides the
center-outward ordering that is necessary and sufficient to define nearest
neighbors. Like all their depth-based competitors, the resulting classifiers
are affine-invariant, hence in particular are insensitive to unit changes.
Unlike the former, however, the latter achieve Bayes consistency under
virtually any absolutely continuous distributions - a concept we call
nonparametric consistency, to stress the difference with the stronger universal
consistency of the standard $k$NN classifiers. We investigate the finite-sample
performances of the proposed classifiers through simulations and show that they
outperform affine-invariant nearest-neighbor classifiers obtained through an
obvious standardization construction. We illustrate the practical value of our
classifiers on two real data examples. Finally, we shortly discuss the possible
uses of our depth-based neighbors in other inference problems."@2012
Constantinos Georghiou@http://arxiv.org/abs/1204.3186v4@On the modes of the Poisson distribution of order k@"Sharp upper and lower bounds are established for the modes of the Poisson
distribution of order k. The lower bound established in this paper is better
than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a
recent conjecture is presently proved solving partially an open problem since
1983."@2012
Andreas N. Philippou@http://arxiv.org/abs/1204.3186v4@On the modes of the Poisson distribution of order k@"Sharp upper and lower bounds are established for the modes of the Poisson
distribution of order k. The lower bound established in this paper is better
than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a
recent conjecture is presently proved solving partially an open problem since
1983."@2012
Abolfazl Saghafi@http://arxiv.org/abs/1204.3186v4@On the modes of the Poisson distribution of order k@"Sharp upper and lower bounds are established for the modes of the Poisson
distribution of order k. The lower bound established in this paper is better
than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a
recent conjecture is presently proved solving partially an open problem since
1983."@2012
Samuel Vaiter@http://arxiv.org/abs/1204.3212v2@"Local Behavior of Sparse Analysis Regularization: Applications to Risk
  Estimation"@"In this paper, we aim at recovering an unknown signal x0 from noisy
L1measurements y=Phi*x0+w, where Phi is an ill-conditioned or singular linear
operator and w accounts for some noise. To regularize such an ill-posed inverse
problem, we impose an analysis sparsity prior. More precisely, the recovery is
cast as a convex optimization program where the objective is the sum of a
quadratic data fidelity term and a regularization term formed of the L1-norm of
the correlations between the sought after signal and atoms in a given
(generally overcomplete) dictionary. The L1-sparsity analysis prior is weighted
by a regularization parameter lambda>0. In this paper, we prove that any
minimizers of this problem is a piecewise-affine function of the observations y
and the regularization parameter lambda. As a byproduct, we exploit these
properties to get an objectively guided choice of lambda. In particular, we
develop an extension of the Generalized Stein Unbiased Risk Estimator (GSURE)
and show that it is an unbiased and reliable estimator of an appropriately
defined risk. The latter encompasses special cases such as the prediction risk,
the projection risk and the estimation risk. We apply these risk estimators to
the special case of L1-sparsity analysis regularization. We also discuss
implementation issues and propose fast algorithms to solve the L1 analysis
minimization problem and to compute the associated GSURE. We finally illustrate
the applicability of our framework to parameter(s) selection on several imaging
problems."@2012
Charles Deledalle@http://arxiv.org/abs/1204.3212v2@"Local Behavior of Sparse Analysis Regularization: Applications to Risk
  Estimation"@"In this paper, we aim at recovering an unknown signal x0 from noisy
L1measurements y=Phi*x0+w, where Phi is an ill-conditioned or singular linear
operator and w accounts for some noise. To regularize such an ill-posed inverse
problem, we impose an analysis sparsity prior. More precisely, the recovery is
cast as a convex optimization program where the objective is the sum of a
quadratic data fidelity term and a regularization term formed of the L1-norm of
the correlations between the sought after signal and atoms in a given
(generally overcomplete) dictionary. The L1-sparsity analysis prior is weighted
by a regularization parameter lambda>0. In this paper, we prove that any
minimizers of this problem is a piecewise-affine function of the observations y
and the regularization parameter lambda. As a byproduct, we exploit these
properties to get an objectively guided choice of lambda. In particular, we
develop an extension of the Generalized Stein Unbiased Risk Estimator (GSURE)
and show that it is an unbiased and reliable estimator of an appropriately
defined risk. The latter encompasses special cases such as the prediction risk,
the projection risk and the estimation risk. We apply these risk estimators to
the special case of L1-sparsity analysis regularization. We also discuss
implementation issues and propose fast algorithms to solve the L1 analysis
minimization problem and to compute the associated GSURE. We finally illustrate
the applicability of our framework to parameter(s) selection on several imaging
problems."@2012
Gabriel Peyré@http://arxiv.org/abs/1204.3212v2@"Local Behavior of Sparse Analysis Regularization: Applications to Risk
  Estimation"@"In this paper, we aim at recovering an unknown signal x0 from noisy
L1measurements y=Phi*x0+w, where Phi is an ill-conditioned or singular linear
operator and w accounts for some noise. To regularize such an ill-posed inverse
problem, we impose an analysis sparsity prior. More precisely, the recovery is
cast as a convex optimization program where the objective is the sum of a
quadratic data fidelity term and a regularization term formed of the L1-norm of
the correlations between the sought after signal and atoms in a given
(generally overcomplete) dictionary. The L1-sparsity analysis prior is weighted
by a regularization parameter lambda>0. In this paper, we prove that any
minimizers of this problem is a piecewise-affine function of the observations y
and the regularization parameter lambda. As a byproduct, we exploit these
properties to get an objectively guided choice of lambda. In particular, we
develop an extension of the Generalized Stein Unbiased Risk Estimator (GSURE)
and show that it is an unbiased and reliable estimator of an appropriately
defined risk. The latter encompasses special cases such as the prediction risk,
the projection risk and the estimation risk. We apply these risk estimators to
the special case of L1-sparsity analysis regularization. We also discuss
implementation issues and propose fast algorithms to solve the L1 analysis
minimization problem and to compute the associated GSURE. We finally illustrate
the applicability of our framework to parameter(s) selection on several imaging
problems."@2012
Charles Dossal@http://arxiv.org/abs/1204.3212v2@"Local Behavior of Sparse Analysis Regularization: Applications to Risk
  Estimation"@"In this paper, we aim at recovering an unknown signal x0 from noisy
L1measurements y=Phi*x0+w, where Phi is an ill-conditioned or singular linear
operator and w accounts for some noise. To regularize such an ill-posed inverse
problem, we impose an analysis sparsity prior. More precisely, the recovery is
cast as a convex optimization program where the objective is the sum of a
quadratic data fidelity term and a regularization term formed of the L1-norm of
the correlations between the sought after signal and atoms in a given
(generally overcomplete) dictionary. The L1-sparsity analysis prior is weighted
by a regularization parameter lambda>0. In this paper, we prove that any
minimizers of this problem is a piecewise-affine function of the observations y
and the regularization parameter lambda. As a byproduct, we exploit these
properties to get an objectively guided choice of lambda. In particular, we
develop an extension of the Generalized Stein Unbiased Risk Estimator (GSURE)
and show that it is an unbiased and reliable estimator of an appropriately
defined risk. The latter encompasses special cases such as the prediction risk,
the projection risk and the estimation risk. We apply these risk estimators to
the special case of L1-sparsity analysis regularization. We also discuss
implementation issues and propose fast algorithms to solve the L1 analysis
minimization problem and to compute the associated GSURE. We finally illustrate
the applicability of our framework to parameter(s) selection on several imaging
problems."@2012
Jalal Fadili@http://arxiv.org/abs/1204.3212v2@"Local Behavior of Sparse Analysis Regularization: Applications to Risk
  Estimation"@"In this paper, we aim at recovering an unknown signal x0 from noisy
L1measurements y=Phi*x0+w, where Phi is an ill-conditioned or singular linear
operator and w accounts for some noise. To regularize such an ill-posed inverse
problem, we impose an analysis sparsity prior. More precisely, the recovery is
cast as a convex optimization program where the objective is the sum of a
quadratic data fidelity term and a regularization term formed of the L1-norm of
the correlations between the sought after signal and atoms in a given
(generally overcomplete) dictionary. The L1-sparsity analysis prior is weighted
by a regularization parameter lambda>0. In this paper, we prove that any
minimizers of this problem is a piecewise-affine function of the observations y
and the regularization parameter lambda. As a byproduct, we exploit these
properties to get an objectively guided choice of lambda. In particular, we
develop an extension of the Generalized Stein Unbiased Risk Estimator (GSURE)
and show that it is an unbiased and reliable estimator of an appropriately
defined risk. The latter encompasses special cases such as the prediction risk,
the projection risk and the estimation risk. We apply these risk estimators to
the special case of L1-sparsity analysis regularization. We also discuss
implementation issues and propose fast algorithms to solve the L1 analysis
minimization problem and to compute the associated GSURE. We finally illustrate
the applicability of our framework to parameter(s) selection on several imaging
problems."@2012
Hervé Cardot@http://arxiv.org/abs/1204.3213v1@"Recursive estimation of the conditional geometric median in Hilbert
  spaces"@"A recursive estimator of the conditional geometric median in Hilbert spaces
is studied. It is based on a stochastic gradient algorithm whose aim is to
minimize a weighted L1 criterion and is consequently well adapted for robust
online estimation. The weights are controlled by a kernel function and an
associated bandwidth. Almost sure convergence and L2 rates of convergence are
proved under general conditions on the conditional distribution as well as the
sequence of descent steps of the algorithm and the sequence of bandwidths.
Asymptotic normality is also proved for the averaged version of the algorithm
with an optimal rate of convergence. A simulation study confirms the interest
of this new and fast algorithm when the sample sizes are large. Finally, the
ability of these recursive algorithms to deal with very high-dimensional data
is illustrated on the robust estimation of television audience profiles
conditional on the total time spent watching television over a period of 24
hours."@2012
Peggy Cénac@http://arxiv.org/abs/1204.3213v1@"Recursive estimation of the conditional geometric median in Hilbert
  spaces"@"A recursive estimator of the conditional geometric median in Hilbert spaces
is studied. It is based on a stochastic gradient algorithm whose aim is to
minimize a weighted L1 criterion and is consequently well adapted for robust
online estimation. The weights are controlled by a kernel function and an
associated bandwidth. Almost sure convergence and L2 rates of convergence are
proved under general conditions on the conditional distribution as well as the
sequence of descent steps of the algorithm and the sequence of bandwidths.
Asymptotic normality is also proved for the averaged version of the algorithm
with an optimal rate of convergence. A simulation study confirms the interest
of this new and fast algorithm when the sample sizes are large. Finally, the
ability of these recursive algorithms to deal with very high-dimensional data
is illustrated on the robust estimation of television audience profiles
conditional on the total time spent watching television over a period of 24
hours."@2012
Pierre-André Zitt@http://arxiv.org/abs/1204.3213v1@"Recursive estimation of the conditional geometric median in Hilbert
  spaces"@"A recursive estimator of the conditional geometric median in Hilbert spaces
is studied. It is based on a stochastic gradient algorithm whose aim is to
minimize a weighted L1 criterion and is consequently well adapted for robust
online estimation. The weights are controlled by a kernel function and an
associated bandwidth. Almost sure convergence and L2 rates of convergence are
proved under general conditions on the conditional distribution as well as the
sequence of descent steps of the algorithm and the sequence of bandwidths.
Asymptotic normality is also proved for the averaged version of the algorithm
with an optimal rate of convergence. A simulation study confirms the interest
of this new and fast algorithm when the sample sizes are large. Finally, the
ability of these recursive algorithms to deal with very high-dimensional data
is illustrated on the robust estimation of television audience profiles
conditional on the total time spent watching television over a period of 24
hours."@2012
Richard A. Davis@http://arxiv.org/abs/1204.3915v1@"Theory and Inference for a Class of Observation-driven Models with
  Application to Time Series of Counts"@"This paper studies theory and inference related to a class of time series
models that incorporates nonlinear dynamics. It is assumed that the
observations follow a one-parameter exponential family of distributions given
an accompanying process that evolves as a function of lagged observations. We
employ an iterated random function approach and a special coupling technique to
show that, under suitable conditions on the parameter space, the conditional
mean process is a geometric moment contracting Markov chain and that the
observation process is absolutely regular with geometrically decaying
coefficients. Moreover the asymptotic theory of the maximum likelihood
estimates of the parameters is established under some mild assumptions. These
models are applied to two examples; the first is the number of transactions per
minute of Ericsson stock and the second is related to return times of extreme
events of Goldman Sachs Group stock."@2012
Heng Liu@http://arxiv.org/abs/1204.3915v1@"Theory and Inference for a Class of Observation-driven Models with
  Application to Time Series of Counts"@"This paper studies theory and inference related to a class of time series
models that incorporates nonlinear dynamics. It is assumed that the
observations follow a one-parameter exponential family of distributions given
an accompanying process that evolves as a function of lagged observations. We
employ an iterated random function approach and a special coupling technique to
show that, under suitable conditions on the parameter space, the conditional
mean process is a geometric moment contracting Markov chain and that the
observation process is absolutely regular with geometrically decaying
coefficients. Moreover the asymptotic theory of the maximum likelihood
estimates of the parameters is established under some mild assumptions. These
models are applied to two examples; the first is the number of transactions per
minute of Ericsson stock and the second is related to return times of extreme
events of Goldman Sachs Group stock."@2012
Silvia L. P. Ferrari@http://arxiv.org/abs/1204.3949v3@Small-sample likelihood inference in extreme-value regression models@"We deal with a general class of extreme-value regression models introduced by
Barreto- Souza and Vasconcellos (2011). Our goal is to derive an adjusted
likelihood ratio statistic that is approximately distributed as \c{hi}2 with a
high degree of accuracy. Although the adjusted statistic requires more
computational effort than its unadjusted counterpart, it is shown that the
adjustment term has a simple compact form that can be easily implemented in
standard statistical software. Further, we compare the finite sample
performance of the three classical tests (likelihood ratio, Wald, and score),
the gradient test that has been recently proposed by Terrell (2002), and the
adjusted likelihood ratio test obtained in this paper. Our simulations favor
the latter. Applications of our results are presented. Key words: Extreme-value
regression; Gradient test; Gumbel distribution; Likelihood ratio test;
Nonlinear models; Score test; Small-sample adjustments; Wald test."@2012
Eliane C. Pinheiro@http://arxiv.org/abs/1204.3949v3@Small-sample likelihood inference in extreme-value regression models@"We deal with a general class of extreme-value regression models introduced by
Barreto- Souza and Vasconcellos (2011). Our goal is to derive an adjusted
likelihood ratio statistic that is approximately distributed as \c{hi}2 with a
high degree of accuracy. Although the adjusted statistic requires more
computational effort than its unadjusted counterpart, it is shown that the
adjustment term has a simple compact form that can be easily implemented in
standard statistical software. Further, we compare the finite sample
performance of the three classical tests (likelihood ratio, Wald, and score),
the gradient test that has been recently proposed by Terrell (2002), and the
adjusted likelihood ratio test obtained in this paper. Our simulations favor
the latter. Applications of our results are presented. Key words: Extreme-value
regression; Gradient test; Gumbel distribution; Likelihood ratio test;
Nonlinear models; Score test; Small-sample adjustments; Wald test."@2012
Xianyang Zhang@http://arxiv.org/abs/1204.4228v2@Fixed-smoothing asymptotics for time series@"In this paper, we derive higher order Edgeworth expansions for the finite
sample distributions of the subsampling-based t-statistic and the Wald
statistic in the Gaussian location model under the so-called fixed-smoothing
paradigm. In particular, we show that the error of asymptotic approximation is
at the order of the reciprocal of the sample size and obtain explicit forms for
the leading error terms in the expansions. The results are used to justify the
second-order correctness of a new bootstrap method, the Gaussian dependent
bootstrap, in the context of Gaussian location model."@2012
Xiaofeng Shao@http://arxiv.org/abs/1204.4228v2@Fixed-smoothing asymptotics for time series@"In this paper, we derive higher order Edgeworth expansions for the finite
sample distributions of the subsampling-based t-statistic and the Wald
statistic in the Gaussian location model under the so-called fixed-smoothing
paradigm. In particular, we show that the error of asymptotic approximation is
at the order of the reciprocal of the sample size and obtain explicit forms for
the leading error terms in the expansions. The results are used to justify the
second-order correctness of a new bootstrap method, the Gaussian dependent
bootstrap, in the context of Gaussian location model."@2012
Weining Shen@http://arxiv.org/abs/1204.4238v2@MCMC-free adaptive Bayesian procedures using random series prior@"We consider priors for several nonparametric Bayesian models which use finite
random series with a random number of terms. The prior is constructed through
distributions on the number of basis functions and the associated coefficients.
We derive a general result on the construction of an appropriate sieve and
obtain adaptive posterior contraction rates for all smoothness levels of the
function in the true model. We apply this general result on several statistical
problems such as signal processing, density estimation, nonparametric additive
regression, classification, spectral density estimation, functional regression
etc. The prior can be viewed as an alternative to commonly used Gaussian
process prior, but can be analyzed by relatively simpler techniques and in many
cases allows a simpler approach to computation without using Markov chain
Monte-Carlo (MCMC) methods. A simulation study was conducted to show that the
performance of the random series prior is comparable to that of a Gaussian
process prior."@2012
Subhashis Ghosal@http://arxiv.org/abs/1204.4238v2@MCMC-free adaptive Bayesian procedures using random series prior@"We consider priors for several nonparametric Bayesian models which use finite
random series with a random number of terms. The prior is constructed through
distributions on the number of basis functions and the associated coefficients.
We derive a general result on the construction of an appropriate sieve and
obtain adaptive posterior contraction rates for all smoothness levels of the
function in the true model. We apply this general result on several statistical
problems such as signal processing, density estimation, nonparametric additive
regression, classification, spectral density estimation, functional regression
etc. The prior can be viewed as an alternative to commonly used Gaussian
process prior, but can be analyzed by relatively simpler techniques and in many
cases allows a simpler approach to computation without using Markov chain
Monte-Carlo (MCMC) methods. A simulation study was conducted to show that the
performance of the random series prior is comparable to that of a Gaussian
process prior."@2012
Victor I. Ivanenko@http://arxiv.org/abs/1204.4440v6@On regularities of mass random phenomena@"This paper contains an answer to the question of existence of regularities of
the so called \textit{random in a broad sense} mass phenomena, asked by A. N.
Kolmogorov in \cite{Kolmogorov}. It turns out that some family of
finitely-additive probabilities is the statistical regularity of any such
phenomenon. If the mass phenomenon is stochastic, then this family degenerates
into a single probability measure. The paper provides definitions, the
formulation and the proof of the theorem of existence of statistical
regularities, as well as the examples of their application."@2012
Valery A. Labkovsky@http://arxiv.org/abs/1204.4440v6@On regularities of mass random phenomena@"This paper contains an answer to the question of existence of regularities of
the so called \textit{random in a broad sense} mass phenomena, asked by A. N.
Kolmogorov in \cite{Kolmogorov}. It turns out that some family of
finitely-additive probabilities is the statistical regularity of any such
phenomenon. If the mass phenomenon is stochastic, then this family degenerates
into a single probability measure. The paper provides definitions, the
formulation and the proof of the theorem of existence of statistical
regularities, as well as the examples of their application."@2012
Arnold Janssen@http://arxiv.org/abs/1204.4611v1@Applications of the Likelihood Theory in Finance: Modelling and Pricing@"This paper discusses the connection between mathematical finance and
statistical modelling which turns out to be more than a formal mathematical
correspondence. We like to figure out how common results and notions in
statistics and their meaning can be translated to the world of mathematical
finance and vice versa. A lot of similarities can be expressed in terms of
LeCam's theory for statistical experiments which is the theory of the behaviour
of likelihood processes. For positive prices the arbitrage free financial
assets fit into filtered experiments. It is shown that they are given by
filtered likelihood ratio processes. From the statistical point of view,
martingale measures, completeness and pricing formulas are revisited. The
pricing formulas for various options are connected with the power functions of
tests. For instance the Black-Scholes price of a European option has an
interpretation as Bayes risk of a Neyman Pearson test. Under contiguity the
convergence of financial experiments and option prices are obtained. In
particular, the approximation of Ito type price processes by discrete models
and the convergence of associated option prices is studied. The result relies
on the central limit theorem for statistical experiments, which is well known
in statistics in connection with local asymptotic normal (LAN) families. As
application certain continuous time option prices can be approximated by
related discrete time pricing formulas."@2012
Martin Tietje@http://arxiv.org/abs/1204.4611v1@Applications of the Likelihood Theory in Finance: Modelling and Pricing@"This paper discusses the connection between mathematical finance and
statistical modelling which turns out to be more than a formal mathematical
correspondence. We like to figure out how common results and notions in
statistics and their meaning can be translated to the world of mathematical
finance and vice versa. A lot of similarities can be expressed in terms of
LeCam's theory for statistical experiments which is the theory of the behaviour
of likelihood processes. For positive prices the arbitrage free financial
assets fit into filtered experiments. It is shown that they are given by
filtered likelihood ratio processes. From the statistical point of view,
martingale measures, completeness and pricing formulas are revisited. The
pricing formulas for various options are connected with the power functions of
tests. For instance the Black-Scholes price of a European option has an
interpretation as Bayes risk of a Neyman Pearson test. Under contiguity the
convergence of financial experiments and option prices are obtained. In
particular, the approximation of Ito type price processes by discrete models
and the convergence of associated option prices is studied. The result relies
on the central limit theorem for statistical experiments, which is well known
in statistics in connection with local asymptotic normal (LAN) families. As
application certain continuous time option prices can be approximated by
related discrete time pricing formulas."@2012
Jianqing Fan@http://arxiv.org/abs/1204.5536v2@Endogeneity in high dimensions@"Most papers on high-dimensional statistics are based on the assumption that
none of the regressors are correlated with the regression error, namely, they
are exogenous. Yet, endogeneity can arise incidentally from a large pool of
regressors in a high-dimensional regression. This causes the inconsistency of
the penalized least-squares method and possible false scientific discoveries. A
necessary condition for model selection consistency of a general class of
penalized regression methods is given, which allows us to prove formally the
inconsistency claim. To cope with the incidental endogeneity, we construct a
novel penalized focused generalized method of moments (FGMM) criterion
function. The FGMM effectively achieves the dimension reduction and applies the
instrumental variable methods. We show that it possesses the oracle property
even in the presence of endogenous predictors, and that the solution is also
near global minimum under the over-identification assumption. Finally, we also
show how the semi-parametric efficiency of estimation can be achieved via a
two-step approach."@2012
Yuan Liao@http://arxiv.org/abs/1204.5536v2@Endogeneity in high dimensions@"Most papers on high-dimensional statistics are based on the assumption that
none of the regressors are correlated with the regression error, namely, they
are exogenous. Yet, endogeneity can arise incidentally from a large pool of
regressors in a high-dimensional regression. This causes the inconsistency of
the penalized least-squares method and possible false scientific discoveries. A
necessary condition for model selection consistency of a general class of
penalized regression methods is given, which allows us to prove formally the
inconsistency claim. To cope with the incidental endogeneity, we construct a
novel penalized focused generalized method of moments (FGMM) criterion
function. The FGMM effectively achieves the dimension reduction and applies the
instrumental variable methods. We show that it possesses the oracle property
even in the presence of endogenous predictors, and that the solution is also
near global minimum under the over-identification assumption. Finally, we also
show how the semi-parametric efficiency of estimation can be achieved via a
two-step approach."@2012
Othmane Kortbi@http://arxiv.org/abs/1204.6054v1@"Estimation of a multivariate normal mean with a bounded signal to noise
  ratio"@"For normal canonical models with $X \sim N_p(\theta, \sigma^{2} I_{p}), \;\;
S^{2} \sim \sigma^{2}\chi^{2}_{k}, \;{independent}$, we consider the problem of
estimating $\theta$ under scale invariant squared error loss $\frac{\|d-\theta
\|^{2}}{\sigma^{2}}$, when it is known that the signal-to-noise ratio
$\frac{\|\theta\|}{\sigma}$ is bounded above by $m$. Risk analysis is achieved
by making use of a conditional risk decomposition and we obtain in particular
sufficient conditions for an estimator to dominate either the unbiased
estimator $\delta_{UB}(X)=X$, or the maximum likelihood estimator
$\delta_{\hbox{mle}}(X,S^2)$, or both of these benchmark procedures. The given
developments bring into play the pivotal role of the boundary Bayes estimator
$\delta_{BU}$ associated with a prior on $(\theta,\sigma)$ such that
$\theta|\sigma$ is uniformly distributed on the (boundary) sphere of radius $m$
and a non-informative $\frac{1}{\sigma}$ prior measure is placed marginally on
$\sigma$. With a series of technical results related to $\delta_{BU}$; which
relate to particular ratios of confluent hypergeometric functions; we show
that, whenever $m \leq \sqrt{p}$ and $p \geq 2$, $\delta_{BU}$ dominates both
$\delta_{UB}$ and $\delta_{\hbox{mle}}$. The finding can be viewed as both a
multivariate extension of $p=1$ result due to Kubokawa (2005) and a unknown
variance extension of a similar dominance finding due to Marchand and Perron
(2001). Various other dominance results are obtained, illustrations are
provided and commented upon. In particular, for $m \leq \sqrt{\frac{p}{2}}$, a
wide class of Bayes estimators, which include priors where $\theta|\sigma$ is
uniformly distributed on the ball of radius $m$, are shown to dominate
$\delta_{UB}$."@2012
Éric Marchand@http://arxiv.org/abs/1204.6054v1@"Estimation of a multivariate normal mean with a bounded signal to noise
  ratio"@"For normal canonical models with $X \sim N_p(\theta, \sigma^{2} I_{p}), \;\;
S^{2} \sim \sigma^{2}\chi^{2}_{k}, \;{independent}$, we consider the problem of
estimating $\theta$ under scale invariant squared error loss $\frac{\|d-\theta
\|^{2}}{\sigma^{2}}$, when it is known that the signal-to-noise ratio
$\frac{\|\theta\|}{\sigma}$ is bounded above by $m$. Risk analysis is achieved
by making use of a conditional risk decomposition and we obtain in particular
sufficient conditions for an estimator to dominate either the unbiased
estimator $\delta_{UB}(X)=X$, or the maximum likelihood estimator
$\delta_{\hbox{mle}}(X,S^2)$, or both of these benchmark procedures. The given
developments bring into play the pivotal role of the boundary Bayes estimator
$\delta_{BU}$ associated with a prior on $(\theta,\sigma)$ such that
$\theta|\sigma$ is uniformly distributed on the (boundary) sphere of radius $m$
and a non-informative $\frac{1}{\sigma}$ prior measure is placed marginally on
$\sigma$. With a series of technical results related to $\delta_{BU}$; which
relate to particular ratios of confluent hypergeometric functions; we show
that, whenever $m \leq \sqrt{p}$ and $p \geq 2$, $\delta_{BU}$ dominates both
$\delta_{UB}$ and $\delta_{\hbox{mle}}$. The finding can be viewed as both a
multivariate extension of $p=1$ result due to Kubokawa (2005) and a unknown
variance extension of a similar dominance finding due to Marchand and Perron
(2001). Various other dominance results are obtained, illustrations are
provided and commented upon. In particular, for $m \leq \sqrt{\frac{p}{2}}$, a
wide class of Bayes estimators, which include priors where $\theta|\sigma$ is
uniformly distributed on the ball of radius $m$, are shown to dominate
$\delta_{UB}$."@2012
Hervé Cardot@http://arxiv.org/abs/1204.6382v3@"Uniform convergence and asymptotic confidence bands for model-assisted
  estimators of the mean of sampled functional data"@"When the study variable is functional and storage capacities are limited or
transmission costs are high, selecting with survey sampling techniques a small
fraction of the observations is an interesting alternative to signal
compression techniques, particularly when the goal is the estimation of simple
quantities such as means or totals. We extend, in this functional framework,
model-assisted estimators with linear regression models that can take account
of auxiliary variables whose totals over the population are known. We first
show, under weak hypotheses on the sampling design and the regularity of the
trajectories, that the estimator of the mean function as well as its variance
estimator are uniformly consistent. Then, under additional assumptions, we
prove a functional central limit theorem and we assess rigorously a fast
technique based on simulations of Gaussian processes which is employed to build
asymptotic confidence bands. The accuracy of the variance function estimator is
evaluated on a real dataset of sampled electricity consumption curves measured
every half an hour over a period of one week."@2012
Camelia Goga@http://arxiv.org/abs/1204.6382v3@"Uniform convergence and asymptotic confidence bands for model-assisted
  estimators of the mean of sampled functional data"@"When the study variable is functional and storage capacities are limited or
transmission costs are high, selecting with survey sampling techniques a small
fraction of the observations is an interesting alternative to signal
compression techniques, particularly when the goal is the estimation of simple
quantities such as means or totals. We extend, in this functional framework,
model-assisted estimators with linear regression models that can take account
of auxiliary variables whose totals over the population are known. We first
show, under weak hypotheses on the sampling design and the regularity of the
trajectories, that the estimator of the mean function as well as its variance
estimator are uniformly consistent. Then, under additional assumptions, we
prove a functional central limit theorem and we assess rigorously a fast
technique based on simulations of Gaussian processes which is employed to build
asymptotic confidence bands. The accuracy of the variance function estimator is
evaluated on a real dataset of sampled electricity consumption curves measured
every half an hour over a period of one week."@2012
Pauline Lardin@http://arxiv.org/abs/1204.6382v3@"Uniform convergence and asymptotic confidence bands for model-assisted
  estimators of the mean of sampled functional data"@"When the study variable is functional and storage capacities are limited or
transmission costs are high, selecting with survey sampling techniques a small
fraction of the observations is an interesting alternative to signal
compression techniques, particularly when the goal is the estimation of simple
quantities such as means or totals. We extend, in this functional framework,
model-assisted estimators with linear regression models that can take account
of auxiliary variables whose totals over the population are known. We first
show, under weak hypotheses on the sampling design and the regularity of the
trajectories, that the estimator of the mean function as well as its variance
estimator are uniformly consistent. Then, under additional assumptions, we
prove a functional central limit theorem and we assess rigorously a fast
technique based on simulations of Gaussian processes which is employed to build
asymptotic confidence bands. The accuracy of the variance function estimator is
evaluated on a real dataset of sampled electricity consumption curves measured
every half an hour over a period of one week."@2012
Álvaro Calvache@http://arxiv.org/abs/1204.6641v1@"Theory of two-parameter Markov chain with an application in warranty
  study"@"In this paper we present the classical results of Kolmogorov's backward and
forward equations to the case of a two-parameter Markov process. These
equations relates the infinitesimal transition matrix of the two-parameter
Markov process. However, solving these equations is not possible and we require
a numerical procedure. In this paper, we give an alternative method by use of
double Laplace transform of the transition probability matrix and of the
infinitesimal transition matrix of the process. An illustrative example is
presented for the method proposed. In this example, we consider a two-parameter
warranty model, in which a system can be any of these states: working, failure.
We calculate the transition density matrix of these states and also the cost of
the warranty for the proposed model."@2012
Viswanathan Arunachalam@http://arxiv.org/abs/1204.6641v1@"Theory of two-parameter Markov chain with an application in warranty
  study"@"In this paper we present the classical results of Kolmogorov's backward and
forward equations to the case of a two-parameter Markov process. These
equations relates the infinitesimal transition matrix of the two-parameter
Markov process. However, solving these equations is not possible and we require
a numerical procedure. In this paper, we give an alternative method by use of
double Laplace transform of the transition probability matrix and of the
infinitesimal transition matrix of the process. An illustrative example is
presented for the method proposed. In this example, we consider a two-parameter
warranty model, in which a system can be any of these states: working, failure.
We calculate the transition density matrix of these states and also the cost of
the warranty for the proposed model."@2012
Xiao Wang@http://arxiv.org/abs/1205.0023v1@Uniform Convergence and Rate Adaptive Estimation of a Convex Function@"This paper addresses the problem of estimating a convex regression function
under both the sup-norm risk and the pointwise risk using B-splines. The
presence of the convex constraint complicates various issues in asymptotic
analysis, particularly uniform convergence analysis. To overcome this
difficulty, we establish the uniform Lipschitz property of optimal spline
coefficients in the $\ell_\infty$-norm by exploiting piecewise linear and
polyhedral theory. Based upon this property, it is shown that this estimator
attains optimal rates of convergence on the entire interval of interest over
the H\""older class under both the risks. In addition, adaptive estimates are
constructed under both the sup-norm risk and the pointwise risk when the
exponent of the H\""older class is between one and two. These estimates achieve
a maximal risk within a constant factor of the minimax risk over the H\""older
class."@2012
Jinglai Shen@http://arxiv.org/abs/1205.0023v1@Uniform Convergence and Rate Adaptive Estimation of a Convex Function@"This paper addresses the problem of estimating a convex regression function
under both the sup-norm risk and the pointwise risk using B-splines. The
presence of the convex constraint complicates various issues in asymptotic
analysis, particularly uniform convergence analysis. To overcome this
difficulty, we establish the uniform Lipschitz property of optimal spline
coefficients in the $\ell_\infty$-norm by exploiting piecewise linear and
polyhedral theory. Based upon this property, it is shown that this estimator
attains optimal rates of convergence on the entire interval of interest over
the H\""older class under both the risks. In addition, adaptive estimates are
constructed under both the sup-norm risk and the pointwise risk when the
exponent of the H\""older class is between one and two. These estimates achieve
a maximal risk within a constant factor of the minimax risk over the H\""older
class."@2012
Axel Bücher@http://arxiv.org/abs/1205.0417v2@Nonparametric inference on Lévy measures and copulas@"In this paper nonparametric methods to assess the multivariate L\'{e}vy
measure are introduced. Starting from high-frequency observations of a L\'{e}vy
process $\mathbf{X}$, we construct estimators for its tail integrals and the
Pareto-L\'{e}vy copula and prove weak convergence of these estimators in
certain function spaces. Given n observations of increments over intervals of
length $\Delta_n$, the rate of convergence is $k_n^{-1/2}$ for $k_n=n\Delta_n$
which is natural concerning inference on the L\'{e}vy measure. Besides
extensions to nonequidistant sampling schemes analytic properties of the
Pareto-L\'{e}vy copula which, to the best of our knowledge, have not been
mentioned before in the literature are provided as well. We conclude with a
short simulation study on the performance of our estimators and apply them to
real data."@2012
Mathias Vetter@http://arxiv.org/abs/1205.0417v2@Nonparametric inference on Lévy measures and copulas@"In this paper nonparametric methods to assess the multivariate L\'{e}vy
measure are introduced. Starting from high-frequency observations of a L\'{e}vy
process $\mathbf{X}$, we construct estimators for its tail integrals and the
Pareto-L\'{e}vy copula and prove weak convergence of these estimators in
certain function spaces. Given n observations of increments over intervals of
length $\Delta_n$, the rate of convergence is $k_n^{-1/2}$ for $k_n=n\Delta_n$
which is natural concerning inference on the L\'{e}vy measure. Besides
extensions to nonequidistant sampling schemes analytic properties of the
Pareto-L\'{e}vy copula which, to the best of our knowledge, have not been
mentioned before in the literature are provided as well. We conclude with a
short simulation study on the performance of our estimators and apply them to
real data."@2012
M. Doostparast@http://arxiv.org/abs/1205.0638v1@Pareto analysis based on records@"Estimation of the parameters of an exponential distribution based on record
data has been treated by Samaniego and Whitaker (1986) and Doostparast (2009).
Recently, Doostparast and Balakrishnan (2011) obtained optimal confidence
intervals as well as uniformly most powerful tests for one- and two-sided
hypotheses concerning location and scale parameters based on record data from a
two-parameter exponential model. In this paper, we derive optimal statistical
procedures including point and interval estimation as well as most powerful
tests based on record data from a two-parameter Pareto model. For illustrative
purpose, a data set on annual wages of a sample production-line workers in a
large industrial firm is analyzed using the proposed procedures."@2012
N. Balakrishnan@http://arxiv.org/abs/1205.0638v1@Pareto analysis based on records@"Estimation of the parameters of an exponential distribution based on record
data has been treated by Samaniego and Whitaker (1986) and Doostparast (2009).
Recently, Doostparast and Balakrishnan (2011) obtained optimal confidence
intervals as well as uniformly most powerful tests for one- and two-sided
hypotheses concerning location and scale parameters based on record data from a
two-parameter exponential model. In this paper, we derive optimal statistical
procedures including point and interval estimation as well as most powerful
tests based on record data from a two-parameter Pareto model. For illustrative
purpose, a data set on annual wages of a sample production-line workers in a
large industrial firm is analyzed using the proposed procedures."@2012
Rida Benhaddou@http://arxiv.org/abs/1205.0990v2@"Adaptive Nonparametric Empirical Bayes Estimation Via Wavelet Series:
  the Minimax Study"@"In the present paper, we derive lower bounds for the risk of the
nonparametric empirical Bayes estimators. In order to attain the optimal
convergence rate, we propose generalization of the linear empirical Bayes
estimation method which takes advantage of the flexibility of the wavelet
techniques. We present an empirical Bayes estimator as a wavelet series
expansion and estimate coefficients by minimizing the prior risk of the
estimator. As a result, estimation of wavelet coefficients requires solution of
a well-posed low-dimensional sparse system of linear equations. The dimension
of the system depends on the size of wavelet support and smoothness of the
Bayes estimator. An adaptive choice of the resolution level is carried out
using Lepski (1997) method. The method is computationally efficient and
provides asymptotically optimal adaptive EB estimators. The theory is
supplemented by numerous examples."@2012
Marianna Pensky@http://arxiv.org/abs/1205.0990v2@"Adaptive Nonparametric Empirical Bayes Estimation Via Wavelet Series:
  the Minimax Study"@"In the present paper, we derive lower bounds for the risk of the
nonparametric empirical Bayes estimators. In order to attain the optimal
convergence rate, we propose generalization of the linear empirical Bayes
estimation method which takes advantage of the flexibility of the wavelet
techniques. We present an empirical Bayes estimator as a wavelet series
expansion and estimate coefficients by minimizing the prior risk of the
estimator. As a result, estimation of wavelet coefficients requires solution of
a well-posed low-dimensional sparse system of linear equations. The dimension
of the system depends on the size of wavelet support and smoothness of the
Bayes estimator. An adaptive choice of the resolution level is carried out
using Lepski (1997) method. The method is computationally efficient and
provides asymptotically optimal adaptive EB estimators. The theory is
supplemented by numerous examples."@2012
Laszlo Gyorfi@http://arxiv.org/abs/1205.1005v1@Some Refinements of Large Deviation Tail Probabilities@"We study tail probabilities via some Gaussian approximations. Our results
make refinements to large deviation theory. The proof builds on classical
results by Bahadur and Rao. Binomial distributions and their tail probabilities
are discussed in more detail."@2012
Peter Harremoes@http://arxiv.org/abs/1205.1005v1@Some Refinements of Large Deviation Tail Probabilities@"We study tail probabilities via some Gaussian approximations. Our results
make refinements to large deviation theory. The proof builds on classical
results by Bahadur and Rao. Binomial distributions and their tail probabilities
are discussed in more detail."@2012
Gabor Tusnady@http://arxiv.org/abs/1205.1005v1@Some Refinements of Large Deviation Tail Probabilities@"We study tail probabilities via some Gaussian approximations. Our results
make refinements to large deviation theory. The proof builds on classical
results by Bahadur and Rao. Binomial distributions and their tail probabilities
are discussed in more detail."@2012
Anthony J Webster@http://arxiv.org/abs/1205.1150v3@Estimating Omissions from Searches@"The mark-recapture method was devised by Petersen in 1896 to estimate the
number of fish migrating into the Limfjord, and independently by Lincoln in
1930 to estimate waterfowl abundance. The technique applies to any search for a
finite number of items by two or more people or agents, allowing the number of
searched-for items to be estimated. This ubiquitous problem appears in fields
from ecology and epidemiology, through to mathematics, social sciences, and
computing. Here we exactly calculate the moments of the hypergeometric
distribution associated with this long-standing problem, confirming that widely
used estimates conjectured in 1951 are often too small. Our Bayesian approach
highlights how different search strategies will modify the estimates. As an
example, we assess the accuracy of a systematic literature review, an
application we recommend."@2012
Richard Kemp@http://arxiv.org/abs/1205.1150v3@Estimating Omissions from Searches@"The mark-recapture method was devised by Petersen in 1896 to estimate the
number of fish migrating into the Limfjord, and independently by Lincoln in
1930 to estimate waterfowl abundance. The technique applies to any search for a
finite number of items by two or more people or agents, allowing the number of
searched-for items to be estimated. This ubiquitous problem appears in fields
from ecology and epidemiology, through to mathematics, social sciences, and
computing. Here we exactly calculate the moments of the hypergeometric
distribution associated with this long-standing problem, confirming that widely
used estimates conjectured in 1951 are often too small. Our Bayesian approach
highlights how different search strategies will modify the estimates. As an
example, we assess the accuracy of a systematic literature review, an
application we recommend."@2012
Philippe Rigollet@http://arxiv.org/abs/1205.1210v1@Estimation of Covariance Matrices under Sparsity Constraints@"We prove optimal sparsity oracle inequalities for the estimation of
covariance matrix under the Frobenius norm. In particular we explore various
sparsity structures on the underlying matrix."@2012
Alexandre Tsybakov@http://arxiv.org/abs/1205.1210v1@Estimation of Covariance Matrices under Sparsity Constraints@"We prove optimal sparsity oracle inequalities for the estimation of
covariance matrix under the Frobenius norm. In particular we explore various
sparsity structures on the underlying matrix."@2012
Samuel Vaiter@http://arxiv.org/abs/1205.1481v1@The Degrees of Freedom of the Group Lasso@"This paper studies the sensitivity to the observations of the block/group
Lasso solution to an overdetermined linear regression model. Such a
regularization is known to promote sparsity patterns structured as
nonoverlapping groups of coefficients. Our main contribution provides a local
parameterization of the solution with respect to the observations. As a
byproduct, we give an unbiased estimate of the degrees of freedom of the group
Lasso. Among other applications of such results, one can choose in a principled
and objective way the regularization parameter of the Lasso through model
selection criteria."@2012
Charles Deledalle@http://arxiv.org/abs/1205.1481v1@The Degrees of Freedom of the Group Lasso@"This paper studies the sensitivity to the observations of the block/group
Lasso solution to an overdetermined linear regression model. Such a
regularization is known to promote sparsity patterns structured as
nonoverlapping groups of coefficients. Our main contribution provides a local
parameterization of the solution with respect to the observations. As a
byproduct, we give an unbiased estimate of the degrees of freedom of the group
Lasso. Among other applications of such results, one can choose in a principled
and objective way the regularization parameter of the Lasso through model
selection criteria."@2012
Gabriel Peyré@http://arxiv.org/abs/1205.1481v1@The Degrees of Freedom of the Group Lasso@"This paper studies the sensitivity to the observations of the block/group
Lasso solution to an overdetermined linear regression model. Such a
regularization is known to promote sparsity patterns structured as
nonoverlapping groups of coefficients. Our main contribution provides a local
parameterization of the solution with respect to the observations. As a
byproduct, we give an unbiased estimate of the degrees of freedom of the group
Lasso. Among other applications of such results, one can choose in a principled
and objective way the regularization parameter of the Lasso through model
selection criteria."@2012
Jalal Fadili@http://arxiv.org/abs/1205.1481v1@The Degrees of Freedom of the Group Lasso@"This paper studies the sensitivity to the observations of the block/group
Lasso solution to an overdetermined linear regression model. Such a
regularization is known to promote sparsity patterns structured as
nonoverlapping groups of coefficients. Our main contribution provides a local
parameterization of the solution with respect to the observations. As a
byproduct, we give an unbiased estimate of the degrees of freedom of the group
Lasso. Among other applications of such results, one can choose in a principled
and objective way the regularization parameter of the Lasso through model
selection criteria."@2012
Charles Dossal@http://arxiv.org/abs/1205.1481v1@The Degrees of Freedom of the Group Lasso@"This paper studies the sensitivity to the observations of the block/group
Lasso solution to an overdetermined linear regression model. Such a
regularization is known to promote sparsity patterns structured as
nonoverlapping groups of coefficients. Our main contribution provides a local
parameterization of the solution with respect to the observations. As a
byproduct, we give an unbiased estimate of the degrees of freedom of the group
Lasso. Among other applications of such results, one can choose in a principled
and objective way the regularization parameter of the Lasso through model
selection criteria."@2012
Makoto Maejima@http://arxiv.org/abs/1205.1654v1@"A class of multivariate infinitely divisible distributions related to
  arcsine density"@"Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures
on $\mathbb{R}^d$ based on the arcsine density are studied and their relation
to general Upsilon transformations is considered. The domains of definition of
$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they
have the same range. The class of infinitely divisible distributions on
$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the
class $A$ and any distribution in the class $A$ is expressed as the law of a
stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect
to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass
the Jurek class of distributions. It is shown that generalized type $G$
distributions are the image of distributions in the class $A$ under a mapping
defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as
an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be."@2012
Víctor Pérez-Abreu@http://arxiv.org/abs/1205.1654v1@"A class of multivariate infinitely divisible distributions related to
  arcsine density"@"Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures
on $\mathbb{R}^d$ based on the arcsine density are studied and their relation
to general Upsilon transformations is considered. The domains of definition of
$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they
have the same range. The class of infinitely divisible distributions on
$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the
class $A$ and any distribution in the class $A$ is expressed as the law of a
stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect
to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass
the Jurek class of distributions. It is shown that generalized type $G$
distributions are the image of distributions in the class $A$ under a mapping
defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as
an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be."@2012
Ken-iti Sato@http://arxiv.org/abs/1205.1654v1@"A class of multivariate infinitely divisible distributions related to
  arcsine density"@"Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures
on $\mathbb{R}^d$ based on the arcsine density are studied and their relation
to general Upsilon transformations is considered. The domains of definition of
$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they
have the same range. The class of infinitely divisible distributions on
$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the
class $A$ and any distribution in the class $A$ is expressed as the law of a
stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect
to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass
the Jurek class of distributions. It is shown that generalized type $G$
distributions are the image of distributions in the class $A$ under a mapping
defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as
an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be."@2012
Vladimir Koltchinskii@http://arxiv.org/abs/1205.1868v2@Low Rank Estimation of Similarities on Graphs@"Let (V, E) be a graph with vertex set V and edge set E. Let (X, X', Y) \in V
\times V \times {-1, 1} be a random triple, where X, X' are independent
uniformly distributed vertices and Y is a label indicating whether X, X' are
""similar"" (Y = +1), or not (Y = -1). Our goal is to estimate the regression
function S\ast (u, v) = E(Y |X = u, X = v), u, v \in V based on training data
consisting of n i.i.d. copies of (X, X',Y). We are interested in this problem
in the case when S\ast is a symmetric low rank kernel and, in addition to this,
it is assumed that S\ast is ""smooth"" on the graph. We study estimators based on
a modified least squares method with complexity penalization involving both the
nuclear norm and Sobolev type norms of symmetric kernels on the graph and prove
upper bounds on L2 -type errors of such estimators with explicit dependence
both on the rank of S\ast and on the degree of its smoothness."@2012
Pedro Rangel@http://arxiv.org/abs/1205.1868v2@Low Rank Estimation of Similarities on Graphs@"Let (V, E) be a graph with vertex set V and edge set E. Let (X, X', Y) \in V
\times V \times {-1, 1} be a random triple, where X, X' are independent
uniformly distributed vertices and Y is a label indicating whether X, X' are
""similar"" (Y = +1), or not (Y = -1). Our goal is to estimate the regression
function S\ast (u, v) = E(Y |X = u, X = v), u, v \in V based on training data
consisting of n i.i.d. copies of (X, X',Y). We are interested in this problem
in the case when S\ast is a symmetric low rank kernel and, in addition to this,
it is assumed that S\ast is ""smooth"" on the graph. We study estimators based on
a modified least squares method with complexity penalization involving both the
nuclear norm and Sobolev type norms of symmetric kernels on the graph and prove
upper bounds on L2 -type errors of such estimators with explicit dependence
both on the rank of S\ast and on the degree of its smoothness."@2012
Yuichi Hirose@http://arxiv.org/abs/1205.1920v1@"Reparametrization of the least favorable submodel in semi-parametric
  multisample models"@"The method of estimation in Scott and Wild (Biometrika 84 (1997) 57--71 and
J. Statist. Plann. Inference 96 (2001) 3--27) uses a reparametrization of the
profile likelihood that often reduces the computation times dramatically.
Showing the efficiency of estimators for this method has been a challenging
problem. In this paper, we try to solve the problem by investigating conditions
under which the efficient score function and the efficient information matrix
can be expressed in terms of the parameters in the reparametrized model."@2012
Alan Lee@http://arxiv.org/abs/1205.1920v1@"Reparametrization of the least favorable submodel in semi-parametric
  multisample models"@"The method of estimation in Scott and Wild (Biometrika 84 (1997) 57--71 and
J. Statist. Plann. Inference 96 (2001) 3--27) uses a reparametrization of the
profile likelihood that often reduces the computation times dramatically.
Showing the efficiency of estimators for this method has been a challenging
problem. In this paper, we try to solve the problem by investigating conditions
under which the efficient score function and the efficient information matrix
can be expressed in terms of the parameters in the reparametrized model."@2012
Pedro C. Álvarez-Esteban@http://arxiv.org/abs/1205.1950v1@Similarity of samples and trimming@"We say that two probabilities are similar at level $\alpha$ if they are
contaminated versions (up to an $\alpha$ fraction) of the same common
probability. We show how this model is related to minimal distances between
sets of trimmed probabilities. Empirical versions turn out to present an
overfitting effect in the sense that trimming beyond the similarity level
results in trimmed samples that are closer than expected to each other. We show
how this can be combined with a bootstrap approach to assess similarity from
two data samples."@2012
Eustasio del Barrio@http://arxiv.org/abs/1205.1950v1@Similarity of samples and trimming@"We say that two probabilities are similar at level $\alpha$ if they are
contaminated versions (up to an $\alpha$ fraction) of the same common
probability. We show how this model is related to minimal distances between
sets of trimmed probabilities. Empirical versions turn out to present an
overfitting effect in the sense that trimming beyond the similarity level
results in trimmed samples that are closer than expected to each other. We show
how this can be combined with a bootstrap approach to assess similarity from
two data samples."@2012
Juan A. Cuesta-Albertos@http://arxiv.org/abs/1205.1950v1@Similarity of samples and trimming@"We say that two probabilities are similar at level $\alpha$ if they are
contaminated versions (up to an $\alpha$ fraction) of the same common
probability. We show how this model is related to minimal distances between
sets of trimmed probabilities. Empirical versions turn out to present an
overfitting effect in the sense that trimming beyond the similarity level
results in trimmed samples that are closer than expected to each other. We show
how this can be combined with a bootstrap approach to assess similarity from
two data samples."@2012
Carlos Matrán@http://arxiv.org/abs/1205.1950v1@Similarity of samples and trimming@"We say that two probabilities are similar at level $\alpha$ if they are
contaminated versions (up to an $\alpha$ fraction) of the same common
probability. We show how this model is related to minimal distances between
sets of trimmed probabilities. Empirical versions turn out to present an
overfitting effect in the sense that trimming beyond the similarity level
results in trimmed samples that are closer than expected to each other. We show
how this can be combined with a bootstrap approach to assess similarity from
two data samples."@2012
Éric Marchand@http://arxiv.org/abs/1205.1964v1@A unified minimax result for restricted parameter spaces@"We provide a development that unifies, simplifies and extends considerably a
number of minimax results in the restricted parameter space literature. Various
applications follow, such as that of estimating location or scale parameters
under a lower (or upper) bound restriction, location parameter vectors
restricted to a polyhedral cone, scale parameters subject to restricted ratios
or products, linear combinations of restricted location parameters, location
parameters bounded to an interval with unknown scale, quantiles for
location-scale families with parametric restrictions and restricted covariance
matrices."@2012
William E. Strawderman@http://arxiv.org/abs/1205.1964v1@A unified minimax result for restricted parameter spaces@"We provide a development that unifies, simplifies and extends considerably a
number of minimax results in the restricted parameter space literature. Various
applications follow, such as that of estimating location or scale parameters
under a lower (or upper) bound restriction, location parameter vectors
restricted to a polyhedral cone, scale parameters subject to restricted ratios
or products, linear combinations of restricted location parameters, location
parameters bounded to an interval with unknown scale, quantiles for
location-scale families with parametric restrictions and restricted covariance
matrices."@2012
A. N. Varaksin@http://arxiv.org/abs/1205.2446v1@"On relationship between regression models and interpretation of multiple
  regression coefficients"@"In this paper, we consider the problem of treating linear regression equation
coefficients in the case of correlated predictors. It is shown that in general
there are no natural ways of interpreting these coefficients similar to the
case of single predictor. Nevertheless we suggest linear transformations of
predictors, reducing multiple regression to a simple one and retaining the
coefficient at variable of interest. The new variable can be treated as the
part of the old variable that has no linear statistical dependence on other
presented variables."@2012
V. G. Panov@http://arxiv.org/abs/1205.2446v1@"On relationship between regression models and interpretation of multiple
  regression coefficients"@"In this paper, we consider the problem of treating linear regression equation
coefficients in the case of correlated predictors. It is shown that in general
there are no natural ways of interpreting these coefficients similar to the
case of single predictor. Nevertheless we suggest linear transformations of
predictors, reducing multiple regression to a simple one and retaining the
coefficient at variable of interest. The new variable can be treated as the
part of the old variable that has no linear statistical dependence on other
presented variables."@2012
Florent Autin@http://arxiv.org/abs/1205.2679v2@Testing means from sampling populations with undefined labels@"We consider the problem of testing means from samples of two populations for
which the labels are not defined with certainty. We show that this problem is
connected to another one that is testing expected values of components of
mixture-models from two data samples. The underlying mixture-model is
associated with known varying mixing-weights. We provide a testing procedure
that performs well. Then we point out the loss of performance of our method due
to the mixing-effect by comparing its numerical performances to the Welch's
t-test on means which would have been done if true labels were available."@2012
Christophe Pouet@http://arxiv.org/abs/1205.2679v2@Testing means from sampling populations with undefined labels@"We consider the problem of testing means from samples of two populations for
which the labels are not defined with certainty. We show that this problem is
connected to another one that is testing expected values of components of
mixture-models from two data samples. The underlying mixture-model is
associated with known varying mixing-weights. We provide a testing procedure
that performs well. Then we point out the loss of performance of our method due
to the mixing-effect by comparing its numerical performances to the Welch's
t-test on means which would have been done if true labels were available."@2012
Loïc Hervé@http://arxiv.org/abs/1205.2947v1@"A uniform Berry--Esseen theorem on $M$-estimators for geometrically
  ergodic Markov chains"@"Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some
real-valued functional $F$, define
$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,
$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator
$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying
$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with
$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some
standard regularity and moment assumptions, close to those of the i.i.d. case,
the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with
respect to the underlying probability distribution of the Markov chain."@2012
James Ledoux@http://arxiv.org/abs/1205.2947v1@"A uniform Berry--Esseen theorem on $M$-estimators for geometrically
  ergodic Markov chains"@"Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some
real-valued functional $F$, define
$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,
$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator
$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying
$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with
$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some
standard regularity and moment assumptions, close to those of the i.i.d. case,
the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with
respect to the underlying probability distribution of the Markov chain."@2012
Valentin Patilea@http://arxiv.org/abs/1205.2947v1@"A uniform Berry--Esseen theorem on $M$-estimators for geometrically
  ergodic Markov chains"@"Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some
real-valued functional $F$, define
$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,
$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator
$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying
$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with
$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some
standard regularity and moment assumptions, close to those of the i.i.d. case,
the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with
respect to the underlying probability distribution of the Markov chain."@2012
Dong Li@http://arxiv.org/abs/1205.2948v1@On moving-average models with feedback@"Moving average models, linear or nonlinear, are characterized by their short
memory. This paper shows that, in the presence of feedback in the dynamics, the
above characteristic can disappear."@2012
Shiqing Ling@http://arxiv.org/abs/1205.2948v1@On moving-average models with feedback@"Moving average models, linear or nonlinear, are characterized by their short
memory. This paper shows that, in the presence of feedback in the dynamics, the
above characteristic can disappear."@2012
Howell Tong@http://arxiv.org/abs/1205.2948v1@On moving-average models with feedback@"Moving average models, linear or nonlinear, are characterized by their short
memory. This paper shows that, in the presence of feedback in the dynamics, the
above characteristic can disappear."@2012
Jia Chen@http://arxiv.org/abs/1205.3324v1@Estimation in semi-parametric regression with non-stationary regressors@"In this paper, we consider a partially linear model of the form
$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a
$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly
stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary
sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a
semi-parametric least-squares (SLS) estimation method. Under certain
conditions, we then show that the proposed SLS estimator of $\theta_0$ is still
asymptotically normal with the same rate as for the case of stationary time
series. In addition, we also establish an asymptotic distribution for the
nonparametric estimator of the function $g(\cdot)$. Some numerical examples are
provided to show that our theory and estimation method work well in practice."@2012
Jiti Gao@http://arxiv.org/abs/1205.3324v1@Estimation in semi-parametric regression with non-stationary regressors@"In this paper, we consider a partially linear model of the form
$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a
$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly
stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary
sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a
semi-parametric least-squares (SLS) estimation method. Under certain
conditions, we then show that the proposed SLS estimator of $\theta_0$ is still
asymptotically normal with the same rate as for the case of stationary time
series. In addition, we also establish an asymptotic distribution for the
nonparametric estimator of the function $g(\cdot)$. Some numerical examples are
provided to show that our theory and estimation method work well in practice."@2012
Degui Li@http://arxiv.org/abs/1205.3324v1@Estimation in semi-parametric regression with non-stationary regressors@"In this paper, we consider a partially linear model of the form
$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a
$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly
stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary
sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a
semi-parametric least-squares (SLS) estimation method. Under certain
conditions, we then show that the proposed SLS estimator of $\theta_0$ is still
asymptotically normal with the same rate as for the case of stationary time
series. In addition, we also establish an asymptotic distribution for the
nonparametric estimator of the function $g(\cdot)$. Some numerical examples are
provided to show that our theory and estimation method work well in practice."@2012
Alexandra Carpentier@http://arxiv.org/abs/1205.4094v1@"Bandit Theory meets Compressed Sensing for high dimensional Stochastic
  Linear Bandit"@"We consider a linear stochastic bandit problem where the dimension $K$ of the
unknown parameter $\theta$ is larger than the sampling budget $n$. In such
cases, it is in general impossible to derive sub-linear regret bounds since
usual linear bandit algorithms have a regret in $O(K\sqrt{n})$. In this paper
we assume that $\theta$ is $S-$sparse, i.e. has at most $S-$non-zero
components, and that the space of arms is the unit ball for the $||.||_2$ norm.
We combine ideas from Compressed Sensing and Bandit Theory and derive
algorithms with regret bounds in $O(S\sqrt{n})$."@2012
Rémi Munos@http://arxiv.org/abs/1205.4094v1@"Bandit Theory meets Compressed Sensing for high dimensional Stochastic
  Linear Bandit"@"We consider a linear stochastic bandit problem where the dimension $K$ of the
unknown parameter $\theta$ is larger than the sampling budget $n$. In such
cases, it is in general impossible to derive sub-linear regret bounds since
usual linear bandit algorithms have a regret in $O(K\sqrt{n})$. In this paper
we assume that $\theta$ is $S-$sparse, i.e. has at most $S-$non-zero
components, and that the space of arms is the unit ball for the $||.||_2$ norm.
We combine ideas from Compressed Sensing and Bandit Theory and derive
algorithms with regret bounds in $O(S\sqrt{n})$."@2012
Alexandra Carpentier@http://arxiv.org/abs/1205.4095v1@"Minimax Number of Strata for Online Stratified Sampling given Noisy
  Samples"@"We consider the problem of online stratified sampling for Monte Carlo
integration of a function given a finite budget of $n$ noisy evaluations to the
function. More precisely we focus on the problem of choosing the number of
strata $K$ as a function of the budget $n$. We provide asymptotic and
finite-time results on how an oracle that has access to the function would
choose the partition optimally. In addition we prove a \textit{lower bound} on
the learning rate for the problem of stratified Monte-Carlo. As a result, we
are able to state, by improving the bound on its performance, that algorithm
MC-UCB, defined in \citep{MC-UCB}, is minimax optimal both in terms of the
number of samples n and the number of strata K, up to a $\sqrt{\log(nK)}$. This
enables to deduce a minimax optimal bound on the difference between the
performance of the estimate outputted by MC-UCB, and the performance of the
estimate outputted by the best oracle static strategy, on the class of H\""older
continuous functions, and upt to a $\sqrt{\log(n)}$."@2012
Rémi Munos@http://arxiv.org/abs/1205.4095v1@"Minimax Number of Strata for Online Stratified Sampling given Noisy
  Samples"@"We consider the problem of online stratified sampling for Monte Carlo
integration of a function given a finite budget of $n$ noisy evaluations to the
function. More precisely we focus on the problem of choosing the number of
strata $K$ as a function of the budget $n$. We provide asymptotic and
finite-time results on how an oracle that has access to the function would
choose the partition optimally. In addition we prove a \textit{lower bound} on
the learning rate for the problem of stratified Monte-Carlo. As a result, we
are able to state, by improving the bound on its performance, that algorithm
MC-UCB, defined in \citep{MC-UCB}, is minimax optimal both in terms of the
number of samples n and the number of strata K, up to a $\sqrt{\log(nK)}$. This
enables to deduce a minimax optimal bound on the difference between the
performance of the estimate outputted by MC-UCB, and the performance of the
estimate outputted by the best oracle static strategy, on the class of H\""older
continuous functions, and upt to a $\sqrt{\log(n)}$."@2012
Piotr Pokarowski@http://arxiv.org/abs/1205.4146v1@"Linear regression model selection using p-values when the model
  dimension grows"@"We consider a new criterion-based approach to model selection in linear
regression. Properties of selection criteria based on p-values of a likelihood
ratio statistic are studied for families of linear regression models. We prove
that such procedures are consistent i.e. the minimal true model is chosen with
probability tending to 1 even when the number of models under consideration
slowly increases with a sample size. The simulation study indicates that
introduced methods perform promisingly when compared with Akaike and Bayesian
Information Criteria."@2012
Jan Mielniczuk@http://arxiv.org/abs/1205.4146v1@"Linear regression model selection using p-values when the model
  dimension grows"@"We consider a new criterion-based approach to model selection in linear
regression. Properties of selection criteria based on p-values of a likelihood
ratio statistic are studied for families of linear regression models. We prove
that such procedures are consistent i.e. the minimal true model is chosen with
probability tending to 1 even when the number of models under consideration
slowly increases with a sample size. The simulation study indicates that
introduced methods perform promisingly when compared with Akaike and Bayesian
Information Criteria."@2012
Paweł Teisseyre@http://arxiv.org/abs/1205.4146v1@"Linear regression model selection using p-values when the model
  dimension grows"@"We consider a new criterion-based approach to model selection in linear
regression. Properties of selection criteria based on p-values of a likelihood
ratio statistic are studied for families of linear regression models. We prove
that such procedures are consistent i.e. the minimal true model is chosen with
probability tending to 1 even when the number of models under consideration
slowly increases with a sample size. The simulation study indicates that
introduced methods perform promisingly when compared with Akaike and Bayesian
Information Criteria."@2012
T. Tony Cai@http://arxiv.org/abs/1205.4219v2@Optimal hypothesis testing for high dimensional covariance matrices@"This paper considers testing a covariance matrix $\Sigma$ in the high
dimensional setting where the dimension $p$ can be comparable or much larger
than the sample size $n$. The problem of testing the hypothesis
$H_0:\Sigma=\Sigma_0$ for a given covariance matrix $\Sigma_0$ is studied from
a minimax point of view. We first characterize the boundary that separates the
testable region from the non-testable region by the Frobenius norm when the
ratio between the dimension $p$ over the sample size $n$ is bounded. A test
based on a $U$-statistic is introduced and is shown to be rate optimal over
this asymptotic regime. Furthermore, it is shown that the power of this test
uniformly dominates that of the corrected likelihood ratio test (CLRT) over the
entire asymptotic regime under which the CLRT is applicable. The power of the
$U$-statistic based test is also analyzed when $p/n$ is unbounded."@2012
Zongming Ma@http://arxiv.org/abs/1205.4219v2@Optimal hypothesis testing for high dimensional covariance matrices@"This paper considers testing a covariance matrix $\Sigma$ in the high
dimensional setting where the dimension $p$ can be comparable or much larger
than the sample size $n$. The problem of testing the hypothesis
$H_0:\Sigma=\Sigma_0$ for a given covariance matrix $\Sigma_0$ is studied from
a minimax point of view. We first characterize the boundary that separates the
testable region from the non-testable region by the Frobenius norm when the
ratio between the dimension $p$ over the sample size $n$ is bounded. A test
based on a $U$-statistic is introduced and is shown to be rate optimal over
this asymptotic regime. Furthermore, it is shown that the power of this test
uniformly dominates that of the corrected likelihood ratio test (CLRT) over the
entire asymptotic regime under which the CLRT is applicable. The power of the
$U$-statistic based test is also analyzed when $p/n$ is unbounded."@2012
Christophe Ley@http://arxiv.org/abs/1205.4259v2@Efficient ANOVA for directional data@"In this paper we tackle the ANOVA problem for directional data (with
particular emphasis on geological data) by having recourse to the Le Cam
methodology usually reserved for linear multivariate analysis. We construct
locally and asymptotically most stringent parametric tests for ANOVA for
directional data within the class of rotationally symmetric distributions. We
turn these parametric tests into semi-parametric ones by (i) using a
studentization argument (which leads to what we call pseudo-FvML tests) and by
(ii) resorting to the invariance principle (which leads to efficient rank-based
tests). Within each construction the semi-parametric tests inherit optimality
under a given distribution (the FvML distribution in the first case, any
rotationally symmetric distribution in the second) from their parametric
antecedents and also improve on the latter by being valid under the whole class
of rotationally symmetric distributions. Asymptotic relative efficiencies are
calculated and the finite-sample behavior of the proposed tests is investigated
by means of a Monte Carlo simulation. We conclude by applying our findings on a
real-data example involving geological data."@2012
Yvik Swan@http://arxiv.org/abs/1205.4259v2@Efficient ANOVA for directional data@"In this paper we tackle the ANOVA problem for directional data (with
particular emphasis on geological data) by having recourse to the Le Cam
methodology usually reserved for linear multivariate analysis. We construct
locally and asymptotically most stringent parametric tests for ANOVA for
directional data within the class of rotationally symmetric distributions. We
turn these parametric tests into semi-parametric ones by (i) using a
studentization argument (which leads to what we call pseudo-FvML tests) and by
(ii) resorting to the invariance principle (which leads to efficient rank-based
tests). Within each construction the semi-parametric tests inherit optimality
under a given distribution (the FvML distribution in the first case, any
rotationally symmetric distribution in the second) from their parametric
antecedents and also improve on the latter by being valid under the whole class
of rotationally symmetric distributions. Asymptotic relative efficiencies are
calculated and the finite-sample behavior of the proposed tests is investigated
by means of a Monte Carlo simulation. We conclude by applying our findings on a
real-data example involving geological data."@2012
Thomas Verdebout@http://arxiv.org/abs/1205.4259v2@Efficient ANOVA for directional data@"In this paper we tackle the ANOVA problem for directional data (with
particular emphasis on geological data) by having recourse to the Le Cam
methodology usually reserved for linear multivariate analysis. We construct
locally and asymptotically most stringent parametric tests for ANOVA for
directional data within the class of rotationally symmetric distributions. We
turn these parametric tests into semi-parametric ones by (i) using a
studentization argument (which leads to what we call pseudo-FvML tests) and by
(ii) resorting to the invariance principle (which leads to efficient rank-based
tests). Within each construction the semi-parametric tests inherit optimality
under a given distribution (the FvML distribution in the first case, any
rotationally symmetric distribution in the second) from their parametric
antecedents and also improve on the latter by being valid under the whole class
of rotationally symmetric distributions. Asymptotic relative efficiencies are
calculated and the finite-sample behavior of the proposed tests is investigated
by means of a Monte Carlo simulation. We conclude by applying our findings on a
real-data example involving geological data."@2012
Mélina Bec@http://arxiv.org/abs/1205.4692v2@Adaptive pointwise estimation for pure jump Lévy processes@"This paper is concerned with adaptive kernel estimation of the L\'evy density
N(x) for bounded-variation pure-jump L\'evy processes. The sample path is
observed at n discrete instants in the ""high frequency"" context (\Delta =
\Delta(n) tends to zero while n\Delta tends to infinity). We construct a
collection of kernel estimators of the function g(x)=xN(x) and propose a method
of local adaptive selection of the bandwidth. We provide an oracle inequality
and a rate of convergence for the quadratic pointwise risk. This rate is proved
to be the optimal minimax rate. We give examples and simulation results for
processes fitting in our framework. We also consider the case of irregular
sampling."@2012
Claire Lacour@http://arxiv.org/abs/1205.4692v2@Adaptive pointwise estimation for pure jump Lévy processes@"This paper is concerned with adaptive kernel estimation of the L\'evy density
N(x) for bounded-variation pure-jump L\'evy processes. The sample path is
observed at n discrete instants in the ""high frequency"" context (\Delta =
\Delta(n) tends to zero while n\Delta tends to infinity). We construct a
collection of kernel estimators of the function g(x)=xN(x) and propose a method
of local adaptive selection of the bandwidth. We provide an oracle inequality
and a rate of convergence for the quadratic pointwise risk. This rate is proved
to be the optimal minimax rate. We give examples and simulation results for
processes fitting in our framework. We also consider the case of irregular
sampling."@2012
Jianqing Fan@http://arxiv.org/abs/1205.4795v3@Adaptive robust variable selection@"Heavy-tailed high-dimensional data are commonly encountered in various
scientific fields and pose great challenges to modern statistical analysis. A
natural procedure to address this problem is to use penalized quantile
regression with weighted $L_1$-penalty, called weighted robust Lasso
(WR-Lasso), in which weights are introduced to ameliorate the bias problem
induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the
dimensionality can grow exponentially with the sample size, we investigate the
model selection oracle property and establish the asymptotic normality of the
WR-Lasso. We show that only mild conditions on the model error distribution are
needed. Our theoretical results also reveal that adaptive choice of the weight
vector is essential for the WR-Lasso to enjoy these nice asymptotic properties.
To make the WR-Lasso practically feasible, we propose a two-step procedure,
called adaptive robust Lasso (AR-Lasso), in which the weight vector in the
second step is constructed based on the $L_1$-penalized quantile regression
estimate from the first step. This two-step procedure is justified
theoretically to possess the oracle property and the asymptotic normality.
Numerical studies demonstrate the favorable finite-sample performance of the
AR-Lasso."@2012
Yingying Fan@http://arxiv.org/abs/1205.4795v3@Adaptive robust variable selection@"Heavy-tailed high-dimensional data are commonly encountered in various
scientific fields and pose great challenges to modern statistical analysis. A
natural procedure to address this problem is to use penalized quantile
regression with weighted $L_1$-penalty, called weighted robust Lasso
(WR-Lasso), in which weights are introduced to ameliorate the bias problem
induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the
dimensionality can grow exponentially with the sample size, we investigate the
model selection oracle property and establish the asymptotic normality of the
WR-Lasso. We show that only mild conditions on the model error distribution are
needed. Our theoretical results also reveal that adaptive choice of the weight
vector is essential for the WR-Lasso to enjoy these nice asymptotic properties.
To make the WR-Lasso practically feasible, we propose a two-step procedure,
called adaptive robust Lasso (AR-Lasso), in which the weight vector in the
second step is constructed based on the $L_1$-penalized quantile regression
estimate from the first step. This two-step procedure is justified
theoretically to possess the oracle property and the asymptotic normality.
Numerical studies demonstrate the favorable finite-sample performance of the
AR-Lasso."@2012
Emre Barut@http://arxiv.org/abs/1205.4795v3@Adaptive robust variable selection@"Heavy-tailed high-dimensional data are commonly encountered in various
scientific fields and pose great challenges to modern statistical analysis. A
natural procedure to address this problem is to use penalized quantile
regression with weighted $L_1$-penalty, called weighted robust Lasso
(WR-Lasso), in which weights are introduced to ameliorate the bias problem
induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the
dimensionality can grow exponentially with the sample size, we investigate the
model selection oracle property and establish the asymptotic normality of the
WR-Lasso. We show that only mild conditions on the model error distribution are
needed. Our theoretical results also reveal that adaptive choice of the weight
vector is essential for the WR-Lasso to enjoy these nice asymptotic properties.
To make the WR-Lasso practically feasible, we propose a two-step procedure,
called adaptive robust Lasso (AR-Lasso), in which the weight vector in the
second step is constructed based on the $L_1$-penalized quantile regression
estimate from the first step. This two-step procedure is justified
theoretically to possess the oracle property and the asymptotic normality.
Numerical studies demonstrate the favorable finite-sample performance of the
AR-Lasso."@2012
Frédéric Lavancier@http://arxiv.org/abs/1205.4818v5@"Determinantal point process models and statistical inference : Extended
  version"@"Statistical models and methods for determinantal point processes (DPPs) seem
largely unexplored. We demonstrate that DPPs provide useful models for the
description of spatial point pattern datasets where nearby points repel each
other. Such data are usually modelled by Gibbs point processes, where the
likelihood and moment expressions are intractable and simulations are time
consuming. We exploit the appealing probabilistic properties of DPPs to develop
parametric models, where the likelihood and moment expressions can be easily
evaluated and realizations can be quickly simulated. We discuss how statistical
inference is conducted using the likelihood or moment properties of DPP models,
and we provide freely available software for simulation and statistical
inference."@2012
Jesper Møller@http://arxiv.org/abs/1205.4818v5@"Determinantal point process models and statistical inference : Extended
  version"@"Statistical models and methods for determinantal point processes (DPPs) seem
largely unexplored. We demonstrate that DPPs provide useful models for the
description of spatial point pattern datasets where nearby points repel each
other. Such data are usually modelled by Gibbs point processes, where the
likelihood and moment expressions are intractable and simulations are time
consuming. We exploit the appealing probabilistic properties of DPPs to develop
parametric models, where the likelihood and moment expressions can be easily
evaluated and realizations can be quickly simulated. We discuss how statistical
inference is conducted using the likelihood or moment properties of DPP models,
and we provide freely available software for simulation and statistical
inference."@2012
Ege Rubak@http://arxiv.org/abs/1205.4818v5@"Determinantal point process models and statistical inference : Extended
  version"@"Statistical models and methods for determinantal point processes (DPPs) seem
largely unexplored. We demonstrate that DPPs provide useful models for the
description of spatial point pattern datasets where nearby points repel each
other. Such data are usually modelled by Gibbs point processes, where the
likelihood and moment expressions are intractable and simulations are time
consuming. We exploit the appealing probabilistic properties of DPPs to develop
parametric models, where the likelihood and moment expressions can be easily
evaluated and realizations can be quickly simulated. We discuss how statistical
inference is conducted using the likelihood or moment properties of DPP models,
and we provide freely available software for simulation and statistical
inference."@2012
Tatiane F. N. Melo@http://arxiv.org/abs/1205.5039v2@"Modified likelihood ratio tests in heteroskedastic multivariate
  regression models with measurement error"@"In this paper, we develop modified versions of the likelihood ratio test for
multivariate heteroskedastic errors-in-variables regression models. The error
terms are allowed to follow a multivariate distribution in the elliptical class
of distributions, which has the normal distribution as a special case. We
derive the Skovgaard adjusted likelihood ratio statistics, which follow a
chi-squared distribution with a high degree of accuracy. We conduct a
simulation study and show that the proposed tests display superior finite
sample behavior as compared to the standard likelihood ratio test. We
illustrate the usefulness of our results in applied settings using a data set
from the WHO MONICA Projection cardiovascular disease."@2012
Silvia L. P. Ferrari@http://arxiv.org/abs/1205.5039v2@"Modified likelihood ratio tests in heteroskedastic multivariate
  regression models with measurement error"@"In this paper, we develop modified versions of the likelihood ratio test for
multivariate heteroskedastic errors-in-variables regression models. The error
terms are allowed to follow a multivariate distribution in the elliptical class
of distributions, which has the normal distribution as a special case. We
derive the Skovgaard adjusted likelihood ratio statistics, which follow a
chi-squared distribution with a high degree of accuracy. We conduct a
simulation study and show that the proposed tests display superior finite
sample behavior as compared to the standard likelihood ratio test. We
illustrate the usefulness of our results in applied settings using a data set
from the WHO MONICA Projection cardiovascular disease."@2012
Alexandre G. Patriota@http://arxiv.org/abs/1205.5039v2@"Modified likelihood ratio tests in heteroskedastic multivariate
  regression models with measurement error"@"In this paper, we develop modified versions of the likelihood ratio test for
multivariate heteroskedastic errors-in-variables regression models. The error
terms are allowed to follow a multivariate distribution in the elliptical class
of distributions, which has the normal distribution as a special case. We
derive the Skovgaard adjusted likelihood ratio statistics, which follow a
chi-squared distribution with a high degree of accuracy. We conduct a
simulation study and show that the proposed tests display superior finite
sample behavior as compared to the standard likelihood ratio test. We
illustrate the usefulness of our results in applied settings using a data set
from the WHO MONICA Projection cardiovascular disease."@2012
Lothar Heinrich@http://arxiv.org/abs/1205.5044v1@"Non-parametric asymptotic statistics for the Palm mark distribution of
  β-mixing marked point processes"@"We consider spatially homogeneous marked point patterns in an unboundedly
expanding convex sampling window. Our main objective is to identify the
distribution of the typical mark by constructing an asymptotic
\chi^2-goodness-of-fit test. The corresponding test statistic is based on a
natural empirical version of the Palm mark distribution and a smoothed
covariance estimator which turns out to be mean-square consistent. Our approach
does not require independent marks and allows dependences between the mark
field and the point pattern. Instead we impose a suitable \beta-mixing
condition on the underlying stationary marked point process which can be
checked for a number of Poisson-based models and, in particular, in the case of
geostatistical marking. Our method needs a central limit theorem for
\beta-mixing random fields which is proved by extending Bernstein's blocking
technique to non-cubic index sets and seems to be of interest in its own right.
By large-scale model-based simulations the performance of our test is studied
in dependence of the model parameters which determine the range of spatial
correlations."@2012
Sebastian Lück@http://arxiv.org/abs/1205.5044v1@"Non-parametric asymptotic statistics for the Palm mark distribution of
  β-mixing marked point processes"@"We consider spatially homogeneous marked point patterns in an unboundedly
expanding convex sampling window. Our main objective is to identify the
distribution of the typical mark by constructing an asymptotic
\chi^2-goodness-of-fit test. The corresponding test statistic is based on a
natural empirical version of the Palm mark distribution and a smoothed
covariance estimator which turns out to be mean-square consistent. Our approach
does not require independent marks and allows dependences between the mark
field and the point pattern. Instead we impose a suitable \beta-mixing
condition on the underlying stationary marked point process which can be
checked for a number of Poisson-based models and, in particular, in the case of
geostatistical marking. Our method needs a central limit theorem for
\beta-mixing random fields which is proved by extending Bernstein's blocking
technique to non-cubic index sets and seems to be of interest in its own right.
By large-scale model-based simulations the performance of our test is studied
in dependence of the model parameters which determine the range of spatial
correlations."@2012
Volker Schmidt@http://arxiv.org/abs/1205.5044v1@"Non-parametric asymptotic statistics for the Palm mark distribution of
  β-mixing marked point processes"@"We consider spatially homogeneous marked point patterns in an unboundedly
expanding convex sampling window. Our main objective is to identify the
distribution of the typical mark by constructing an asymptotic
\chi^2-goodness-of-fit test. The corresponding test statistic is based on a
natural empirical version of the Palm mark distribution and a smoothed
covariance estimator which turns out to be mean-square consistent. Our approach
does not require independent marks and allows dependences between the mark
field and the point pattern. Instead we impose a suitable \beta-mixing
condition on the underlying stationary marked point process which can be
checked for a number of Poisson-based models and, in particular, in the case of
geostatistical marking. Our method needs a central limit theorem for
\beta-mixing random fields which is proved by extending Bernstein's blocking
technique to non-cubic index sets and seems to be of interest in its own right.
By large-scale model-based simulations the performance of our test is studied
in dependence of the model parameters which determine the range of spatial
correlations."@2012
Sara van de Geer@http://arxiv.org/abs/1205.5473v2@$\ell_0$-penalized maximum likelihood for sparse directed acyclic graphs@"We consider the problem of regularized maximum likelihood estimation for the
structure and parameters of a high-dimensional, sparse directed acyclic
graphical (DAG) model with Gaussian distribution, or equivalently, of a
Gaussian structural equation model. We show that the $\ell_0$-penalized maximum
likelihood estimator of a DAG has about the same number of edges as the
minimal-edge I-MAP (a DAG with minimal number of edges representing the
distribution), and that it converges in Frobenius norm. We allow the number of
nodes p to be much larger than sample size n but assume a sparsity condition
and that any representation of the true DAG has at least a fixed proportion of
its nonzero edge weights above the noise level. Our results do not rely on the
faithfulness assumption nor on the restrictive strong faithfulness condition
which are required for methods based on conditional independence testing such
as the PC-algorithm."@2012
Peter Bühlmann@http://arxiv.org/abs/1205.5473v2@$\ell_0$-penalized maximum likelihood for sparse directed acyclic graphs@"We consider the problem of regularized maximum likelihood estimation for the
structure and parameters of a high-dimensional, sparse directed acyclic
graphical (DAG) model with Gaussian distribution, or equivalently, of a
Gaussian structural equation model. We show that the $\ell_0$-penalized maximum
likelihood estimator of a DAG has about the same number of edges as the
minimal-edge I-MAP (a DAG with minimal number of edges representing the
distribution), and that it converges in Frobenius norm. We allow the number of
nodes p to be much larger than sample size n but assume a sparsity condition
and that any representation of the true DAG has at least a fixed proportion of
its nonzero edge weights above the noise level. Our results do not rely on the
faithfulness assumption nor on the restrictive strong faithfulness condition
which are required for methods based on conditional independence testing such
as the PC-algorithm."@2012
Sabyasachi Mukhopadhyay@http://arxiv.org/abs/1205.5508v5@"Bayesian MISE convergence rates of Polya urn based density estimators:
  asymptotic comparisons and choice of prior parameters"@"Mixture models are well-known for their versatility, and the Bayesian
paradigm is a suitable platform for mixture analysis, particularly when the
number of components is unknown. Bhattacharya (2008) introduced a mixture model
based on the Dirichlet process, where an upper bound on the unknown number of
components is to be specified. Here we consider a Bayesian asymptotic framework
for objectively specifying the upper bound, which we assume to depend on the
sample size. In particular, we define a Bayesian analogue of the mean
integrated squared error (Bayesian MISE), and select that form of the upper
bound, and also that form of the precision parameter of the underlying
Dirichlet process, for which Bayesian MISE of a specific density estimator,
which is a suitable modification of the Polya-urn based prior predictive model,
converges at a desired rate. As a byproduct of our approach, we investigate
asymptotic choice of the precision parameter of the traditional Dirichlet
process mixture model; the density estimator we consider here is a modification
of the prior predictive distribution of Escobar & West (1995) associated with
the Polya urn model. Various asymptotic issues related to the two
aforementioned mixtures, including comparative performances, are also
investigated."@2012
Sourabh Bhattacharya@http://arxiv.org/abs/1205.5508v5@"Bayesian MISE convergence rates of Polya urn based density estimators:
  asymptotic comparisons and choice of prior parameters"@"Mixture models are well-known for their versatility, and the Bayesian
paradigm is a suitable platform for mixture analysis, particularly when the
number of components is unknown. Bhattacharya (2008) introduced a mixture model
based on the Dirichlet process, where an upper bound on the unknown number of
components is to be specified. Here we consider a Bayesian asymptotic framework
for objectively specifying the upper bound, which we assume to depend on the
sample size. In particular, we define a Bayesian analogue of the mean
integrated squared error (Bayesian MISE), and select that form of the upper
bound, and also that form of the precision parameter of the underlying
Dirichlet process, for which Bayesian MISE of a specific density estimator,
which is a suitable modification of the Polya-urn based prior predictive model,
converges at a desired rate. As a byproduct of our approach, we investigate
asymptotic choice of the precision parameter of the traditional Dirichlet
process mixture model; the density estimator we consider here is a modification
of the prior predictive distribution of Escobar & West (1995) associated with
the Polya urn model. Various asymptotic issues related to the two
aforementioned mixtures, including comparative performances, are also
investigated."@2012
Valentin Patilea@http://arxiv.org/abs/1205.5578v1@"Projection-based nonparametric goodness-of-fit testing with functional
  covariates"@"This paper studies the problem of nonparametric testing for the effect of a
random functional covariate on a real-valued error term. The covariate takes
values in $L^2[0,1]$, the Hilbert space of the square-integrable real-valued
functions on the unit interval. The error term could be directly observed as a
response or \emph{estimated} from a functional parametric model, like for
instance the functional linear regression. Our test is based on the remark that
checking the no-effect of the functional covariate is equivalent to checking
the nullity of the conditional expectation of the error term given a
sufficiently rich set of projections of the covariate. Such projections could
be on elements of norm 1 from finite-dimension subspaces of $L^2[0,1]$. Next,
the idea is to search a finite-dimension element of norm 1 that is, in some
sense, the least favorable for the null hypothesis. Finally, it remains to
perform a nonparametric check of the nullity of the conditional expectation of
the error term given the scalar product between the covariate and the selected
least favorable direction. For such finite-dimension search and nonparametric
check we use a kernel-based approach. As a result, our test statistic is a
quadratic form based on univariate kernel smoothing and the asymptotic critical
values are given by the standard normal law. The test is able to detect
nonparametric alternatives, including the polynomial ones. The error term could
present heteroscedasticity of unknown form. We do no require the law of the
covariate $X$ to be known. The test could be implemented quite easily and
performs well in simulations and real data applications. We illustrate the
performance of our test for checking the functional linear regression model."@2012
Cesar Sanchez-Sellero@http://arxiv.org/abs/1205.5578v1@"Projection-based nonparametric goodness-of-fit testing with functional
  covariates"@"This paper studies the problem of nonparametric testing for the effect of a
random functional covariate on a real-valued error term. The covariate takes
values in $L^2[0,1]$, the Hilbert space of the square-integrable real-valued
functions on the unit interval. The error term could be directly observed as a
response or \emph{estimated} from a functional parametric model, like for
instance the functional linear regression. Our test is based on the remark that
checking the no-effect of the functional covariate is equivalent to checking
the nullity of the conditional expectation of the error term given a
sufficiently rich set of projections of the covariate. Such projections could
be on elements of norm 1 from finite-dimension subspaces of $L^2[0,1]$. Next,
the idea is to search a finite-dimension element of norm 1 that is, in some
sense, the least favorable for the null hypothesis. Finally, it remains to
perform a nonparametric check of the nullity of the conditional expectation of
the error term given the scalar product between the covariate and the selected
least favorable direction. For such finite-dimension search and nonparametric
check we use a kernel-based approach. As a result, our test statistic is a
quadratic form based on univariate kernel smoothing and the asymptotic critical
values are given by the standard normal law. The test is able to detect
nonparametric alternatives, including the polynomial ones. The error term could
present heteroscedasticity of unknown form. We do no require the law of the
covariate $X$ to be known. The test could be implemented quite easily and
performs well in simulations and real data applications. We illustrate the
performance of our test for checking the functional linear regression model."@2012
Matthieu Saumard@http://arxiv.org/abs/1205.5578v1@"Projection-based nonparametric goodness-of-fit testing with functional
  covariates"@"This paper studies the problem of nonparametric testing for the effect of a
random functional covariate on a real-valued error term. The covariate takes
values in $L^2[0,1]$, the Hilbert space of the square-integrable real-valued
functions on the unit interval. The error term could be directly observed as a
response or \emph{estimated} from a functional parametric model, like for
instance the functional linear regression. Our test is based on the remark that
checking the no-effect of the functional covariate is equivalent to checking
the nullity of the conditional expectation of the error term given a
sufficiently rich set of projections of the covariate. Such projections could
be on elements of norm 1 from finite-dimension subspaces of $L^2[0,1]$. Next,
the idea is to search a finite-dimension element of norm 1 that is, in some
sense, the least favorable for the null hypothesis. Finally, it remains to
perform a nonparametric check of the nullity of the conditional expectation of
the error term given the scalar product between the covariate and the selected
least favorable direction. For such finite-dimension search and nonparametric
check we use a kernel-based approach. As a result, our test statistic is a
quadratic form based on univariate kernel smoothing and the asymptotic critical
values are given by the standard normal law. The test is able to detect
nonparametric alternatives, including the polynomial ones. The error term could
present heteroscedasticity of unknown form. We do no require the law of the
covariate $X$ to be known. The test could be implemented quite easily and
performs well in simulations and real data applications. We illustrate the
performance of our test for checking the functional linear regression model."@2012
Dong Chen@http://arxiv.org/abs/1205.6040v1@Nonlinear manifold representations for functional data@"For functional data lying on an unknown nonlinear low-dimensional space, we
study manifold learning and introduce the notions of manifold mean, manifold
modes of functional variation and of functional manifold components. These
constitute nonlinear representations of functional data that complement
classical linear representations such as eigenfunctions and functional
principal components. Our manifold learning procedures borrow ideas from
existing nonlinear dimension reduction methods, which we modify to address
functional data settings. In simulations and applications, we study examples of
functional data which lie on a manifold and validate the superior behavior of
manifold mean and functional manifold components over traditional
cross-sectional mean and functional principal components. We also include
consistency proofs for our estimators under certain assumptions."@2012
Hans-Georg Müller@http://arxiv.org/abs/1205.6040v1@Nonlinear manifold representations for functional data@"For functional data lying on an unknown nonlinear low-dimensional space, we
study manifold learning and introduce the notions of manifold mean, manifold
modes of functional variation and of functional manifold components. These
constitute nonlinear representations of functional data that complement
classical linear representations such as eigenfunctions and functional
principal components. Our manifold learning procedures borrow ideas from
existing nonlinear dimension reduction methods, which we modify to address
functional data settings. In simulations and applications, we study examples of
functional data which lie on a manifold and validate the superior behavior of
manifold mean and functional manifold components over traditional
cross-sectional mean and functional principal components. We also include
consistency proofs for our estimators under certain assumptions."@2012
Victoria Plamadeala@http://arxiv.org/abs/1205.6043v1@Sequential monitoring with conditional randomization tests@"Sequential monitoring in clinical trials is often employed to allow for early
stopping and other interim decisions, while maintaining the type I error rate.
However, sequential monitoring is typically described only in the context of a
population model. We describe a computational method to implement sequential
monitoring in a randomization-based context. In particular, we discuss a new
technique for the computation of approximate conditional tests following
restricted randomization procedures and then apply this technique to
approximate the joint distribution of sequentially computed conditional
randomization tests. We also describe the computation of a randomization-based
analog of the information fraction. We apply these techniques to a restricted
randomization procedure, Efron's [Biometrika 58 (1971) 403--417] biased coin
design. These techniques require derivation of certain conditional
probabilities and conditional covariances of the randomization procedure. We
employ combinatoric techniques to derive these for the biased coin design."@2012
William F. Rosenberger@http://arxiv.org/abs/1205.6043v1@Sequential monitoring with conditional randomization tests@"Sequential monitoring in clinical trials is often employed to allow for early
stopping and other interim decisions, while maintaining the type I error rate.
However, sequential monitoring is typically described only in the context of a
population model. We describe a computational method to implement sequential
monitoring in a randomization-based context. In particular, we discuss a new
technique for the computation of approximate conditional tests following
restricted randomization procedures and then apply this technique to
approximate the joint distribution of sequentially computed conditional
randomization tests. We also describe the computation of a randomization-based
analog of the information fraction. We apply these techniques to a restricted
randomization procedure, Efron's [Biometrika 58 (1971) 403--417] biased coin
design. These techniques require derivation of certain conditional
probabilities and conditional covariances of the randomization procedure. We
employ combinatoric techniques to derive these for the biased coin design."@2012
Runlong Tang@http://arxiv.org/abs/1205.6055v1@"Likelihood based inference for current status data on a grid: A boundary
  phenomenon and an adaptive inference procedure"@"In this paper, we study the nonparametric maximum likelihood estimator for an
event time distribution function at a point in the current status model with
observation times supported on a grid of potentially unknown sparsity and with
multiple subjects sharing the same observation time. This is of interest since
observation time ties occur frequently with current status data. The grid
resolution is specified as $cn^{-\gamma}$ with $c>0$ being a scaling constant
and $\gamma>0$ regulating the sparsity of the grid relative to $n$, the number
of subjects. The asymptotic behavior falls into three cases depending on
$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma<1/3$,
nonstandard cube-root asymptotics prevail when $\gamma>1/3$ and $\gamma=1/3$
serves as a boundary at which the transition happens. The limit distribution at
the boundary is different from either of the previous cases and converges
weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as
$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to
develop an adaptive procedure to construct confidence intervals for the value
of the event time distribution at a point of interest without needing to know
or estimate $\gamma$, which is of enormous advantage from the perspective of
inference. A simulation study of the adaptive procedure is presented."@2012
Moulinath Banerjee@http://arxiv.org/abs/1205.6055v1@"Likelihood based inference for current status data on a grid: A boundary
  phenomenon and an adaptive inference procedure"@"In this paper, we study the nonparametric maximum likelihood estimator for an
event time distribution function at a point in the current status model with
observation times supported on a grid of potentially unknown sparsity and with
multiple subjects sharing the same observation time. This is of interest since
observation time ties occur frequently with current status data. The grid
resolution is specified as $cn^{-\gamma}$ with $c>0$ being a scaling constant
and $\gamma>0$ regulating the sparsity of the grid relative to $n$, the number
of subjects. The asymptotic behavior falls into three cases depending on
$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma<1/3$,
nonstandard cube-root asymptotics prevail when $\gamma>1/3$ and $\gamma=1/3$
serves as a boundary at which the transition happens. The limit distribution at
the boundary is different from either of the previous cases and converges
weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as
$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to
develop an adaptive procedure to construct confidence intervals for the value
of the event time distribution at a point of interest without needing to know
or estimate $\gamma$, which is of enormous advantage from the perspective of
inference. A simulation study of the adaptive procedure is presented."@2012
Michael R. Kosorok@http://arxiv.org/abs/1205.6055v1@"Likelihood based inference for current status data on a grid: A boundary
  phenomenon and an adaptive inference procedure"@"In this paper, we study the nonparametric maximum likelihood estimator for an
event time distribution function at a point in the current status model with
observation times supported on a grid of potentially unknown sparsity and with
multiple subjects sharing the same observation time. This is of interest since
observation time ties occur frequently with current status data. The grid
resolution is specified as $cn^{-\gamma}$ with $c>0$ being a scaling constant
and $\gamma>0$ regulating the sparsity of the grid relative to $n$, the number
of subjects. The asymptotic behavior falls into three cases depending on
$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma<1/3$,
nonstandard cube-root asymptotics prevail when $\gamma>1/3$ and $\gamma=1/3$
serves as a boundary at which the transition happens. The limit distribution at
the boundary is different from either of the previous cases and converges
weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as
$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to
develop an adaptive procedure to construct confidence intervals for the value
of the event time distribution at a point of interest without needing to know
or estimate $\gamma$, which is of enormous advantage from the perspective of
inference. A simulation study of the adaptive procedure is presented."@2012
Mark S. Kaiser@http://arxiv.org/abs/1205.6086v1@Goodness of fit tests for a class of Markov random field models@"This paper develops goodness of fit statistics that can be used to formally
assess Markov random field models for spatial data, when the model
distributions are discrete or continuous and potentially parametric. Test
statistics are formed from generalized spatial residuals which are collected
over groups of nonneighboring spatial observations, called concliques. Under a
hypothesized Markov model structure, spatial residuals within each conclique
are shown to be independent and identically distributed as uniform variables.
The information from a series of concliques can be then pooled into goodness of
fit statistics. Under some conditions, large sample distributions of these
statistics are explicitly derived for testing both simple and composite
hypotheses, where the latter involves additional parametric estimation steps.
The distributional results are verified through simulation, and a data example
illustrates the method for model assessment."@2012
Soumendra N. Lahiri@http://arxiv.org/abs/1205.6086v1@Goodness of fit tests for a class of Markov random field models@"This paper develops goodness of fit statistics that can be used to formally
assess Markov random field models for spatial data, when the model
distributions are discrete or continuous and potentially parametric. Test
statistics are formed from generalized spatial residuals which are collected
over groups of nonneighboring spatial observations, called concliques. Under a
hypothesized Markov model structure, spatial residuals within each conclique
are shown to be independent and identically distributed as uniform variables.
The information from a series of concliques can be then pooled into goodness of
fit statistics. Under some conditions, large sample distributions of these
statistics are explicitly derived for testing both simple and composite
hypotheses, where the latter involves additional parametric estimation steps.
The distributional results are verified through simulation, and a data example
illustrates the method for model assessment."@2012
Daniel J. Nordman@http://arxiv.org/abs/1205.6086v1@Goodness of fit tests for a class of Markov random field models@"This paper develops goodness of fit statistics that can be used to formally
assess Markov random field models for spatial data, when the model
distributions are discrete or continuous and potentially parametric. Test
statistics are formed from generalized spatial residuals which are collected
over groups of nonneighboring spatial observations, called concliques. Under a
hypothesized Markov model structure, spatial residuals within each conclique
are shown to be independent and identically distributed as uniform variables.
The information from a series of concliques can be then pooled into goodness of
fit statistics. Under some conditions, large sample distributions of these
statistics are explicitly derived for testing both simple and composite
hypotheses, where the latter involves additional parametric estimation steps.
The distributional results are verified through simulation, and a data example
illustrates the method for model assessment."@2012
Aurore Delaigle@http://arxiv.org/abs/1205.6102v1@Nonparametric regression with homogeneous group testing data@"We introduce new nonparametric predictors for homogeneous pooled data in the
context of group testing for rare abnormalities and show that they achieve
optimal rates of convergence. In particular, when the level of pooling is
moderate, then despite the cost savings, the method enjoys the same convergence
rate as in the case of no pooling. In the setting of ""over-pooling"" the
convergence rate differs from that of an optimal estimator by no more than a
logarithmic factor. Our approach improves on the random-pooling nonparametric
predictor, which is currently the only nonparametric method available, unless
there is no pooling, in which case the two approaches are identical."@2012
Peter Hall@http://arxiv.org/abs/1205.6102v1@Nonparametric regression with homogeneous group testing data@"We introduce new nonparametric predictors for homogeneous pooled data in the
context of group testing for rare abnormalities and show that they achieve
optimal rates of convergence. In particular, when the level of pooling is
moderate, then despite the cost savings, the method enjoys the same convergence
rate as in the case of no pooling. In the setting of ""over-pooling"" the
convergence rate differs from that of an optimal estimator by no more than a
logarithmic factor. Our approach improves on the random-pooling nonparametric
predictor, which is currently the only nonparametric method available, unless
there is no pooling, in which case the two approaches are identical."@2012
Masoud Asgharian@http://arxiv.org/abs/1205.6275v1@"Large-sample study of the kernel density estimators under multiplicative
  censoring"@"The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)
751--761] is an incomplete data problem whereby two independent samples from
the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and
$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.
Specifically, sample $\mathcal{X}_m$ is fully observed while
$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where
$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard
uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this
model unifies several important statistical problems, such as the deconvolution
of an exponential random variable, estimation under a decreasing density
constraint and an estimation problem in renewal processes. In this paper, we
establish the large-sample properties of kernel density estimators under the
multiplicative censoring model. We first construct a strong approximation for
the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the
nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and
$k=m+n$ is the total sample size. Using this strong approximation and a result
on the global modulus of continuity, we establish conditions for the strong
uniform consistency of kernel density estimators. We also make use of this
strong approximation to study the weak convergence and integrated squared error
properties of these estimators. We conclude by extending our results to the
setting of length-biased sampling."@2012
Marco Carone@http://arxiv.org/abs/1205.6275v1@"Large-sample study of the kernel density estimators under multiplicative
  censoring"@"The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)
751--761] is an incomplete data problem whereby two independent samples from
the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and
$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.
Specifically, sample $\mathcal{X}_m$ is fully observed while
$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where
$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard
uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this
model unifies several important statistical problems, such as the deconvolution
of an exponential random variable, estimation under a decreasing density
constraint and an estimation problem in renewal processes. In this paper, we
establish the large-sample properties of kernel density estimators under the
multiplicative censoring model. We first construct a strong approximation for
the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the
nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and
$k=m+n$ is the total sample size. Using this strong approximation and a result
on the global modulus of continuity, we establish conditions for the strong
uniform consistency of kernel density estimators. We also make use of this
strong approximation to study the weak convergence and integrated squared error
properties of these estimators. We conclude by extending our results to the
setting of length-biased sampling."@2012
Vahid Fakoor@http://arxiv.org/abs/1205.6275v1@"Large-sample study of the kernel density estimators under multiplicative
  censoring"@"The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)
751--761] is an incomplete data problem whereby two independent samples from
the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and
$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.
Specifically, sample $\mathcal{X}_m$ is fully observed while
$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where
$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard
uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this
model unifies several important statistical problems, such as the deconvolution
of an exponential random variable, estimation under a decreasing density
constraint and an estimation problem in renewal processes. In this paper, we
establish the large-sample properties of kernel density estimators under the
multiplicative censoring model. We first construct a strong approximation for
the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the
nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and
$k=m+n$ is the total sample size. Using this strong approximation and a result
on the global modulus of continuity, we establish conditions for the strong
uniform consistency of kernel density estimators. We also make use of this
strong approximation to study the weak convergence and integrated squared error
properties of these estimators. We conclude by extending our results to the
setting of length-biased sampling."@2012
Holger Dette@http://arxiv.org/abs/1205.6283v1@$T$-optimal designs for discrimination between two polynomial models@"This paper is devoted to the explicit construction of optimal designs for
discrimination between two polynomial regression models of degree $n-2$ and
$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)
57--70] proposed the $T$-optimality criterion for this purpose. Recently,
Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]
determined $T$-optimal designs for polynomials up to degree 6 numerically and
based on these results he conjectured that the support points of the optimal
design are cosines of the angles that divide half of the circle into equal
parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree
vanishes. In the present paper we give a strong justification of the conjecture
and determine all $T$-optimal designs explicitly for any degree
$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional
class of $T$-optimal designs. Moreover, we also present a generalization to the
case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller
than a certain critical value. Because of the complexity of the optimization
problem, $T$-optimal designs have only been determined numerically so far, and
this paper provides the first explicit solution of the $T$-optimal design
problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)
57--70]. Finally, for the remaining cases (where the ratio of coefficients is
larger than the critical value), we propose a numerical procedure to calculate
the $T$-optimal designs. The results are also illustrated in an example."@2012
Viatcheslav B. Melas@http://arxiv.org/abs/1205.6283v1@$T$-optimal designs for discrimination between two polynomial models@"This paper is devoted to the explicit construction of optimal designs for
discrimination between two polynomial regression models of degree $n-2$ and
$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)
57--70] proposed the $T$-optimality criterion for this purpose. Recently,
Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]
determined $T$-optimal designs for polynomials up to degree 6 numerically and
based on these results he conjectured that the support points of the optimal
design are cosines of the angles that divide half of the circle into equal
parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree
vanishes. In the present paper we give a strong justification of the conjecture
and determine all $T$-optimal designs explicitly for any degree
$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional
class of $T$-optimal designs. Moreover, we also present a generalization to the
case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller
than a certain critical value. Because of the complexity of the optimization
problem, $T$-optimal designs have only been determined numerically so far, and
this paper provides the first explicit solution of the $T$-optimal design
problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)
57--70]. Finally, for the remaining cases (where the ratio of coefficients is
larger than the critical value), we propose a numerical procedure to calculate
the $T$-optimal designs. The results are also illustrated in an example."@2012
Petr Shpilev@http://arxiv.org/abs/1205.6283v1@$T$-optimal designs for discrimination between two polynomial models@"This paper is devoted to the explicit construction of optimal designs for
discrimination between two polynomial regression models of degree $n-2$ and
$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)
57--70] proposed the $T$-optimality criterion for this purpose. Recently,
Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]
determined $T$-optimal designs for polynomials up to degree 6 numerically and
based on these results he conjectured that the support points of the optimal
design are cosines of the angles that divide half of the circle into equal
parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree
vanishes. In the present paper we give a strong justification of the conjecture
and determine all $T$-optimal designs explicitly for any degree
$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional
class of $T$-optimal designs. Moreover, we also present a generalization to the
case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller
than a certain critical value. Because of the complexity of the optimization
problem, $T$-optimal designs have only been determined numerically so far, and
this paper provides the first explicit solution of the $T$-optimal design
problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)
57--70]. Finally, for the remaining cases (where the ratio of coefficients is
larger than the critical value), we propose a numerical procedure to calculate
the $T$-optimal designs. The results are also illustrated in an example."@2012
Aurore Delaigle@http://arxiv.org/abs/1205.6367v1@"Methodology and theory for partial least squares applied to functional
  data"@"The partial least squares procedure was originally developed to estimate the
slope parameter in multivariate parametric models. More recently it has gained
popularity in the functional data literature. There, the partial least squares
estimator of slope is either used to construct linear predictive models, or as
a tool to project the data onto a one-dimensional quantity that is employed for
further statistical analysis. Although the partial least squares approach is
often viewed as an attractive alternative to projections onto the principal
component basis, its properties are less well known than those of the latter,
mainly because of its iterative nature. We develop an explicit formulation of
partial least squares for functional data, which leads to insightful results
and motivates new theory, demonstrating consistency and establishing
convergence rates."@2012
Peter Hall@http://arxiv.org/abs/1205.6367v1@"Methodology and theory for partial least squares applied to functional
  data"@"The partial least squares procedure was originally developed to estimate the
slope parameter in multivariate parametric models. More recently it has gained
popularity in the functional data literature. There, the partial least squares
estimator of slope is either used to construct linear predictive models, or as
a tool to project the data onto a one-dimensional quantity that is employed for
further statistical analysis. Although the partial least squares approach is
often viewed as an attractive alternative to projections onto the principal
component basis, its properties are less well known than those of the latter,
mainly because of its iterative nature. We develop an explicit formulation of
partial least squares for functional data, which leads to insightful results
and motivates new theory, demonstrating consistency and establishing
convergence rates."@2012
R. Dennis Cook@http://arxiv.org/abs/1205.6556v1@"Estimating sufficient reductions of the predictors in abundant
  high-dimensional regressions"@"We study the asymptotic behavior of a class of methods for sufficient
dimension reduction in high-dimension regressions, as the sample size and
number of predictors grow in various alignments. It is demonstrated that these
methods are consistent in a variety of settings, particularly in abundant
regressions where most predictors contribute some information on the response,
and oracle rates are possible. Simulation results are presented to support the
theoretical conclusion."@2012
Liliana Forzani@http://arxiv.org/abs/1205.6556v1@"Estimating sufficient reductions of the predictors in abundant
  high-dimensional regressions"@"We study the asymptotic behavior of a class of methods for sufficient
dimension reduction in high-dimension regressions, as the sample size and
number of predictors grow in various alignments. It is demonstrated that these
methods are consistent in a variety of settings, particularly in abundant
regressions where most predictors contribute some information on the response,
and oracle rates are possible. Simulation results are presented to support the
theoretical conclusion."@2012
Adam J. Rothman@http://arxiv.org/abs/1205.6556v1@"Estimating sufficient reductions of the predictors in abundant
  high-dimensional regressions"@"We study the asymptotic behavior of a class of methods for sufficient
dimension reduction in high-dimension regressions, as the sample size and
number of predictors grow in various alignments. It is demonstrated that these
methods are consistent in a variety of settings, particularly in abundant
regressions where most predictors contribute some information on the response,
and oracle rates are possible. Simulation results are presented to support the
theoretical conclusion."@2012
G. M. Pan@http://arxiv.org/abs/1205.6607v1@Independence Test for High Dimensional Random Vectors@"This paper proposes a new mutual independence test for a large number of high
dimensional random vectors. The test statistic is based on the characteristic
function of the empirical spectral distribution of the sample covariance
matrix. The asymptotic distributions of the test statistic under the null and
local alternative hypotheses are established as dimensionality and the sample
size of the data are comparable. We apply this test to examine multiple MA(1)
and AR(1) models, panel data models with some spatial cross-sectional
structures. In addition, in a flexible applied fashion, the proposed test can
capture some dependent but uncorrelated structures, for example, nonlinear
MA(1) models, multiple ARCH(1) models and vandermonde matrices.
  Simulation results are provided for detecting these dependent structures. An
empirical study of dependence between closed stock prices of several companies
from New York Stock Exchange (NYSE) demonstrates that the feature of
cross--sectional dependence is popular in stock markets."@2012
J. Gao@http://arxiv.org/abs/1205.6607v1@Independence Test for High Dimensional Random Vectors@"This paper proposes a new mutual independence test for a large number of high
dimensional random vectors. The test statistic is based on the characteristic
function of the empirical spectral distribution of the sample covariance
matrix. The asymptotic distributions of the test statistic under the null and
local alternative hypotheses are established as dimensionality and the sample
size of the data are comparable. We apply this test to examine multiple MA(1)
and AR(1) models, panel data models with some spatial cross-sectional
structures. In addition, in a flexible applied fashion, the proposed test can
capture some dependent but uncorrelated structures, for example, nonlinear
MA(1) models, multiple ARCH(1) models and vandermonde matrices.
  Simulation results are provided for detecting these dependent structures. An
empirical study of dependence between closed stock prices of several companies
from New York Stock Exchange (NYSE) demonstrates that the feature of
cross--sectional dependence is popular in stock markets."@2012
Y. Yang@http://arxiv.org/abs/1205.6607v1@Independence Test for High Dimensional Random Vectors@"This paper proposes a new mutual independence test for a large number of high
dimensional random vectors. The test statistic is based on the characteristic
function of the empirical spectral distribution of the sample covariance
matrix. The asymptotic distributions of the test statistic under the null and
local alternative hypotheses are established as dimensionality and the sample
size of the data are comparable. We apply this test to examine multiple MA(1)
and AR(1) models, panel data models with some spatial cross-sectional
structures. In addition, in a flexible applied fashion, the proposed test can
capture some dependent but uncorrelated structures, for example, nonlinear
MA(1) models, multiple ARCH(1) models and vandermonde matrices.
  Simulation results are provided for detecting these dependent structures. An
empirical study of dependence between closed stock prices of several companies
from New York Stock Exchange (NYSE) demonstrates that the feature of
cross--sectional dependence is popular in stock markets."@2012
M. Guo@http://arxiv.org/abs/1205.6607v1@Independence Test for High Dimensional Random Vectors@"This paper proposes a new mutual independence test for a large number of high
dimensional random vectors. The test statistic is based on the characteristic
function of the empirical spectral distribution of the sample covariance
matrix. The asymptotic distributions of the test statistic under the null and
local alternative hypotheses are established as dimensionality and the sample
size of the data are comparable. We apply this test to examine multiple MA(1)
and AR(1) models, panel data models with some spatial cross-sectional
structures. In addition, in a flexible applied fashion, the proposed test can
capture some dependent but uncorrelated structures, for example, nonlinear
MA(1) models, multiple ARCH(1) models and vandermonde matrices.
  Simulation results are provided for detecting these dependent structures. An
empirical study of dependence between closed stock prices of several companies
from New York Stock Exchange (NYSE) demonstrates that the feature of
cross--sectional dependence is popular in stock markets."@2012
Jushan Bai@http://arxiv.org/abs/1205.6617v1@Statistical analysis of factor models of high dimension@"This paper considers the maximum likelihood estimation of factor models of
high dimension, where the number of variables (N) is comparable with or even
greater than the number of observations (T). An inferential theory is
developed. We establish not only consistency but also the rate of convergence
and the limiting distributions. Five different sets of identification
conditions are considered. We show that the distributions of the MLE estimators
depend on the identification restrictions. Unlike the principal components
approach, the maximum likelihood estimator explicitly allows
heteroskedasticities, which are jointly estimated with other parameters.
Efficiency of MLE relative to the principal components method is also
considered."@2012
Kunpeng Li@http://arxiv.org/abs/1205.6617v1@Statistical analysis of factor models of high dimension@"This paper considers the maximum likelihood estimation of factor models of
high dimension, where the number of variables (N) is comparable with or even
greater than the number of observations (T). An inferential theory is
developed. We establish not only consistency but also the rate of convergence
and the limiting distributions. Five different sets of identification
conditions are considered. We show that the distributions of the MLE estimators
depend on the identification restrictions. Unlike the principal components
approach, the maximum likelihood estimator explicitly allows
heteroskedasticities, which are jointly estimated with other parameters.
Efficiency of MLE relative to the principal components method is also
considered."@2012
Yair Goldberg@http://arxiv.org/abs/1205.6659v1@Q-learning with censored data@"We develop methodology for a multistage decision problem with flexible number
of stages in which the rewards are survival times that are subject to
censoring. We present a novel Q-learning algorithm that is adjusted for
censored data and allows a flexible number of stages. We provide finite sample
bounds on the generalization error of the policy learned by the algorithm, and
show that when the optimal Q-function belongs to the approximation space, the
expected survival time for policies obtained by the algorithm converges to that
of the optimal policy. We simulate a multistage clinical trial with flexible
number of stages and apply the proposed censored-Q-learning algorithm to find
individualized treatment regimens. The methodology presented in this paper has
implications in the design of personalized medicine trials in cancer and in
other life-threatening diseases."@2012
Michael R. Kosorok@http://arxiv.org/abs/1205.6659v1@Q-learning with censored data@"We develop methodology for a multistage decision problem with flexible number
of stages in which the rewards are survival times that are subject to
censoring. We present a novel Q-learning algorithm that is adjusted for
censored data and allows a flexible number of stages. We provide finite sample
bounds on the generalization error of the policy learned by the algorithm, and
show that when the optimal Q-function belongs to the approximation space, the
expected survival time for policies obtained by the algorithm converges to that
of the optimal policy. We simulate a multistage clinical trial with flexible
number of stages and apply the proposed censored-Q-learning algorithm to find
individualized treatment regimens. The methodology presented in this paper has
implications in the design of personalized medicine trials in cancer and in
other life-threatening diseases."@2012
Daniel Berend@http://arxiv.org/abs/1205.6711v2@On the Convergence of the Empirical Distribution@"We develop a general technique for bounding the tail of the total variation
distance between the empirical and the true distributions over countable sets.
Our methods sharpen a deviation bound of Devroye (1983) for distributions over
finite sets, and also hold for the broader class of distributions with
countable support. We also provide some lower bounds of possible independent
interest."@2012
Aryeh Kontorovich@http://arxiv.org/abs/1205.6711v2@On the Convergence of the Empirical Distribution@"We develop a general technique for bounding the tail of the total variation
distance between the empirical and the true distributions over countable sets.
Our methods sharpen a deviation bound of Devroye (1983) for distributions over
finite sets, and also hold for the broader class of distributions with
countable support. We also provide some lower bounds of possible independent
interest."@2012
Richard J. Samworth@http://arxiv.org/abs/1206.0457v1@"Independent component analysis via nonparametric maximum likelihood
  estimation"@"Independent Component Analysis (ICA) models are very popular semiparametric
models in which we observe independent copies of a random vector $X = AS$,
where $A$ is a non-singular matrix and $S$ has independent components. We
propose a new way of estimating the unmixing matrix $W = A^{-1}$ and the
marginal distributions of the components of $S$ using nonparametric maximum
likelihood. Specifically, we study the projection of the empirical distribution
onto the subset of ICA distributions having log-concave marginals. We show
that, from the point of view of estimating the unmixing matrix, it makes no
difference whether or not the log-concavity is correctly specified. The
approach is further justified by both theoretical results and a simulation
study."@2012
Ming Yuan@http://arxiv.org/abs/1206.0457v1@"Independent component analysis via nonparametric maximum likelihood
  estimation"@"Independent Component Analysis (ICA) models are very popular semiparametric
models in which we observe independent copies of a random vector $X = AS$,
where $A$ is a non-singular matrix and $S$ has independent components. We
propose a new way of estimating the unmixing matrix $W = A^{-1}$ and the
marginal distributions of the components of $S$ using nonparametric maximum
likelihood. Specifically, we study the projection of the empirical distribution
onto the subset of ICA distributions having log-concave marginals. We show
that, from the point of view of estimating the unmixing matrix, it makes no
difference whether or not the log-concavity is correctly specified. The
approach is further justified by both theoretical results and a simulation
study."@2012
Ismael Castillo@http://arxiv.org/abs/1206.0459v1@Thomas Bayes' walk on manifolds@"Convergence of the Bayes posterior measure is considered in canonical
statistical settings where observations sit on a geometrical object such as a
compact manifold, or more generally on a compact metric space verifying some
conditions. A natural geometric prior based on randomly rescaled solutions of
the heat equation is considered. Upper and lower bound posterior contraction
rates are derived."@2012
Gerard Kerkyacharian@http://arxiv.org/abs/1206.0459v1@Thomas Bayes' walk on manifolds@"Convergence of the Bayes posterior measure is considered in canonical
statistical settings where observations sit on a geometrical object such as a
compact manifold, or more generally on a compact metric space verifying some
conditions. A natural geometric prior based on randomly rescaled solutions of
the heat equation is considered. Upper and lower bound posterior contraction
rates are derived."@2012
Dominique Picard@http://arxiv.org/abs/1206.0459v1@Thomas Bayes' walk on manifolds@"Convergence of the Bayes posterior measure is considered in canonical
statistical settings where observations sit on a geometrical object such as a
compact manifold, or more generally on a compact metric space verifying some
conditions. A natural geometric prior based on randomly rescaled solutions of
the heat equation is considered. Upper and lower bound posterior contraction
rates are derived."@2012
Clifford Lam@http://arxiv.org/abs/1206.0613v1@"Factor modeling for high-dimensional time series: Inference for the
  number of factors"@"This paper deals with the factor modeling for high-dimensional time series
based on a dimension-reduction viewpoint. Under stationary settings, the
inference is simple in the sense that both the number of factors and the factor
loadings are estimated in terms of an eigenanalysis for a nonnegative definite
matrix, and is therefore applicable when the dimension of time series is on the
order of a few thousands. Asymptotic properties of the proposed method are
investigated under two settings: (i) the sample size goes to infinity while the
dimension of time series is fixed; and (ii) both the sample size and the
dimension of time series go to infinity together. In particular, our estimators
for zero-eigenvalues enjoy faster convergence (or slower divergence) rates,
hence making the estimation for the number of factors easier. In particular,
when the sample size and the dimension of time series go to infinity together,
the estimators for the eigenvalues are no longer consistent. However, our
estimator for the number of the factors, which is based on the ratios of the
estimated eigenvalues, still works fine. Furthermore, this estimation shows the
so-called ""blessing of dimensionality"" property in the sense that the
performance of the estimation may improve when the dimension of time series
increases. A two-step procedure is investigated when the factors are of
different degrees of strength. Numerical illustration with both simulated and
real data is also reported."@2012
Qiwei Yao@http://arxiv.org/abs/1206.0613v1@"Factor modeling for high-dimensional time series: Inference for the
  number of factors"@"This paper deals with the factor modeling for high-dimensional time series
based on a dimension-reduction viewpoint. Under stationary settings, the
inference is simple in the sense that both the number of factors and the factor
loadings are estimated in terms of an eigenanalysis for a nonnegative definite
matrix, and is therefore applicable when the dimension of time series is on the
order of a few thousands. Asymptotic properties of the proposed method are
investigated under two settings: (i) the sample size goes to infinity while the
dimension of time series is fixed; and (ii) both the sample size and the
dimension of time series go to infinity together. In particular, our estimators
for zero-eigenvalues enjoy faster convergence (or slower divergence) rates,
hence making the estimation for the number of factors easier. In particular,
when the sample size and the dimension of time series go to infinity together,
the estimators for the eigenvalues are no longer consistent. However, our
estimator for the number of the factors, which is based on the ratios of the
estimated eigenvalues, still works fine. Furthermore, this estimation shows the
so-called ""blessing of dimensionality"" property in the sense that the
performance of the estimation may improve when the dimension of time series
increases. A two-step procedure is investigated when the factors are of
different degrees of strength. Numerical illustration with both simulated and
real data is also reported."@2012
Qiying Wang@http://arxiv.org/abs/1206.0825v1@A specification test for nonlinear nonstationary models@"We provide a limit theory for a general class of kernel smoothed U-statistics
that may be used for specification testing in time series regression with
nonstationary data. The test framework allows for linear and nonlinear models
with endogenous regressors that have autoregressive unit roots or near unit
roots. The limit theory for the specification test depends on the
self-intersection local time of a Gaussian process. A new weak convergence
result is developed for certain partial sums of functions involving
nonstationary time series that converges to the intersection local time
process. This result is of independent interest and is useful in other
applications. Simulations examine the finite sample performance of the test."@2012
Peter C. B. Phillips@http://arxiv.org/abs/1206.0825v1@A specification test for nonlinear nonstationary models@"We provide a limit theory for a general class of kernel smoothed U-statistics
that may be used for specification testing in time series regression with
nonstationary data. The test framework allows for linear and nonlinear models
with endogenous regressors that have autoregressive unit roots or near unit
roots. The limit theory for the specification test depends on the
self-intersection local time of a Gaussian process. A new weak convergence
result is developed for certain partial sums of functions involving
nonstationary time series that converges to the intersection local time
process. This result is of independent interest and is useful in other
applications. Simulations examine the finite sample performance of the test."@2012
Bing-Yi Jing@http://arxiv.org/abs/1206.0827v1@Modeling high-frequency financial data by pure jump processes@"It is generally accepted that the asset price processes contain jumps. In
fact, pure jump models have been widely used to model asset prices and/or
stochastic volatilities. The question is: is there any statistical evidence
from the high-frequency financial data to support using pure jump models alone?
The purpose of this paper is to develop such a statistical test against the
necessity of a diffusion component. The test is very simple to use and yet
effective. Asymptotic properties of the proposed test statistic will be
studied. Simulation studies and some real-life examples are included to
illustrate our results."@2012
Xin-Bing Kong@http://arxiv.org/abs/1206.0827v1@Modeling high-frequency financial data by pure jump processes@"It is generally accepted that the asset price processes contain jumps. In
fact, pure jump models have been widely used to model asset prices and/or
stochastic volatilities. The question is: is there any statistical evidence
from the high-frequency financial data to support using pure jump models alone?
The purpose of this paper is to develop such a statistical test against the
necessity of a diffusion component. The test is very simple to use and yet
effective. Asymptotic properties of the proposed test statistic will be
studied. Simulation studies and some real-life examples are included to
illustrate our results."@2012
Zhi Liu@http://arxiv.org/abs/1206.0827v1@Modeling high-frequency financial data by pure jump processes@"It is generally accepted that the asset price processes contain jumps. In
fact, pure jump models have been widely used to model asset prices and/or
stochastic volatilities. The question is: is there any statistical evidence
from the high-frequency financial data to support using pure jump models alone?
The purpose of this paper is to develop such a statistical test against the
necessity of a diffusion component. The test is very simple to use and yet
effective. Asymptotic properties of the proposed test statistic will be
studied. Simulation studies and some real-life examples are included to
illustrate our results."@2012
Jun Shao@http://arxiv.org/abs/1206.0847v1@"Estimation in high-dimensional linear models with deterministic design
  matrices"@"Because of the advance in technologies, modern statistical studies often
encounter linear models with the number of explanatory variables much larger
than the sample size. Estimation and variable selection in these
high-dimensional problems with deterministic design points is very different
from those in the case of random covariates, due to the identifiability of the
high-dimensional regression parameter vector. We show that a reasonable
approach is to focus on the projection of the regression parameter vector onto
the linear space generated by the design matrix. In this work, we consider the
ridge regression estimator of the projection vector and propose to threshold
the ridge regression estimator when the projection vector is sparse in the
sense that many of its components are small. The proposed estimator has an
explicit form and is easy to use in application. Asymptotic properties such as
the consistency of variable selection and estimation and the convergence rate
of the prediction mean squared error are established under some sparsity
conditions on the projection vector. A simulation study is also conducted to
examine the performance of the proposed estimator."@2012
Xinwei Deng@http://arxiv.org/abs/1206.0847v1@"Estimation in high-dimensional linear models with deterministic design
  matrices"@"Because of the advance in technologies, modern statistical studies often
encounter linear models with the number of explanatory variables much larger
than the sample size. Estimation and variable selection in these
high-dimensional problems with deterministic design points is very different
from those in the case of random covariates, due to the identifiability of the
high-dimensional regression parameter vector. We show that a reasonable
approach is to focus on the projection of the regression parameter vector onto
the linear space generated by the design matrix. In this work, we consider the
ridge regression estimator of the projection vector and propose to threshold
the ridge regression estimator when the projection vector is sparse in the
sense that many of its components are small. The proposed estimator has an
explicit form and is easy to use in application. Asymptotic properties such as
the consistency of variable selection and estimation and the convergence rate
of the prediction mean squared error are established under some sparsity
conditions on the projection vector. A simulation study is also conducted to
examine the performance of the proposed estimator."@2012
Guillaume Lecué@http://arxiv.org/abs/1206.0871v1@"General nonexact oracle inequalities for classes with a subexponential
  envelope"@"We show that empirical risk minimization procedures and regularized empirical
risk minimization procedures satisfy nonexact oracle inequalities in an
unbounded framework, under the assumption that the class has a subexponential
envelope function. The main novelty, in addition to the boundedness assumption
free setup, is that those inequalities can yield fast rates even in situations
in which exact oracle inequalities only hold with slower rates. We apply these
results to show that procedures based on $\ell_1$ and nuclear norms
regularization functions satisfy oracle inequalities with a residual term that
decreases like $1/n$ for every $L_q$-loss functions ($q\geq2$), while only
assuming that the tail behavior of the input and output variables are well
behaved. In particular, no RIP type of assumption or ""incoherence condition""
are needed to obtain fast residual terms in those setups. We also apply these
results to the problems of convex aggregation and model selection."@2012
Shahar Mendelson@http://arxiv.org/abs/1206.0871v1@"General nonexact oracle inequalities for classes with a subexponential
  envelope"@"We show that empirical risk minimization procedures and regularized empirical
risk minimization procedures satisfy nonexact oracle inequalities in an
unbounded framework, under the assumption that the class has a subexponential
envelope function. The main novelty, in addition to the boundedness assumption
free setup, is that those inequalities can yield fast rates even in situations
in which exact oracle inequalities only hold with slower rates. We apply these
results to show that procedures based on $\ell_1$ and nuclear norms
regularization functions satisfy oracle inequalities with a residual term that
decreases like $1/n$ for every $L_q$-loss functions ($q\geq2$), while only
assuming that the tail behavior of the input and output variables are well
behaved. In particular, no RIP type of assumption or ""incoherence condition""
are needed to obtain fast residual terms in those setups. We also apply these
results to the problems of convex aggregation and model selection."@2012
Yu Tang@http://arxiv.org/abs/1206.0897v1@Uniform fractional factorial designs@"The minimum aberration criterion has been frequently used in the selection of
fractional factorial designs with nominal factors. For designs with
quantitative factors, however, level permutation of factors could alter their
geometrical structures and statistical properties. In this paper uniformity is
used to further distinguish fractional factorial designs, besides the minimum
aberration criterion. We show that minimum aberration designs have low
discrepancies on average. An efficient method for constructing uniform minimum
aberration designs is proposed and optimal designs with 27 and 81 runs are
obtained for practical use. These designs have good uniformity and are
effective for studying quantitative factors."@2012
Hongquan Xu@http://arxiv.org/abs/1206.0897v1@Uniform fractional factorial designs@"The minimum aberration criterion has been frequently used in the selection of
fractional factorial designs with nominal factors. For designs with
quantitative factors, however, level permutation of factors could alter their
geometrical structures and statistical properties. In this paper uniformity is
used to further distinguish fractional factorial designs, besides the minimum
aberration criterion. We show that minimum aberration designs have low
discrepancies on average. An efficient method for constructing uniform minimum
aberration designs is proposed and optimal designs with 27 and 81 runs are
obtained for practical use. These designs have good uniformity and are
effective for studying quantitative factors."@2012
Dennis K. J. Lin@http://arxiv.org/abs/1206.0897v1@Uniform fractional factorial designs@"The minimum aberration criterion has been frequently used in the selection of
fractional factorial designs with nominal factors. For designs with
quantitative factors, however, level permutation of factors could alter their
geometrical structures and statistical properties. In this paper uniformity is
used to further distinguish fractional factorial designs, besides the minimum
aberration criterion. We show that minimum aberration designs have low
discrepancies on average. An efficient method for constructing uniform minimum
aberration designs is proposed and optimal designs with 27 and 81 runs are
obtained for practical use. These designs have good uniformity and are
effective for studying quantitative factors."@2012
Romain Guy@http://arxiv.org/abs/1206.0916v2@"Parametric inference for discretely observed multidimensional diffusions
  with small diffusion coefficient"@"We consider a multidimensional diffusion X with drift coefficient
b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).
The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a
fixed interval [0,T]. We study minimum contrast estimators derived from the
Gaussian process approximating X for small {\epsilon}. We obtain consistent and
asymptotically normal estimators of {\alpha} for fixed {\Delta} and
{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and
{\epsilon}\rightarrow0. We compare the estimators obtained with various methods
and for various magnitudes of {\Delta} and {\epsilon} based on simulation
studies. Finally, we investigate the interest of using such methods in an
epidemiological framework."@2012
Catherine Laredo@http://arxiv.org/abs/1206.0916v2@"Parametric inference for discretely observed multidimensional diffusions
  with small diffusion coefficient"@"We consider a multidimensional diffusion X with drift coefficient
b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).
The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a
fixed interval [0,T]. We study minimum contrast estimators derived from the
Gaussian process approximating X for small {\epsilon}. We obtain consistent and
asymptotically normal estimators of {\alpha} for fixed {\Delta} and
{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and
{\epsilon}\rightarrow0. We compare the estimators obtained with various methods
and for various magnitudes of {\Delta} and {\epsilon} based on simulation
studies. Finally, we investigate the interest of using such methods in an
epidemiological framework."@2012
Elisabeta Vergu@http://arxiv.org/abs/1206.0916v2@"Parametric inference for discretely observed multidimensional diffusions
  with small diffusion coefficient"@"We consider a multidimensional diffusion X with drift coefficient
b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).
The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a
fixed interval [0,T]. We study minimum contrast estimators derived from the
Gaussian process approximating X for small {\epsilon}. We obtain consistent and
asymptotically normal estimators of {\alpha} for fixed {\Delta} and
{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and
{\epsilon}\rightarrow0. We compare the estimators obtained with various methods
and for various magnitudes of {\Delta} and {\epsilon} based on simulation
studies. Finally, we investigate the interest of using such methods in an
epidemiological framework."@2012
Jun Li@http://arxiv.org/abs/1206.0917v1@Two sample tests for high-dimensional covariance matrices@"We propose two tests for the equality of covariance matrices between two
high-dimensional populations. One test is on the whole variance--covariance
matrices, and the other is on off-diagonal sub-matrices, which define the
covariance between two nonoverlapping segments of the high-dimensional random
vectors. The tests are applicable (i) when the data dimension is much larger
than the sample sizes, namely the ""large $p$, small $n$"" situations and (ii)
without assuming parametric distributions for the two populations. These two
aspects surpass the capability of the conventional likelihood ratio test. The
proposed tests can be used to test on covariances associated with gene ontology
terms."@2012
Song Xi Chen@http://arxiv.org/abs/1206.0917v1@Two sample tests for high-dimensional covariance matrices@"We propose two tests for the equality of covariance matrices between two
high-dimensional populations. One test is on the whole variance--covariance
matrices, and the other is on off-diagonal sub-matrices, which define the
covariance between two nonoverlapping segments of the high-dimensional random
vectors. The tests are applicable (i) when the data dimension is much larger
than the sample sizes, namely the ""large $p$, small $n$"" situations and (ii)
without assuming parametric distributions for the two populations. These two
aspects surpass the capability of the conventional likelihood ratio test. The
proposed tests can be used to test on covariances associated with gene ontology
terms."@2012
Emre Barut@http://arxiv.org/abs/1206.1024v2@Conditional Sure Independence Screening@"Independence screening is a powerful method for variable selection for `Big
Data' when the number of variables is massive. Commonly used independence
screening methods are based on marginal correlations or variations of it. In
many applications, researchers often have some prior knowledge that a certain
set of variables is related to the response. In such a situation, a natural
assessment on the relative importance of the other predictors is the
conditional contributions of the individual predictors in presence of the known
set of variables. This results in conditional sure independence screening
(CSIS). Conditioning helps for reducing the false positive and the false
negative rates in the variable selection process. In this paper, we propose and
study CSIS in the context of generalized linear models. For
ultrahigh-dimensional statistical problems, we give conditions under which sure
screening is possible and derive an upper bound on the number of selected
variables. We also spell out the situation under which CSIS yields model
selection consistency. Moreover, we provide two data-driven methods to select
the thresholding parameter of conditional screening. The utility of the
procedure is illustrated by simulation studies and analysis of two real data
sets."@2012
Jianqing Fan@http://arxiv.org/abs/1206.1024v2@Conditional Sure Independence Screening@"Independence screening is a powerful method for variable selection for `Big
Data' when the number of variables is massive. Commonly used independence
screening methods are based on marginal correlations or variations of it. In
many applications, researchers often have some prior knowledge that a certain
set of variables is related to the response. In such a situation, a natural
assessment on the relative importance of the other predictors is the
conditional contributions of the individual predictors in presence of the known
set of variables. This results in conditional sure independence screening
(CSIS). Conditioning helps for reducing the false positive and the false
negative rates in the variable selection process. In this paper, we propose and
study CSIS in the context of generalized linear models. For
ultrahigh-dimensional statistical problems, we give conditions under which sure
screening is possible and derive an upper bound on the number of selected
variables. We also spell out the situation under which CSIS yields model
selection consistency. Moreover, we provide two data-driven methods to select
the thresholding parameter of conditional screening. The utility of the
procedure is illustrated by simulation studies and analysis of two real data
sets."@2012
Anneleen Verhasselt@http://arxiv.org/abs/1206.1024v2@Conditional Sure Independence Screening@"Independence screening is a powerful method for variable selection for `Big
Data' when the number of variables is massive. Commonly used independence
screening methods are based on marginal correlations or variations of it. In
many applications, researchers often have some prior knowledge that a certain
set of variables is related to the response. In such a situation, a natural
assessment on the relative importance of the other predictors is the
conditional contributions of the individual predictors in presence of the known
set of variables. This results in conditional sure independence screening
(CSIS). Conditioning helps for reducing the false positive and the false
negative rates in the variable selection process. In this paper, we propose and
study CSIS in the context of generalized linear models. For
ultrahigh-dimensional statistical problems, we give conditions under which sure
screening is possible and derive an upper bound on the number of selected
variables. We also spell out the situation under which CSIS yields model
selection consistency. Moreover, we provide two data-driven methods to select
the thresholding parameter of conditional screening. The utility of the
procedure is illustrated by simulation studies and analysis of two real data
sets."@2012
C. Fonseca@http://arxiv.org/abs/1206.1228v1@Stability and contagion measures for spatial extreme value analyses@"As part of global climate change an accelerated hydrologic cycle (including
an increase in heavy precipitation) is anticipated. So, it is of great
importance to be able to quantify high-impact hydrologic relationships, for
example, the impact that an extreme precipitation (or temperature) in a
location has on a surrounding region. Building on the Multivariate Extreme
Value Theory we propose a contagion index and a stability index. The contagion
index makes it possible to quantify the effect that an exceedance above a high
threshold can have on a region. The stability index reflects the expected
number of crossings of a high threshold in a region associated to a specific
location i, given the occurrence of at least one crossing at that location. We
will find some relations with well-known extremal dependence measures found in
the literature, which will provide immediate estimators. For these estimators
an application to the annual maxima precipitation in Portuguese regions is
presented."@2012
H. Ferreira@http://arxiv.org/abs/1206.1228v1@Stability and contagion measures for spatial extreme value analyses@"As part of global climate change an accelerated hydrologic cycle (including
an increase in heavy precipitation) is anticipated. So, it is of great
importance to be able to quantify high-impact hydrologic relationships, for
example, the impact that an extreme precipitation (or temperature) in a
location has on a surrounding region. Building on the Multivariate Extreme
Value Theory we propose a contagion index and a stability index. The contagion
index makes it possible to quantify the effect that an exceedance above a high
threshold can have on a region. The stability index reflects the expected
number of crossings of a high threshold in a region associated to a specific
location i, given the occurrence of at least one crossing at that location. We
will find some relations with well-known extremal dependence measures found in
the literature, which will provide immediate estimators. For these estimators
an application to the annual maxima precipitation in Portuguese regions is
presented."@2012
L. Pereira A. P.@http://arxiv.org/abs/1206.1228v1@Stability and contagion measures for spatial extreme value analyses@"As part of global climate change an accelerated hydrologic cycle (including
an increase in heavy precipitation) is anticipated. So, it is of great
importance to be able to quantify high-impact hydrologic relationships, for
example, the impact that an extreme precipitation (or temperature) in a
location has on a surrounding region. Building on the Multivariate Extreme
Value Theory we propose a contagion index and a stability index. The contagion
index makes it possible to quantify the effect that an exceedance above a high
threshold can have on a region. The stability index reflects the expected
number of crossings of a high threshold in a region associated to a specific
location i, given the occurrence of at least one crossing at that location. We
will find some relations with well-known extremal dependence measures found in
the literature, which will provide immediate estimators. For these estimators
an application to the annual maxima precipitation in Portuguese regions is
presented."@2012
Martins@http://arxiv.org/abs/1206.1228v1@Stability and contagion measures for spatial extreme value analyses@"As part of global climate change an accelerated hydrologic cycle (including
an increase in heavy precipitation) is anticipated. So, it is of great
importance to be able to quantify high-impact hydrologic relationships, for
example, the impact that an extreme precipitation (or temperature) in a
location has on a surrounding region. Building on the Multivariate Extreme
Value Theory we propose a contagion index and a stability index. The contagion
index makes it possible to quantify the effect that an exceedance above a high
threshold can have on a region. The stability index reflects the expected
number of crossings of a high threshold in a region associated to a specific
location i, given the occurrence of at least one crossing at that location. We
will find some relations with well-known extremal dependence measures found in
the literature, which will provide immediate estimators. For these estimators
an application to the annual maxima precipitation in Portuguese regions is
presented."@2012
Axel Bücher@http://arxiv.org/abs/1206.1675v1@"Consistent testing for a constant copula under strong mixing based on
  the tapered block multiplier technique"@"Considering multivariate strongly mixing time series, nonparametric tests for
a constant copula with specified or unspecified change point (candidate) are
derived; the tests are consistent against general alternatives. A tapered block
multiplier technique based on serially dependent multiplier random variables is
provided to estimate p-values of the test statistics. Size and power of the
tests in finite samples are evaluated with Monte Carlo simulations. The block
multiplier technique might have several other applications for statistical
inference on copulas of serially dependent data."@2012
Martin Ruppert@http://arxiv.org/abs/1206.1675v1@"Consistent testing for a constant copula under strong mixing based on
  the tapered block multiplier technique"@"Considering multivariate strongly mixing time series, nonparametric tests for
a constant copula with specified or unspecified change point (candidate) are
derived; the tests are consistent against general alternatives. A tapered block
multiplier technique based on serially dependent multiplier random variables is
provided to estimate p-values of the test statistics. Size and power of the
tests in finite samples are evaluated with Monte Carlo simulations. The block
multiplier technique might have several other applications for statistical
inference on copulas of serially dependent data."@2012
Pierre Alquier@http://arxiv.org/abs/1206.1711v3@Rank penalized estimation of a quantum system@"We introduce a new method to reconstruct the density matrix $\rho$ of a
system of $n$-qubits and estimate its rank $d$ from data obtained by quantum
state tomography measurements repeated $m$ times. The procedure consists in
minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by
given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the
moment method. We obtain simultaneously an estimator of the rank and the
resulting density matrix associated to this rank. We establish an upper bound
for the error of penalized estimator, evaluated with the Frobenius norm, which
is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The
proposed methodology is computationaly efficient and is illustrated with some
example states and real experimental data sets."@2012
Cristina Butucea@http://arxiv.org/abs/1206.1711v3@Rank penalized estimation of a quantum system@"We introduce a new method to reconstruct the density matrix $\rho$ of a
system of $n$-qubits and estimate its rank $d$ from data obtained by quantum
state tomography measurements repeated $m$ times. The procedure consists in
minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by
given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the
moment method. We obtain simultaneously an estimator of the rank and the
resulting density matrix associated to this rank. We establish an upper bound
for the error of penalized estimator, evaluated with the Frobenius norm, which
is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The
proposed methodology is computationaly efficient and is illustrated with some
example states and real experimental data sets."@2012
Mohamed Hebiri@http://arxiv.org/abs/1206.1711v3@Rank penalized estimation of a quantum system@"We introduce a new method to reconstruct the density matrix $\rho$ of a
system of $n$-qubits and estimate its rank $d$ from data obtained by quantum
state tomography measurements repeated $m$ times. The procedure consists in
minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by
given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the
moment method. We obtain simultaneously an estimator of the rank and the
resulting density matrix associated to this rank. We establish an upper bound
for the error of penalized estimator, evaluated with the Frobenius norm, which
is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The
proposed methodology is computationaly efficient and is illustrated with some
example states and real experimental data sets."@2012
Katia Meziani@http://arxiv.org/abs/1206.1711v3@Rank penalized estimation of a quantum system@"We introduce a new method to reconstruct the density matrix $\rho$ of a
system of $n$-qubits and estimate its rank $d$ from data obtained by quantum
state tomography measurements repeated $m$ times. The procedure consists in
minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by
given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the
moment method. We obtain simultaneously an estimator of the rank and the
resulting density matrix associated to this rank. We establish an upper bound
for the error of penalized estimator, evaluated with the Frobenius norm, which
is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The
proposed methodology is computationaly efficient and is illustrated with some
example states and real experimental data sets."@2012
Morimae Tomoyuki@http://arxiv.org/abs/1206.1711v3@Rank penalized estimation of a quantum system@"We introduce a new method to reconstruct the density matrix $\rho$ of a
system of $n$-qubits and estimate its rank $d$ from data obtained by quantum
state tomography measurements repeated $m$ times. The procedure consists in
minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by
given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the
moment method. We obtain simultaneously an estimator of the rank and the
resulting density matrix associated to this rank. We establish an upper bound
for the error of penalized estimator, evaluated with the Frobenius norm, which
is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The
proposed methodology is computationaly efficient and is illustrated with some
example states and real experimental data sets."@2012
Daniel Commenges@http://arxiv.org/abs/1206.1753v1@"A universal approximate cross-validation criterion and its asymptotic
  distribution"@"A general framework is that the estimators of a distribution are obtained by
minimizing a function (the estimating function) and they are assessed through
another function (the assessment function). The estimating and assessment
functions generally estimate risks. A classical case is that both functions
estimate an information risk (specifically cross entropy); in that case Akaike
information criterion (AIC) is relevant. In more general cases, the assessment
risk can be estimated by leave-one-out crossvalidation. Since leave-one-out
crossvalidation is computationally very demanding, an approximation formula can
be very useful. A universal approximate crossvalidation criterion (UACV) for
the leave-one-out crossvalidation is given. This criterion can be adapted to
different types of estimators, including penalized likelihood and maximum a
posteriori estimators, and of assessment risk functions, including information
risk functions and continuous rank probability score (CRPS). This formula
reduces to Takeuchi information criterion (TIC) when cross entropy is the risk
for both estimation and assessment. The asymptotic distribution of UACV and of
a difference of UACV is given. UACV can be used for comparing estimators of the
distributions of ordered categorical data derived from threshold models and
models based on continuous approximations. A simulation study and an analysis
of real psychometric data are presented."@2012
Cécile Proust-Lima@http://arxiv.org/abs/1206.1753v1@"A universal approximate cross-validation criterion and its asymptotic
  distribution"@"A general framework is that the estimators of a distribution are obtained by
minimizing a function (the estimating function) and they are assessed through
another function (the assessment function). The estimating and assessment
functions generally estimate risks. A classical case is that both functions
estimate an information risk (specifically cross entropy); in that case Akaike
information criterion (AIC) is relevant. In more general cases, the assessment
risk can be estimated by leave-one-out crossvalidation. Since leave-one-out
crossvalidation is computationally very demanding, an approximation formula can
be very useful. A universal approximate crossvalidation criterion (UACV) for
the leave-one-out crossvalidation is given. This criterion can be adapted to
different types of estimators, including penalized likelihood and maximum a
posteriori estimators, and of assessment risk functions, including information
risk functions and continuous rank probability score (CRPS). This formula
reduces to Takeuchi information criterion (TIC) when cross entropy is the risk
for both estimation and assessment. The asymptotic distribution of UACV and of
a difference of UACV is given. UACV can be used for comparing estimators of the
distributions of ordered categorical data derived from threshold models and
models based on continuous approximations. A simulation study and an analysis
of real psychometric data are presented."@2012
Cécilia Samieri@http://arxiv.org/abs/1206.1753v1@"A universal approximate cross-validation criterion and its asymptotic
  distribution"@"A general framework is that the estimators of a distribution are obtained by
minimizing a function (the estimating function) and they are assessed through
another function (the assessment function). The estimating and assessment
functions generally estimate risks. A classical case is that both functions
estimate an information risk (specifically cross entropy); in that case Akaike
information criterion (AIC) is relevant. In more general cases, the assessment
risk can be estimated by leave-one-out crossvalidation. Since leave-one-out
crossvalidation is computationally very demanding, an approximation formula can
be very useful. A universal approximate crossvalidation criterion (UACV) for
the leave-one-out crossvalidation is given. This criterion can be adapted to
different types of estimators, including penalized likelihood and maximum a
posteriori estimators, and of assessment risk functions, including information
risk functions and continuous rank probability score (CRPS). This formula
reduces to Takeuchi information criterion (TIC) when cross entropy is the risk
for both estimation and assessment. The asymptotic distribution of UACV and of
a difference of UACV is given. UACV can be used for comparing estimators of the
distributions of ordered categorical data derived from threshold models and
models based on continuous approximations. A simulation study and an analysis
of real psychometric data are presented."@2012
Benoit Liquet@http://arxiv.org/abs/1206.1753v1@"A universal approximate cross-validation criterion and its asymptotic
  distribution"@"A general framework is that the estimators of a distribution are obtained by
minimizing a function (the estimating function) and they are assessed through
another function (the assessment function). The estimating and assessment
functions generally estimate risks. A classical case is that both functions
estimate an information risk (specifically cross entropy); in that case Akaike
information criterion (AIC) is relevant. In more general cases, the assessment
risk can be estimated by leave-one-out crossvalidation. Since leave-one-out
crossvalidation is computationally very demanding, an approximation formula can
be very useful. A universal approximate crossvalidation criterion (UACV) for
the leave-one-out crossvalidation is given. This criterion can be adapted to
different types of estimators, including penalized likelihood and maximum a
posteriori estimators, and of assessment risk functions, including information
risk functions and continuous rank probability score (CRPS). This formula
reduces to Takeuchi information criterion (TIC) when cross entropy is the risk
for both estimation and assessment. The asymptotic distribution of UACV and of
a difference of UACV is given. UACV can be used for comparing estimators of the
distributions of ordered categorical data derived from threshold models and
models based on continuous approximations. A simulation study and an analysis
of real psychometric data are presented."@2012
Tiago M. Vargas@http://arxiv.org/abs/1206.2206v1@"Gradient statistic: higher-order asymptotics and Bartlett-type
  correction"@"We obtain an asymptotic expansion for the null distribution function of
thegradient statistic for testing composite null hypotheses in the presence of
nuisance parameters. The expansion is derived using a Bayesian route based on
the shrinkage argument described in Ghosh and Mukerjee (1991). Using this
expansion, we propose a Bartlett-type corrected gradient statistic with
chi-square distribution up to an error of order o(n^{-1}) under the null
hypothesis. Further, we also use the expansion to modify the percentage points
of the large sample reference chi-square distribution. A small Monte Carlo
experiment and various examples are presented and discussed."@2012
Silvia L. P. Ferrari@http://arxiv.org/abs/1206.2206v1@"Gradient statistic: higher-order asymptotics and Bartlett-type
  correction"@"We obtain an asymptotic expansion for the null distribution function of
thegradient statistic for testing composite null hypotheses in the presence of
nuisance parameters. The expansion is derived using a Bayesian route based on
the shrinkage argument described in Ghosh and Mukerjee (1991). Using this
expansion, we propose a Bartlett-type corrected gradient statistic with
chi-square distribution up to an error of order o(n^{-1}) under the null
hypothesis. Further, we also use the expansion to modify the percentage points
of the large sample reference chi-square distribution. A small Monte Carlo
experiment and various examples are presented and discussed."@2012
Artur J. Lemonte@http://arxiv.org/abs/1206.2206v1@"Gradient statistic: higher-order asymptotics and Bartlett-type
  correction"@"We obtain an asymptotic expansion for the null distribution function of
thegradient statistic for testing composite null hypotheses in the presence of
nuisance parameters. The expansion is derived using a Bayesian route based on
the shrinkage argument described in Ghosh and Mukerjee (1991). Using this
expansion, we propose a Bartlett-type corrected gradient statistic with
chi-square distribution up to an error of order o(n^{-1}) under the null
hypothesis. Further, we also use the expansion to modify the percentage points
of the large sample reference chi-square distribution. A small Monte Carlo
experiment and various examples are presented and discussed."@2012
Dave Zachariah@http://arxiv.org/abs/1206.2493v1@Alternating Least-Squares for Low-Rank Matrix Reconstruction@"For reconstruction of low-rank matrices from undersampled measurements, we
develop an iterative algorithm based on least-squares estimation. While the
algorithm can be used for any low-rank matrix, it is also capable of exploiting
a-priori knowledge of matrix structure. In particular, we consider linearly
structured matrices, such as Hankel and Toeplitz, as well as positive
semidefinite matrices. The performance of the algorithm, referred to as
alternating least-squares (ALS), is evaluated by simulations and compared to
the Cram\'er-Rao bounds."@2012
Martin Sundin@http://arxiv.org/abs/1206.2493v1@Alternating Least-Squares for Low-Rank Matrix Reconstruction@"For reconstruction of low-rank matrices from undersampled measurements, we
develop an iterative algorithm based on least-squares estimation. While the
algorithm can be used for any low-rank matrix, it is also capable of exploiting
a-priori knowledge of matrix structure. In particular, we consider linearly
structured matrices, such as Hankel and Toeplitz, as well as positive
semidefinite matrices. The performance of the algorithm, referred to as
alternating least-squares (ALS), is evaluated by simulations and compared to
the Cram\'er-Rao bounds."@2012
Magnus Jansson@http://arxiv.org/abs/1206.2493v1@Alternating Least-Squares for Low-Rank Matrix Reconstruction@"For reconstruction of low-rank matrices from undersampled measurements, we
develop an iterative algorithm based on least-squares estimation. While the
algorithm can be used for any low-rank matrix, it is also capable of exploiting
a-priori knowledge of matrix structure. In particular, we consider linearly
structured matrices, such as Hankel and Toeplitz, as well as positive
semidefinite matrices. The performance of the algorithm, referred to as
alternating least-squares (ALS), is evaluated by simulations and compared to
the Cram\'er-Rao bounds."@2012
Saikat Chatterjee@http://arxiv.org/abs/1206.2493v1@Alternating Least-Squares for Low-Rank Matrix Reconstruction@"For reconstruction of low-rank matrices from undersampled measurements, we
develop an iterative algorithm based on least-squares estimation. While the
algorithm can be used for any low-rank matrix, it is also capable of exploiting
a-priori knowledge of matrix structure. In particular, we consider linearly
structured matrices, such as Hankel and Toeplitz, as well as positive
semidefinite matrices. The performance of the algorithm, referred to as
alternating least-squares (ALS), is evaluated by simulations and compared to
the Cram\'er-Rao bounds."@2012
Dave Zachariah@http://arxiv.org/abs/1206.2496v1@Dynamic Iterative Pursuit@"For compressive sensing of dynamic sparse signals, we develop an iterative
pursuit algorithm. A dynamic sparse signal process is characterized by varying
sparsity patterns over time/space. For such signals, the developed algorithm is
able to incorporate sequential predictions, thereby providing better
compressive sensing recovery performance, but not at the cost of high
complexity. Through experimental evaluations, we observe that the new algorithm
exhibits a graceful degradation at deteriorating signal conditions while
capable of yielding substantial performance gains as conditions improve."@2012
Saikat Chatterjee@http://arxiv.org/abs/1206.2496v1@Dynamic Iterative Pursuit@"For compressive sensing of dynamic sparse signals, we develop an iterative
pursuit algorithm. A dynamic sparse signal process is characterized by varying
sparsity patterns over time/space. For such signals, the developed algorithm is
able to incorporate sequential predictions, thereby providing better
compressive sensing recovery performance, but not at the cost of high
complexity. Through experimental evaluations, we observe that the new algorithm
exhibits a graceful degradation at deteriorating signal conditions while
capable of yielding substantial performance gains as conditions improve."@2012
Magnus Jansson@http://arxiv.org/abs/1206.2496v1@Dynamic Iterative Pursuit@"For compressive sensing of dynamic sparse signals, we develop an iterative
pursuit algorithm. A dynamic sparse signal process is characterized by varying
sparsity patterns over time/space. For such signals, the developed algorithm is
able to incorporate sequential predictions, thereby providing better
compressive sensing recovery performance, but not at the cost of high
complexity. Through experimental evaluations, we observe that the new algorithm
exhibits a graceful degradation at deteriorating signal conditions while
capable of yielding substantial performance gains as conditions improve."@2012
Felix Abramovich@http://arxiv.org/abs/1206.3422v2@Model selection in regression under structural constraints@"The paper considers model selection in regression under the additional
structural constraints on admissible models where the number of potential
predictors might be even larger than the available sample size. We develop a
Bayesian formalism as a natural tool for generating a wide class of model
selection criteria based on penalized least squares estimation with various
complexity penalties associated with a prior on a model size. The resulting
criteria are adaptive to structural constraints. We establish the upper bound
for the quadratic risk of the resulting MAP estimator and the corresponding
lower bound for the minimax risk over a set of admissible models of a given
size. We then specify the class of priors (and, therefore, the class of
complexity penalties) where for the ""nearly-orthogonal"" design the MAP
estimator is asymptotically at least nearly-minimax (up to a log-factor)
simultaneously over an entire range of sparse and dense setups. Moreover, when
the numbers of admissible models are ""small"" (e.g., ordered variable selection)
or, on the opposite, for the case of complete variable selection, the proposed
estimator achieves the exact minimax rates."@2012
Vadim Grinshtein@http://arxiv.org/abs/1206.3422v2@Model selection in regression under structural constraints@"The paper considers model selection in regression under the additional
structural constraints on admissible models where the number of potential
predictors might be even larger than the available sample size. We develop a
Bayesian formalism as a natural tool for generating a wide class of model
selection criteria based on penalized least squares estimation with various
complexity penalties associated with a prior on a model size. The resulting
criteria are adaptive to structural constraints. We establish the upper bound
for the quadratic risk of the resulting MAP estimator and the corresponding
lower bound for the minimax risk over a set of admissible models of a given
size. We then specify the class of priors (and, therefore, the class of
complexity penalties) where for the ""nearly-orthogonal"" design the MAP
estimator is asymptotically at least nearly-minimax (up to a log-factor)
simultaneously over an entire range of sparse and dense setups. Moreover, when
the numbers of admissible models are ""small"" (e.g., ordered variable selection)
or, on the opposite, for the case of complete variable selection, the proposed
estimator achieves the exact minimax rates."@2012
Debdeep Pati@http://arxiv.org/abs/1206.3627v4@"Posterior contraction in sparse Bayesian factor models for massive
  covariance matrices"@"Sparse Bayesian factor models are routinely implemented for parsimonious
dependence modeling and dimensionality reduction in high-dimensional
applications. We provide theoretical understanding of such Bayesian procedures
in terms of posterior convergence rates in inferring high-dimensional
covariance matrices where the dimension can be larger than the sample size.
Under relevant sparsity assumptions on the true covariance matrix, we show that
commonly-used point mass mixture priors on the factor loadings lead to
consistent estimation in the operator norm even when $p\gg n$. One of our major
contributions is to develop a new class of continuous shrinkage priors and
provide insights into their concentration around sparse vectors. Using such
priors for the factor loadings, we obtain similar rate of convergence as
obtained with point mass mixture priors. To obtain the convergence rates, we
construct test functions to separate points in the space of high-dimensional
covariance matrices using insights from random matrix theory; the tools
developed may be of independent interest. We also derive minimax rates and show
that the Bayesian posterior rates of convergence coincide with the minimax
rates upto a $\sqrt{\log n}$ term."@2012
Anirban Bhattacharya@http://arxiv.org/abs/1206.3627v4@"Posterior contraction in sparse Bayesian factor models for massive
  covariance matrices"@"Sparse Bayesian factor models are routinely implemented for parsimonious
dependence modeling and dimensionality reduction in high-dimensional
applications. We provide theoretical understanding of such Bayesian procedures
in terms of posterior convergence rates in inferring high-dimensional
covariance matrices where the dimension can be larger than the sample size.
Under relevant sparsity assumptions on the true covariance matrix, we show that
commonly-used point mass mixture priors on the factor loadings lead to
consistent estimation in the operator norm even when $p\gg n$. One of our major
contributions is to develop a new class of continuous shrinkage priors and
provide insights into their concentration around sparse vectors. Using such
priors for the factor loadings, we obtain similar rate of convergence as
obtained with point mass mixture priors. To obtain the convergence rates, we
construct test functions to separate points in the space of high-dimensional
covariance matrices using insights from random matrix theory; the tools
developed may be of independent interest. We also derive minimax rates and show
that the Bayesian posterior rates of convergence coincide with the minimax
rates upto a $\sqrt{\log n}$ term."@2012
Natesh S. Pillai@http://arxiv.org/abs/1206.3627v4@"Posterior contraction in sparse Bayesian factor models for massive
  covariance matrices"@"Sparse Bayesian factor models are routinely implemented for parsimonious
dependence modeling and dimensionality reduction in high-dimensional
applications. We provide theoretical understanding of such Bayesian procedures
in terms of posterior convergence rates in inferring high-dimensional
covariance matrices where the dimension can be larger than the sample size.
Under relevant sparsity assumptions on the true covariance matrix, we show that
commonly-used point mass mixture priors on the factor loadings lead to
consistent estimation in the operator norm even when $p\gg n$. One of our major
contributions is to develop a new class of continuous shrinkage priors and
provide insights into their concentration around sparse vectors. Using such
priors for the factor loadings, we obtain similar rate of convergence as
obtained with point mass mixture priors. To obtain the convergence rates, we
construct test functions to separate points in the space of high-dimensional
covariance matrices using insights from random matrix theory; the tools
developed may be of independent interest. We also derive minimax rates and show
that the Bayesian posterior rates of convergence coincide with the minimax
rates upto a $\sqrt{\log n}$ term."@2012
David Dunson@http://arxiv.org/abs/1206.3627v4@"Posterior contraction in sparse Bayesian factor models for massive
  covariance matrices"@"Sparse Bayesian factor models are routinely implemented for parsimonious
dependence modeling and dimensionality reduction in high-dimensional
applications. We provide theoretical understanding of such Bayesian procedures
in terms of posterior convergence rates in inferring high-dimensional
covariance matrices where the dimension can be larger than the sample size.
Under relevant sparsity assumptions on the true covariance matrix, we show that
commonly-used point mass mixture priors on the factor loadings lead to
consistent estimation in the operator norm even when $p\gg n$. One of our major
contributions is to develop a new class of continuous shrinkage priors and
provide insights into their concentration around sparse vectors. Using such
priors for the factor loadings, we obtain similar rate of convergence as
obtained with point mass mixture priors. To obtain the convergence rates, we
construct test functions to separate points in the space of high-dimensional
covariance matrices using insights from random matrix theory; the tools
developed may be of independent interest. We also derive minimax rates and show
that the Bayesian posterior rates of convergence coincide with the minimax
rates upto a $\sqrt{\log n}$ term."@2012
Roberto Fontana@http://arxiv.org/abs/1206.3911v2@Saturated fractions of two-factor designs@"In this paper we study saturated fractions of a two-factor design under the
simple effect model. In particular, we define a criterion to check whether a
given fraction is saturated or not, and we compute the number of saturated
fractions. All proofs are constructive and can be used as actual methods to
build saturated fractions. Moreover, we show how the theory of Markov bases for
contingency tables can be applied to two-factor designs for moving between the
designs with given margins."@2012
Fabio Rapallo@http://arxiv.org/abs/1206.3911v2@Saturated fractions of two-factor designs@"In this paper we study saturated fractions of a two-factor design under the
simple effect model. In particular, we define a criterion to check whether a
given fraction is saturated or not, and we compute the number of saturated
fractions. All proofs are constructive and can be used as actual methods to
build saturated fractions. Moreover, we show how the theory of Markov bases for
contingency tables can be applied to two-factor designs for moving between the
designs with given margins."@2012
Maria Piera Rogantin@http://arxiv.org/abs/1206.3911v2@Saturated fractions of two-factor designs@"In this paper we study saturated fractions of a two-factor design under the
simple effect model. In particular, we define a criterion to check whether a
given fraction is saturated or not, and we compute the number of saturated
fractions. All proofs are constructive and can be used as actual methods to
build saturated fractions. Moreover, we show how the theory of Markov bases for
contingency tables can be applied to two-factor designs for moving between the
designs with given margins."@2012
Yindeng Jiang@http://arxiv.org/abs/1206.4765v1@Reverse Exchangeability and Extreme Order Statistics@"For a bivariate random vector (X,Y), symmetry conditions are presented that
yield stochastic orderings among |X|, |Y|, |max(X,Y)|, and | min(X, Y)|.
Partial extensions of these results for multivariate random vectors (X1,...,Xn)
are also given."@2012
Michael D. Perlman@http://arxiv.org/abs/1206.4765v1@Reverse Exchangeability and Extreme Order Statistics@"For a bivariate random vector (X,Y), symmetry conditions are presented that
yield stochastic orderings among |X|, |Y|, |max(X,Y)|, and | min(X, Y)|.
Partial extensions of these results for multivariate random vectors (X1,...,Xn)
are also given."@2012
Aixin Tan@http://arxiv.org/abs/1206.4770v1@On the Geometric Ergodicity of Two-Variable Gibbs Samplers@"A Markov chain is geometrically ergodic if it converges to its in- variant
distribution at a geometric rate in total variation norm. We study geo- metric
ergodicity of deterministic and random scan versions of the two-variable Gibbs
sampler. We give a sufficient condition which simultaneously guarantees both
versions are geometrically ergodic. We also develop a method for simul-
taneously establishing that both versions are subgeometrically ergodic. These
general results allow us to characterize the convergence rate of two-variable
Gibbs samplers in a particular family of discrete bivariate distributions."@2012
Galin L. Jones@http://arxiv.org/abs/1206.4770v1@On the Geometric Ergodicity of Two-Variable Gibbs Samplers@"A Markov chain is geometrically ergodic if it converges to its in- variant
distribution at a geometric rate in total variation norm. We study geo- metric
ergodicity of deterministic and random scan versions of the two-variable Gibbs
sampler. We give a sufficient condition which simultaneously guarantees both
versions are geometrically ergodic. We also develop a method for simul-
taneously establishing that both versions are subgeometrically ergodic. These
general results allow us to characterize the convergence rate of two-variable
Gibbs samplers in a particular family of discrete bivariate distributions."@2012
James P. Hobert@http://arxiv.org/abs/1206.4770v1@On the Geometric Ergodicity of Two-Variable Gibbs Samplers@"A Markov chain is geometrically ergodic if it converges to its in- variant
distribution at a geometric rate in total variation norm. We study geo- metric
ergodicity of deterministic and random scan versions of the two-variable Gibbs
sampler. We give a sufficient condition which simultaneously guarantees both
versions are geometrically ergodic. We also develop a method for simul-
taneously establishing that both versions are subgeometrically ergodic. These
general results allow us to characterize the convergence rate of two-variable
Gibbs samplers in a particular family of discrete bivariate distributions."@2012
Shota Gugushvili@http://arxiv.org/abs/1206.4981v2@"Non-parametric Bayesian drift estimation for stochastic differential
  equations"@"We consider non-parametric Bayesian estimation of the drift coefficient of a
one-dimensional stochastic differential equation from discrete-time
observations on the solution of this equation. Under suitable regularity
conditions that are weaker than those previosly suggested in the literature, we
establish posterior consistency in this context. Furthermore, we show that
posterior consistency extends to the multidimensional setting as well, which,
to the best of our knowledge, is a new result in this setting."@2012
Peter Spreij@http://arxiv.org/abs/1206.4981v2@"Non-parametric Bayesian drift estimation for stochastic differential
  equations"@"We consider non-parametric Bayesian estimation of the drift coefficient of a
one-dimensional stochastic differential equation from discrete-time
observations on the solution of this equation. Under suitable regularity
conditions that are weaker than those previosly suggested in the literature, we
establish posterior consistency in this context. Furthermore, we show that
posterior consistency extends to the multidimensional setting as well, which,
to the best of our knowledge, is a new result in this setting."@2012
Darren Homrighausen@http://arxiv.org/abs/1206.6128v2@Leave-one-out cross-validation is risk consistent for lasso@"The lasso procedure is ubiquitous in the statistical and signal processing
literature, and as such, is the target of substantial theoretical and applied
research. While much of this research focuses on the desirable properties that
lasso possesses---predictive risk consistency, sign consistency, correct model
selection---all of it has assumes that the tuning parameter is chosen in an
oracle fashion. Yet, this is impossible in practice. Instead, data analysts
must use the data twice, once to choose the tuning parameter and again to
estimate the model. But only heuristics have ever justified such a procedure.
To this end, we give the first definitive answer about the risk consistency of
lasso when the smoothing parameter is chosen via cross-validation. We show that
under some restrictions on the design matrix, the lasso estimator is still risk
consistent with an empirically chosen tuning parameter."@2012
Daniel J. McDonald@http://arxiv.org/abs/1206.6128v2@Leave-one-out cross-validation is risk consistent for lasso@"The lasso procedure is ubiquitous in the statistical and signal processing
literature, and as such, is the target of substantial theoretical and applied
research. While much of this research focuses on the desirable properties that
lasso possesses---predictive risk consistency, sign consistency, correct model
selection---all of it has assumes that the tuning parameter is chosen in an
oracle fashion. Yet, this is impossible in practice. Instead, data analysts
must use the data twice, once to choose the tuning parameter and again to
estimate the model. But only heuristics have ever justified such a procedure.
To this end, we give the first definitive answer about the risk consistency of
lasso when the smoothing parameter is chosen via cross-validation. We show that
under some restrictions on the design matrix, the lasso estimator is still risk
consistent with an empirically chosen tuning parameter."@2012
Luai Al Labadi@http://arxiv.org/abs/1206.6658v1@"On Some Asymptotic Properties and an Almost Sure Approximation of the
  Normalized Inverse-Gaussian Process"@"In this paper, we present some asymptotic properties of the normalized
inverse-Gaussian process. In particular, when the concentration parameter is
large, we establish an analogue of the empirical functional central limit
theorem, the strong law of large numbers and the Glivenko-Cantelli theorem for
the normalized inverse-Gaussian process and its corresponding quantile process.
We also derive a finite sum-representation that converges almost surely to the
Ferguson and Klass representation of the normalized inverse-Gaussian process.
This almost sure approximation can be used to simulate efficiently the
normalized inverse-Gaussian process."@2012
Mahmoud Zarepour@http://arxiv.org/abs/1206.6658v1@"On Some Asymptotic Properties and an Almost Sure Approximation of the
  Normalized Inverse-Gaussian Process"@"In this paper, we present some asymptotic properties of the normalized
inverse-Gaussian process. In particular, when the concentration parameter is
large, we establish an analogue of the empirical functional central limit
theorem, the strong law of large numbers and the Glivenko-Cantelli theorem for
the normalized inverse-Gaussian process and its corresponding quantile process.
We also derive a finite sum-representation that converges almost surely to the
Ferguson and Klass representation of the normalized inverse-Gaussian process.
This almost sure approximation can be used to simulate efficiently the
normalized inverse-Gaussian process."@2012
Persi Diaconis@http://arxiv.org/abs/1206.6913v2@Sampling From A Manifold@"We develop algorithms for sampling from a probability distribution on a
submanifold embedded in Rn. Applications are given to the evaluation of
algorithms in 'Topological Statistics'; to goodness of fit tests in exponential
families and to Neyman's smooth test. This article is partially expository,
giving an introduction to the tools of geometric measure theory."@2012
Susan Holmes@http://arxiv.org/abs/1206.6913v2@Sampling From A Manifold@"We develop algorithms for sampling from a probability distribution on a
submanifold embedded in Rn. Applications are given to the evaluation of
algorithms in 'Topological Statistics'; to goodness of fit tests in exponential
families and to Neyman's smooth test. This article is partially expository,
giving an introduction to the tools of geometric measure theory."@2012
Mehrdad Shahshahani@http://arxiv.org/abs/1206.6913v2@Sampling From A Manifold@"We develop algorithms for sampling from a probability distribution on a
submanifold embedded in Rn. Applications are given to the evaluation of
algorithms in 'Topological Statistics'; to goodness of fit tests in exponential
families and to Neyman's smooth test. This article is partially expository,
giving an introduction to the tools of geometric measure theory."@2012
Ilia Negri@http://arxiv.org/abs/1206.6961v1@"Moment convergence of $Z$-estimators and $Z$-process method for change
  point problems"@"The problem to establish not only the asymptotic distribution results for
statistical estimators but also the moment convergence of the estimators has
been recognized as an important issue in advanced theories of statistics. One
of the main goals of this paper is to present a metod to derive the moment
convergence of $Z$-estimators as it has been done for $M$-estimators. Another
goal of this paper is to develop a general, unified approach, based on some
partial estimation functions which we call ""$Z$-process"", to the change point
problems for ergodic models as well as some models where the Fisher information
matrix is random and inhomogeneous in time. Applications to some diffusion
process models and Cox's regression model are also discussed."@2012
Yoichi Nishiyama@http://arxiv.org/abs/1206.6961v1@"Moment convergence of $Z$-estimators and $Z$-process method for change
  point problems"@"The problem to establish not only the asymptotic distribution results for
statistical estimators but also the moment convergence of the estimators has
been recognized as an important issue in advanced theories of statistics. One
of the main goals of this paper is to present a metod to derive the moment
convergence of $Z$-estimators as it has been done for $M$-estimators. Another
goal of this paper is to develop a general, unified approach, based on some
partial estimation functions which we call ""$Z$-process"", to the change point
problems for ergodic models as well as some models where the Fisher information
matrix is random and inhomogeneous in time. Applications to some diffusion
process models and Cox's regression model are also discussed."@2012
Mahendra Mariadassou@http://arxiv.org/abs/1206.7101v3@"Convergence of the groups posterior distribution in latent or stochastic
  block models"@"We propose a unified framework for studying both latent and stochastic block
models, which are used to cluster simultaneously rows and columns of a data
matrix. In this new framework, we study the behaviour of the groups posterior
distribution, given the data. We characterize whether it is possible to
asymptotically recover the actual groups on the rows and columns of the matrix,
relying on a consistent estimate of the parameter. In other words, we establish
sufficient conditions for the groups posterior distribution to converge (as the
size of the data increases) to a Dirac mass located at the actual (random)
groups configuration. In particular, we highlight some cases where the model
assumes symmetries in the matrix of connection probabilities that prevents
recovering the original groups. We also discuss the validity of these results
when the proportion of non-null entries in the data matrix converges to zero."@2012
Catherine Matias@http://arxiv.org/abs/1206.7101v3@"Convergence of the groups posterior distribution in latent or stochastic
  block models"@"We propose a unified framework for studying both latent and stochastic block
models, which are used to cluster simultaneously rows and columns of a data
matrix. In this new framework, we study the behaviour of the groups posterior
distribution, given the data. We characterize whether it is possible to
asymptotically recover the actual groups on the rows and columns of the matrix,
relying on a consistent estimate of the parameter. In other words, we establish
sufficient conditions for the groups posterior distribution to converge (as the
size of the data increases) to a Dirac mass located at the actual (random)
groups configuration. In particular, we highlight some cases where the model
assumes symmetries in the matrix of connection probabilities that prevents
recovering the original groups. We also discuss the validity of these results
when the proportion of non-null entries in the data matrix converges to zero."@2012
Naftali Harris@http://arxiv.org/abs/1207.0242v1@PC algorithm for Gaussian copula graphical models@"The PC algorithm uses conditional independence tests for model selection in
graphical modeling with acyclic directed graphs. In Gaussian models, tests of
conditional independence are typically based on Pearson correlations, and
high-dimensional consistency results have been obtained for the PC algorithm in
this setting. We prove that high-dimensional consistency carries over to the
broader class of Gaussian copula or \textit{nonparanormal} models when using
rank-based measures of correlation. For graphs with bounded degree, our result
is as strong as prior Gaussian results. In simulations, the `Rank PC' algorithm
works as well as the `Pearson PC' algorithm for normal data and considerably
better for non-normal Gaussian copula data, all the while incurring a
negligible increase of computation time. Simulations with contaminated data
show that rank correlations can also perform better than other robust estimates
considered in previous work when the underlying distribution does not belong to
the nonparanormal family."@2012
Mathias Drton@http://arxiv.org/abs/1207.0242v1@PC algorithm for Gaussian copula graphical models@"The PC algorithm uses conditional independence tests for model selection in
graphical modeling with acyclic directed graphs. In Gaussian models, tests of
conditional independence are typically based on Pearson correlations, and
high-dimensional consistency results have been obtained for the PC algorithm in
this setting. We prove that high-dimensional consistency carries over to the
broader class of Gaussian copula or \textit{nonparanormal} models when using
rank-based measures of correlation. For graphs with bounded degree, our result
is as strong as prior Gaussian results. In simulations, the `Rank PC' algorithm
works as well as the `Pearson PC' algorithm for normal data and considerably
better for non-normal Gaussian copula data, all the while incurring a
negligible increase of computation time. Simulations with contaminated data
show that rank correlations can also perform better than other robust estimates
considered in previous work when the underlying distribution does not belong to
the nonparanormal family."@2012
Marc Hallin@http://arxiv.org/abs/1207.0282v1@"Skew-symmetric distributions and Fisher information -- a tale of two
  densities"@"Skew-symmetric densities recently received much attention in the literature,
giving rise to increasingly general families of univariate and multivariate
skewed densities. Most of those families, however, suffer from the inferential
drawback of a potentially singular Fisher information in the vicinity of
symmetry. All existing results indicate that Gaussian densities (possibly after
restriction to some linear subspace) play a special and somewhat intriguing
role in that context. We dispel that widespread opinion by providing a full
characterization, in a general multivariate context, of the information
singularity phenomenon, highlighting its relation to a possible link between
symmetric kernels and skewing functions -- a link that can be interpreted as
the mismatch of two densities."@2012
Christophe Ley@http://arxiv.org/abs/1207.0282v1@"Skew-symmetric distributions and Fisher information -- a tale of two
  densities"@"Skew-symmetric densities recently received much attention in the literature,
giving rise to increasingly general families of univariate and multivariate
skewed densities. Most of those families, however, suffer from the inferential
drawback of a potentially singular Fisher information in the vicinity of
symmetry. All existing results indicate that Gaussian densities (possibly after
restriction to some linear subspace) play a special and somewhat intriguing
role in that context. We dispel that widespread opinion by providing a full
characterization, in a general multivariate context, of the information
singularity phenomenon, highlighting its relation to a possible link between
symmetric kernels and skewing functions -- a link that can be interpreted as
the mismatch of two densities."@2012
Fasano María Victoria@http://arxiv.org/abs/1207.0473v1@Consistency of M estimates for separable nonlinear regression models@"Consider a nonlinear regression model : y_{i}=g(x_{i},{\theta})+e_{i},
i=1,...,n, where the x_{i} are random predictors x_{i} and {\theta} is the
unknown parameter vector ranging in a set {\Theta}\subsetR^{p}. All known
results on the consistency of the least squares estimator and in general of M
estimators assume that either {\Theta} is compact or g is bounded, which
excludes frequently employed models such as the Michaelis-Menten, logistic
growth and exponential decay models. In this article we deal with the so-called
separable models, where p=p_{1}+p_{2}, {\theta}=({\alpha},{\beta}) with
{\alpha}\inA\subsetR^{p_{1}}, {\beta}\inB\subsetR^{p_{2},}and g has the form
g(x,{\theta})={\beta}^{T}h(x,{\alpha}) where h is a function with values in
R^{p_{2}}. We prove the strong consistency of M estimators under very general
assumptions, assuming that h is a bounded function of {\alpha}, which includes
the three models mentioned above. Key words and phrases: Nonlinear regression,
separable models, consistency, robust estimation."@2012
Ricardo A. Maronna@http://arxiv.org/abs/1207.0473v1@Consistency of M estimates for separable nonlinear regression models@"Consider a nonlinear regression model : y_{i}=g(x_{i},{\theta})+e_{i},
i=1,...,n, where the x_{i} are random predictors x_{i} and {\theta} is the
unknown parameter vector ranging in a set {\Theta}\subsetR^{p}. All known
results on the consistency of the least squares estimator and in general of M
estimators assume that either {\Theta} is compact or g is bounded, which
excludes frequently employed models such as the Michaelis-Menten, logistic
growth and exponential decay models. In this article we deal with the so-called
separable models, where p=p_{1}+p_{2}, {\theta}=({\alpha},{\beta}) with
{\alpha}\inA\subsetR^{p_{1}}, {\beta}\inB\subsetR^{p_{2},}and g has the form
g(x,{\theta})={\beta}^{T}h(x,{\alpha}) where h is a function with values in
R^{p_{2}}. We prove the strong consistency of M estimators under very general
assumptions, assuming that h is a bounded function of {\alpha}, which includes
the three models mentioned above. Key words and phrases: Nonlinear regression,
separable models, consistency, robust estimation."@2012
Caroline Uhler@http://arxiv.org/abs/1207.0547v3@Geometry of the faithfulness assumption in causal inference@"Many algorithms for inferring causality rely heavily on the faithfulness
assumption. The main justification for imposing this assumption is that the set
of unfaithful distributions has Lebesgue measure zero, since it can be seen as
a collection of hypersurfaces in a hypercube. However, due to sampling error
the faithfulness condition alone is not sufficient for statistical estimation,
and strong-faithfulness has been proposed and assumed to achieve uniform or
high-dimensional consistency. In contrast to the plain faithfulness assumption,
the set of distributions that is not strong-faithful has nonzero Lebesgue
measure and in fact, can be surprisingly large as we show in this paper. We
study the strong-faithfulness condition from a geometric and combinatorial
point of view and give upper and lower bounds on the Lebesgue measure of
strong-faithful distributions for various classes of directed acyclic graphs.
Our results imply fundamental limitations for the PC-algorithm and potentially
also for other algorithms based on partial correlation testing in the Gaussian
case."@2012
Garvesh Raskutti@http://arxiv.org/abs/1207.0547v3@Geometry of the faithfulness assumption in causal inference@"Many algorithms for inferring causality rely heavily on the faithfulness
assumption. The main justification for imposing this assumption is that the set
of unfaithful distributions has Lebesgue measure zero, since it can be seen as
a collection of hypersurfaces in a hypercube. However, due to sampling error
the faithfulness condition alone is not sufficient for statistical estimation,
and strong-faithfulness has been proposed and assumed to achieve uniform or
high-dimensional consistency. In contrast to the plain faithfulness assumption,
the set of distributions that is not strong-faithful has nonzero Lebesgue
measure and in fact, can be surprisingly large as we show in this paper. We
study the strong-faithfulness condition from a geometric and combinatorial
point of view and give upper and lower bounds on the Lebesgue measure of
strong-faithful distributions for various classes of directed acyclic graphs.
Our results imply fundamental limitations for the PC-algorithm and potentially
also for other algorithms based on partial correlation testing in the Gaussian
case."@2012
Peter Bühlmann@http://arxiv.org/abs/1207.0547v3@Geometry of the faithfulness assumption in causal inference@"Many algorithms for inferring causality rely heavily on the faithfulness
assumption. The main justification for imposing this assumption is that the set
of unfaithful distributions has Lebesgue measure zero, since it can be seen as
a collection of hypersurfaces in a hypercube. However, due to sampling error
the faithfulness condition alone is not sufficient for statistical estimation,
and strong-faithfulness has been proposed and assumed to achieve uniform or
high-dimensional consistency. In contrast to the plain faithfulness assumption,
the set of distributions that is not strong-faithful has nonzero Lebesgue
measure and in fact, can be surprisingly large as we show in this paper. We
study the strong-faithfulness condition from a geometric and combinatorial
point of view and give upper and lower bounds on the Lebesgue measure of
strong-faithful distributions for various classes of directed acyclic graphs.
Our results imply fundamental limitations for the PC-algorithm and potentially
also for other algorithms based on partial correlation testing in the Gaussian
case."@2012
Bin Yu@http://arxiv.org/abs/1207.0547v3@Geometry of the faithfulness assumption in causal inference@"Many algorithms for inferring causality rely heavily on the faithfulness
assumption. The main justification for imposing this assumption is that the set
of unfaithful distributions has Lebesgue measure zero, since it can be seen as
a collection of hypersurfaces in a hypercube. However, due to sampling error
the faithfulness condition alone is not sufficient for statistical estimation,
and strong-faithfulness has been proposed and assumed to achieve uniform or
high-dimensional consistency. In contrast to the plain faithfulness assumption,
the set of distributions that is not strong-faithful has nonzero Lebesgue
measure and in fact, can be surprisingly large as we show in this paper. We
study the strong-faithfulness condition from a geometric and combinatorial
point of view and give upper and lower bounds on the Lebesgue measure of
strong-faithful distributions for various classes of directed acyclic graphs.
Our results imply fundamental limitations for the PC-algorithm and potentially
also for other algorithms based on partial correlation testing in the Gaussian
case."@2012
Amandine Schreck@http://arxiv.org/abs/1207.0662v2@Adaptive Equi-Energy Sampler : Convergence and Illustration@"Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known
up to a multiplicative constant. Classical MCMC samplers are known to have very
poor mixing properties when sampling multimodal distributions. The Equi-Energy
sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006
to sample difficult multimodal distributions. This algorithm runs several
chains at different temperatures in parallel, and allow lower-tempered chains
to jump to a state from a higher-tempered chain having an energy 'close' to
that of the current state. A major drawback of this algorithm is that it
depends on many design parameters and thus, requires a significant effort to
tune these parameters. In this paper, we introduce an Adaptive Equi-Energy
(AEE) sampler which automates the choice of the selection mecanism when jumping
onto a state of the higher-temperature chain. We prove the ergodicity and a
strong law of large numbers for AEE, and for the original Equi-Energy sampler
as well. Finally, we apply our algorithm to motif sampling in DNA sequences."@2012
Gersende Fort@http://arxiv.org/abs/1207.0662v2@Adaptive Equi-Energy Sampler : Convergence and Illustration@"Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known
up to a multiplicative constant. Classical MCMC samplers are known to have very
poor mixing properties when sampling multimodal distributions. The Equi-Energy
sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006
to sample difficult multimodal distributions. This algorithm runs several
chains at different temperatures in parallel, and allow lower-tempered chains
to jump to a state from a higher-tempered chain having an energy 'close' to
that of the current state. A major drawback of this algorithm is that it
depends on many design parameters and thus, requires a significant effort to
tune these parameters. In this paper, we introduce an Adaptive Equi-Energy
(AEE) sampler which automates the choice of the selection mecanism when jumping
onto a state of the higher-temperature chain. We prove the ergodicity and a
strong law of large numbers for AEE, and for the original Equi-Energy sampler
as well. Finally, we apply our algorithm to motif sampling in DNA sequences."@2012
Eric Moulines@http://arxiv.org/abs/1207.0662v2@Adaptive Equi-Energy Sampler : Convergence and Illustration@"Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known
up to a multiplicative constant. Classical MCMC samplers are known to have very
poor mixing properties when sampling multimodal distributions. The Equi-Energy
sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006
to sample difficult multimodal distributions. This algorithm runs several
chains at different temperatures in parallel, and allow lower-tempered chains
to jump to a state from a higher-tempered chain having an energy 'close' to
that of the current state. A major drawback of this algorithm is that it
depends on many design parameters and thus, requires a significant effort to
tune these parameters. In this paper, we introduce an Adaptive Equi-Energy
(AEE) sampler which automates the choice of the selection mecanism when jumping
onto a state of the higher-temperature chain. We prove the ergodicity and a
strong law of large numbers for AEE, and for the original Equi-Energy sampler
as well. Finally, we apply our algorithm to motif sampling in DNA sequences."@2012
Fabien Navarro@http://arxiv.org/abs/1207.1056v2@On adaptive wavelet estimation of a class of weighted densities@"We investigate the estimation of a weighted density taking the form
$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated
distribution function and $w$ is a known (non-negative) weight. Such a class
encompasses many examples, including those arising in order statistics or when
$g$ is related to the maximum or the minimum of $N$ (random or fixed)
independent and identically distributed (\iid) random variables. We here
construct a new adaptive non-parametric estimator for $g$ based on a plug-in
approach and the wavelets methodology. For a wide class of models, we prove
that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with
$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared
error) over Besov balls. The theoretical findings are illustrated through
several simulations."@2012
Christophe Chesneau@http://arxiv.org/abs/1207.1056v2@On adaptive wavelet estimation of a class of weighted densities@"We investigate the estimation of a weighted density taking the form
$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated
distribution function and $w$ is a known (non-negative) weight. Such a class
encompasses many examples, including those arising in order statistics or when
$g$ is related to the maximum or the minimum of $N$ (random or fixed)
independent and identically distributed (\iid) random variables. We here
construct a new adaptive non-parametric estimator for $g$ based on a plug-in
approach and the wavelets methodology. For a wide class of models, we prove
that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with
$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared
error) over Besov balls. The theoretical findings are illustrated through
several simulations."@2012
Jalal Fadili@http://arxiv.org/abs/1207.1056v2@On adaptive wavelet estimation of a class of weighted densities@"We investigate the estimation of a weighted density taking the form
$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated
distribution function and $w$ is a known (non-negative) weight. Such a class
encompasses many examples, including those arising in order statistics or when
$g$ is related to the maximum or the minimum of $N$ (random or fixed)
independent and identically distributed (\iid) random variables. We here
construct a new adaptive non-parametric estimator for $g$ based on a plug-in
approach and the wavelets methodology. For a wide class of models, we prove
that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with
$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared
error) over Besov balls. The theoretical findings are illustrated through
several simulations."@2012
Jose A. Diaz-Garcia@http://arxiv.org/abs/1207.1993v1@Jacobians of singular matrix transformations: Extensions@"This article presents a unified approach to simultaneously compute the
Jacobians of several singular matrix transformations in the real, complex,
quaternion and octonion cases. Formally, these Jacobians are obtained for real
normed division algebras with respect to the Hausdorff measure."@2012
Ramón Gutierrez-Sanchez@http://arxiv.org/abs/1207.1993v1@Jacobians of singular matrix transformations: Extensions@"This article presents a unified approach to simultaneously compute the
Jacobians of several singular matrix transformations in the real, complex,
quaternion and octonion cases. Formally, these Jacobians are obtained for real
normed division algebras with respect to the Hausdorff measure."@2012
Elisabeth Gassiat@http://arxiv.org/abs/1207.2064v2@"About the posterior distribution in hidden Markov models with unknown
  number of states"@"We consider finite state space stationary hidden Markov models (HMMs) in the
situation where the number of hidden states is unknown. We provide a
frequentist asymptotic evaluation of Bayesian analysis methods. Our main result
gives posterior concentration rates for the marginal densities, that is for the
density of a fixed number of consecutive observations. Using conditions on the
prior, we are then able to define a consistent Bayesian estimator of the number
of hidden states. It is known that the likelihood ratio test statistic for
overfitted HMMs has a nonstandard behaviour and is unbounded. Our conditions on
the prior may be seen as a way to penalize parameters to avoid this phenomenon.
Inference of parameters is a much more difficult task than inference of
marginal densities, we still provide a precise description of the situation
when the observations are i.i.d. and we allow for $2$ possible hidden states."@2012
Judith Rousseau@http://arxiv.org/abs/1207.2064v2@"About the posterior distribution in hidden Markov models with unknown
  number of states"@"We consider finite state space stationary hidden Markov models (HMMs) in the
situation where the number of hidden states is unknown. We provide a
frequentist asymptotic evaluation of Bayesian analysis methods. Our main result
gives posterior concentration rates for the marginal densities, that is for the
density of a fixed number of consecutive observations. Using conditions on the
prior, we are then able to define a consistent Bayesian estimator of the number
of hidden states. It is known that the likelihood ratio test statistic for
overfitted HMMs has a nonstandard behaviour and is unbounded. Our conditions on
the prior may be seen as a way to penalize parameters to avoid this phenomenon.
Inference of parameters is a much more difficult task than inference of
marginal densities, we still provide a precise description of the situation
when the observations are i.i.d. and we allow for $2$ possible hidden states."@2012
Mathilde Mougeot@http://arxiv.org/abs/1207.2067v1@Grouping Strategies and Thresholding for High Dimensional Linear Models@"The estimation problem in a high regression model with structured sparsity is
investigated. An algorithm using a two steps block thresholding procedure
called GR-LOL is provided. Convergence rates are produced: they depend on
simple coherence-type indices of the Gram matrix -easily checkable on the data-
as well as sparsity assumptions of the model parameters measured by a
combination of $l_1$ within-blocks with $l_q,q<1$ between-blocks norms. The
simplicity of the coherence indicator suggests ways to optimize the rates of
convergence when the group structure is not naturally given by the problem and
is unknown. In such a case, an auto-driven procedure is provided to determine
the regressors groups (number and contents). An intensive practical study
compares our grouping methods with the standard LOL algorithm. We prove that
the grouping rarely deteriorates the results but can improve them very
significantly. GR-LOL is also compared with group-Lasso procedures and exhibits
a very encouraging behavior. The results are quite impressive, especially when
GR-LOL algorithm is combined with a grouping pre-processing."@2012
Dominique Picard@http://arxiv.org/abs/1207.2067v1@Grouping Strategies and Thresholding for High Dimensional Linear Models@"The estimation problem in a high regression model with structured sparsity is
investigated. An algorithm using a two steps block thresholding procedure
called GR-LOL is provided. Convergence rates are produced: they depend on
simple coherence-type indices of the Gram matrix -easily checkable on the data-
as well as sparsity assumptions of the model parameters measured by a
combination of $l_1$ within-blocks with $l_q,q<1$ between-blocks norms. The
simplicity of the coherence indicator suggests ways to optimize the rates of
convergence when the group structure is not naturally given by the problem and
is unknown. In such a case, an auto-driven procedure is provided to determine
the regressors groups (number and contents). An intensive practical study
compares our grouping methods with the standard LOL algorithm. We prove that
the grouping rarely deteriorates the results but can improve them very
significantly. GR-LOL is also compared with group-Lasso procedures and exhibits
a very encouraging behavior. The results are quite impressive, especially when
GR-LOL algorithm is combined with a grouping pre-processing."@2012
Karine Tribouley@http://arxiv.org/abs/1207.2067v1@Grouping Strategies and Thresholding for High Dimensional Linear Models@"The estimation problem in a high regression model with structured sparsity is
investigated. An algorithm using a two steps block thresholding procedure
called GR-LOL is provided. Convergence rates are produced: they depend on
simple coherence-type indices of the Gram matrix -easily checkable on the data-
as well as sparsity assumptions of the model parameters measured by a
combination of $l_1$ within-blocks with $l_q,q<1$ between-blocks norms. The
simplicity of the coherence indicator suggests ways to optimize the rates of
convergence when the group structure is not naturally given by the problem and
is unknown. In such a case, an auto-driven procedure is provided to determine
the regressors groups (number and contents). An intensive practical study
compares our grouping methods with the standard LOL algorithm. We prove that
the grouping rarely deteriorates the results but can improve them very
significantly. GR-LOL is also compared with group-Lasso procedures and exhibits
a very encouraging behavior. The results are quite impressive, especially when
GR-LOL algorithm is combined with a grouping pre-processing."@2012
Cécile Durot@http://arxiv.org/abs/1207.2118v2@Testing equality of functions under monotonicity constraints@"We consider the problem of testing equality of functions $f_j:[0,1]\to
\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from
possibly different distributions under the assumption that the functions are
monotone. We provide a uniform approach that covers testing equality of
monotone regression curves, equality of monotone densities and equality of
monotone hazards in the random censorship model. Two test statistics are
proposed based on $L_1$-distances. We show that both statistics are
asymptotically normal and we provide bootstrap implementations, which are shown
to have critical regions with asymptotic level $\alpha$."@2012
Piet Groeneboom@http://arxiv.org/abs/1207.2118v2@Testing equality of functions under monotonicity constraints@"We consider the problem of testing equality of functions $f_j:[0,1]\to
\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from
possibly different distributions under the assumption that the functions are
monotone. We provide a uniform approach that covers testing equality of
monotone regression curves, equality of monotone densities and equality of
monotone hazards in the random censorship model. Two test statistics are
proposed based on $L_1$-distances. We show that both statistics are
asymptotically normal and we provide bootstrap implementations, which are shown
to have critical regions with asymptotic level $\alpha$."@2012
Hendrik P. Lopuhaä@http://arxiv.org/abs/1207.2118v2@Testing equality of functions under monotonicity constraints@"We consider the problem of testing equality of functions $f_j:[0,1]\to
\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from
possibly different distributions under the assumption that the functions are
monotone. We provide a uniform approach that covers testing equality of
monotone regression curves, equality of monotone densities and equality of
monotone hazards in the random censorship model. Two test statistics are
proposed based on $L_1$-distances. We show that both statistics are
asymptotically normal and we provide bootstrap implementations, which are shown
to have critical regions with asymptotic level $\alpha$."@2012
Fabienne Comte@http://arxiv.org/abs/1207.2231v1@"Laplace deconvolution and its application to Dynamic Contrast Enhanced
  imaging"@"In the present paper we consider the problem of Laplace deconvolution with
noisy discrete observations. The study is motivated by Dynamic Contrast
Enhanced imaging using a bolus of contrast agent, a procedure which allows
considerable improvement in {evaluating} the quality of a vascular network and
its permeability and is widely used in medical assessment of brain flows or
cancerous tumors. Although the study is motivated by medical imaging
application, we obtain a solution of a general problem of Laplace deconvolution
based on noisy data which appears in many different contexts. We propose a new
method for Laplace deconvolution which is based on expansions of the
convolution kernel, the unknown function and the observed signal over Laguerre
functions basis. The expansion results in a small system of linear equations
with the matrix of the system being triangular and Toeplitz. The number $m$ of
the terms in the expansion of the estimator is controlled via complexity
penalty. The advantage of this methodology is that it leads to very fast
computations, does not require exact knowledge of the kernel and produces no
boundary effects due to extension at zero and cut-off at $T$. The technique
leads to an estimator with the risk within a logarithmic factor of $m$ of the
oracle risk under no assumptions on the model and within a constant factor of
the oracle risk under mild assumptions. The methodology is illustrated by a
finite sample simulation study which includes an example of the kernel obtained
in the real life DCE experiments. Simulations confirm that the proposed
technique is fast, efficient, accurate, usable from a practical point of view
and competitive."@2012
Charles-André Cuénod@http://arxiv.org/abs/1207.2231v1@"Laplace deconvolution and its application to Dynamic Contrast Enhanced
  imaging"@"In the present paper we consider the problem of Laplace deconvolution with
noisy discrete observations. The study is motivated by Dynamic Contrast
Enhanced imaging using a bolus of contrast agent, a procedure which allows
considerable improvement in {evaluating} the quality of a vascular network and
its permeability and is widely used in medical assessment of brain flows or
cancerous tumors. Although the study is motivated by medical imaging
application, we obtain a solution of a general problem of Laplace deconvolution
based on noisy data which appears in many different contexts. We propose a new
method for Laplace deconvolution which is based on expansions of the
convolution kernel, the unknown function and the observed signal over Laguerre
functions basis. The expansion results in a small system of linear equations
with the matrix of the system being triangular and Toeplitz. The number $m$ of
the terms in the expansion of the estimator is controlled via complexity
penalty. The advantage of this methodology is that it leads to very fast
computations, does not require exact knowledge of the kernel and produces no
boundary effects due to extension at zero and cut-off at $T$. The technique
leads to an estimator with the risk within a logarithmic factor of $m$ of the
oracle risk under no assumptions on the model and within a constant factor of
the oracle risk under mild assumptions. The methodology is illustrated by a
finite sample simulation study which includes an example of the kernel obtained
in the real life DCE experiments. Simulations confirm that the proposed
technique is fast, efficient, accurate, usable from a practical point of view
and competitive."@2012
Marianna Pensky@http://arxiv.org/abs/1207.2231v1@"Laplace deconvolution and its application to Dynamic Contrast Enhanced
  imaging"@"In the present paper we consider the problem of Laplace deconvolution with
noisy discrete observations. The study is motivated by Dynamic Contrast
Enhanced imaging using a bolus of contrast agent, a procedure which allows
considerable improvement in {evaluating} the quality of a vascular network and
its permeability and is widely used in medical assessment of brain flows or
cancerous tumors. Although the study is motivated by medical imaging
application, we obtain a solution of a general problem of Laplace deconvolution
based on noisy data which appears in many different contexts. We propose a new
method for Laplace deconvolution which is based on expansions of the
convolution kernel, the unknown function and the observed signal over Laguerre
functions basis. The expansion results in a small system of linear equations
with the matrix of the system being triangular and Toeplitz. The number $m$ of
the terms in the expansion of the estimator is controlled via complexity
penalty. The advantage of this methodology is that it leads to very fast
computations, does not require exact knowledge of the kernel and produces no
boundary effects due to extension at zero and cut-off at $T$. The technique
leads to an estimator with the risk within a logarithmic factor of $m$ of the
oracle risk under no assumptions on the model and within a constant factor of
the oracle risk under mild assumptions. The methodology is illustrated by a
finite sample simulation study which includes an example of the kernel obtained
in the real life DCE experiments. Simulations confirm that the proposed
technique is fast, efficient, accurate, usable from a practical point of view
and competitive."@2012
Yves Rozenholc@http://arxiv.org/abs/1207.2231v1@"Laplace deconvolution and its application to Dynamic Contrast Enhanced
  imaging"@"In the present paper we consider the problem of Laplace deconvolution with
noisy discrete observations. The study is motivated by Dynamic Contrast
Enhanced imaging using a bolus of contrast agent, a procedure which allows
considerable improvement in {evaluating} the quality of a vascular network and
its permeability and is widely used in medical assessment of brain flows or
cancerous tumors. Although the study is motivated by medical imaging
application, we obtain a solution of a general problem of Laplace deconvolution
based on noisy data which appears in many different contexts. We propose a new
method for Laplace deconvolution which is based on expansions of the
convolution kernel, the unknown function and the observed signal over Laguerre
functions basis. The expansion results in a small system of linear equations
with the matrix of the system being triangular and Toeplitz. The number $m$ of
the terms in the expansion of the estimator is controlled via complexity
penalty. The advantage of this methodology is that it leads to very fast
computations, does not require exact knowledge of the kernel and produces no
boundary effects due to extension at zero and cut-off at $T$. The technique
leads to an estimator with the risk within a logarithmic factor of $m$ of the
oracle risk under no assumptions on the model and within a constant factor of
the oracle risk under mild assumptions. The methodology is illustrated by a
finite sample simulation study which includes an example of the kernel obtained
in the real life DCE experiments. Simulations confirm that the proposed
technique is fast, efficient, accurate, usable from a practical point of view
and competitive."@2012
Yao Xie@http://arxiv.org/abs/1207.2386v2@Sequential multi-sensor change-point detection@"We develop a mixture procedure to monitor parallel streams of data for a
change-point that affects only a subset of them, without assuming a spatial
structure relating the data streams to one another. Observations are assumed
initially to be independent standard normal random variables. After a
change-point the observations in a subset of the streams of data have nonzero
mean values. The subset and the post-change means are unknown. The procedure we
study uses stream specific generalized likelihood ratio statistics, which are
combined to form an overall detection statistic in a mixture model that
hypothesizes an assumed fraction $p_0$ of affected data streams. An analytic
expression is obtained for the average run length (ARL) when there is no change
and is shown by simulations to be very accurate. Similarly, an approximation
for the expected detection delay (EDD) after a change-point is also obtained.
Numerical examples are given to compare the suggested procedure to other
procedures for unstructured problems and in one case where the problem is
assumed to have a well-defined geometric structure. Finally we discuss
sensitivity of the procedure to the assumed value of $p_0$ and suggest a
generalization."@2012
David Siegmund@http://arxiv.org/abs/1207.2386v2@Sequential multi-sensor change-point detection@"We develop a mixture procedure to monitor parallel streams of data for a
change-point that affects only a subset of them, without assuming a spatial
structure relating the data streams to one another. Observations are assumed
initially to be independent standard normal random variables. After a
change-point the observations in a subset of the streams of data have nonzero
mean values. The subset and the post-change means are unknown. The procedure we
study uses stream specific generalized likelihood ratio statistics, which are
combined to form an overall detection statistic in a mixture model that
hypothesizes an assumed fraction $p_0$ of affected data streams. An analytic
expression is obtained for the average run length (ARL) when there is no change
and is shown by simulations to be very accurate. Similarly, an approximation
for the expected detection delay (EDD) after a change-point is also obtained.
Numerical examples are given to compare the suggested procedure to other
procedures for unstructured problems and in one case where the problem is
assumed to have a well-defined geometric structure. Finally we discuss
sensitivity of the procedure to the assumed value of $p_0$ and suggest a
generalization."@2012
Saeid Rezakhah@http://arxiv.org/abs/1207.2450v1@Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes@"The characteristic feature of semi-selfsimilar process is the invariance of
its finite dimensional distributions by certain dilation for specific scaling
factor. Estimating the scale parameter $\lambda$ and the Hurst index of such
processes is one of the fundamental problem in the literature. We present some
iterative method for estimation of the scale and Hurst parameters which is
addressed for semi-selfsimilar processes with stationary increments. This
method is based on some flexible sampling scheme and evaluating sample variance
of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in
\mathbb{N}$. For such iterative method we find the initial estimation for the
scale parameter by evaluating cumulative sum of moving sample variances and
also by evaluating sample variance of preceding and succeeding moving sample
variance of increments. We also present a new efficient method for estimation
of Hurst parameter of selfsimilar processes. As an example we introduce simple
fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary
increments. We present some simulations and numerical evaluation to illustrate
the results and to estimate the scale for sfBm as a semi-selfsimilar process.
We also present another simulation and show the efficiency of our method in
estimation of Hurst parameter by comparing its performance with some previous
methods."@2012
Anne Philippe@http://arxiv.org/abs/1207.2450v1@Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes@"The characteristic feature of semi-selfsimilar process is the invariance of
its finite dimensional distributions by certain dilation for specific scaling
factor. Estimating the scale parameter $\lambda$ and the Hurst index of such
processes is one of the fundamental problem in the literature. We present some
iterative method for estimation of the scale and Hurst parameters which is
addressed for semi-selfsimilar processes with stationary increments. This
method is based on some flexible sampling scheme and evaluating sample variance
of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in
\mathbb{N}$. For such iterative method we find the initial estimation for the
scale parameter by evaluating cumulative sum of moving sample variances and
also by evaluating sample variance of preceding and succeeding moving sample
variance of increments. We also present a new efficient method for estimation
of Hurst parameter of selfsimilar processes. As an example we introduce simple
fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary
increments. We present some simulations and numerical evaluation to illustrate
the results and to estimate the scale for sfBm as a semi-selfsimilar process.
We also present another simulation and show the efficiency of our method in
estimation of Hurst parameter by comparing its performance with some previous
methods."@2012
Navideh Modarresi@http://arxiv.org/abs/1207.2450v1@Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes@"The characteristic feature of semi-selfsimilar process is the invariance of
its finite dimensional distributions by certain dilation for specific scaling
factor. Estimating the scale parameter $\lambda$ and the Hurst index of such
processes is one of the fundamental problem in the literature. We present some
iterative method for estimation of the scale and Hurst parameters which is
addressed for semi-selfsimilar processes with stationary increments. This
method is based on some flexible sampling scheme and evaluating sample variance
of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in
\mathbb{N}$. For such iterative method we find the initial estimation for the
scale parameter by evaluating cumulative sum of moving sample variances and
also by evaluating sample variance of preceding and succeeding moving sample
variance of increments. We also present a new efficient method for estimation
of Hurst parameter of selfsimilar processes. As an example we introduce simple
fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary
increments. We present some simulations and numerical evaluation to illustrate
the results and to estimate the scale for sfBm as a semi-selfsimilar process.
We also present another simulation and show the efficiency of our method in
estimation of Hurst parameter by comparing its performance with some previous
methods."@2012
Jean-Marc Bardet@http://arxiv.org/abs/1207.2453v2@"Semiparametric stationarity tests based on adaptive multidimensional
  increment ratio statistics"@"In this paper, we show that the adaptive multidimensional increment ratio
estimator of the long range memory parameter defined in Bardet and Dola (2012)
satisfies a central limit theorem (CLT in the sequel) for a large
semiparametric class of Gaussian fractionally integrated processes with memory
parameter $d \in (-0.5,1.25)$. Since the asymptotic variance of this CLT can be
computed, tests of stationarity or nonstationarity distinguishing the
assumptions $d<0.5$ and $d \geq 0.5$ are constructed. These tests are also
consistent tests of unit root. Simulations done on a large benchmark of short
memory, long memory and non stationary processes show the accuracy of the tests
with respect to other usual stationarity or nonstationarity tests (LMC, V/S,
ADF and PP tests). Finally, the estimator and tests are applied to log-returns
of famous economic data and to their absolute value power laws."@2012
Béchir Dola@http://arxiv.org/abs/1207.2453v2@"Semiparametric stationarity tests based on adaptive multidimensional
  increment ratio statistics"@"In this paper, we show that the adaptive multidimensional increment ratio
estimator of the long range memory parameter defined in Bardet and Dola (2012)
satisfies a central limit theorem (CLT in the sequel) for a large
semiparametric class of Gaussian fractionally integrated processes with memory
parameter $d \in (-0.5,1.25)$. Since the asymptotic variance of this CLT can be
computed, tests of stationarity or nonstationarity distinguishing the
assumptions $d<0.5$ and $d \geq 0.5$ are constructed. These tests are also
consistent tests of unit root. Simulations done on a large benchmark of short
memory, long memory and non stationary processes show the accuracy of the tests
with respect to other usual stationarity or nonstationarity tests (LMC, V/S,
ADF and PP tests). Finally, the estimator and tests are applied to log-returns
of famous economic data and to their absolute value power laws."@2012
Yan Sun@http://arxiv.org/abs/1207.2740v2@"A Normal Hierarchical Model and Minimum Contrast Estimation for Random
  Intervals"@"Many statistical data are imprecise due to factors such as measurement
errors, computation errors, and lack of information. In such cases, data are
better represented by intervals rather than by single numbers. Existing methods
for analyzing interval-valued data include regressions in the metric space of
intervals and symbolic data analysis, the latter being proposed in a more
general setting. However, there has been a lack of literature on the
distribution-based inferences for interval-valued data. In an attempt to fill
this gap, we extend the concept of normality for random sets by Lyashenko
(1983) and propose a normal hierarchical model for random intervals. In
addition, we develop a minimum contrast estimator (MCE) for the model
parameters, which we show is both consistent and asymptotically normal.
Simulation studies support our theoretical findings, and show very promising
results. Finally, we successfully apply our model and MCE to a real dataset."@2012
Dan Ralescu@http://arxiv.org/abs/1207.2740v2@"A Normal Hierarchical Model and Minimum Contrast Estimation for Random
  Intervals"@"Many statistical data are imprecise due to factors such as measurement
errors, computation errors, and lack of information. In such cases, data are
better represented by intervals rather than by single numbers. Existing methods
for analyzing interval-valued data include regressions in the metric space of
intervals and symbolic data analysis, the latter being proposed in a more
general setting. However, there has been a lack of literature on the
distribution-based inferences for interval-valued data. In an attempt to fill
this gap, we extend the concept of normality for random sets by Lyashenko
(1983) and propose a normal hierarchical model for random intervals. In
addition, we develop a minimum contrast estimator (MCE) for the model
parameters, which we show is both consistent and asymptotically normal.
Simulation studies support our theoretical findings, and show very promising
results. Finally, we successfully apply our model and MCE to a real dataset."@2012
Y. Maleki@http://arxiv.org/abs/1207.2831v1@"The Scale Invariant Wigner Spectrum Estimation of Gaussian Locally
  Self-Similar Processes"@"We study locally self-similar processes (LSSPs) in Silverman's sense. By
deriving the minimum mean-square optimal kernel within Cohen's class
counterpart of time-frequency representations, we obtain an optimal estimation
for the scale invariant Wigner spectrum (SIWS) of Gaussian LSSPs. The class of
estimators is completely characterized in terms of kernels, so the optimal
kernel minimizes the mean-square error of the estimation. We obtain the SIWS
estimation for two cases: global and local, where in the local case, the kernel
is allowed to vary with time and frequency. We also introduce two
generalizations of LSSPs: the locally self-similar chrip process and the
multicomponent locally self-similar process, and obtain their optimal kernels.
Finally, the performance and accuracy of the estimation is studied via
simulation."@2012
S. Rezakhah@http://arxiv.org/abs/1207.2831v1@"The Scale Invariant Wigner Spectrum Estimation of Gaussian Locally
  Self-Similar Processes"@"We study locally self-similar processes (LSSPs) in Silverman's sense. By
deriving the minimum mean-square optimal kernel within Cohen's class
counterpart of time-frequency representations, we obtain an optimal estimation
for the scale invariant Wigner spectrum (SIWS) of Gaussian LSSPs. The class of
estimators is completely characterized in terms of kernels, so the optimal
kernel minimizes the mean-square error of the estimation. We obtain the SIWS
estimation for two cases: global and local, where in the local case, the kernel
is allowed to vary with time and frequency. We also introduce two
generalizations of LSSPs: the locally self-similar chrip process and the
multicomponent locally self-similar process, and obtain their optimal kernels.
Finally, the performance and accuracy of the estimation is studied via
simulation."@2012
A. Kohansal@http://arxiv.org/abs/1207.3238v2@"Two New Entropy Estimators for Testing Exponentiality with Type-II
  Censored Data"@"This paper proposes two estimators of the joint entropy of the Type-II
censored data. Consistency of both estimators is proved. Simulation results
show that the second one shows less bias and root of mean square error (RMSE)
than leading estimator. Also, two goodness of fit test statistics based on the
Kullback-Leibler information with the Type-II censored data are established and
their performances with the leading test statistics are compared. We provide a
Monte Carlo simulation study which shows that the test statistics
$T^{(1)}_{m,n,r}$ and $T^{(2)}_{m,n,r}$ show better powers than leading test
statistics against the alternatives with monotone decreasing and monotone
increasing hazard functions, respectively."@2012
S. Rezakhah@http://arxiv.org/abs/1207.3238v2@"Two New Entropy Estimators for Testing Exponentiality with Type-II
  Censored Data"@"This paper proposes two estimators of the joint entropy of the Type-II
censored data. Consistency of both estimators is proved. Simulation results
show that the second one shows less bias and root of mean square error (RMSE)
than leading estimator. Also, two goodness of fit test statistics based on the
Kullback-Leibler information with the Type-II censored data are established and
their performances with the leading test statistics are compared. We provide a
Monte Carlo simulation study which shows that the test statistics
$T^{(1)}_{m,n,r}$ and $T^{(2)}_{m,n,r}$ show better powers than leading test
statistics against the alternatives with monotone decreasing and monotone
increasing hazard functions, respectively."@2012
Jean Jacod@http://arxiv.org/abs/1207.3757v3@Quarticity and other functionals of volatility: Efficient estimation@"We consider a multidimensional Ito semimartingale regularly sampled on [0,t]
at high frequency $1/\Delta_n$, with $\Delta_n$ going to zero. The goal of this
paper is to provide an estimator for the integral over [0,t] of a given
function of the volatility matrix. To approximate the integral, we simply use a
Riemann sum based on local estimators of the pointwise volatility. We show that
although the accuracy of the pointwise estimation is at most $\Delta_n^{1/4}$,
this procedure reaches the parametric rate $\Delta_n^{1/2}$, as it is usually
the case in integrated functionals estimation. After a suitable bias
correction, we obtain an unbiased central limit theorem for our estimator and
show that it is asymptotically efficient within some classes of sub models."@2012
Mathieu Rosenbaum@http://arxiv.org/abs/1207.3757v3@Quarticity and other functionals of volatility: Efficient estimation@"We consider a multidimensional Ito semimartingale regularly sampled on [0,t]
at high frequency $1/\Delta_n$, with $\Delta_n$ going to zero. The goal of this
paper is to provide an estimator for the integral over [0,t] of a given
function of the volatility matrix. To approximate the integral, we simply use a
Riemann sum based on local estimators of the pointwise volatility. We show that
although the accuracy of the pointwise estimation is at most $\Delta_n^{1/4}$,
this procedure reaches the parametric rate $\Delta_n^{1/2}$, as it is usually
the case in integrated functionals estimation. After a suitable bias
correction, we obtain an unbiased central limit theorem for our estimator and
show that it is asymptotically efficient within some classes of sub models."@2012
Ahmed A. Quadeer@http://arxiv.org/abs/1207.3847v1@Structure-Based Bayesian Sparse Reconstruction@"Sparse signal reconstruction algorithms have attracted research attention due
to their wide applications in various fields. In this paper, we present a
simple Bayesian approach that utilizes the sparsity constraint and a priori
statistical information (Gaussian or otherwise) to obtain near optimal
estimates. In addition, we make use of the rich structure of the sensing matrix
encountered in many signal processing applications to develop a fast sparse
recovery algorithm. The computational complexity of the proposed algorithm is
relatively low compared with the widely used convex relaxation methods as well
as greedy matching pursuit techniques, especially at a low sparsity rate."@2012
Tareq Y. Al-Naffouri@http://arxiv.org/abs/1207.3847v1@Structure-Based Bayesian Sparse Reconstruction@"Sparse signal reconstruction algorithms have attracted research attention due
to their wide applications in various fields. In this paper, we present a
simple Bayesian approach that utilizes the sparsity constraint and a priori
statistical information (Gaussian or otherwise) to obtain near optimal
estimates. In addition, we make use of the rich structure of the sensing matrix
encountered in many signal processing applications to develop a fast sparse
recovery algorithm. The computational complexity of the proposed algorithm is
relatively low compared with the widely used convex relaxation methods as well
as greedy matching pursuit techniques, especially at a low sparsity rate."@2012
Pierre-Yves Massé@http://arxiv.org/abs/1207.3975v3@"Adaptive confidence bands in the nonparametric fixed design regression
  model"@"In this note, we consider the problem of existence of adaptive confidence
bands in the fixed design regression model, adapting ideas in Hoffmann and
Nickl (2011) to the present case. In the course of the proof, we show that
sup-norm adaptive estimators exist as well in regression."@2012
William Meiniel@http://arxiv.org/abs/1207.3975v3@"Adaptive confidence bands in the nonparametric fixed design regression
  model"@"In this note, we consider the problem of existence of adaptive confidence
bands in the fixed design regression model, adapting ideas in Hoffmann and
Nickl (2011) to the present case. In the course of the proof, we show that
sup-norm adaptive estimators exist as well in regression."@2012
José A. Díaz-García@http://arxiv.org/abs/1207.4391v1@"Asymptotic normality of the optimal solution in multiresponse surface
  methodology"@"In this work is obtained an explicit form for the perturbation effect on the
matrix of regression coefficients on the optimal solution in multiresponse
surface methodology. Then, the sensitivity analysis of the optimal solution is
studied and the critical point characterisation of the convex program,
associated with the optimum of a multiresponse surface, is also analysed.
Finally, the asymptotic normality of the optimal solution is derived by
standard methods."@2012
Francisco J. Caro-Lopera@http://arxiv.org/abs/1207.4391v1@"Asymptotic normality of the optimal solution in multiresponse surface
  methodology"@"In this work is obtained an explicit form for the perturbation effect on the
matrix of regression coefficients on the optimal solution in multiresponse
surface methodology. Then, the sensitivity analysis of the optimal solution is
studied and the critical point characterisation of the convex program,
associated with the optimum of a multiresponse surface, is also analysed.
Finally, the asymptotic normality of the optimal solution is derived by
standard methods."@2012
Michaël Chichignoud@http://arxiv.org/abs/1207.4447v4@"A robust, adaptive M-estimator for pointwise estimation in
  heteroscedastic regression"@"We introduce a robust and fully adaptive method for pointwise estimation in
heteroscedastic regression. We allow for noise and design distributions that
are unknown and fulfill very weak assumptions only. In particular, we do not
impose moment conditions on the noise distribution. Moreover, we do not require
a positive density for the design distribution. In a first step, we study the
consistency of locally polynomial M-estimators that consist of a contrast and a
kernel. Afterwards, minimax results are established over unidimensional
H\""older spaces for degenerate design. We then choose the contrast and the
kernel that minimize an empirical variance term and demonstrate that the
corresponding M-estimator is adaptive with respect to the noise and design
distributions and adaptive (Huber) minimax for contamination models. In a
second step, we additionally choose a data-driven bandwidth via Lepski's
method. This leads to an M-estimator that is adaptive with respect to the noise
and design distributions and, additionally, adaptive with respect to the
smoothness of an isotropic, multivariate, locally polynomial target function.
These results are also extended to anisotropic, locally constant target
functions. Our data-driven approach provides, in particular, a level of
robustness that adapts to the noise, contamination, and outliers."@2012
Johannes Lederer@http://arxiv.org/abs/1207.4447v4@"A robust, adaptive M-estimator for pointwise estimation in
  heteroscedastic regression"@"We introduce a robust and fully adaptive method for pointwise estimation in
heteroscedastic regression. We allow for noise and design distributions that
are unknown and fulfill very weak assumptions only. In particular, we do not
impose moment conditions on the noise distribution. Moreover, we do not require
a positive density for the design distribution. In a first step, we study the
consistency of locally polynomial M-estimators that consist of a contrast and a
kernel. Afterwards, minimax results are established over unidimensional
H\""older spaces for degenerate design. We then choose the contrast and the
kernel that minimize an empirical variance term and demonstrate that the
corresponding M-estimator is adaptive with respect to the noise and design
distributions and adaptive (Huber) minimax for contamination models. In a
second step, we additionally choose a data-driven bandwidth via Lepski's
method. This leads to an M-estimator that is adaptive with respect to the noise
and design distributions and, additionally, adaptive with respect to the
smoothness of an isotropic, multivariate, locally polynomial target function.
These results are also extended to anisotropic, locally constant target
functions. Our data-driven approach provides, in particular, a level of
robustness that adapts to the noise, contamination, and outliers."@2012
Jelena Bradic@http://arxiv.org/abs/1207.4510v3@Structured Estimation in Nonparameteric Cox Model@"To better understand the interplay of censoring and sparsity we develop
finite sample properties of nonparametric Cox proportional hazard's model. Due
to high impact of sequencing data, carrying genetic information of each
individual, we work with over-parametrized problem and propose general class of
group penalties suitable for sparse structured variable selection and
estimation. Novel non-asymptotic sandwich bounds for the partial likelihood are
developed. We establish how they extend notion of local asymptotic normality
(LAN) of Le Cam's. Such non-asymptotic LAN principles are further extended to
high dimensional spaces where $p \gg n$. Finite sample prediction properties of
penalized estimator in non-parametric Cox proportional hazards model, under
suitable censoring conditions, agree with those of penalized estimator in
linear models."@2012
Rui Song@http://arxiv.org/abs/1207.4510v3@Structured Estimation in Nonparameteric Cox Model@"To better understand the interplay of censoring and sparsity we develop
finite sample properties of nonparametric Cox proportional hazard's model. Due
to high impact of sequencing data, carrying genetic information of each
individual, we work with over-parametrized problem and propose general class of
group penalties suitable for sparse structured variable selection and
estimation. Novel non-asymptotic sandwich bounds for the partial likelihood are
developed. We establish how they extend notion of local asymptotic normality
(LAN) of Le Cam's. Such non-asymptotic LAN principles are further extended to
high dimensional spaces where $p \gg n$. Finite sample prediction properties of
penalized estimator in non-parametric Cox proportional hazards model, under
suitable censoring conditions, agree with those of penalized estimator in
linear models."@2012
Vladimir Koltchinskii@http://arxiv.org/abs/1207.4819v4@Low rank estimation of smooth kernels on graphs@"Let (V,A) be a weighted graph with a finite vertex set V, with a symmetric
matrix of nonnegative weights A and with Laplacian $\Delta$. Let $S_*:V\times
V\mapsto{\mathbb{R}}$ be a symmetric kernel defined on the vertex set V.
Consider n i.i.d. observations $(X_j,X_j',Y_j),j=1,\ldots,n$, where $X_j,X_j'$
are independent random vertices sampled from the uniform distribution in V and
$Y_j\in{\mathbb{R}}$ is a real valued response variable such that
${\mathbb{E}}(Y_j|X_j,X_j')=S_*(X_j,X_j'),j=1,\ldots,n$. The goal is to
estimate the kernel $S_*$ based on the data
$(X_1,X_1',Y_1),\ldots,(X_n,X_n',Y_n)$ and under the assumption that $S_*$ is
low rank and, at the same time, smooth on the graph (the smoothness being
characterized by discrete Sobolev norms defined in terms of the graph
Laplacian). We obtain several results for such problems including minimax lower
bounds on the $L_2$-error and upper bounds for penalized least squares
estimators both with nonconvex and with convex penalties."@2012
Pedro Rangel@http://arxiv.org/abs/1207.4819v4@Low rank estimation of smooth kernels on graphs@"Let (V,A) be a weighted graph with a finite vertex set V, with a symmetric
matrix of nonnegative weights A and with Laplacian $\Delta$. Let $S_*:V\times
V\mapsto{\mathbb{R}}$ be a symmetric kernel defined on the vertex set V.
Consider n i.i.d. observations $(X_j,X_j',Y_j),j=1,\ldots,n$, where $X_j,X_j'$
are independent random vertices sampled from the uniform distribution in V and
$Y_j\in{\mathbb{R}}$ is a real valued response variable such that
${\mathbb{E}}(Y_j|X_j,X_j')=S_*(X_j,X_j'),j=1,\ldots,n$. The goal is to
estimate the kernel $S_*$ based on the data
$(X_1,X_1',Y_1),\ldots,(X_n,X_n',Y_n)$ and under the assumption that $S_*$ is
low rank and, at the same time, smooth on the graph (the smoothness being
characterized by discrete Sobolev norms defined in terms of the graph
Laplacian). We obtain several results for such problems including minimax lower
bounds on the $L_2$-error and upper bounds for penalized least squares
estimators both with nonconvex and with convex penalties."@2012
Nate Strawn@http://arxiv.org/abs/1207.4854v3@Finite sample posterior concentration in high-dimensional regression@"We study the behavior of the posterior distribution in high-dimensional
Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number
of predictors and $n$ the sample size. Our focus is on obtaining quantitative
finite sample bounds ensuring sufficient posterior probability assigned in
neighborhoods of the true regression coefficient vector, $\beta^0$, with high
probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain
universal bounds, which provide insight into the role of the prior in
controlling concentration of the posterior. Based on these finite sample
bounds, we examine the implied asymptotic contraction rates for several
examples showing that sparsely-structured and heavy-tail shrinkage priors
exhibit rapid contraction rates. We also demonstrate that a stronger result
holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators
($\gamma$) is drawn from the uniform distribution on the set of binary
sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$
if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite
sample bounds provide guidelines for designing and evaluating priors for
high-dimensional problems."@2012
Artin Armagan@http://arxiv.org/abs/1207.4854v3@Finite sample posterior concentration in high-dimensional regression@"We study the behavior of the posterior distribution in high-dimensional
Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number
of predictors and $n$ the sample size. Our focus is on obtaining quantitative
finite sample bounds ensuring sufficient posterior probability assigned in
neighborhoods of the true regression coefficient vector, $\beta^0$, with high
probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain
universal bounds, which provide insight into the role of the prior in
controlling concentration of the posterior. Based on these finite sample
bounds, we examine the implied asymptotic contraction rates for several
examples showing that sparsely-structured and heavy-tail shrinkage priors
exhibit rapid contraction rates. We also demonstrate that a stronger result
holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators
($\gamma$) is drawn from the uniform distribution on the set of binary
sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$
if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite
sample bounds provide guidelines for designing and evaluating priors for
high-dimensional problems."@2012
Rayan Saab@http://arxiv.org/abs/1207.4854v3@Finite sample posterior concentration in high-dimensional regression@"We study the behavior of the posterior distribution in high-dimensional
Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number
of predictors and $n$ the sample size. Our focus is on obtaining quantitative
finite sample bounds ensuring sufficient posterior probability assigned in
neighborhoods of the true regression coefficient vector, $\beta^0$, with high
probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain
universal bounds, which provide insight into the role of the prior in
controlling concentration of the posterior. Based on these finite sample
bounds, we examine the implied asymptotic contraction rates for several
examples showing that sparsely-structured and heavy-tail shrinkage priors
exhibit rapid contraction rates. We also demonstrate that a stronger result
holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators
($\gamma$) is drawn from the uniform distribution on the set of binary
sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$
if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite
sample bounds provide guidelines for designing and evaluating priors for
high-dimensional problems."@2012
Lawrence Carin@http://arxiv.org/abs/1207.4854v3@Finite sample posterior concentration in high-dimensional regression@"We study the behavior of the posterior distribution in high-dimensional
Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number
of predictors and $n$ the sample size. Our focus is on obtaining quantitative
finite sample bounds ensuring sufficient posterior probability assigned in
neighborhoods of the true regression coefficient vector, $\beta^0$, with high
probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain
universal bounds, which provide insight into the role of the prior in
controlling concentration of the posterior. Based on these finite sample
bounds, we examine the implied asymptotic contraction rates for several
examples showing that sparsely-structured and heavy-tail shrinkage priors
exhibit rapid contraction rates. We also demonstrate that a stronger result
holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators
($\gamma$) is drawn from the uniform distribution on the set of binary
sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$
if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite
sample bounds provide guidelines for designing and evaluating priors for
high-dimensional problems."@2012
David Dunson@http://arxiv.org/abs/1207.4854v3@Finite sample posterior concentration in high-dimensional regression@"We study the behavior of the posterior distribution in high-dimensional
Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number
of predictors and $n$ the sample size. Our focus is on obtaining quantitative
finite sample bounds ensuring sufficient posterior probability assigned in
neighborhoods of the true regression coefficient vector, $\beta^0$, with high
probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain
universal bounds, which provide insight into the role of the prior in
controlling concentration of the posterior. Based on these finite sample
bounds, we examine the implied asymptotic contraction rates for several
examples showing that sparsely-structured and heavy-tail shrinkage priors
exhibit rapid contraction rates. We also demonstrate that a stronger result
holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators
($\gamma$) is drawn from the uniform distribution on the set of binary
sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$
if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite
sample bounds provide guidelines for designing and evaluating priors for
high-dimensional problems."@2012
Mohammad Jafari Jozani@http://arxiv.org/abs/1207.5097v1@Estimation of a nonnegative location parameter with unknown scale@"For normal canonical models, and more generally a vast array of general
spherically symmetric location-scale models with a residual vector, we consider
estimating the (univariate) location parameter when it is lower bounded. We
provide conditions for estimators to dominate the benchmark minimax MRE
estimator, and thus be minimax under scale invariant loss. These minimax
estimators include the generalized Bayes estimator with respect to the
truncation of the common non-informative prior onto the restricted parameter
space for normal models under general convex symmetric loss, as well as
non-normal models under scale invariant $L^p$ loss with $p>0$. We cover many
other situations when the loss is asymmetric, and where other generalized Bayes
estimators, obtained with different powers of the scale parameter in the prior
measure, are proven to be minimax. We rely on various novel representations,
sharp sign change analyses, as well as capitalize on Kubokawa's integral
expression for risk difference technique. Several other analytical properties
are obtained, including a robustness property of the generalized Bayes
estimators above when the loss is either scale invariant $L^p$ or asymmetrized
versions. Applications include inference in two-sample normal model with order
constraints on the means."@2012
Eric Marchand@http://arxiv.org/abs/1207.5097v1@Estimation of a nonnegative location parameter with unknown scale@"For normal canonical models, and more generally a vast array of general
spherically symmetric location-scale models with a residual vector, we consider
estimating the (univariate) location parameter when it is lower bounded. We
provide conditions for estimators to dominate the benchmark minimax MRE
estimator, and thus be minimax under scale invariant loss. These minimax
estimators include the generalized Bayes estimator with respect to the
truncation of the common non-informative prior onto the restricted parameter
space for normal models under general convex symmetric loss, as well as
non-normal models under scale invariant $L^p$ loss with $p>0$. We cover many
other situations when the loss is asymmetric, and where other generalized Bayes
estimators, obtained with different powers of the scale parameter in the prior
measure, are proven to be minimax. We rely on various novel representations,
sharp sign change analyses, as well as capitalize on Kubokawa's integral
expression for risk difference technique. Several other analytical properties
are obtained, including a robustness property of the generalized Bayes
estimators above when the loss is either scale invariant $L^p$ or asymmetrized
versions. Applications include inference in two-sample normal model with order
constraints on the means."@2012
William Strawderman@http://arxiv.org/abs/1207.5097v1@Estimation of a nonnegative location parameter with unknown scale@"For normal canonical models, and more generally a vast array of general
spherically symmetric location-scale models with a residual vector, we consider
estimating the (univariate) location parameter when it is lower bounded. We
provide conditions for estimators to dominate the benchmark minimax MRE
estimator, and thus be minimax under scale invariant loss. These minimax
estimators include the generalized Bayes estimator with respect to the
truncation of the common non-informative prior onto the restricted parameter
space for normal models under general convex symmetric loss, as well as
non-normal models under scale invariant $L^p$ loss with $p>0$. We cover many
other situations when the loss is asymmetric, and where other generalized Bayes
estimators, obtained with different powers of the scale parameter in the prior
measure, are proven to be minimax. We rely on various novel representations,
sharp sign change analyses, as well as capitalize on Kubokawa's integral
expression for risk difference technique. Several other analytical properties
are obtained, including a robustness property of the generalized Bayes
estimators above when the loss is either scale invariant $L^p$ or asymmetrized
versions. Applications include inference in two-sample normal model with order
constraints on the means."@2012
M. Jafari Jozani@http://arxiv.org/abs/1207.5270v1@Some Pitman Closeness Properties Pertinent to Symmetric Populations@"In this paper, we focus on Pitman closeness probabilities when the estimators
are symmetrically distributed about the unknown parameter $\theta$. We first
consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and
obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman
closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish
some properties in the context of estimation under Pitman closeness criterion.
We define a Pitman closeness probability which measures the frequency with
which an individual order statistic is Pitman closer to $\theta$ than some
symmetric estimator. We show that, for symmetric populations, the sample median
is Pitman closer to the population median than any other symmetrically
distributed estimator of $\theta$. Finally, we discuss the use of Pitman
closeness probabilities in the determination of an optimal ranked set sampling
scheme (denoted by RSS) for the estimation of the population median when the
underlying distribution is symmetric. We show that the best RSS scheme from
symmetric populations in the sense of Pitman closeness is the median and
randomized median RSS for the cases of odd and even sample sizes, respectively."@2012
N. Balakrishnan@http://arxiv.org/abs/1207.5270v1@Some Pitman Closeness Properties Pertinent to Symmetric Populations@"In this paper, we focus on Pitman closeness probabilities when the estimators
are symmetrically distributed about the unknown parameter $\theta$. We first
consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and
obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman
closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish
some properties in the context of estimation under Pitman closeness criterion.
We define a Pitman closeness probability which measures the frequency with
which an individual order statistic is Pitman closer to $\theta$ than some
symmetric estimator. We show that, for symmetric populations, the sample median
is Pitman closer to the population median than any other symmetrically
distributed estimator of $\theta$. Finally, we discuss the use of Pitman
closeness probabilities in the determination of an optimal ranked set sampling
scheme (denoted by RSS) for the estimation of the population median when the
underlying distribution is symmetric. We show that the best RSS scheme from
symmetric populations in the sense of Pitman closeness is the median and
randomized median RSS for the cases of odd and even sample sizes, respectively."@2012
K. F. Davies@http://arxiv.org/abs/1207.5270v1@Some Pitman Closeness Properties Pertinent to Symmetric Populations@"In this paper, we focus on Pitman closeness probabilities when the estimators
are symmetrically distributed about the unknown parameter $\theta$. We first
consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and
obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman
closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish
some properties in the context of estimation under Pitman closeness criterion.
We define a Pitman closeness probability which measures the frequency with
which an individual order statistic is Pitman closer to $\theta$ than some
symmetric estimator. We show that, for symmetric populations, the sample median
is Pitman closer to the population median than any other symmetrically
distributed estimator of $\theta$. Finally, we discuss the use of Pitman
closeness probabilities in the determination of an optimal ranked set sampling
scheme (denoted by RSS) for the estimation of the population median when the
underlying distribution is symmetric. We show that the best RSS scheme from
symmetric populations in the sense of Pitman closeness is the median and
randomized median RSS for the cases of odd and even sample sizes, respectively."@2012
Olivier Ledoit@http://arxiv.org/abs/1207.5322v1@Nonlinear shrinkage estimation of large-dimensional covariance matrices@"Many statistical applications require an estimate of a covariance matrix
and/or its inverse. When the matrix dimension is large compared to the sample
size, which happens frequently, the sample covariance matrix is known to
perform poorly and may suffer from ill-conditioning. There already exists an
extensive literature concerning improved estimators in such situations. In the
absence of further knowledge about the structure of the true covariance matrix,
the most successful approach so far, arguably, has been shrinkage estimation.
Shrinking the sample covariance matrix to a multiple of the identity, by taking
a weighted average of the two, turns out to be equivalent to linearly shrinking
the sample eigenvalues to their grand mean, while retaining the sample
eigenvectors. Our paper extends this approach by considering nonlinear
transformations of the sample eigenvalues. We show how to construct an
estimator that is asymptotically equivalent to an oracle estimator suggested in
previous work. As demonstrated in extensive Monte Carlo simulations, the
resulting bona fide estimator can result in sizeable improvements over the
sample covariance matrix and also over linear shrinkage."@2012
Michael Wolf@http://arxiv.org/abs/1207.5322v1@Nonlinear shrinkage estimation of large-dimensional covariance matrices@"Many statistical applications require an estimate of a covariance matrix
and/or its inverse. When the matrix dimension is large compared to the sample
size, which happens frequently, the sample covariance matrix is known to
perform poorly and may suffer from ill-conditioning. There already exists an
extensive literature concerning improved estimators in such situations. In the
absence of further knowledge about the structure of the true covariance matrix,
the most successful approach so far, arguably, has been shrinkage estimation.
Shrinking the sample covariance matrix to a multiple of the identity, by taking
a weighted average of the two, turns out to be equivalent to linearly shrinking
the sample eigenvalues to their grand mean, while retaining the sample
eigenvectors. Our paper extends this approach by considering nonlinear
transformations of the sample eigenvalues. We show how to construct an
estimator that is asymptotically equivalent to an oracle estimator suggested in
previous work. As demonstrated in extensive Monte Carlo simulations, the
resulting bona fide estimator can result in sizeable improvements over the
sample covariance matrix and also over linear shrinkage."@2012
John Ehrlinger@http://arxiv.org/abs/1207.5367v1@Characterizing $L_2$Boosting@"We consider $L_2$Boosting, a special case of Friedman's generic boosting
algorithm applied to linear regression under $L_2$-loss. We study $L_2$Boosting
for an arbitrary regularization parameter and derive an exact closed form
expression for the number of steps taken along a fixed coordinate direction.
This relationship is used to describe $L_2$Boosting's solution path, to
describe new tools for studying its path, and to characterize some of the
algorithm's unique properties, including active set cycling, a property where
the algorithm spends lengthy periods of time cycling between the same
coordinates when the regularization parameter is arbitrarily small. Our fixed
descent analysis also reveals a repressible condition that limits the
effectiveness of $L_2$Boosting in correlated problems by preventing desirable
variables from entering the solution path. As a simple remedy, a data
augmentation method similar to that used for the elastic net is used to
introduce $L_2$-penalization and is shown, in combination with decorrelation,
to reverse the repressible condition and circumvents $L_2$Boosting's
deficiencies in correlated problems. In itself, this presents a new explanation
for why the elastic net is successful in correlated problems and why methods
like LAR and lasso can perform poorly in such settings."@2012
Hemant Ishwaran@http://arxiv.org/abs/1207.5367v1@Characterizing $L_2$Boosting@"We consider $L_2$Boosting, a special case of Friedman's generic boosting
algorithm applied to linear regression under $L_2$-loss. We study $L_2$Boosting
for an arbitrary regularization parameter and derive an exact closed form
expression for the number of steps taken along a fixed coordinate direction.
This relationship is used to describe $L_2$Boosting's solution path, to
describe new tools for studying its path, and to characterize some of the
algorithm's unique properties, including active set cycling, a property where
the algorithm spends lengthy periods of time cycling between the same
coordinates when the regularization parameter is arbitrarily small. Our fixed
descent analysis also reveals a repressible condition that limits the
effectiveness of $L_2$Boosting in correlated problems by preventing desirable
variables from entering the solution path. As a simple remedy, a data
augmentation method similar to that used for the elastic net is used to
introduce $L_2$-penalization and is shown, in combination with decorrelation,
to reverse the repressible condition and circumvents $L_2$Boosting's
deficiencies in correlated problems. In itself, this presents a new explanation
for why the elastic net is successful in correlated problems and why methods
like LAR and lasso can perform poorly in such settings."@2012
Yunwen Yang@http://arxiv.org/abs/1207.5378v1@Bayesian empirical likelihood for quantile regression@"Bayesian inference provides a flexible way of combining data with prior
information. However, quantile regression is not equipped with a parametric
likelihood, and therefore, Bayesian inference for quantile regression demands
careful investigation. This paper considers the Bayesian empirical likelihood
approach to quantile regression. Taking the empirical likelihood into a
Bayesian framework, we show that the resultant posterior from any fixed prior
is asymptotically normal; its mean shrinks toward the true parameter values,
and its variance approaches that of the maximum empirical likelihood estimator.
A more interesting case can be made for the Bayesian empirical likelihood when
informative priors are used to explore commonality across quantiles. Regression
quantiles that are computed separately at each percentile level tend to be
highly variable in the data sparse areas (e.g., high or low percentile levels).
Through empirical likelihood, the proposed method enables us to explore various
forms of commonality across quantiles for efficiency gains. By using an MCMC
algorithm in the computation, we avoid the daunting task of directly maximizing
empirical likelihood. The finite sample performance of the proposed method is
investigated empirically, where substantial efficiency gains are demonstrated
with informative priors on common features across several percentile levels. A
theoretical framework of shrinking priors is used in the paper to better
understand the power of the proposed method."@2012
Xuming He@http://arxiv.org/abs/1207.5378v1@Bayesian empirical likelihood for quantile regression@"Bayesian inference provides a flexible way of combining data with prior
information. However, quantile regression is not equipped with a parametric
likelihood, and therefore, Bayesian inference for quantile regression demands
careful investigation. This paper considers the Bayesian empirical likelihood
approach to quantile regression. Taking the empirical likelihood into a
Bayesian framework, we show that the resultant posterior from any fixed prior
is asymptotically normal; its mean shrinks toward the true parameter values,
and its variance approaches that of the maximum empirical likelihood estimator.
A more interesting case can be made for the Bayesian empirical likelihood when
informative priors are used to explore commonality across quantiles. Regression
quantiles that are computed separately at each percentile level tend to be
highly variable in the data sparse areas (e.g., high or low percentile levels).
Through empirical likelihood, the proposed method enables us to explore various
forms of commonality across quantiles for efficiency gains. By using an MCMC
algorithm in the computation, we avoid the daunting task of directly maximizing
empirical likelihood. The finite sample performance of the proposed method is
investigated empirically, where substantial efficiency gains are demonstrated
with informative priors on common features across several percentile levels. A
theoretical framework of shrinking priors is used in the paper to better
understand the power of the proposed method."@2012
Enno Mammen@http://arxiv.org/abs/1207.5594v1@Nonparametric regression with nonparametrically generated covariates@"We analyze the statistical properties of nonparametric regression estimators
using covariates which are not directly observable, but have be estimated from
data in a preliminary step. These so-called generated covariates appear in
numerous applications, including two-stage nonparametric regression, estimation
of simultaneous equation models or censored regression models. Yet so far there
seems to be no general theory for their impact on the final estimator's
statistical properties. Our paper provides such results. We derive a stochastic
expansion that characterizes the influence of the generation step on the final
estimator, and use it to derive rates of consistency and asymptotic
distributions accounting for the presence of generated covariates."@2012
Christoph Rothe@http://arxiv.org/abs/1207.5594v1@Nonparametric regression with nonparametrically generated covariates@"We analyze the statistical properties of nonparametric regression estimators
using covariates which are not directly observable, but have be estimated from
data in a preliminary step. These so-called generated covariates appear in
numerous applications, including two-stage nonparametric regression, estimation
of simultaneous equation models or censored regression models. Yet so far there
seems to be no general theory for their impact on the final estimator's
statistical properties. Our paper provides such results. We derive a stochastic
expansion that characterizes the influence of the generation step on the final
estimator, and use it to derive rates of consistency and asymptotic
distributions accounting for the presence of generated covariates."@2012
Melanie Schienle@http://arxiv.org/abs/1207.5594v1@Nonparametric regression with nonparametrically generated covariates@"We analyze the statistical properties of nonparametric regression estimators
using covariates which are not directly observable, but have be estimated from
data in a preliminary step. These so-called generated covariates appear in
numerous applications, including two-stage nonparametric regression, estimation
of simultaneous equation models or censored regression models. Yet so far there
seems to be no general theory for their impact on the final estimator's
statistical properties. Our paper provides such results. We derive a stochastic
expansion that characterizes the influence of the generation step on the final
estimator, and use it to derive rates of consistency and asymptotic
distributions accounting for the presence of generated covariates."@2012
Viktor Todorov@http://arxiv.org/abs/1207.5615v1@Realized Laplace transforms for pure-jump semimartingales@"We consider specification and inference for the stochastic scale of
discretely-observed pure-jump semimartingales with locally stable L\'{e}vy
densities in the setting where both the time span of the data set increases,
and the mesh of the observation grid decreases. The estimation is based on
constructing a nonparametric estimate for the empirical Laplace transform of
the stochastic scale over a given interval of time by aggregating
high-frequency increments of the observed process on that time interval into a
statistic we call realized Laplace transform. The realized Laplace transform
depends on the activity of the driving pure-jump martingale, and we consider
both cases when the latter is known or has to be inferred from the data."@2012
George Tauchen@http://arxiv.org/abs/1207.5615v1@Realized Laplace transforms for pure-jump semimartingales@"We consider specification and inference for the stochastic scale of
discretely-observed pure-jump semimartingales with locally stable L\'{e}vy
densities in the setting where both the time span of the data set increases,
and the mesh of the observation grid decreases. The estimation is based on
constructing a nonparametric estimate for the empirical Laplace transform of
the stochastic scale over a given interval of time by aggregating
high-frequency increments of the observed process on that time interval into a
statistic we call realized Laplace transform. The realized Laplace transform
depends on the activity of the driving pure-jump martingale, and we consider
both cases when the latter is known or has to be inferred from the data."@2012
Kari Lock Morgan@http://arxiv.org/abs/1207.5625v1@Rerandomization to improve covariate balance in experiments@"Randomized experiments are the ""gold standard"" for estimating causal effects,
yet often in practice, chance imbalances exist in covariate distributions
between treatment groups. If covariate data are available before units are
exposed to treatments, these chance imbalances can be mitigated by first
checking covariate balance before the physical experiment takes place. Provided
a precise definition of imbalance has been specified in advance, unbalanced
randomizations can be discarded, followed by a rerandomization, and this
process can continue until a randomization yielding balance according to the
definition is achieved. By improving covariate balance, rerandomization
provides more precise and trustworthy estimates of treatment effects."@2012
Donald B. Rubin@http://arxiv.org/abs/1207.5625v1@Rerandomization to improve covariate balance in experiments@"Randomized experiments are the ""gold standard"" for estimating causal effects,
yet often in practice, chance imbalances exist in covariate distributions
between treatment groups. If covariate data are available before units are
exposed to treatments, these chance imbalances can be mitigated by first
checking covariate balance before the physical experiment takes place. Provided
a precise definition of imbalance has been specified in advance, unbalanced
randomizations can be discarded, followed by a rerandomization, and this
process can continue until a randomization yielding balance according to the
definition is achieved. By improving covariate balance, rerandomization
provides more precise and trustworthy estimates of treatment effects."@2012
Hélène Boistard@http://arxiv.org/abs/1207.5654v1@"Approximation of rejective sampling inclusion probabilities and
  application to high order correlations"@"This paper is devoted to rejective sampling. We provide an expansion of joint
inclusion probabilities of any order in terms of the inclusion probabilities of
order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and
making the remainder term more precise. Following H\'ajek (1981), the proof is
based on Edgeworth expansions. The main result is applied to derive bounds on
higher order correlations, which are needed for the consistency and asymptotic
normality of several complex estimators."@2012
Hendrik P. Lopuhaä@http://arxiv.org/abs/1207.5654v1@"Approximation of rejective sampling inclusion probabilities and
  application to high order correlations"@"This paper is devoted to rejective sampling. We provide an expansion of joint
inclusion probabilities of any order in terms of the inclusion probabilities of
order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and
making the remainder term more precise. Following H\'ajek (1981), the proof is
based on Edgeworth expansions. The main result is applied to derive bounds on
higher order correlations, which are needed for the consistency and asymptotic
normality of several complex estimators."@2012
Anne Ruiz-Gazen@http://arxiv.org/abs/1207.5654v1@"Approximation of rejective sampling inclusion probabilities and
  application to high order correlations"@"This paper is devoted to rejective sampling. We provide an expansion of joint
inclusion probabilities of any order in terms of the inclusion probabilities of
order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and
making the remainder term more precise. Following H\'ajek (1981), the proof is
based on Edgeworth expansions. The main result is applied to derive bounds on
higher order correlations, which are needed for the consistency and asymptotic
normality of several complex estimators."@2012
Eric Beutner@http://arxiv.org/abs/1207.5899v1@"Deriving the asymptotic distribution of U- and V-statistics of dependent
  data using weighted empirical processes"@"It is commonly acknowledged that V-functionals with an unbounded kernel are
not Hadamard differentiable and that therefore the asymptotic distribution of
U- and V-statistics with an unbounded kernel cannot be derived by the
Functional Delta Method (FDM). However, in this article we show that
V-functionals are quasi-Hadamard differentiable and that therefore a modified
version of the FDM (introduced recently in (J. Multivariate Anal. 101 (2010)
2452--2463)) can be applied to this problem. The modified FDM requires weak
convergence of a weighted version of the underlying empirical process. The
latter is not problematic since there exist several results on weighted
empirical processes in the literature; see, for example, (J. Econometrics 130
(2006) 307--335, Ann. Probab. 24 (1996) 2098--2127, Empirical Processes with
Applications to Statistics (1986) Wiley, Statist. Sinica 18 (2008) 313--333).
The modified FDM approach has the advantage that it is very flexible w.r.t.
both the underlying data and the estimator of the unknown distribution
function. Both will be demonstrated by various examples. In particular, we will
show that our FDM approach covers mainly all the results known in literature
for the asymptotic distribution of U- and V-statistics based on dependent data
-- and our assumptions are by tendency even weaker. Moreover, using our FDM
approach we extend these results to dependence concepts that are not covered by
the existing literature."@2012
Henryk Zähle@http://arxiv.org/abs/1207.5899v1@"Deriving the asymptotic distribution of U- and V-statistics of dependent
  data using weighted empirical processes"@"It is commonly acknowledged that V-functionals with an unbounded kernel are
not Hadamard differentiable and that therefore the asymptotic distribution of
U- and V-statistics with an unbounded kernel cannot be derived by the
Functional Delta Method (FDM). However, in this article we show that
V-functionals are quasi-Hadamard differentiable and that therefore a modified
version of the FDM (introduced recently in (J. Multivariate Anal. 101 (2010)
2452--2463)) can be applied to this problem. The modified FDM requires weak
convergence of a weighted version of the underlying empirical process. The
latter is not problematic since there exist several results on weighted
empirical processes in the literature; see, for example, (J. Econometrics 130
(2006) 307--335, Ann. Probab. 24 (1996) 2098--2127, Empirical Processes with
Applications to Statistics (1986) Wiley, Statist. Sinica 18 (2008) 313--333).
The modified FDM approach has the advantage that it is very flexible w.r.t.
both the underlying data and the estimator of the unknown distribution
function. Both will be demonstrated by various examples. In particular, we will
show that our FDM approach covers mainly all the results known in literature
for the asymptotic distribution of U- and V-statistics based on dependent data
-- and our assumptions are by tendency even weaker. Moreover, using our FDM
approach we extend these results to dependence concepts that are not covered by
the existing literature."@2012
Shaul K. Bar-Lev@http://arxiv.org/abs/1207.5902v1@On the small-time behavior of subordinators@"We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain
$(0,\infty)$-valued stochastic processes $(Y_t)_{t>0}$. In particular, we show
for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only
possible weak limit and provide necessary and sufficient conditions for the
convergence. More generally, we also consider the weak convergence of $tL(Y_t)$
as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.
Various examples demonstrating the applicability of the results are presented."@2012
Andreas Löpker@http://arxiv.org/abs/1207.5902v1@On the small-time behavior of subordinators@"We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain
$(0,\infty)$-valued stochastic processes $(Y_t)_{t>0}$. In particular, we show
for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only
possible weak limit and provide necessary and sufficient conditions for the
convergence. More generally, we also consider the weak convergence of $tL(Y_t)$
as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.
Various examples demonstrating the applicability of the results are presented."@2012
Wolfgang Stadje@http://arxiv.org/abs/1207.5902v1@On the small-time behavior of subordinators@"We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain
$(0,\infty)$-valued stochastic processes $(Y_t)_{t>0}$. In particular, we show
for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only
possible weak limit and provide necessary and sufficient conditions for the
convergence. More generally, we also consider the weak convergence of $tL(Y_t)$
as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.
Various examples demonstrating the applicability of the results are presented."@2012
Jan Draisma@http://arxiv.org/abs/1207.5910v3@Groups acting on Gaussian graphical models@"Gaussian graphical models have become a well-recognized tool for the analysis
of conditional independencies within a set of continuous random variables. From
an inferential point of view, it is important to realize that they are
composite exponential transformation families. We reveal this structure by
explicitly describing, for any undirected graph, the (maximal) matrix group
acting on the space of concentration matrices in the model. The continuous part
of this group is captured by a poset naturally associated to the graph, while
automorphisms of the graph account for the discrete part of the group. We
compute the dimension of the space of orbits of this group on concentration
matrices, in terms of the combinatorics of the graph; and for dimension zero we
recover the characterization by Letac and Massam of models that are
transformation families. Furthermore, we describe the maximal invariant of this
group on the sample space, and we give a sharp lower bound on the sample size
needed for the existence of equivariant estimators of the concentration matrix.
Finally, we address the issue of robustness of these estimators by computing
upper bounds on finite sample breakdown points."@2012
Sonja Kuhnt@http://arxiv.org/abs/1207.5910v3@Groups acting on Gaussian graphical models@"Gaussian graphical models have become a well-recognized tool for the analysis
of conditional independencies within a set of continuous random variables. From
an inferential point of view, it is important to realize that they are
composite exponential transformation families. We reveal this structure by
explicitly describing, for any undirected graph, the (maximal) matrix group
acting on the space of concentration matrices in the model. The continuous part
of this group is captured by a poset naturally associated to the graph, while
automorphisms of the graph account for the discrete part of the group. We
compute the dimension of the space of orbits of this group on concentration
matrices, in terms of the combinatorics of the graph; and for dimension zero we
recover the characterization by Letac and Massam of models that are
transformation families. Furthermore, we describe the maximal invariant of this
group on the sample space, and we give a sharp lower bound on the sample size
needed for the existence of equivariant estimators of the concentration matrix.
Finally, we address the issue of robustness of these estimators by computing
upper bounds on finite sample breakdown points."@2012
Piotr Zwiernik@http://arxiv.org/abs/1207.5910v3@Groups acting on Gaussian graphical models@"Gaussian graphical models have become a well-recognized tool for the analysis
of conditional independencies within a set of continuous random variables. From
an inferential point of view, it is important to realize that they are
composite exponential transformation families. We reveal this structure by
explicitly describing, for any undirected graph, the (maximal) matrix group
acting on the space of concentration matrices in the model. The continuous part
of this group is captured by a poset naturally associated to the graph, while
automorphisms of the graph account for the discrete part of the group. We
compute the dimension of the space of orbits of this group on concentration
matrices, in terms of the combinatorics of the graph; and for dimension zero we
recover the characterization by Letac and Massam of models that are
transformation families. Furthermore, we describe the maximal invariant of this
group on the sample space, and we give a sharp lower bound on the sample size
needed for the existence of equivariant estimators of the concentration matrix.
Finally, we address the issue of robustness of these estimators by computing
upper bounds on finite sample breakdown points."@2012
Stéphanie Allassonniere@http://arxiv.org/abs/1207.5938v4@"Convergent Stochastic Expectation Maximization algorithm with efficient
  sampling in high dimension. Application to deformable template model
  estimation"@"Estimation in the deformable template model is a big challenge in image
analysis. The issue is to estimate an atlas of a population. This atlas
contains a template and the corresponding geometrical variability of the
observed shapes. The goal is to propose an accurate algorithm with low
computational cost and with theoretical guaranties of relevance. This becomes
very demanding when dealing with high dimensional data which is particularly
the case of medical images. We propose to use an optimized Monte Carlo Markov
Chain method into a stochastic Expectation Maximization algorithm in order to
estimate the model parameters by maximizing the likelihood. In this paper, we
present a new Anisotropic Metropolis Adjusted Langevin Algorithm which we use
as transition in the MCMC method. We first prove that this new sampler leads to
a geometrically uniformly ergodic Markov chain. We prove also that under mild
conditions, the estimated parameters converge almost surely and are
asymptotically Gaussian distributed. The methodology developed is then tested
on handwritten digits and some 2D and 3D medical images for the deformable
model estimation. More widely, the proposed algorithm can be used for a large
range of models in many fields of applications such as pharmacology or genetic."@2012
Estelle Kuhn@http://arxiv.org/abs/1207.5938v4@"Convergent Stochastic Expectation Maximization algorithm with efficient
  sampling in high dimension. Application to deformable template model
  estimation"@"Estimation in the deformable template model is a big challenge in image
analysis. The issue is to estimate an atlas of a population. This atlas
contains a template and the corresponding geometrical variability of the
observed shapes. The goal is to propose an accurate algorithm with low
computational cost and with theoretical guaranties of relevance. This becomes
very demanding when dealing with high dimensional data which is particularly
the case of medical images. We propose to use an optimized Monte Carlo Markov
Chain method into a stochastic Expectation Maximization algorithm in order to
estimate the model parameters by maximizing the likelihood. In this paper, we
present a new Anisotropic Metropolis Adjusted Langevin Algorithm which we use
as transition in the MCMC method. We first prove that this new sampler leads to
a geometrically uniformly ergodic Markov chain. We prove also that under mild
conditions, the estimated parameters converge almost surely and are
asymptotically Gaussian distributed. The methodology developed is then tested
on handwritten digits and some 2D and 3D medical images for the deformable
model estimation. More widely, the proposed algorithm can be used for a large
range of models in many fields of applications such as pharmacology or genetic."@2012
Liugen Xue@http://arxiv.org/abs/1207.5960v1@Empirical likelihood for single-index varying-coefficient models@"In this paper, we develop statistical inference techniques for the unknown
coefficient functions and single-index parameters in single-index
varying-coefficient models. We first estimate the nonparametric component via
the local linear fitting, then construct an estimated empirical likelihood
ratio function and hence obtain a maximum empirical likelihood estimator for
the parametric component. Our estimator for parametric component is
asymptotically efficient, and the estimator of nonparametric component has an
optimal convergence rate. Our results provide ways to construct the confidence
region for the involved unknown parameter. We also develop an adjusted
empirical likelihood ratio for constructing the confidence regions of
parameters of interest. A simulation study is conducted to evaluate the finite
sample behaviors of the proposed methods."@2012
Qihua Wang@http://arxiv.org/abs/1207.5960v1@Empirical likelihood for single-index varying-coefficient models@"In this paper, we develop statistical inference techniques for the unknown
coefficient functions and single-index parameters in single-index
varying-coefficient models. We first estimate the nonparametric component via
the local linear fitting, then construct an estimated empirical likelihood
ratio function and hence obtain a maximum empirical likelihood estimator for
the parametric component. Our estimator for parametric component is
asymptotically efficient, and the estimator of nonparametric component has an
optimal convergence rate. Our results provide ways to construct the confidence
region for the involved unknown parameter. We also develop an adjusted
empirical likelihood ratio for constructing the confidence regions of
parameters of interest. A simulation study is conducted to evaluate the finite
sample behaviors of the proposed methods."@2012
David Dereudre@http://arxiv.org/abs/1207.5998v3@"Estimation of the intensity parameter of the germ-grain
  Quermass-interaction model when the number of germs is not observed"@"The Quermass-interaction model allows to generalise the classical germ-grain
Boolean model in adding a morphological interaction between the grains. It
enables to model random structures with specific morphologies which are
unlikely to be generated from a Boolean model. The Quermass-interaction model
depends in particular on an intensity parameter, which is impossible to
estimate from classical likelihood or pseudo-likelihood approaches because the
number of points is not observable from a germ-grain set. In this paper, we
present a procedure based on the Takacs-Fiksel method which is able to estimate
all parameters of the Quermass-interaction model, including the intensity. An
intensive simulation study is conducted to assess the efficiency of the
procedure and to provide practical recommendations. It also illustrates that
the estimation of the intensity parameter is crucial in order to identify the
model. The Quermass-interaction model is finally fitted by our method to P.
Diggle's heather dataset."@2012
Frédéric Lavancier@http://arxiv.org/abs/1207.5998v3@"Estimation of the intensity parameter of the germ-grain
  Quermass-interaction model when the number of germs is not observed"@"The Quermass-interaction model allows to generalise the classical germ-grain
Boolean model in adding a morphological interaction between the grains. It
enables to model random structures with specific morphologies which are
unlikely to be generated from a Boolean model. The Quermass-interaction model
depends in particular on an intensity parameter, which is impossible to
estimate from classical likelihood or pseudo-likelihood approaches because the
number of points is not observable from a germ-grain set. In this paper, we
present a procedure based on the Takacs-Fiksel method which is able to estimate
all parameters of the Quermass-interaction model, including the intensity. An
intensive simulation study is conducted to assess the efficiency of the
procedure and to provide practical recommendations. It also illustrates that
the estimation of the intensity parameter is crucial in order to identify the
model. The Quermass-interaction model is finally fitted by our method to P.
Diggle's heather dataset."@2012
Katerina Helisova Stankova@http://arxiv.org/abs/1207.5998v3@"Estimation of the intensity parameter of the germ-grain
  Quermass-interaction model when the number of germs is not observed"@"The Quermass-interaction model allows to generalise the classical germ-grain
Boolean model in adding a morphological interaction between the grains. It
enables to model random structures with specific morphologies which are
unlikely to be generated from a Boolean model. The Quermass-interaction model
depends in particular on an intensity parameter, which is impossible to
estimate from classical likelihood or pseudo-likelihood approaches because the
number of points is not observable from a germ-grain set. In this paper, we
present a procedure based on the Takacs-Fiksel method which is able to estimate
all parameters of the Quermass-interaction model, including the intensity. An
intensive simulation study is conducted to assess the efficiency of the
procedure and to provide practical recommendations. It also illustrates that
the estimation of the intensity parameter is crucial in order to identify the
model. The Quermass-interaction model is finally fitted by our method to P.
Diggle's heather dataset."@2012
Stefano Favaro@http://arxiv.org/abs/1207.6228v1@A class of measure-valued Markov chains and Bayesian nonparametrics@"Measure-valued Markov chains have raised interest in Bayesian nonparametrics
since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)
579--585) where a Markov chain having the law of the Dirichlet process as
unique invariant measure has been introduced. In the present paper, we propose
and investigate a new class of measure-valued Markov chains defined via
exchangeable sequences of random variables. Asymptotic properties for this new
class are derived and applications related to Bayesian nonparametric mixture
modeling, and to a generalization of the Markov chain proposed by (Math. Proc.
Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and
their applications highlight once again the interplay between Bayesian
nonparametrics and the theory of measure-valued Markov chains."@2012
Alessandra Guglielmi@http://arxiv.org/abs/1207.6228v1@A class of measure-valued Markov chains and Bayesian nonparametrics@"Measure-valued Markov chains have raised interest in Bayesian nonparametrics
since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)
579--585) where a Markov chain having the law of the Dirichlet process as
unique invariant measure has been introduced. In the present paper, we propose
and investigate a new class of measure-valued Markov chains defined via
exchangeable sequences of random variables. Asymptotic properties for this new
class are derived and applications related to Bayesian nonparametric mixture
modeling, and to a generalization of the Markov chain proposed by (Math. Proc.
Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and
their applications highlight once again the interplay between Bayesian
nonparametrics and the theory of measure-valued Markov chains."@2012
Stephen G. Walker@http://arxiv.org/abs/1207.6228v1@A class of measure-valued Markov chains and Bayesian nonparametrics@"Measure-valued Markov chains have raised interest in Bayesian nonparametrics
since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)
579--585) where a Markov chain having the law of the Dirichlet process as
unique invariant measure has been introduced. In the present paper, we propose
and investigate a new class of measure-valued Markov chains defined via
exchangeable sequences of random variables. Asymptotic properties for this new
class are derived and applications related to Bayesian nonparametric mixture
modeling, and to a generalization of the Markov chain proposed by (Math. Proc.
Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and
their applications highlight once again the interplay between Bayesian
nonparametrics and the theory of measure-valued Markov chains."@2012
Xiang Zhang@http://arxiv.org/abs/1207.6363v1@"A Note on Spatial-Temporal Lattice Modeling and Maximum Likelihood
  Estimation"@"Spatial-temporal linear model and the corresponding likelihood-based
statistical inference are important tools for the analysis of spatial-temporal
lattice data. In this paper, we study the asymptotic properties of maximum
likelihood estimates under a general asymptotic framework for spatial-temporal
linear models. We propose mild regularity conditions on the spatial-temporal
weight matrices and derive the asymptotic properties (consistency and
asymptotic normality) of maximum likelihood estimates. A simulation study is
conducted to examine the finite-sample properties of the maximum likelihood
estimates."@2012
Yanbing Zheng@http://arxiv.org/abs/1207.6363v1@"A Note on Spatial-Temporal Lattice Modeling and Maximum Likelihood
  Estimation"@"Spatial-temporal linear model and the corresponding likelihood-based
statistical inference are important tools for the analysis of spatial-temporal
lattice data. In this paper, we study the asymptotic properties of maximum
likelihood estimates under a general asymptotic framework for spatial-temporal
linear models. We propose mild regularity conditions on the spatial-temporal
weight matrices and derive the asymptotic properties (consistency and
asymptotic normality) of maximum likelihood estimates. A simulation study is
conducted to examine the finite-sample properties of the maximum likelihood
estimates."@2012
Gérard Biau@http://arxiv.org/abs/1207.6461v2@New Insights Into Approximate Bayesian Computation@"Approximate Bayesian Computation (ABC for short) is a family of computational
techniques which offer an almost automated solution in situations where
evaluation of the posterior likelihood is computationally prohibitive, or
whenever suitable likelihoods are not available. In the present paper, we
analyze the procedure from the point of view of k-nearest neighbor theory and
explore the statistical properties of its outputs. We discuss in particular
some asymptotic features of the genuine conditional density estimate associated
with ABC, which is an interesting hybrid between a k-nearest neighbor and a
kernel method."@2012
Frédéric Cérou@http://arxiv.org/abs/1207.6461v2@New Insights Into Approximate Bayesian Computation@"Approximate Bayesian Computation (ABC for short) is a family of computational
techniques which offer an almost automated solution in situations where
evaluation of the posterior likelihood is computationally prohibitive, or
whenever suitable likelihoods are not available. In the present paper, we
analyze the procedure from the point of view of k-nearest neighbor theory and
explore the statistical properties of its outputs. We discuss in particular
some asymptotic features of the genuine conditional density estimate associated
with ABC, which is an interesting hybrid between a k-nearest neighbor and a
kernel method."@2012
Arnaud Guyader@http://arxiv.org/abs/1207.6461v2@New Insights Into Approximate Bayesian Computation@"Approximate Bayesian Computation (ABC for short) is a family of computational
techniques which offer an almost automated solution in situations where
evaluation of the posterior likelihood is computationally prohibitive, or
whenever suitable likelihoods are not available. In the present paper, we
analyze the procedure from the point of view of k-nearest neighbor theory and
explore the statistical properties of its outputs. We discuss in particular
some asymptotic features of the genuine conditional density estimate associated
with ABC, which is an interesting hybrid between a k-nearest neighbor and a
kernel method."@2012
M. Z. Anis@http://arxiv.org/abs/1207.6763v1@"The exact null distribution of the generalized Hollander-Proschan type
  test for NBUE alternatives"@"In this note we derive the exact null distribution for the test statistic
proposed by Anis and Mitra (2011) for testing exponentiality against NBUE
alternatives. As a special case, we obtain the exact null distribution for the
test statistic proposed by Hollander and Proschan (1975). Selected critical
values for different size are tabulated for these two statistics. Some remarks
concerning the benefits of using the exact distribution are made."@2012
Kinjal Basu@http://arxiv.org/abs/1207.6763v1@"The exact null distribution of the generalized Hollander-Proschan type
  test for NBUE alternatives"@"In this note we derive the exact null distribution for the test statistic
proposed by Anis and Mitra (2011) for testing exponentiality against NBUE
alternatives. As a special case, we obtain the exact null distribution for the
test statistic proposed by Hollander and Proschan (1975). Selected critical
values for different size are tabulated for these two statistics. Some remarks
concerning the benefits of using the exact distribution are made."@2012
Eric Marchand@http://arxiv.org/abs/1208.0028v2@"On Bayesian credible sets in restricted parameter space problems and
  lower bounds for frequentist coverage"@"For estimating a lower bounded parametric function in the framework of
Marchand and Strawderman (2006), we provide through a unified approach a class
of Bayesian confidence intervals with credibility $1-\alpha$ and frequentist
coverage probability bounded below by $\frac{1-\alpha}{1+\alpha}$. In cases
where the underlying pivotal distribution is symmetric, the findings represent
extensions with respect to the specification of the credible set achieved
through the choice of a {\it spending function}, and include Marchand and
Strawderman's HPD procedure result. For non-symmetric cases, the determination
of a such a class of Bayesian credible sets fills a gap in the literature and
includes an ""equal-tails"" modification of the HPD procedure. Several examples
are presented demonstrating wide applicability."@2012
William E. Strawderman@http://arxiv.org/abs/1208.0028v2@"On Bayesian credible sets in restricted parameter space problems and
  lower bounds for frequentist coverage"@"For estimating a lower bounded parametric function in the framework of
Marchand and Strawderman (2006), we provide through a unified approach a class
of Bayesian confidence intervals with credibility $1-\alpha$ and frequentist
coverage probability bounded below by $\frac{1-\alpha}{1+\alpha}$. In cases
where the underlying pivotal distribution is symmetric, the findings represent
extensions with respect to the specification of the credible set achieved
through the choice of a {\it spending function}, and include Marchand and
Strawderman's HPD procedure result. For non-symmetric cases, the determination
of a such a class of Bayesian credible sets fills a gap in the literature and
includes an ""equal-tails"" modification of the HPD procedure. Several examples
are presented demonstrating wide applicability."@2012
Niels Richard Hansen@http://arxiv.org/abs/1208.0570v2@Lasso and probabilistic inequalities for multivariate point processes@"Due to its low computational cost, Lasso is an attractive regularization
method for high-dimensional statistical settings. In this paper, we consider
multivariate counting processes depending on an unknown function parameter to
be estimated by linear combinations of a fixed dictionary. To select
coefficients, we propose an adaptive $\ell_1$-penalization methodology, where
data-driven weights of the penalty are derived from new Bernstein type
inequalities for martingales. Oracle inequalities are established under
assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic
results for multivariate Hawkes processes are proven, which allows us to check
these assumptions by considering general dictionaries based on histograms,
Fourier or wavelet bases. Motivated by problems of neuronal activity inference,
we finally carry out a simulation study for multivariate Hawkes processes and
compare our methodology with the adaptive Lasso procedure proposed by Zou in
(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent
behavior of our procedure. We rely on theoretical aspects for the essential
question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.
Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with
respect to all the parameters of the problem, revealing its potential for
concrete purposes, in particular in neuroscience."@2012
Patricia Reynaud-Bouret@http://arxiv.org/abs/1208.0570v2@Lasso and probabilistic inequalities for multivariate point processes@"Due to its low computational cost, Lasso is an attractive regularization
method for high-dimensional statistical settings. In this paper, we consider
multivariate counting processes depending on an unknown function parameter to
be estimated by linear combinations of a fixed dictionary. To select
coefficients, we propose an adaptive $\ell_1$-penalization methodology, where
data-driven weights of the penalty are derived from new Bernstein type
inequalities for martingales. Oracle inequalities are established under
assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic
results for multivariate Hawkes processes are proven, which allows us to check
these assumptions by considering general dictionaries based on histograms,
Fourier or wavelet bases. Motivated by problems of neuronal activity inference,
we finally carry out a simulation study for multivariate Hawkes processes and
compare our methodology with the adaptive Lasso procedure proposed by Zou in
(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent
behavior of our procedure. We rely on theoretical aspects for the essential
question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.
Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with
respect to all the parameters of the problem, revealing its potential for
concrete purposes, in particular in neuroscience."@2012
Vincent Rivoirard@http://arxiv.org/abs/1208.0570v2@Lasso and probabilistic inequalities for multivariate point processes@"Due to its low computational cost, Lasso is an attractive regularization
method for high-dimensional statistical settings. In this paper, we consider
multivariate counting processes depending on an unknown function parameter to
be estimated by linear combinations of a fixed dictionary. To select
coefficients, we propose an adaptive $\ell_1$-penalization methodology, where
data-driven weights of the penalty are derived from new Bernstein type
inequalities for martingales. Oracle inequalities are established under
assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic
results for multivariate Hawkes processes are proven, which allows us to check
these assumptions by considering general dictionaries based on histograms,
Fourier or wavelet bases. Motivated by problems of neuronal activity inference,
we finally carry out a simulation study for multivariate Hawkes processes and
compare our methodology with the adaptive Lasso procedure proposed by Zou in
(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent
behavior of our procedure. We rely on theoretical aspects for the essential
question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.
Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with
respect to all the parameters of the problem, revealing its potential for
concrete purposes, in particular in neuroscience."@2012
Nathan Huntley@http://arxiv.org/abs/1208.1154v1@Identifying subtree perfectness in decision trees@"In decision problems, often, utilities and probabilities are hard to
determine. In such cases, one can resort to so-called choice functions. They
provide a means to determine which options in a particular set are optimal, and
allow incomparability among any number of options. Applying choice functions in
sequential decision problems can be highly non-trivial, as the usual properties
of maximising expected utility may no longer be satisfied. In this paper, we
study one of these properties: we revisit and reinterpret Selten's concept of
subgame perfectness in the context of decision trees, leading us to the concept
of subtree perfectness, which basically says that the optimal solution of a
decision tree should not depend on any larger tree it may be embedded in. In
other words, subtree perfectness excludes counterfactual reasoning, and
therefore may be desirable from some philosophical points of view. Subtree
perfectness is also desirable from a practical point of view, because it admits
efficient algorithms for solving decision trees, such as backward induction.
The main contribution of this paper is a very simple non-technical criterion
for determining whether any given choice function will satisfy subtree
perfectness or not. We demonstrate the theorem and illustrate subtree
perfectness, or the lack thereof, through numerous examples, for a wide variety
of choice functions, where incomparability amongst strategies can be caused by
imprecision in either probabilities or utilities. We find that almost no choice
function, except for maximising expected utility, satisfies it in general. We
also find that choice functions other than maximising expected utility can
satisfy it, provided that we restrict either the structure of the tree, or the
structure of the choice function."@2012
Matthias C. M. Troffaes@http://arxiv.org/abs/1208.1154v1@Identifying subtree perfectness in decision trees@"In decision problems, often, utilities and probabilities are hard to
determine. In such cases, one can resort to so-called choice functions. They
provide a means to determine which options in a particular set are optimal, and
allow incomparability among any number of options. Applying choice functions in
sequential decision problems can be highly non-trivial, as the usual properties
of maximising expected utility may no longer be satisfied. In this paper, we
study one of these properties: we revisit and reinterpret Selten's concept of
subgame perfectness in the context of decision trees, leading us to the concept
of subtree perfectness, which basically says that the optimal solution of a
decision tree should not depend on any larger tree it may be embedded in. In
other words, subtree perfectness excludes counterfactual reasoning, and
therefore may be desirable from some philosophical points of view. Subtree
perfectness is also desirable from a practical point of view, because it admits
efficient algorithms for solving decision trees, such as backward induction.
The main contribution of this paper is a very simple non-technical criterion
for determining whether any given choice function will satisfy subtree
perfectness or not. We demonstrate the theorem and illustrate subtree
perfectness, or the lack thereof, through numerous examples, for a wide variety
of choice functions, where incomparability amongst strategies can be caused by
imprecision in either probabilities or utilities. We find that almost no choice
function, except for maximising expected utility, satisfies it in general. We
also find that choice functions other than maximising expected utility can
satisfy it, provided that we restrict either the structure of the tree, or the
structure of the choice function."@2012
Laëtitia Comminges@http://arxiv.org/abs/1208.1823v3@"Minimax testing of a composite null hypothesis defined via a quadratic
  functional in the model of regression"@"We consider the problem of testing a particular type of composite null
hypothesis under a nonparametric multivariate regression model. For a given
quadratic functional $Q$, the null hypothesis states that the regression
function $f$ satisfies the constraint $Q[f]=0$, while the alternative
corresponds to the functions for which $Q[f]$ is bounded away from zero. On the
one hand, we provide minimax rates of testing and the exact separation
constants, along with a sharp-optimal testing procedure, for diagonal and
nonnegative quadratic functionals. We consider smoothness classes of
ellipsoidal form and check that our conditions are fulfilled in the particular
case of ellipsoids corresponding to anisotropic Sobolev classes. In this case,
we present a closed form of the minimax rate and the separation constant. On
the other hand, minimax rates for quadratic functionals which are neither
positive nor negative makes appear two different regimes: ""regular"" and
""irregular"". In the ""regular"" case, the minimax rate is equal to $n^{-1/4}$
while in the ""irregular"" case, the rate depends on the smoothness class and is
slower than in the ""regular"" case. We apply this to the issue of testing the
equality of norms of two functions observed in noisy environments."@2012
Arnak Dalalyan@http://arxiv.org/abs/1208.1823v3@"Minimax testing of a composite null hypothesis defined via a quadratic
  functional in the model of regression"@"We consider the problem of testing a particular type of composite null
hypothesis under a nonparametric multivariate regression model. For a given
quadratic functional $Q$, the null hypothesis states that the regression
function $f$ satisfies the constraint $Q[f]=0$, while the alternative
corresponds to the functions for which $Q[f]$ is bounded away from zero. On the
one hand, we provide minimax rates of testing and the exact separation
constants, along with a sharp-optimal testing procedure, for diagonal and
nonnegative quadratic functionals. We consider smoothness classes of
ellipsoidal form and check that our conditions are fulfilled in the particular
case of ellipsoids corresponding to anisotropic Sobolev classes. In this case,
we present a closed form of the minimax rate and the separation constant. On
the other hand, minimax rates for quadratic functionals which are neither
positive nor negative makes appear two different regimes: ""regular"" and
""irregular"". In the ""regular"" case, the minimax rate is equal to $n^{-1/4}$
while in the ""irregular"" case, the rate depends on the smoothness class and is
slower than in the ""regular"" case. We apply this to the issue of testing the
equality of norms of two functions observed in noisy environments."@2012
F. Bartolucci@http://arxiv.org/abs/1208.1864v1@"Nested hidden Markov chains for modeling dynamic unobserved
  heterogeneity in multilevel longitudinal data"@"In the context of multilevel longitudinal data, where sample units are
collected in clusters, an important aspect that should be accounted for is the
unobserved heterogeneity between sample units and between clusters. For this
aim we propose an approach based on nested hidden (latent) Markov chains, which
are associated to every sample unit and to every cluster. The approach allows
us to account for the mentioned forms of unobserved heterogeneity in a dynamic
fashion; it also allows us to account for the correlation which may arise
between the responses provided by the units belonging to the same cluster.
Given the complexity in computing the manifest distribution of these response
variables, we make inference on the proposed model through a composite
likelihood function based on all the possible pairs of subjects within every
cluster. The proposed approach is illustrated through an application to a
dataset concerning a sample of Italian workers in which a binary response
variable for the worker receiving an illness benefit was repeatedly observed."@2012
M. Lupparelli@http://arxiv.org/abs/1208.1864v1@"Nested hidden Markov chains for modeling dynamic unobserved
  heterogeneity in multilevel longitudinal data"@"In the context of multilevel longitudinal data, where sample units are
collected in clusters, an important aspect that should be accounted for is the
unobserved heterogeneity between sample units and between clusters. For this
aim we propose an approach based on nested hidden (latent) Markov chains, which
are associated to every sample unit and to every cluster. The approach allows
us to account for the mentioned forms of unobserved heterogeneity in a dynamic
fashion; it also allows us to account for the correlation which may arise
between the responses provided by the units belonging to the same cluster.
Given the complexity in computing the manifest distribution of these response
variables, we make inference on the proposed model through a composite
likelihood function based on all the possible pairs of subjects within every
cluster. The proposed approach is illustrated through an application to a
dataset concerning a sample of Italian workers in which a binary response
variable for the worker receiving an illness benefit was repeatedly observed."@2012
Ery Arias-Castro@http://arxiv.org/abs/1208.2635v2@Variable Selection with Exponential Weights and $l_0$-Penalization@"In the context of a linear model with a sparse coefficient vector,
exponential weights methods have been shown to be achieve oracle inequalities
for prediction. We show that such methods also succeed at variable selection
and estimation under the necessary identifiability condition on the design
matrix, instead of much stronger assumptions required by other methods such as
the Lasso or the Dantzig Selector. The same analysis yields consistency results
for Bayesian methods and BIC-type variable selection under similar conditions."@2012
Karim Lounici@http://arxiv.org/abs/1208.2635v2@Variable Selection with Exponential Weights and $l_0$-Penalization@"In the context of a linear model with a sparse coefficient vector,
exponential weights methods have been shown to be achieve oracle inequalities
for prediction. We show that such methods also succeed at variable selection
and estimation under the necessary identifiability condition on the design
matrix, instead of much stronger assumptions required by other methods such as
the Lasso or the Dantzig Selector. The same analysis yields consistency results
for Bayesian methods and BIC-type variable selection under similar conditions."@2012
Siegfried Hörmann@http://arxiv.org/abs/1208.2895v3@A note on estimation in Hilbertian linear models@"We study estimation and prediction in linear models where the response and
the regressor variable both take values in some Hilbert space. Our main
objective is to obtain consistency of a principal components based estimator
for the regression operator under minimal assumptions. In particular, we avoid
some inconvenient technical restrictions that have been used throughout the
literature. We develop our theory in a time dependent setup which comprises as
important special case the autoregressive Hilbertian model."@2012
Łukasz Kidziński@http://arxiv.org/abs/1208.2895v3@A note on estimation in Hilbertian linear models@"We study estimation and prediction in linear models where the response and
the regressor variable both take values in some Hilbert space. Our main
objective is to obtain consistency of a principal components based estimator
for the regression operator under minimal assumptions. In particular, we avoid
some inconvenient technical restrictions that have been used throughout the
literature. We develop our theory in a time dependent setup which comprises as
important special case the autoregressive Hilbertian model."@2012
Yumou Qiu@http://arxiv.org/abs/1208.3321v1@"Test for bandedness of high-dimensional covariance matrices and
  bandwidth estimation"@"Motivated by the latest effort to employ banded matrices to estimate a
high-dimensional covariance $\Sigma$, we propose a test for $\Sigma$ being
banded with possible diverging bandwidth. The test is adaptive to the ""large
$p$, small $n$"" situations without assuming a specific parametric distribution
for the data. We also formulate a consistent estimator for the bandwidth of a
banded high-dimensional covariance matrix. The properties of the test and the
bandwidth estimator are investigated by theoretical evaluations and simulation
studies, as well as an empirical analysis on a protein mass spectroscopy data."@2012
Song Xi Chen@http://arxiv.org/abs/1208.3321v1@"Test for bandedness of high-dimensional covariance matrices and
  bandwidth estimation"@"Motivated by the latest effort to employ banded matrices to estimate a
high-dimensional covariance $\Sigma$, we propose a test for $\Sigma$ being
banded with possible diverging bandwidth. The test is adaptive to the ""large
$p$, small $n$"" situations without assuming a specific parametric distribution
for the data. We also formulate a consistent estimator for the bandwidth of a
banded high-dimensional covariance matrix. The properties of the test and the
bandwidth estimator are investigated by theoretical evaluations and simulation
studies, as well as an empirical analysis on a protein mass spectroscopy data."@2012
Mathias Drton@http://arxiv.org/abs/1208.3360v1@Correction on Moments of minors of Wishart matrices@"Correction on Moments of minors of Wishart matrices by M. Drton and A. Goia
(Ann. Statist. 36 (2008) 2261-2283), arXiv:math/0604488"@2012
Aldo Goia@http://arxiv.org/abs/1208.3360v1@Correction on Moments of minors of Wishart matrices@"Correction on Moments of minors of Wishart matrices by M. Drton and A. Goia
(Ann. Statist. 36 (2008) 2261-2283), arXiv:math/0604488"@2012
Ting Zhang@http://arxiv.org/abs/1208.3552v1@Inference of time-varying regression models@"We consider parameter estimation, hypothesis testing and variable selection
for partially time-varying coefficient models. Our asymptotic theory has the
useful feature that it can allow dependent, nonstationary error and covariate
processes. With a two-stage method, the parametric component can be estimated
with a $n^{1/2}$-convergence rate. A simulation-assisted hypothesis testing
procedure is proposed for testing significance and parameter constancy. We
further propose an information criterion that can consistently select the true
set of significant predictors. Our method is applied to autoregressive models
with time-varying coefficients. Simulation results and a real data application
are provided."@2012
Wei Biao Wu@http://arxiv.org/abs/1208.3552v1@Inference of time-varying regression models@"We consider parameter estimation, hypothesis testing and variable selection
for partially time-varying coefficient models. Our asymptotic theory has the
useful feature that it can allow dependent, nonstationary error and covariate
processes. With a two-stage method, the parametric component can be estimated
with a $n^{1/2}$-convergence rate. A simulation-assisted hypothesis testing
procedure is proposed for testing significance and parameter constancy. We
further propose an information criterion that can consistently select the true
set of significant predictors. Our method is applied to autoregressive models
with time-varying coefficients. Simulation results and a real data application
are provided."@2012
Lingzhou Xue@http://arxiv.org/abs/1208.3555v1@"Nonconcave penalized composite conditional likelihood estimation of
  sparse Ising models"@"The Ising model is a useful tool for studying complex interactions within a
system. The estimation of such a model, however, is rather challenging,
especially in the presence of high-dimensional parameters. In this work, we
propose efficient procedures for learning a sparse Ising model based on a
penalized composite conditional likelihood with nonconcave penalties.
Nonconcave penalized likelihood estimation has received a lot of attention in
recent years. However, such an approach is computationally prohibitive under
high-dimensional Ising models. To overcome such difficulties, we extend the
methodology and theory of nonconcave penalized likelihood to penalized
composite conditional likelihood estimation. The proposed method can be
efficiently implemented by taking advantage of coordinate-ascent and
minorization--maximization principles. Asymptotic oracle properties of the
proposed method are established with NP-dimensionality. Optimality of the
computed local solution is discussed. We demonstrate its finite sample
performance via simulation studies and further illustrate our proposal by
studying the Human Immunodeficiency Virus type 1 protease structure based on
data from the Stanford HIV drug resistance database. Our statistical learning
results match the known biological findings very well, although no prior
biological information is used in the data analysis procedure."@2012
Hui Zou@http://arxiv.org/abs/1208.3555v1@"Nonconcave penalized composite conditional likelihood estimation of
  sparse Ising models"@"The Ising model is a useful tool for studying complex interactions within a
system. The estimation of such a model, however, is rather challenging,
especially in the presence of high-dimensional parameters. In this work, we
propose efficient procedures for learning a sparse Ising model based on a
penalized composite conditional likelihood with nonconcave penalties.
Nonconcave penalized likelihood estimation has received a lot of attention in
recent years. However, such an approach is computationally prohibitive under
high-dimensional Ising models. To overcome such difficulties, we extend the
methodology and theory of nonconcave penalized likelihood to penalized
composite conditional likelihood estimation. The proposed method can be
efficiently implemented by taking advantage of coordinate-ascent and
minorization--maximization principles. Asymptotic oracle properties of the
proposed method are established with NP-dimensionality. Optimality of the
computed local solution is discussed. We demonstrate its finite sample
performance via simulation studies and further illustrate our proposal by
studying the Human Immunodeficiency Virus type 1 protease structure based on
data from the Stanford HIV drug resistance database. Our statistical learning
results match the known biological findings very well, although no prior
biological information is used in the data analysis procedure."@2012
Tianxi Cai@http://arxiv.org/abs/1208.3555v1@"Nonconcave penalized composite conditional likelihood estimation of
  sparse Ising models"@"The Ising model is a useful tool for studying complex interactions within a
system. The estimation of such a model, however, is rather challenging,
especially in the presence of high-dimensional parameters. In this work, we
propose efficient procedures for learning a sparse Ising model based on a
penalized composite conditional likelihood with nonconcave penalties.
Nonconcave penalized likelihood estimation has received a lot of attention in
recent years. However, such an approach is computationally prohibitive under
high-dimensional Ising models. To overcome such difficulties, we extend the
methodology and theory of nonconcave penalized likelihood to penalized
composite conditional likelihood estimation. The proposed method can be
efficiently implemented by taking advantage of coordinate-ascent and
minorization--maximization principles. Asymptotic oracle properties of the
proposed method are established with NP-dimensionality. Optimality of the
computed local solution is discussed. We demonstrate its finite sample
performance via simulation studies and further illustrate our proposal by
studying the Human Immunodeficiency Virus type 1 protease structure based on
data from the Stanford HIV drug resistance database. Our statistical learning
results match the known biological findings very well, although no prior
biological information is used in the data analysis procedure."@2012
Ismaël Castillo@http://arxiv.org/abs/1208.3862v4@Nonparametric Bernstein-von Mises theorems in Gaussian white noise@"Bernstein-von Mises theorems for nonparametric Bayes priors in the Gaussian
white noise model are proved. It is demonstrated how such results justify Bayes
methods as efficient frequentist inference procedures in a variety of concrete
nonparametric problems. Particularly Bayesian credible sets are constructed
that have asymptotically exact $1-\alpha$ frequentist coverage level and whose
$L^2$-diameter shrinks at the minimax rate of convergence (within logarithmic
factors) over H\""{o}lder balls. Other applications include general classes of
linear and nonlinear functionals and credible bands for auto-convolutions. The
assumptions cover nonconjugate product priors defined on general orthonormal
bases of $L^2$ satisfying weak conditions."@2012
Richard Nickl@http://arxiv.org/abs/1208.3862v4@Nonparametric Bernstein-von Mises theorems in Gaussian white noise@"Bernstein-von Mises theorems for nonparametric Bayes priors in the Gaussian
white noise model are proved. It is demonstrated how such results justify Bayes
methods as efficient frequentist inference procedures in a variety of concrete
nonparametric problems. Particularly Bayesian credible sets are constructed
that have asymptotically exact $1-\alpha$ frequentist coverage level and whose
$L^2$-diameter shrinks at the minimax rate of convergence (within logarithmic
factors) over H\""{o}lder balls. Other applications include general classes of
linear and nonlinear functionals and credible bands for auto-convolutions. The
assumptions cover nonconjugate product priors defined on general orthonormal
bases of $L^2$ satisfying weak conditions."@2012
Takuma Yoshida@http://arxiv.org/abs/1208.3920v1@Asymptotics for penalized splines in generalized additive models@"This paper discusses asymptotic theory for penalized spline estimators in
generalized additive models. The purpose of this paper is to establish the
asymptotic bias and variance as well as the asymptotic normality of the
penalized spline estimators proposed by Marx and Eilers (1998). Furthermore,
the asymptotics for the penalized quasi likelihood fit in mixed models are also
discussed."@2012
Kanta Naito@http://arxiv.org/abs/1208.3920v1@Asymptotics for penalized splines in generalized additive models@"This paper discusses asymptotic theory for penalized spline estimators in
generalized additive models. The purpose of this paper is to establish the
asymptotic bias and variance as well as the asymptotic normality of the
penalized spline estimators proposed by Marx and Eilers (1998). Furthermore,
the asymptotics for the penalized quasi likelihood fit in mixed models are also
discussed."@2012
Xin Gao@http://arxiv.org/abs/1208.4379v1@"Simultaneous Model Selection and Estimation for Mean and Association
  Structures with Clustered Binary Data"@"This paper investigates the property of the penalized estimating equations
when both the mean and association structures are modelled. To select variables
for the mean and association structures sequentially, we propose a hierarchical
penalized generalized estimating equations (HPGEE2) approach. The first set of
penalized estimating equations is solved for the selection of significant mean
parameters. Conditional on the selected mean model, the second set of penalized
estimating equations is solved for the selection of significant association
parameters. The hierarchical approach is designed to accommodate possible model
constraints relating the inclusion of covariates into the mean and the
association models. This two-step penalization strategy enjoys a compelling
advantage of easing computational burdens compared to solving the two sets of
penalized equations simultaneously. HPGEE2 with a smoothly clipped absolute
deviation (SCAD) penalty is shown to have the oracle property for the mean and
association models. The asymptotic behavior of the penalized estimator under
this hierarchical approach is established. An efficient two-stage penalized
weighted least square algorithm is developed to implement the proposed method.
The empirical performance of the proposed HPGEE2 is demonstrated through
Monte-Carlo studies and the analysis of a clinical data set."@2012
Grace Y. Yi@http://arxiv.org/abs/1208.4379v1@"Simultaneous Model Selection and Estimation for Mean and Association
  Structures with Clustered Binary Data"@"This paper investigates the property of the penalized estimating equations
when both the mean and association structures are modelled. To select variables
for the mean and association structures sequentially, we propose a hierarchical
penalized generalized estimating equations (HPGEE2) approach. The first set of
penalized estimating equations is solved for the selection of significant mean
parameters. Conditional on the selected mean model, the second set of penalized
estimating equations is solved for the selection of significant association
parameters. The hierarchical approach is designed to accommodate possible model
constraints relating the inclusion of covariates into the mean and the
association models. This two-step penalization strategy enjoys a compelling
advantage of easing computational burdens compared to solving the two sets of
penalized equations simultaneously. HPGEE2 with a smoothly clipped absolute
deviation (SCAD) penalty is shown to have the oracle property for the mean and
association models. The asymptotic behavior of the penalized estimator under
this hierarchical approach is established. An efficient two-stage penalized
weighted least square algorithm is developed to implement the proposed method.
The empirical performance of the proposed HPGEE2 is demonstrated through
Monte-Carlo studies and the analysis of a clinical data set."@2012
Chun-Yang Wang@http://arxiv.org/abs/1208.4892v1@"Critical Properties of $S^{4}$ System Restudied via Generalized
  Migdal-Kadanoff Bond-moving Renormalization"@"We study the critical properties of the spin-continuous $S^{4}$ system on the
typical translational invariant triangular lattices by combining the
recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures
with the cumulative expansion technique. In three different cases of
nearest-neighbor, next nearest neighbor and external field we obtain the
critical points and further calculate the critical exponents according to the
scaling theory. In all case it is found that there exists three fixed points.
The correlation length critical exponents obtained near the Wilson-Fisher fixed
points are found getting smaller and smaller with the increasing of the system
complexity. Others are found similar to the results of the classical Gaussian
model and different from those of the Ising system."@2012
Wen-Xian Yang@http://arxiv.org/abs/1208.4892v1@"Critical Properties of $S^{4}$ System Restudied via Generalized
  Migdal-Kadanoff Bond-moving Renormalization"@"We study the critical properties of the spin-continuous $S^{4}$ system on the
typical translational invariant triangular lattices by combining the
recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures
with the cumulative expansion technique. In three different cases of
nearest-neighbor, next nearest neighbor and external field we obtain the
critical points and further calculate the critical exponents according to the
scaling theory. In all case it is found that there exists three fixed points.
The correlation length critical exponents obtained near the Wilson-Fisher fixed
points are found getting smaller and smaller with the increasing of the system
complexity. Others are found similar to the results of the classical Gaussian
model and different from those of the Ising system."@2012
Hong Du@http://arxiv.org/abs/1208.4892v1@"Critical Properties of $S^{4}$ System Restudied via Generalized
  Migdal-Kadanoff Bond-moving Renormalization"@"We study the critical properties of the spin-continuous $S^{4}$ system on the
typical translational invariant triangular lattices by combining the
recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures
with the cumulative expansion technique. In three different cases of
nearest-neighbor, next nearest neighbor and external field we obtain the
critical points and further calculate the critical exponents according to the
scaling theory. In all case it is found that there exists three fixed points.
The correlation length critical exponents obtained near the Wilson-Fisher fixed
points are found getting smaller and smaller with the increasing of the system
complexity. Others are found similar to the results of the classical Gaussian
model and different from those of the Ising system."@2012
Vladimir Spokoiny@http://arxiv.org/abs/1208.5384v2@Local Quantile Regression@"Quantile regression is a technique to estimate conditional quantile curves.
It provides a comprehensive picture of a response contingent on explanatory
variables. In a flexible modeling framework, a specific form of the conditional
quantile curve is not a priori fixed. % Indeed, the majority of applications do
not per se require specific functional forms. This motivates a local parametric
rather than a global fixed model fitting approach. A nonparametric smoothing
estimator of the conditional quantile curve requires to balance between local
curvature and stochastic variability. In this paper, we suggest a local model
selection technique that provides an adaptive estimator of the conditional
quantile regression curve at each design point. Theoretical results claim that
the proposed adaptive procedure performs as good as an oracle which would
minimize the local estimation risk for the problem at hand. We illustrate the
performance of the procedure by an extensive simulation study and consider a
couple of applications: to tail dependence analysis for the Hong Kong stock
market and to analysis of the distributions of the risk factors of temperature
dynamics."@2012
Weining Wang@http://arxiv.org/abs/1208.5384v2@Local Quantile Regression@"Quantile regression is a technique to estimate conditional quantile curves.
It provides a comprehensive picture of a response contingent on explanatory
variables. In a flexible modeling framework, a specific form of the conditional
quantile curve is not a priori fixed. % Indeed, the majority of applications do
not per se require specific functional forms. This motivates a local parametric
rather than a global fixed model fitting approach. A nonparametric smoothing
estimator of the conditional quantile curve requires to balance between local
curvature and stochastic variability. In this paper, we suggest a local model
selection technique that provides an adaptive estimator of the conditional
quantile regression curve at each design point. Theoretical results claim that
the proposed adaptive procedure performs as good as an oracle which would
minimize the local estimation risk for the problem at hand. We illustrate the
performance of the procedure by an extensive simulation study and consider a
couple of applications: to tail dependence analysis for the Hong Kong stock
market and to analysis of the distributions of the risk factors of temperature
dynamics."@2012
Wolfgang Karl Härdle@http://arxiv.org/abs/1208.5384v2@Local Quantile Regression@"Quantile regression is a technique to estimate conditional quantile curves.
It provides a comprehensive picture of a response contingent on explanatory
variables. In a flexible modeling framework, a specific form of the conditional
quantile curve is not a priori fixed. % Indeed, the majority of applications do
not per se require specific functional forms. This motivates a local parametric
rather than a global fixed model fitting approach. A nonparametric smoothing
estimator of the conditional quantile curve requires to balance between local
curvature and stochastic variability. In this paper, we suggest a local model
selection technique that provides an adaptive estimator of the conditional
quantile regression curve at each design point. Theoretical results claim that
the proposed adaptive procedure performs as good as an oracle which would
minimize the local estimation risk for the problem at hand. We illustrate the
performance of the procedure by an extensive simulation study and consider a
couple of applications: to tail dependence analysis for the Hong Kong stock
market and to analysis of the distributions of the risk factors of temperature
dynamics."@2012
Till Sabel@http://arxiv.org/abs/1208.5501v3@"Asymptotically efficient estimation of a scale parameter in Gaussian
  time series and closed-form expressions for the Fisher information"@"Mimicking the maximum likelihood estimator, we construct first order
Cramer-Rao efficient and explicitly computable estimators for the scale
parameter $\sigma^2$ in the model $Z_{i,n}=\sigma
n^{-\beta}X_i+Y_i,i=1,\ldots,n,\beta>0$ with independent, stationary Gaussian
processes $(X_i)_{i\in\mathbb{N}}$, $(Y_i)_{i\in\mathbb{N}}$, and
$(X_i)_{i\in\mathbb{N}}$ exhibits possibly long-range dependence. In a second
part, closed-form expressions for the asymptotic behavior of the corresponding
Fisher information are derived. Our main finding is that depending on the
behavior of the spectral densities at zero, the Fisher information has
asymptotically two different scaling regimes, which are separated by a sharp
phase transition. The most prominent example included in our analysis is the
Fisher information for the scaling factor of a high-frequency sample of
fractional Brownian motion under additive noise."@2012
Johannes Schmidt-Hieber@http://arxiv.org/abs/1208.5501v3@"Asymptotically efficient estimation of a scale parameter in Gaussian
  time series and closed-form expressions for the Fisher information"@"Mimicking the maximum likelihood estimator, we construct first order
Cramer-Rao efficient and explicitly computable estimators for the scale
parameter $\sigma^2$ in the model $Z_{i,n}=\sigma
n^{-\beta}X_i+Y_i,i=1,\ldots,n,\beta>0$ with independent, stationary Gaussian
processes $(X_i)_{i\in\mathbb{N}}$, $(Y_i)_{i\in\mathbb{N}}$, and
$(X_i)_{i\in\mathbb{N}}$ exhibits possibly long-range dependence. In a second
part, closed-form expressions for the asymptotic behavior of the corresponding
Fisher information are derived. Our main finding is that depending on the
behavior of the spectral densities at zero, the Fisher information has
asymptotically two different scaling regimes, which are separated by a sharp
phase transition. The most prominent example included in our analysis is the
Fisher information for the scaling factor of a high-frequency sample of
fractional Brownian motion under additive noise."@2012
Jinzhu Jia@http://arxiv.org/abs/1208.5584v1@Preconditioning to comply with the Irrepresentable Condition@"Preconditioning is a technique from numerical linear algebra that can
accelerate algorithms to solve systems of equations. In this paper, we
demonstrate how preconditioning can circumvent a stringent assumption for sign
consistency in sparse linear regression. Given $X \in R^{n \times p}$ and $Y
\in R^n$ that satisfy the standard regression equation, this paper demonstrates
that even if the design matrix $X$ does not satisfy the irrepresentable
condition for the Lasso, the design matrix $F X$ often does, where $F \in
R^{n\times n}$ is a preconditioning matrix defined in this paper. By computing
the Lasso on $(F X, F Y)$, instead of on $(X, Y)$, the necessary assumptions on
$X$ become much less stringent.
  Our preconditioner $F$ ensures that the singular values of the design matrix
are either zero or one. When $n\ge p$, the columns of $F X$ are orthogonal and
the preconditioner always circumvents the stringent assumptions. When $p\ge n$,
$F$ projects the design matrix onto the Stiefel manifold; the rows of $F X$ are
orthogonal. We give both theoretical results and simulation results to show
that, in the high dimensional case, the preconditioner helps to circumvent the
stringent assumptions, improving the statistical performance of a broad class
of model selection techniques in linear regression. Simulation results are
particularly promising."@2012
Karl Rohe@http://arxiv.org/abs/1208.5584v1@Preconditioning to comply with the Irrepresentable Condition@"Preconditioning is a technique from numerical linear algebra that can
accelerate algorithms to solve systems of equations. In this paper, we
demonstrate how preconditioning can circumvent a stringent assumption for sign
consistency in sparse linear regression. Given $X \in R^{n \times p}$ and $Y
\in R^n$ that satisfy the standard regression equation, this paper demonstrates
that even if the design matrix $X$ does not satisfy the irrepresentable
condition for the Lasso, the design matrix $F X$ often does, where $F \in
R^{n\times n}$ is a preconditioning matrix defined in this paper. By computing
the Lasso on $(F X, F Y)$, instead of on $(X, Y)$, the necessary assumptions on
$X$ become much less stringent.
  Our preconditioner $F$ ensures that the singular values of the design matrix
are either zero or one. When $n\ge p$, the columns of $F X$ are orthogonal and
the preconditioner always circumvents the stringent assumptions. When $p\ge n$,
$F$ projects the design matrix onto the Stiefel manifold; the rows of $F X$ are
orthogonal. We give both theoretical results and simulation results to show
that, in the high dimensional case, the preconditioner helps to circumvent the
stringent assumptions, improving the statistical performance of a broad class
of model selection techniques in linear regression. Simulation results are
particularly promising."@2012
Stéphane Girard@http://arxiv.org/abs/1208.6378v1@A note on extreme values and kernel estimators of sample boundaries@"In a previous paper, we studied a kernel estimate of the upper edge of a
two-dimensional bounded set, based upon the extreme values of a Poisson point
process. The initial paper ""Geffroy J. (1964) Sur un probl\`eme d'estimation
g\'eom\'etrique.Publications de l'Institut de Statistique de l'Universit\'e de
Paris, XIII, 191-200"" on the subject treats the frontier as the boundary of the
support set for a density and the points as a random sample. We claimed
in""Girard, S. and Jacob, P. (2004) Extreme values and kernel estimates of point
processes boundaries.ESAIM: Probability and Statistics, 8, 150-168"" that we are
able to deduce the random sample case fr om the point process case. The present
note gives some essential indications to this end, including a method which can
be of general interest."@2012
Pierre Jacob@http://arxiv.org/abs/1208.6378v1@A note on extreme values and kernel estimators of sample boundaries@"In a previous paper, we studied a kernel estimate of the upper edge of a
two-dimensional bounded set, based upon the extreme values of a Poisson point
process. The initial paper ""Geffroy J. (1964) Sur un probl\`eme d'estimation
g\'eom\'etrique.Publications de l'Institut de Statistique de l'Universit\'e de
Paris, XIII, 191-200"" on the subject treats the frontier as the boundary of the
support set for a density and the points as a random sample. We claimed
in""Girard, S. and Jacob, P. (2004) Extreme values and kernel estimates of point
processes boundaries.ESAIM: Probability and Statistics, 8, 150-168"" that we are
able to deduce the random sample case fr om the point process case. The present
note gives some essential indications to this end, including a method which can
be of general interest."@2012
Arnak Dalalyan@http://arxiv.org/abs/1208.6402v3@Statistical inference in compound functional models@"We consider a general nonparametric regression model called the compound
model. It includes, as special cases, sparse additive regression and
nonparametric (or linear) regression with many covariates but possibly a small
number of relevant covariates. The compound model is characterized by three
main parameters: the structure parameter describing the ""macroscopic"" form of
the compound function, the ""microscopic"" sparsity parameter indicating the
maximal number of relevant covariates in each component and the usual
smoothness parameter corresponding to the complexity of the members of the
compound. We find non-asymptotic minimax rate of convergence of estimators in
such a model as a function of these three parameters. We also show that this
rate can be attained in an adaptive way."@2012
Yuri Ingster@http://arxiv.org/abs/1208.6402v3@Statistical inference in compound functional models@"We consider a general nonparametric regression model called the compound
model. It includes, as special cases, sparse additive regression and
nonparametric (or linear) regression with many covariates but possibly a small
number of relevant covariates. The compound model is characterized by three
main parameters: the structure parameter describing the ""macroscopic"" form of
the compound function, the ""microscopic"" sparsity parameter indicating the
maximal number of relevant covariates in each component and the usual
smoothness parameter corresponding to the complexity of the members of the
compound. We find non-asymptotic minimax rate of convergence of estimators in
such a model as a function of these three parameters. We also show that this
rate can be attained in an adaptive way."@2012
Alexandre Tsybakov@http://arxiv.org/abs/1208.6402v3@Statistical inference in compound functional models@"We consider a general nonparametric regression model called the compound
model. It includes, as special cases, sparse additive regression and
nonparametric (or linear) regression with many covariates but possibly a small
number of relevant covariates. The compound model is characterized by three
main parameters: the structure parameter describing the ""macroscopic"" form of
the compound function, the ""microscopic"" sparsity parameter indicating the
maximal number of relevant covariates in each component and the usual
smoothness parameter corresponding to the complexity of the members of the
compound. We find non-asymptotic minimax rate of convergence of estimators in
such a model as a function of these three parameters. We also show that this
rate can be attained in an adaptive way."@2012
Michal Demetrian@http://arxiv.org/abs/1208.6581v1@On computation of clustering coefficient in a class of random networks@"The random networks enriched with additional structures as metric and
group-symmetry in background metric space are investigated. The important
quantities like he clustering coefficient as well as the mean degree of
separation in such networks are effectively computed with help of additional
structures. Representative models are discussed in details."@2012
Martin Nehez@http://arxiv.org/abs/1208.6581v1@On computation of clustering coefficient in a class of random networks@"The random networks enriched with additional structures as metric and
group-symmetry in background metric space are investigated. The important
quantities like he clustering coefficient as well as the mean degree of
separation in such networks are effectively computed with help of additional
structures. Representative models are discussed in details."@2012
Thierry Dumont@http://arxiv.org/abs/1209.0633v4@"Nonparametric regression on hidden phi-mixing variables: identifiability
  and consistency of a pseudo-likelihood based estimation procedure"@"This paper outlines a new nonparametric estimation procedure for unobserved
phi-mixing processes. It is assumed that the only information on the stationary
hidden states (Xk) is given by the process (Yk), where Yk is a noisy
observation of f(Xk). The paper introduces a maximum pseudo-likelihood
procedure to estimate the function f and the distribution of the hidden states
using blocks of observations of length b. The identifiability of the model is
studied in the particular cases b=1 and b=2. The consistency of the estimators
of f and of the distribution of the hidden states as the number of observations
grows to infinity is established."@2012
Sylvain Le Corff@http://arxiv.org/abs/1209.0633v4@"Nonparametric regression on hidden phi-mixing variables: identifiability
  and consistency of a pseudo-likelihood based estimation procedure"@"This paper outlines a new nonparametric estimation procedure for unobserved
phi-mixing processes. It is assumed that the only information on the stationary
hidden states (Xk) is given by the process (Yk), where Yk is a noisy
observation of f(Xk). The paper introduces a maximum pseudo-likelihood
procedure to estimate the function f and the distribution of the hidden states
using blocks of observations of length b. The identifiability of the model is
studied in the particular cases b=1 and b=2. The consistency of the estimators
of f and of the distribution of the hidden states as the number of observations
grows to infinity is established."@2012
Nina Huber@http://arxiv.org/abs/1209.0899v2@"Shrinkage estimators for prediction out-of-sample: Conditional
  performance"@"We find that, in a linear model, the James-Stein estimator, which dominates
the maximum-likelihood estimator in terms of its in-sample prediction error,
can perform poorly compared to the maximum-likelihood estimator in
out-of-sample prediction. We give a detailed analysis of this phenomenon and
discuss its implications. When evaluating the predictive performance of
estimators, we treat the regressor matrix in the training data as fixed, i.e.,
we condition on the design variables. Our findings contrast those obtained by
Baranchik (1973, Ann. Stat. 1:312-321) and, more recently, by Dicker (2012,
arXiv:1102.2952) in an unconditional performance evaluation."@2012
Hannes Leeb@http://arxiv.org/abs/1209.0899v2@"Shrinkage estimators for prediction out-of-sample: Conditional
  performance"@"We find that, in a linear model, the James-Stein estimator, which dominates
the maximum-likelihood estimator in terms of its in-sample prediction error,
can perform poorly compared to the maximum-likelihood estimator in
out-of-sample prediction. We give a detailed analysis of this phenomenon and
discuss its implications. When evaluating the predictive performance of
estimators, we treat the regressor matrix in the training data as fixed, i.e.,
we condition on the design variables. Our findings contrast those obtained by
Baranchik (1973, Ann. Stat. 1:312-321) and, more recently, by Dicker (2012,
arXiv:1102.2952) in an unconditional performance evaluation."@2012
Ahmed Bensalma@http://arxiv.org/abs/1209.1031v3@"Testing the Fractional Integration Parameter Revisited: a Fractional
  Dickey-Fuller Test"@"In this paper, in the first step, we show that the fractional Dickey-Fuller
test proposed by Dolado et al [10] is useless in practice. In the second step,
we propose a new testing procedure for the degree of fractional integration of
a time series inspired on the unit root test of Dickey-Fuller [7]. Through a
simulation study, we show the good performance of the test in terms of size and
power. Finally, in order to show how to use the new testing procedure, the test
is applied to the well-known Nelson and Plosser data."@2012
Mohamed Bentarzi@http://arxiv.org/abs/1209.1031v3@"Testing the Fractional Integration Parameter Revisited: a Fractional
  Dickey-Fuller Test"@"In this paper, in the first step, we show that the fractional Dickey-Fuller
test proposed by Dolado et al [10] is useless in practice. In the second step,
we propose a new testing procedure for the degree of fractional integration of
a time series inspired on the unit root test of Dickey-Fuller [7]. Through a
simulation study, we show the good performance of the test in terms of size and
power. Finally, in order to show how to use the new testing procedure, the test
is applied to the well-known Nelson and Plosser data."@2012
László Varga@http://arxiv.org/abs/1209.1302v1@Weighted bootstrap in GARCH models@"GARCH models are useful tools in the investigation of phenomena, where
volatility changes are prominent features, like most financial data. The
parameter estimation via quasi maximum likelihood (QMLE) and its properties are
by now well understood. However, there is a gap between practical applications
and the theory, as in reality there are usually not enough observations for the
limit results to be valid approximations. We try to fill this gap by this
paper, where the properties of a recent bootstrap methodology in the context of
GARCH modeling are revealed. The results are promising as it turns out that
this remarkably simple method has essentially the same limit distribution, as
the original estimatorwith the advantage of easy confidence interval
construction, as it is demonstrated in the paper.
  The finite-sample properties of the suggested estimators are investigated
through a simulation study, which ensures that the results are practically
applicable for sample sizes as low as a thousand. On the other hand, the
results are not 100% accurate until sample size reaches 100 thousands - but it
is shown that this property is not a feature of our bootstrap procedure only,
as it is shared by the original QMLE, too."@2012
András Zempléni@http://arxiv.org/abs/1209.1302v1@Weighted bootstrap in GARCH models@"GARCH models are useful tools in the investigation of phenomena, where
volatility changes are prominent features, like most financial data. The
parameter estimation via quasi maximum likelihood (QMLE) and its properties are
by now well understood. However, there is a gap between practical applications
and the theory, as in reality there are usually not enough observations for the
limit results to be valid approximations. We try to fill this gap by this
paper, where the properties of a recent bootstrap methodology in the context of
GARCH modeling are revealed. The results are promising as it turns out that
this remarkably simple method has essentially the same limit distribution, as
the original estimatorwith the advantage of easy confidence interval
construction, as it is demonstrated in the paper.
  The finite-sample properties of the suggested estimators are investigated
through a simulation study, which ensures that the results are practically
applicable for sample sizes as low as a thousand. On the other hand, the
results are not 100% accurate until sample size reaches 100 thousands - but it
is shown that this property is not a feature of our bootstrap procedure only,
as it is shared by the original QMLE, too."@2012
Richard Nickl@http://arxiv.org/abs/1209.1508v4@Confidence sets in sparse regression@"The problem of constructing confidence sets in the high-dimensional linear
model with $n$ response variables and $p$ parameters, possibly $p\ge n$, is
considered. Full honest adaptive inference is possible if the rate of sparse
estimation does not exceed $n^{-1/4}$, otherwise sparse adaptive confidence
sets exist only over strict subsets of the parameter spaces for which sparse
estimators exist. Necessary and sufficient conditions for the existence of
confidence sets that adapt to a fixed sparsity level of the parameter vector
are given in terms of minimal $\ell^2$-separation conditions on the parameter
space. The design conditions cover common coherence assumptions used in models
for sparsity, including (possibly correlated) sub-Gaussian designs."@2012
Sara van de Geer@http://arxiv.org/abs/1209.1508v4@Confidence sets in sparse regression@"The problem of constructing confidence sets in the high-dimensional linear
model with $n$ response variables and $p$ parameters, possibly $p\ge n$, is
considered. Full honest adaptive inference is possible if the rate of sparse
estimation does not exceed $n^{-1/4}$, otherwise sparse adaptive confidence
sets exist only over strict subsets of the parameter spaces for which sparse
estimators exist. Necessary and sufficient conditions for the existence of
confidence sets that adapt to a fixed sparsity level of the parameter vector
are given in terms of minimal $\ell^2$-separation conditions on the parameter
space. The design conditions cover common coherence assumptions used in models
for sparsity, including (possibly correlated) sub-Gaussian designs."@2012
Valentin Patilea@http://arxiv.org/abs/1209.2085v2@"Nonparametric testing for no-effect with functional responses and
  functional covariates"@"This paper examines the problem of nonparametric testing for the no-effect of
a random covariate (or predictor) on a functional response. This means testing
whether the conditional expectation of the response given the covariate is
almost surely zero or not, without imposing any model relating response and
covariate. The covariate could be univariate, multivariate or functional. Our
test statistic is a quadratic form involving univariate nearest neighbor
smoothing and the asymptotic critical values are given by the standard normal
law. When the covariate is multidimensional or functional, a preliminary
dimension reduction device is used which allows the effect of the covariate to
be summarized into a univariate random quantity. The test is able to detect not
only linear but nonparametric alternatives. The responses could have
conditional variance of unknown form and the law of the covariate does not need
to be known. An empirical study with simulated and real data shows that the
test performs well in applications."@2012
Cesar Sanchez-Sellero@http://arxiv.org/abs/1209.2085v2@"Nonparametric testing for no-effect with functional responses and
  functional covariates"@"This paper examines the problem of nonparametric testing for the no-effect of
a random covariate (or predictor) on a functional response. This means testing
whether the conditional expectation of the response given the covariate is
almost surely zero or not, without imposing any model relating response and
covariate. The covariate could be univariate, multivariate or functional. Our
test statistic is a quadratic form involving univariate nearest neighbor
smoothing and the asymptotic critical values are given by the standard normal
law. When the covariate is multidimensional or functional, a preliminary
dimension reduction device is used which allows the effect of the covariate to
be summarized into a univariate random quantity. The test is able to detect not
only linear but nonparametric alternatives. The responses could have
conditional variance of unknown form and the law of the covariate does not need
to be known. An empirical study with simulated and real data shows that the
test performs well in applications."@2012
Matthieu Saumard@http://arxiv.org/abs/1209.2085v2@"Nonparametric testing for no-effect with functional responses and
  functional covariates"@"This paper examines the problem of nonparametric testing for the no-effect of
a random covariate (or predictor) on a functional response. This means testing
whether the conditional expectation of the response given the covariate is
almost surely zero or not, without imposing any model relating response and
covariate. The covariate could be univariate, multivariate or functional. Our
test statistic is a quadratic form involving univariate nearest neighbor
smoothing and the asymptotic critical values are given by the standard normal
law. When the covariate is multidimensional or functional, a preliminary
dimension reduction device is used which allows the effect of the covariate to
be summarized into a univariate random quantity. The test is able to detect not
only linear but nonparametric alternatives. The responses could have
conditional variance of unknown form and the law of the covariate does not need
to be known. An empirical study with simulated and real data shows that the
test performs well in applications."@2012
David Källberg@http://arxiv.org/abs/1209.2544v4@Estimation of entropy-type integral functionals@"Entropy-type integral functionals of densities are widely used in
mathematical statistics, information theory, and computer science. Examples
include measures of closeness between distributions (e.g., density power
divergence) and uncertainty characteristics for a random variable (e.g.,
R\'enyi entropy). In this paper, we study U-statistic estimators for a class of
such functionals. The estimators are based on epsilon-close vector observations
in the corresponding independent and identically distributed samples. We prove
asymptotic properties of the estimators (consistency and asymptotic normality)
under mild integrability and smoothness conditions for the densities. The
results can be applied in diverse problems in mathematical statistics and
computer science (e.g., distribution identification problems, approximate
matching for random databases, two-sample problems)."@2012
Oleg Seleznjev@http://arxiv.org/abs/1209.2544v4@Estimation of entropy-type integral functionals@"Entropy-type integral functionals of densities are widely used in
mathematical statistics, information theory, and computer science. Examples
include measures of closeness between distributions (e.g., density power
divergence) and uncertainty characteristics for a random variable (e.g.,
R\'enyi entropy). In this paper, we study U-statistic estimators for a class of
such functionals. The estimators are based on epsilon-close vector observations
in the corresponding independent and identically distributed samples. We prove
asymptotic properties of the estimators (consistency and asymptotic normality)
under mild integrability and smoothness conditions for the densities. The
results can be applied in diverse problems in mathematical statistics and
computer science (e.g., distribution identification problems, approximate
matching for random databases, two-sample problems)."@2012
B. T. Knapik@http://arxiv.org/abs/1209.3628v2@"Bayes procedures for adaptive inference in inverse problems for the
  white noise model"@"We study empirical and hierarchical Bayes approaches to the problem of
estimating an infinite-dimensional parameter in mildly ill-posed inverse
problems. We consider a class of prior distributions indexed by a
hyperparameter that quantifies regularity. We prove that both methods we
consider succeed in automatically selecting this parameter optimally, resulting
in optimal convergence rates for truths with Sobolev or analytic ""smoothness"",
without using knowledge about this regularity. Both methods are illustrated by
simulation examples."@2012
B. T. Szabó@http://arxiv.org/abs/1209.3628v2@"Bayes procedures for adaptive inference in inverse problems for the
  white noise model"@"We study empirical and hierarchical Bayes approaches to the problem of
estimating an infinite-dimensional parameter in mildly ill-posed inverse
problems. We consider a class of prior distributions indexed by a
hyperparameter that quantifies regularity. We prove that both methods we
consider succeed in automatically selecting this parameter optimally, resulting
in optimal convergence rates for truths with Sobolev or analytic ""smoothness"",
without using knowledge about this regularity. Both methods are illustrated by
simulation examples."@2012
A. W. van der Vaart@http://arxiv.org/abs/1209.3628v2@"Bayes procedures for adaptive inference in inverse problems for the
  white noise model"@"We study empirical and hierarchical Bayes approaches to the problem of
estimating an infinite-dimensional parameter in mildly ill-posed inverse
problems. We consider a class of prior distributions indexed by a
hyperparameter that quantifies regularity. We prove that both methods we
consider succeed in automatically selecting this parameter optimally, resulting
in optimal convergence rates for truths with Sobolev or analytic ""smoothness"",
without using knowledge about this regularity. Both methods are illustrated by
simulation examples."@2012
J. H. van Zanten@http://arxiv.org/abs/1209.3628v2@"Bayes procedures for adaptive inference in inverse problems for the
  white noise model"@"We study empirical and hierarchical Bayes approaches to the problem of
estimating an infinite-dimensional parameter in mildly ill-posed inverse
problems. We consider a class of prior distributions indexed by a
hyperparameter that quantifies regularity. We prove that both methods we
consider succeed in automatically selecting this parameter optimally, resulting
in optimal convergence rates for truths with Sobolev or analytic ""smoothness"",
without using knowledge about this regularity. Both methods are illustrated by
simulation examples."@2012
Yunwei Cui@http://arxiv.org/abs/1209.4013v1@Diagnostic Tests for Non-causal Time Series with Infinite Variance@"We study goodness-of-fit testing for non-causal autoregressive time series
with non-Gaussian stable noise. To model time series exhibiting sharp spikes or
occasional bursts of outlying observations, the exponent of the non-Gaussian
stable variables is assumed to be less than two. Under such conditions, the
innovation variables have no finite second moment. We proved that the sample
autocorrelation functions of the trimmed residuals are asymptotically normal.
Nonparametric tests are also investigated. The rank correlations of the
residuals or the squared residuals are shown to be asymptotically normal. Thus,
an assortment of portmanteau statistics are available for model assessment."@2012
Rongning Wu@http://arxiv.org/abs/1209.4013v1@Diagnostic Tests for Non-causal Time Series with Infinite Variance@"We study goodness-of-fit testing for non-causal autoregressive time series
with non-Gaussian stable noise. To model time series exhibiting sharp spikes or
occasional bursts of outlying observations, the exponent of the non-Gaussian
stable variables is assumed to be less than two. Under such conditions, the
innovation variables have no finite second moment. We proved that the sample
autocorrelation functions of the trimmed residuals are asymptotically normal.
Nonparametric tests are also investigated. The rank correlations of the
residuals or the squared residuals are shown to be asymptotically normal. Thus,
an assortment of portmanteau statistics are available for model assessment."@2012
Thomas J. Fisher@http://arxiv.org/abs/1209.4013v1@Diagnostic Tests for Non-causal Time Series with Infinite Variance@"We study goodness-of-fit testing for non-causal autoregressive time series
with non-Gaussian stable noise. To model time series exhibiting sharp spikes or
occasional bursts of outlying observations, the exponent of the non-Gaussian
stable variables is assumed to be less than two. Under such conditions, the
innovation variables have no finite second moment. We proved that the sample
autocorrelation functions of the trimmed residuals are asymptotically normal.
Nonparametric tests are also investigated. The rank correlations of the
residuals or the squared residuals are shown to be asymptotically normal. Thus,
an assortment of portmanteau statistics are available for model assessment."@2012
Miklos Csorgo@http://arxiv.org/abs/1209.4089v4@Another look at Bootstrapping the Student t-statistic@"Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E
X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative
random variables (weights), independent of the data sequence
${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $
X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from
re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq
1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define
$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,
$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put
$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,
$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the
bootstrap sample variance. The main aim of this exposition is to study the
asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=
(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=
\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of
conditioning on the weights via assuming that, as $n,m_n\to \infty$,
$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost
surely or in probability on the probability space of the weights. This view of
justifying the validity of the bootstrap is believed to be new. The need for it
arises naturally in practice when exploring the nature of information contained
in a random sample via re-sampling, for example. Conditioning on the data is
also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as
$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval
$(\lambda_1,\lambda_2)$ with 0< \lambda_1 < \lambda_2 < \infty as in Mason and
Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches
to conditioning is established."@2012
Yuliya Martsynyuk@http://arxiv.org/abs/1209.4089v4@Another look at Bootstrapping the Student t-statistic@"Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E
X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative
random variables (weights), independent of the data sequence
${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $
X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from
re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq
1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define
$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,
$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put
$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,
$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the
bootstrap sample variance. The main aim of this exposition is to study the
asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=
(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=
\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of
conditioning on the weights via assuming that, as $n,m_n\to \infty$,
$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost
surely or in probability on the probability space of the weights. This view of
justifying the validity of the bootstrap is believed to be new. The need for it
arises naturally in practice when exploring the nature of information contained
in a random sample via re-sampling, for example. Conditioning on the data is
also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as
$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval
$(\lambda_1,\lambda_2)$ with 0< \lambda_1 < \lambda_2 < \infty as in Mason and
Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches
to conditioning is established."@2012
Masoud Nasari@http://arxiv.org/abs/1209.4089v4@Another look at Bootstrapping the Student t-statistic@"Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E
X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative
random variables (weights), independent of the data sequence
${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $
X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from
re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq
1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define
$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,
$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put
$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,
$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the
bootstrap sample variance. The main aim of this exposition is to study the
asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=
(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=
\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of
conditioning on the weights via assuming that, as $n,m_n\to \infty$,
$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost
surely or in probability on the probability space of the weights. This view of
justifying the validity of the bootstrap is believed to be new. The need for it
arises naturally in practice when exploring the nature of information contained
in a random sample via re-sampling, for example. Conditioning on the data is
also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as
$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval
$(\lambda_1,\lambda_2)$ with 0< \lambda_1 < \lambda_2 < \infty as in Mason and
Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches
to conditioning is established."@2012
Jean Jacod@http://arxiv.org/abs/1209.4173v2@"A remark on the rates of convergence for integrated volatility
  estimation in the presence of jumps"@"The optimal rate of convergence of estimators of the integrated volatility,
for a discontinuous It\^{o} semimartingale sampled at regularly spaced times
and over a fixed time interval, has been a long-standing problem, at least when
the jumps are not summable. In this paper, we study this optimal rate, in the
minimax sense and for appropriate ""bounded"" nonparametric classes of
semimartingales. We show that, if the $r$th powers of the jumps are summable
for some $r\in[0,2)$, the minimax rate is equal to $\min(\sqrt{n},(n\log
n)^{(2-r)/2})$, where $n$ is the number of observations."@2012
Markus Reiss@http://arxiv.org/abs/1209.4173v2@"A remark on the rates of convergence for integrated volatility
  estimation in the presence of jumps"@"The optimal rate of convergence of estimators of the integrated volatility,
for a discontinuous It\^{o} semimartingale sampled at regularly spaced times
and over a fixed time interval, has been a long-standing problem, at least when
the jumps are not summable. In this paper, we study this optimal rate, in the
minimax sense and for appropriate ""bounded"" nonparametric classes of
semimartingales. We show that, if the $r$th powers of the jumps are summable
for some $r\in[0,2)$, the minimax rate is equal to $\min(\sqrt{n},(n\log
n)^{(2-r)/2})$, where $n$ is the number of observations."@2012
Lyudmila Grigoryeva@http://arxiv.org/abs/1209.4188v1@"Finite sample forecasting with estimated temporally aggregated linear
  processes"@"We propose a finite sample based predictor for estimated linear one
dimensional time series models and compute the associated total forecasting
error. The expression for the error that we present takes into account the
estimation error. Unlike existing solutions in the literature, our formulas
require neither assumptions on the second order stationarity of the sample nor
Monte Carlo simulations for their evaluation. This result is used to prove the
pertinence of a new hybrid scheme that we put forward for the forecast of
linear temporal aggregates. This novel strategy consists of carrying out the
parameter estimation based on disaggregated data and the prediction based on
the corresponding aggregated model and data. We show that in some instances
this scheme has a better performance than the ""all-disaggregated"" approach
presented as optimal in the literature."@2012
Juan-Pablo Ortega@http://arxiv.org/abs/1209.4188v1@"Finite sample forecasting with estimated temporally aggregated linear
  processes"@"We propose a finite sample based predictor for estimated linear one
dimensional time series models and compute the associated total forecasting
error. The expression for the error that we present takes into account the
estimation error. Unlike existing solutions in the literature, our formulas
require neither assumptions on the second order stationarity of the sample nor
Monte Carlo simulations for their evaluation. This result is used to prove the
pertinence of a new hybrid scheme that we put forward for the forecast of
linear temporal aggregates. This novel strategy consists of carrying out the
parameter estimation based on disaggregated data and the prediction based on
the corresponding aggregated model and data. We show that in some instances
this scheme has a better performance than the ""all-disaggregated"" approach
presented as optimal in the literature."@2012
Hannes Leeb@http://arxiv.org/abs/1209.4543v3@"Testing in the Presence of Nuisance Parameters: Some Comments on Tests
  Post-Model-Selection and Random Critical Values"@"We point out that the ideas underlying some test procedures recently proposed
for testing post-model-selection (and for some other test problems) in the
econometrics literature have been around for quite some time in the statistics
literature. We also sharpen some of these results in the statistics literature.
Furthermore, we show that some intuitively appealing testing procedures, that
have found their way into the econometrics literature, lead to tests that do
not have desirable size properties, not even asymptotically."@2012
Benedikt M. Pötscher@http://arxiv.org/abs/1209.4543v3@"Testing in the Presence of Nuisance Parameters: Some Comments on Tests
  Post-Model-Selection and Random Critical Values"@"We point out that the ideas underlying some test procedures recently proposed
for testing post-model-selection (and for some other test problems) in the
econometrics literature have been around for quite some time in the statistics
literature. We also sharpen some of these results in the statistics literature.
Furthermore, we show that some intuitively appealing testing procedures, that
have found their way into the econometrics literature, lead to tests that do
not have desirable size properties, not even asymptotically."@2012
Jean-Marc Bardet@http://arxiv.org/abs/1209.4746v2@Monitoring procedure for parameter change in causal time series@"We propose a new sequential procedure to detect change in the parameters of a
process $ X= (X_t)_{t\in \Z}$ belonging to a large class of causal models (such
as AR($\infty$), ARCH($\infty$), TARCH($\infty$), ARMA-GARCH processes). The
procedure is based on a difference between the historical parameter estimator
and the updated parameter estimator, where both these estimators are based on a
quasi-likelihood of the model. Unlike classical recursive fluctuation test, the
updated estimator is computed without the historical observations. The
asymptotic behavior of the test is studied and the consistency in power as well
as an upper bound of the detection delay are obtained. Some simulation results
are reported with comparisons to some other existing procedures exhibiting the
accuracy of our new procedure. The procedure is also applied to the daily
closing values of the Nikkei 225, S$&$P 500 and FTSE 100 stock index. We show
in this real-data applications how the procedure can be used to solve off-line
multiple breaks detection."@2012
William Chakry Kengne@http://arxiv.org/abs/1209.4746v2@Monitoring procedure for parameter change in causal time series@"We propose a new sequential procedure to detect change in the parameters of a
process $ X= (X_t)_{t\in \Z}$ belonging to a large class of causal models (such
as AR($\infty$), ARCH($\infty$), TARCH($\infty$), ARMA-GARCH processes). The
procedure is based on a difference between the historical parameter estimator
and the updated parameter estimator, where both these estimators are based on a
quasi-likelihood of the model. Unlike classical recursive fluctuation test, the
updated estimator is computed without the historical observations. The
asymptotic behavior of the test is studied and the consistency in power as well
as an upper bound of the detection delay are obtained. Some simulation results
are reported with comparisons to some other existing procedures exhibiting the
accuracy of our new procedure. The procedure is also applied to the daily
closing values of the Nikkei 225, S$&$P 500 and FTSE 100 stock index. We show
in this real-data applications how the procedure can be used to solve off-line
multiple breaks detection."@2012
Sokbae Lee@http://arxiv.org/abs/1209.4875v4@The Lasso for High-Dimensional Regression with a Possible Change-Point@"We consider a high-dimensional regression model with a possible change-point
due to a covariate threshold and develop the Lasso estimator of regression
coefficients as well as the threshold parameter. Our Lasso estimator not only
selects covariates but also selects a model between linear and threshold
regression models. Under a sparsity assumption, we derive non-asymptotic oracle
inequalities for both the prediction risk and the $\ell_1$ estimation loss for
regression coefficients. Since the Lasso estimator selects variables
simultaneously, we show that oracle inequalities can be established without
pretesting the existence of the threshold effect. Furthermore, we establish
conditions under which the estimation error of the unknown threshold parameter
can be bounded by a nearly $n^{-1}$ factor even when the number of regressors
can be much larger than the sample size ($n$). We illustrate the usefulness of
our proposed estimation method via Monte Carlo simulations and an application
to real data."@2012
Myung Hwan Seo@http://arxiv.org/abs/1209.4875v4@The Lasso for High-Dimensional Regression with a Possible Change-Point@"We consider a high-dimensional regression model with a possible change-point
due to a covariate threshold and develop the Lasso estimator of regression
coefficients as well as the threshold parameter. Our Lasso estimator not only
selects covariates but also selects a model between linear and threshold
regression models. Under a sparsity assumption, we derive non-asymptotic oracle
inequalities for both the prediction risk and the $\ell_1$ estimation loss for
regression coefficients. Since the Lasso estimator selects variables
simultaneously, we show that oracle inequalities can be established without
pretesting the existence of the threshold effect. Furthermore, we establish
conditions under which the estimation error of the unknown threshold parameter
can be bounded by a nearly $n^{-1}$ factor even when the number of regressors
can be much larger than the sample size ($n$). We illustrate the usefulness of
our proposed estimation method via Monte Carlo simulations and an application
to real data."@2012
Youngki Shin@http://arxiv.org/abs/1209.4875v4@The Lasso for High-Dimensional Regression with a Possible Change-Point@"We consider a high-dimensional regression model with a possible change-point
due to a covariate threshold and develop the Lasso estimator of regression
coefficients as well as the threshold parameter. Our Lasso estimator not only
selects covariates but also selects a model between linear and threshold
regression models. Under a sparsity assumption, we derive non-asymptotic oracle
inequalities for both the prediction risk and the $\ell_1$ estimation loss for
regression coefficients. Since the Lasso estimator selects variables
simultaneously, we show that oracle inequalities can be established without
pretesting the existence of the threshold effect. Furthermore, we establish
conditions under which the estimation error of the unknown threshold parameter
can be bounded by a nearly $n^{-1}$ factor even when the number of regressors
can be much larger than the sample size ($n$). We illustrate the usefulness of
our proposed estimation method via Monte Carlo simulations and an application
to real data."@2012
Paulo C. Marques F.@http://arxiv.org/abs/1209.4947v2@Bayesian Analysis of Simple Random Densities@"A tractable nonparametric prior over densities is introduced which is closed
under sampling and exhibits proper posterior asymptotics."@2012
Carlos A. de B. Pereira@http://arxiv.org/abs/1209.4947v2@Bayesian Analysis of Simple Random Densities@"A tractable nonparametric prior over densities is introduced which is closed
under sampling and exhibits proper posterior asymptotics."@2012
Yacine Aït-Sahalia@http://arxiv.org/abs/1209.5170v1@"Identifying the successive Blumenthal-Getoor indices of a discretely
  observed process"@"This paper studies the identification of the L\'{e}vy jump measure of a
discretely-sampled semimartingale. We define successive Blumenthal-Getoor
indices of jump activity, and show that the leading index can always be
identified, but that higher order indices are only identifiable if they are
sufficiently close to the previous one, even if the path is fully observed.
This result establishes a clear boundary on which aspects of the jump measure
can be identified on the basis of discrete observations, and which cannot. We
then propose an estimation procedure for the identifiable indices and compare
the rates of convergence of these estimators with the optimal rates in a
special parametric case, which we can compute explicitly."@2012
Jean Jacod@http://arxiv.org/abs/1209.5170v1@"Identifying the successive Blumenthal-Getoor indices of a discretely
  observed process"@"This paper studies the identification of the L\'{e}vy jump measure of a
discretely-sampled semimartingale. We define successive Blumenthal-Getoor
indices of jump activity, and show that the leading index can always be
identified, but that higher order indices are only identifiable if they are
sufficiently close to the previous one, even if the path is fully observed.
This result establishes a clear boundary on which aspects of the jump measure
can be identified on the basis of discrete observations, and which cannot. We
then propose an estimation procedure for the identifiable indices and compare
the rates of convergence of these estimators with the optimal rates in a
special parametric case, which we can compute explicitly."@2012
Yu-Ru Su@http://arxiv.org/abs/1209.5183v1@"Modeling left-truncated and right-censored survival data with
  longitudinal covariates"@"There is a surge in medical follow-up studies that include longitudinal
covariates in the modeling of survival data. So far, the focus has been largely
on right-censored survival data. We consider survival data that are subject to
both left truncation and right censoring. Left truncation is well known to
produce biased sample. The sampling bias issue has been resolved in the
literature for the case which involves baseline or time-varying covariates that
are observable. The problem remains open, however, for the important case where
longitudinal covariates are present in survival models. A joint likelihood
approach has been shown in the literature to provide an effective way to
overcome those difficulties for right-censored data, but this approach faces
substantial additional challenges in the presence of left truncation. Here we
thus propose an alternative likelihood to overcome these difficulties and show
that the regression coefficient in the survival component can be estimated
unbiasedly and efficiently. Issues about the bias for the longitudinal
component are discussed. The new approach is illustrated numerically through
simulations and data from a multi-center AIDS cohort study."@2012
Jane-Ling Wang@http://arxiv.org/abs/1209.5183v1@"Modeling left-truncated and right-censored survival data with
  longitudinal covariates"@"There is a surge in medical follow-up studies that include longitudinal
covariates in the modeling of survival data. So far, the focus has been largely
on right-censored survival data. We consider survival data that are subject to
both left truncation and right censoring. Left truncation is well known to
produce biased sample. The sampling bias issue has been resolved in the
literature for the case which involves baseline or time-varying covariates that
are observable. The problem remains open, however, for the important case where
longitudinal covariates are present in survival models. A joint likelihood
approach has been shown in the literature to provide an effective way to
overcome those difficulties for right-censored data, but this approach faces
substantial additional challenges in the presence of left truncation. Here we
thus propose an alternative likelihood to overcome these difficulties and show
that the regression coefficient in the survival component can be estimated
unbiasedly and efficiently. Issues about the bias for the longitudinal
component are discussed. The new approach is illustrated numerically through
simulations and data from a multi-center AIDS cohort study."@2012
M. J. Bayarri@http://arxiv.org/abs/1209.5240v1@"Criteria for Bayesian model choice with application to variable
  selection"@"In objective Bayesian model selection, no single criterion has emerged as
dominant in defining objective prior distributions. Indeed, many criteria have
been separately proposed and utilized to propose differing prior choices. We
first formalize the most general and compelling of the various criteria that
have been suggested, together with a new criterion. We then illustrate the
potential of these criteria in determining objective model selection priors by
considering their application to the problem of variable selection in normal
linear models. This results in a new model selection objective prior with a
number of compelling properties."@2012
J. O. Berger@http://arxiv.org/abs/1209.5240v1@"Criteria for Bayesian model choice with application to variable
  selection"@"In objective Bayesian model selection, no single criterion has emerged as
dominant in defining objective prior distributions. Indeed, many criteria have
been separately proposed and utilized to propose differing prior choices. We
first formalize the most general and compelling of the various criteria that
have been suggested, together with a new criterion. We then illustrate the
potential of these criteria in determining objective model selection priors by
considering their application to the problem of variable selection in normal
linear models. This results in a new model selection objective prior with a
number of compelling properties."@2012
A. Forte@http://arxiv.org/abs/1209.5240v1@"Criteria for Bayesian model choice with application to variable
  selection"@"In objective Bayesian model selection, no single criterion has emerged as
dominant in defining objective prior distributions. Indeed, many criteria have
been separately proposed and utilized to propose differing prior choices. We
first formalize the most general and compelling of the various criteria that
have been suggested, together with a new criterion. We then illustrate the
potential of these criteria in determining objective model selection priors by
considering their application to the problem of variable selection in normal
linear models. This results in a new model selection objective prior with a
number of compelling properties."@2012
G. García-Donato@http://arxiv.org/abs/1209.5240v1@"Criteria for Bayesian model choice with application to variable
  selection"@"In objective Bayesian model selection, no single criterion has emerged as
dominant in defining objective prior distributions. Indeed, many criteria have
been separately proposed and utilized to propose differing prior choices. We
first formalize the most general and compelling of the various criteria that
have been suggested, together with a new criterion. We then illustrate the
potential of these criteria in determining objective model selection priors by
considering their application to the problem of variable selection in normal
linear models. This results in a new model selection objective prior with a
number of compelling properties."@2012
Yuan Wu@http://arxiv.org/abs/1209.5543v1@"Partially monotone tensor spline estimation of the joint distribution
  function with bivariate current status data"@"The analysis of the joint cumulative distribution function (CDF) with
bivariate event time data is a challenging problem both theoretically and
numerically. This paper develops a tensor spline-based sieve maximum likelihood
estimation method to estimate the joint CDF with bivariate current status data.
The I-splines are used to approximate the joint CDF in order to simplify the
numerical computation of a constrained maximum likelihood estimation problem.
The generalized gradient projection algorithm is used to compute the
constrained optimization problem. Based on the properties of B-spline basis
functions it is shown that the proposed tensor spline-based nonparametric sieve
maximum likelihood estimator is consistent with a rate of convergence
potentially better than $n^{1/3}$ under some mild regularity conditions. The
simulation studies with moderate sample sizes are carried out to demonstrate
that the finite sample performance of the proposed estimator is generally
satisfactory."@2012
Ying Zhang@http://arxiv.org/abs/1209.5543v1@"Partially monotone tensor spline estimation of the joint distribution
  function with bivariate current status data"@"The analysis of the joint cumulative distribution function (CDF) with
bivariate event time data is a challenging problem both theoretically and
numerically. This paper develops a tensor spline-based sieve maximum likelihood
estimation method to estimate the joint CDF with bivariate current status data.
The I-splines are used to approximate the joint CDF in order to simplify the
numerical computation of a constrained maximum likelihood estimation problem.
The generalized gradient projection algorithm is used to compute the
constrained optimization problem. Based on the properties of B-spline basis
functions it is shown that the proposed tensor spline-based nonparametric sieve
maximum likelihood estimator is consistent with a rate of convergence
potentially better than $n^{1/3}$ under some mild regularity conditions. The
simulation studies with moderate sample sizes are carried out to demonstrate
that the finite sample performance of the proposed estimator is generally
satisfactory."@2012
Thierry Dumont@http://arxiv.org/abs/1209.5923v1@"Simultaneous Localization and Mapping Problem in Wireless Sensor
  Networks"@"Mobile device localization in wireless sensor networks is a challenging task.
It has already been addressed when the WiFI propagation maps of the access
points are modeled deterministically. However, this procedure does not take
into account the environmental dynamics and also assumes an offline human
training calibration. In this paper, the maps are made of an average indoor
propagation model combined with a perturbation field which represents the
influence of the environment. This perturbation field is embedded with a prior
distribution. The device localization is dealt with using Sequential Monte
Carlo methods and relies on the estimation of the propagation maps. This
inference task is performed online, i.e. using the observations sequentially,
with a recently proposed online Expectation Maximization based algorithm. The
performance of the algorithm are illustrated through Monte Carlo experiments."@2012
Sylvain Le Corff@http://arxiv.org/abs/1209.5923v1@"Simultaneous Localization and Mapping Problem in Wireless Sensor
  Networks"@"Mobile device localization in wireless sensor networks is a challenging task.
It has already been addressed when the WiFI propagation maps of the access
points are modeled deterministically. However, this procedure does not take
into account the environmental dynamics and also assumes an offline human
training calibration. In this paper, the maps are made of an average indoor
propagation model combined with a perturbation field which represents the
influence of the environment. This perturbation field is embedded with a prior
distribution. The device localization is dealt with using Sequential Monte
Carlo methods and relies on the estimation of the propagation maps. This
inference task is performed online, i.e. using the observations sequentially,
with a recently proposed online Expectation Maximization based algorithm. The
performance of the algorithm are illustrated through Monte Carlo experiments."@2012
Yuejie Chi@http://arxiv.org/abs/1209.6267v1@Coherence-Based Performance Guarantees of Orthogonal Matching Pursuit@"In this paper, we present coherence-based performance guarantees of
Orthogonal Matching Pursuit (OMP) for both support recovery and signal
reconstruction of sparse signals when the measurements are corrupted by noise.
In particular, two variants of OMP either with known sparsity level or with a
stopping rule are analyzed. It is shown that if the measurement matrix
$X\in\mathbb{C}^{n\times p}$ satisfies the strong coherence property, then with
$n\gtrsim\mathcal{O}(k\log p)$, OMP will recover a $k$-sparse signal with high
probability. In particular, the performance guarantees obtained here separate
the properties required of the measurement matrix from the properties required
of the signal, which depends critically on the minimum signal to noise ratio
rather than the power profiles of the signal. We also provide performance
guarantees for partial support recovery. Comparisons are given with other
performance guarantees for OMP using worst-case analysis and the sorted one
step thresholding algorithm."@2012
Robert Calderbank@http://arxiv.org/abs/1209.6267v1@Coherence-Based Performance Guarantees of Orthogonal Matching Pursuit@"In this paper, we present coherence-based performance guarantees of
Orthogonal Matching Pursuit (OMP) for both support recovery and signal
reconstruction of sparse signals when the measurements are corrupted by noise.
In particular, two variants of OMP either with known sparsity level or with a
stopping rule are analyzed. It is shown that if the measurement matrix
$X\in\mathbb{C}^{n\times p}$ satisfies the strong coherence property, then with
$n\gtrsim\mathcal{O}(k\log p)$, OMP will recover a $k$-sparse signal with high
probability. In particular, the performance guarantees obtained here separate
the properties required of the measurement matrix from the properties required
of the signal, which depends critically on the minimum signal to noise ratio
rather than the power profiles of the signal. We also provide performance
guarantees for partial support recovery. Comparisons are given with other
performance guarantees for OMP using worst-case analysis and the sorted one
step thresholding algorithm."@2012
Hervé Cardot@http://arxiv.org/abs/1209.6503v3@"Variance estimation and asymptotic confidence bands for the mean
  estimator of sampled functional data with high entropy unequal probability
  sampling designs"@"For fixed size sampling designs with high entropy it is well known that the
variance of the Horvitz-Thompson estimator can be approximated by the H\'ajek
formula. The interest of this asymptotic variance approximation is that it only
involves the first order inclusion probabilities of the statistical units. We
extend this variance formula when the variable under study is functional and we
prove, under general conditions on the regularity of the individual
trajectories and the sampling design, that we can get a uniformly convergent
estimator of the variance function of the Horvitz-Thompson estimator of the
mean function. Rates of convergence to the true variance function are given for
the rejective sampling. We deduce, under conditions on the entropy of the
sampling design, that it is possible to build confidence bands whose coverage
is asymptotically the desired one via simulation of Gaussian processes with
variance function given by the H\'ajek formula. Finally, the accuracy of the
proposed variance estimator is evaluated on samples of electricity consumption
data measured every half an hour over a period of one week."@2012
Camelia Goga@http://arxiv.org/abs/1209.6503v3@"Variance estimation and asymptotic confidence bands for the mean
  estimator of sampled functional data with high entropy unequal probability
  sampling designs"@"For fixed size sampling designs with high entropy it is well known that the
variance of the Horvitz-Thompson estimator can be approximated by the H\'ajek
formula. The interest of this asymptotic variance approximation is that it only
involves the first order inclusion probabilities of the statistical units. We
extend this variance formula when the variable under study is functional and we
prove, under general conditions on the regularity of the individual
trajectories and the sampling design, that we can get a uniformly convergent
estimator of the variance function of the Horvitz-Thompson estimator of the
mean function. Rates of convergence to the true variance function are given for
the rejective sampling. We deduce, under conditions on the entropy of the
sampling design, that it is possible to build confidence bands whose coverage
is asymptotically the desired one via simulation of Gaussian processes with
variance function given by the H\'ajek formula. Finally, the accuracy of the
proposed variance estimator is evaluated on samples of electricity consumption
data measured every half an hour over a period of one week."@2012
Pauline Lardin@http://arxiv.org/abs/1209.6503v3@"Variance estimation and asymptotic confidence bands for the mean
  estimator of sampled functional data with high entropy unequal probability
  sampling designs"@"For fixed size sampling designs with high entropy it is well known that the
variance of the Horvitz-Thompson estimator can be approximated by the H\'ajek
formula. The interest of this asymptotic variance approximation is that it only
involves the first order inclusion probabilities of the statistical units. We
extend this variance formula when the variable under study is functional and we
prove, under general conditions on the regularity of the individual
trajectories and the sampling design, that we can get a uniformly convergent
estimator of the variance function of the Horvitz-Thompson estimator of the
mean function. Rates of convergence to the true variance function are given for
the rejective sampling. We deduce, under conditions on the entropy of the
sampling design, that it is possible to build confidence bands whose coverage
is asymptotically the desired one via simulation of Gaussian processes with
variance function given by the H\'ajek formula. Finally, the accuracy of the
proposed variance estimator is evaluated on samples of electricity consumption
data measured every half an hour over a period of one week."@2012
Ryan Martin@http://arxiv.org/abs/1210.0103v1@"On convergence rates of Bayesian predictive densities and posterior
  distributions"@"Frequentist-style large-sample properties of Bayesian posterior
distributions, such as consistency and convergence rates, are important
considerations in nonparametric problems. In this paper we give an analysis of
Bayesian asymptotics based primarily on predictive densities. Our analysis is
unified in the sense that essentially the same approach can be taken to develop
convergence rate results in iid, mis-specified iid, independent non-iid, and
dependent data cases."@2012
Liang Hong@http://arxiv.org/abs/1210.0103v1@"On convergence rates of Bayesian predictive densities and posterior
  distributions"@"Frequentist-style large-sample properties of Bayesian posterior
distributions, such as consistency and convergence rates, are important
considerations in nonparametric problems. In this paper we give an analysis of
Bayesian asymptotics based primarily on predictive densities. Our analysis is
unified in the sense that essentially the same approach can be taken to develop
convergence rate results in iid, mis-specified iid, independent non-iid, and
dependent data cases."@2012
Jérémie Bigot@http://arxiv.org/abs/1210.0771v3@Minimax properties of Fréchet means of discretely sampled curves@"We study the problem of estimating a mean pattern from a set of similar
curves in the setting where the variability in the data is due to random
geometric deformations and additive noise. We propose an estimator based on the
notion of Frechet mean that is a generalization of the standard notion of
averaging to non-Euclidean spaces. We derive a minimax rate for this estimation
problem, and we show that our estimator achieves this optimal rate under the
asymptotics where both the number of curves and the number of sampling points
go to infinity."@2012
Xavier Gendre@http://arxiv.org/abs/1210.0771v3@Minimax properties of Fréchet means of discretely sampled curves@"We study the problem of estimating a mean pattern from a set of similar
curves in the setting where the variability in the data is due to random
geometric deformations and additive noise. We propose an estimator based on the
notion of Frechet mean that is a generalization of the standard notion of
averaging to non-Euclidean spaces. We derive a minimax rate for this estimation
problem, and we show that our estimator achieves this optimal rate under the
asymptotics where both the number of curves and the number of sampling points
go to infinity."@2012
Jon A. Wellner@http://arxiv.org/abs/1210.0807v2@"Global Rates of Convergence of the MLE for Multivariate Interval
  Censoring"@"We establish global rates of convergence of the Maximum Likelihood Estimator
(MLE) of a multivariate distribution function in the case of (one type of)
""interval censored"" data. The main finding is that the rate of convergence of
the MLE in the Hellinger metric is no worse than $n^{-1/3} (\log n)^{\gamma}$
for $\gamma = (5d - 4)/6$."@2012
Fuchang Gao@http://arxiv.org/abs/1210.0807v2@"Global Rates of Convergence of the MLE for Multivariate Interval
  Censoring"@"We establish global rates of convergence of the Maximum Likelihood Estimator
(MLE) of a multivariate distribution function in the case of (one type of)
""interval censored"" data. The main finding is that the rate of convergence of
the MLE in the Hellinger metric is no worse than $n^{-1/3} (\log n)^{\gamma}$
for $\gamma = (5d - 4)/6$."@2012
Min Yang@http://arxiv.org/abs/1210.1058v1@"Identifying locally optimal designs for nonlinear models: A simple
  extension with profound consequences"@"We extend the approach in [Ann. Statist. 38 (2010) 2499-2524] for identifying
locally optimal designs for nonlinear models. Conceptually the extension is
relatively simple, but the consequences in terms of applications are profound.
As we will demonstrate, we can obtain results for locally optimal designs under
many optimality criteria and for a larger class of models than has been done
hitherto. In many cases the results lead to optimal designs with the minimal
number of support points."@2012
John Stufken@http://arxiv.org/abs/1210.1058v1@"Identifying locally optimal designs for nonlinear models: A simple
  extension with profound consequences"@"We extend the approach in [Ann. Statist. 38 (2010) 2499-2524] for identifying
locally optimal designs for nonlinear models. Conceptually the extension is
relatively simple, but the consequences in terms of applications are profound.
As we will demonstrate, we can obtain results for locally optimal designs under
many optimality criteria and for a larger class of models than has been done
hitherto. In many cases the results lead to optimal designs with the minimal
number of support points."@2012
Paul Lemaître@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Ekatarina Sergienko@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Aurélie Arnaud@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Nicolas Bousquet@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Fabrice Gamboa@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Bertrand Iooss@http://arxiv.org/abs/1210.1074v3@Density modification based reliability sensitivity analysis@"Sensitivity analysis of a numerical model, for instance simulating physical
phenomena, is useful to quantify the influence of the inputs on the model
responses. This paper proposes a new sensitivity index, based upon the
modification of the probability density function (pdf) of the random inputs,
when the quantity of interest is a failure probability (probability that a
model output exceeds a given threshold). An input is considered influential if
the input pdf modification leads to a broad change in the failure probability.
These sensitivity indices can be computed using the sole set of simulations
that has already been used to estimate the failure probability, thus limiting
the number of calls to the numerical model. In the case of a Monte Carlo
sample, asymptotical properties of the indices are derived. Based on
Kullback-Leibler divergence, several types of input perturbations are
introduced. The relevance of this new sensitivity analysis method is analysed
through three case studies."@2012
Sergios Agapiou@http://arxiv.org/abs/1210.1563v3@"Bayesian Posterior Contraction Rates for Linear Severely Ill-posed
  Inverse Problems"@"We consider a class of linear ill-posed inverse problems arising from
inversion of a compact operator with singular values which decay exponentially
to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown
function. If the observational noise is assumed to be Gaussian then this prior
is conjugate to the likelihood so that the posterior distribution is also
Gaussian. We study Bayesian posterior consistency in the small observational
noise limit. We assume that the forward operator and the prior and noise
covariance operators commute with one another. We show how, for given
smoothness assumptions on the truth, the scale parameter of the prior can be
adjusted to optimize the rate of posterior contraction to the truth, and we
explicitly compute the logarithmic rate."@2012
Andrew M. Stuart@http://arxiv.org/abs/1210.1563v3@"Bayesian Posterior Contraction Rates for Linear Severely Ill-posed
  Inverse Problems"@"We consider a class of linear ill-posed inverse problems arising from
inversion of a compact operator with singular values which decay exponentially
to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown
function. If the observational noise is assumed to be Gaussian then this prior
is conjugate to the likelihood so that the posterior distribution is also
Gaussian. We study Bayesian posterior consistency in the small observational
noise limit. We assume that the forward operator and the prior and noise
covariance operators commute with one another. We show how, for given
smoothness assumptions on the truth, the scale parameter of the prior can be
adjusted to optimize the rate of posterior contraction to the truth, and we
explicitly compute the logarithmic rate."@2012
Yuan-Xiang Zhang@http://arxiv.org/abs/1210.1563v3@"Bayesian Posterior Contraction Rates for Linear Severely Ill-posed
  Inverse Problems"@"We consider a class of linear ill-posed inverse problems arising from
inversion of a compact operator with singular values which decay exponentially
to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown
function. If the observational noise is assumed to be Gaussian then this prior
is conjugate to the likelihood so that the posterior distribution is also
Gaussian. We study Bayesian posterior consistency in the small observational
noise limit. We assume that the forward operator and the prior and noise
covariance operators commute with one another. We show how, for given
smoothness assumptions on the truth, the scale parameter of the prior can be
adjusted to optimize the rate of posterior contraction to the truth, and we
explicitly compute the logarithmic rate."@2012
A. Goldenshluger@http://arxiv.org/abs/1210.1715v2@On adaptive minimax density estimation on $R^d$@"We address the problem of adaptive minimax density estimation on $\bR^d$ with
$\bL_p$--loss on the anisotropic Nikol'skii classes. We fully characterize
behavior of the minimax risk for different relationships between regularity
parameters and norm indexes in definitions of the functional class and of the
risk. In particular, we show that there are four different regimes with respect
to the behavior of the minimax risk. We develop a single estimator which is
(nearly) optimal in orderover the complete scale of the anisotropic Nikol'skii
classes. Our estimation procedure is based on a data-driven selection of an
estimator from a fixed family of kernel estimators."@2012
O. Lepski@http://arxiv.org/abs/1210.1715v2@On adaptive minimax density estimation on $R^d$@"We address the problem of adaptive minimax density estimation on $\bR^d$ with
$\bL_p$--loss on the anisotropic Nikol'skii classes. We fully characterize
behavior of the minimax risk for different relationships between regularity
parameters and norm indexes in definitions of the functional class and of the
risk. In particular, we show that there are four different regimes with respect
to the behavior of the minimax risk. We develop a single estimator which is
(nearly) optimal in orderover the complete scale of the anisotropic Nikol'skii
classes. Our estimation procedure is based on a data-driven selection of an
estimator from a fixed family of kernel estimators."@2012
Sidney I. Resnick@http://arxiv.org/abs/1210.2314v2@Clustering of Markov chain exceedances@"The tail chain of a Markov chain can be used to model the dependence between
extreme observations. For a positive recurrent Markov chain, the tail chain
aids in describing the limit of a sequence of point processes $\{N_n,n\geq1\}$,
consisting of normalized observations plotted against scaled time points. Under
fairly general conditions on extremal behaviour, $\{N_n\}$ converges to a
cluster Poisson process. Our technique decomposes the sample path of the chain
into i.i.d. regenerative cycles rather than using blocking argument typically
employed in the context of stationarity with mixing."@2012
David Zeber@http://arxiv.org/abs/1210.2314v2@Clustering of Markov chain exceedances@"The tail chain of a Markov chain can be used to model the dependence between
extreme observations. For a positive recurrent Markov chain, the tail chain
aids in describing the limit of a sequence of point processes $\{N_n,n\geq1\}$,
consisting of normalized observations plotted against scaled time points. Under
fairly general conditions on extremal behaviour, $\{N_n\}$ converges to a
cluster Poisson process. Our technique decomposes the sample path of the chain
into i.i.d. regenerative cycles rather than using blocking argument typically
employed in the context of stationarity with mixing."@2012
Sylvain Delattre@http://arxiv.org/abs/1210.2489v2@"On empirical distribution function of high-dimensional Gaussian vector
  components with an application to multiple testing"@"This paper introduces a new framework to study the asymptotical behavior of
the empirical distribution function (e.d.f.) of Gaussian vector components,
whose correlation matrix $\Gamma^{(m)}$ is dimension-dependent. Hence, by
contrast with the existing literature, the vector is not assumed to be
stationary. Rather, we make a ""vanishing second order"" assumption ensuring that
the covariance matrix $\Gamma^{(m)}$ is not too far from the identity matrix,
while the behavior of the e.d.f. is affected by $\Gamma^{(m)}$ only through the
sequence $\gamma_m=m^{-2} \sum_{i\neq j} \Gamma_{i,j}^{(m)}$, as $m$ grows to
infinity. This result recovers some of the previous results for stationary
long-range dependencies while it also applies to various, high-dimensional,
non-stationary frameworks, for which the most correlated variables are not
necessarily next to each other. Finally, we present an application of this work
to the multiple testing problem, which was the initial statistical motivation
for developing such a methodology."@2012
Etienne Roquain@http://arxiv.org/abs/1210.2489v2@"On empirical distribution function of high-dimensional Gaussian vector
  components with an application to multiple testing"@"This paper introduces a new framework to study the asymptotical behavior of
the empirical distribution function (e.d.f.) of Gaussian vector components,
whose correlation matrix $\Gamma^{(m)}$ is dimension-dependent. Hence, by
contrast with the existing literature, the vector is not assumed to be
stationary. Rather, we make a ""vanishing second order"" assumption ensuring that
the covariance matrix $\Gamma^{(m)}$ is not too far from the identity matrix,
while the behavior of the e.d.f. is affected by $\Gamma^{(m)}$ only through the
sequence $\gamma_m=m^{-2} \sum_{i\neq j} \Gamma_{i,j}^{(m)}$, as $m$ grows to
infinity. This result recovers some of the previous results for stationary
long-range dependencies while it also applies to various, high-dimensional,
non-stationary frameworks, for which the most correlated variables are not
necessarily next to each other. Finally, we present an application of this work
to the multiple testing problem, which was the initial statistical motivation
for developing such a methodology."@2012
Miklos Csorgo@http://arxiv.org/abs/1210.2757v2@"Asymptotics of Randomly Weighted u- and v-statistics: Application to
  Bootstrap"@"This paper is mainly concerned with asymptotic studies of weighted bootstrap
for u- and v-statistics. We derive the consistency of the weighted bootstrap u-
and v-statistics, based on i.i.d. and non i.i.d. observations, from some more
general results which we first establish for sums of randomly weighted arrays
of random variables. Some of the results in this paper significantly extend
some well-known results on consistency of u-statistics and also consistency of
sums of arrays of random variables. We also employ a new approach to
conditioning to derive a conditional CLT for weighted bootstrap u- and
v-statistics, assuming the same conditions as the classical central limit
theorems for regular u- and v-statistics."@2012
Masoud M. Nasari@http://arxiv.org/abs/1210.2757v2@"Asymptotics of Randomly Weighted u- and v-statistics: Application to
  Bootstrap"@"This paper is mainly concerned with asymptotic studies of weighted bootstrap
for u- and v-statistics. We derive the consistency of the weighted bootstrap u-
and v-statistics, based on i.i.d. and non i.i.d. observations, from some more
general results which we first establish for sums of randomly weighted arrays
of random variables. Some of the results in this paper significantly extend
some well-known results on consistency of u-statistics and also consistency of
sums of arrays of random variables. We also employ a new approach to
conditioning to derive a conditional CLT for weighted bootstrap u- and
v-statistics, assuming the same conditions as the classical central limit
theorems for regular u- and v-statistics."@2012
Loic Le Gratiet@http://arxiv.org/abs/1210.2879v2@"Regularity dependence of the rate of convergence of the learning curve
  for Gaussian process regression"@"This paper deals with the speed of convergence of the learning curve in a
Gaussian process regression framework. The learning curve describes the average
generalization error of the Gaussian process used for the regression. More
specifically, it is defined in this paper as the integral of the mean squared
error over the input parameter space with respect to the probability measure of
the input parameters. The main result is the proof of a theorem giving the mean
squared error in function of the number of observations for a large class of
kernels and for any dimension when the number of observations is large. From
this result, we can deduce the asymptotic behavior of the generalization error.
The presented proof generalizes previous ones that were limited to more
specific kernels or to small dimensions (one or two). The result can be used to
build an optimal strategy for resources allocation. This strategy is applied
successfully to a nuclear safety problem."@2012
Josselin Garnier@http://arxiv.org/abs/1210.2879v2@"Regularity dependence of the rate of convergence of the learning curve
  for Gaussian process regression"@"This paper deals with the speed of convergence of the learning curve in a
Gaussian process regression framework. The learning curve describes the average
generalization error of the Gaussian process used for the regression. More
specifically, it is defined in this paper as the integral of the mean squared
error over the input parameter space with respect to the probability measure of
the input parameters. The main result is the proof of a theorem giving the mean
squared error in function of the number of observations for a large class of
kernels and for any dimension when the number of observations is large. From
this result, we can deduce the asymptotic behavior of the generalization error.
The presented proof generalizes previous ones that were limited to more
specific kernels or to small dimensions (one or two). The result can be used to
build an optimal strategy for resources allocation. This strategy is applied
successfully to a nuclear safety problem."@2012
Sidney Resnick@http://arxiv.org/abs/1210.3060v1@Markov Kernels and the Conditional Extreme Value Model@"The classical approach to multivariate extreme value modelling assumes that
the joint distribution belongs to a multivariate domain of attraction. This
requires each marginal distribution be individually attracted to a univariate
extreme value distribution. An apparently more flexible extremal model for
multivariate data was proposed by Heffernan and Tawn under which not all the
components are required to belong to an extremal domain of attraction but
assumes instead the existence of an asymptotic approximation to the conditional
distribution of the random vector given one of the components is extreme.
Combined with the knowledge that the conditioning component belongs to a
univariate domain of attraction, this leads to an approximation of the
probability of certain risk regions. The original focus on conditional
distributions had technical drawbacks but is natural in several contexts. We
place this approach in the context of the more general approach using
convergence of measures and multivariate regular variation on cones."@2012
David Zeber@http://arxiv.org/abs/1210.3060v1@Markov Kernels and the Conditional Extreme Value Model@"The classical approach to multivariate extreme value modelling assumes that
the joint distribution belongs to a multivariate domain of attraction. This
requires each marginal distribution be individually attracted to a univariate
extreme value distribution. An apparently more flexible extremal model for
multivariate data was proposed by Heffernan and Tawn under which not all the
components are required to belong to an extremal domain of attraction but
assumes instead the existence of an asymptotic approximation to the conditional
distribution of the random vector given one of the components is extreme.
Combined with the knowledge that the conditioning component belongs to a
univariate domain of attraction, this leads to an approximation of the
probability of certain risk regions. The original focus on conditional
distributions had technical drawbacks but is natural in several contexts. We
place this approach in the context of the more general approach using
convergence of measures and multivariate regular variation on cones."@2012
Dave Zachariah@http://arxiv.org/abs/1210.3516v1@Bayesian Estimation with Distance Bounds@"We consider the problem of estimating a random state vector when there is
information about the maximum distances between its subvectors. The estimation
problem is posed in a Bayesian framework in which the minimum mean square error
(MMSE) estimate of the state is given by the conditional mean. Since finding
the conditional mean requires multidimensional integration, an approximate MMSE
estimator is proposed. The performance of the proposed estimator is evaluated
in a positioning problem. Finally, the application of the estimator in
inequality constrained recursive filtering is illustrated by applying the
estimator to a dead-reckoning problem. The MSE of the estimator is compared
with two related posterior Cram\'er-Rao bounds."@2012
Isaac Skog@http://arxiv.org/abs/1210.3516v1@Bayesian Estimation with Distance Bounds@"We consider the problem of estimating a random state vector when there is
information about the maximum distances between its subvectors. The estimation
problem is posed in a Bayesian framework in which the minimum mean square error
(MMSE) estimate of the state is given by the conditional mean. Since finding
the conditional mean requires multidimensional integration, an approximate MMSE
estimator is proposed. The performance of the proposed estimator is evaluated
in a positioning problem. Finally, the application of the estimator in
inequality constrained recursive filtering is illustrated by applying the
estimator to a dead-reckoning problem. The MSE of the estimator is compared
with two related posterior Cram\'er-Rao bounds."@2012
Magnus Jansson@http://arxiv.org/abs/1210.3516v1@Bayesian Estimation with Distance Bounds@"We consider the problem of estimating a random state vector when there is
information about the maximum distances between its subvectors. The estimation
problem is posed in a Bayesian framework in which the minimum mean square error
(MMSE) estimate of the state is given by the conditional mean. Since finding
the conditional mean requires multidimensional integration, an approximate MMSE
estimator is proposed. The performance of the proposed estimator is evaluated
in a positioning problem. Finally, the application of the estimator in
inequality constrained recursive filtering is illustrated by applying the
estimator to a dead-reckoning problem. The MSE of the estimator is compared
with two related posterior Cram\'er-Rao bounds."@2012
Peter Händel@http://arxiv.org/abs/1210.3516v1@Bayesian Estimation with Distance Bounds@"We consider the problem of estimating a random state vector when there is
information about the maximum distances between its subvectors. The estimation
problem is posed in a Bayesian framework in which the minimum mean square error
(MMSE) estimate of the state is given by the conditional mean. Since finding
the conditional mean requires multidimensional integration, an approximate MMSE
estimator is proposed. The performance of the proposed estimator is evaluated
in a positioning problem. Finally, the application of the estimator in
inequality constrained recursive filtering is illustrated by applying the
estimator to a dead-reckoning problem. The MSE of the estimator is compared
with two related posterior Cram\'er-Rao bounds."@2012
Nadia Morsli@http://arxiv.org/abs/1210.4402v2@"Poisson intensity parameter estimation for stationary Gibbs point
  processes of finite interaction range"@"We introduce a semi-parametric estimator of the Poisson intensity parameter
of a spatial stationary Gibbs point process. Under very mild assumptions
satisfied by a large class of Gibbs models, we establish its strong consistency
and asymptotic normality. We also consider its finite-sample properties in a
simulation study."@2012
Jean-François Coeurjolly@http://arxiv.org/abs/1210.4402v2@"Poisson intensity parameter estimation for stationary Gibbs point
  processes of finite interaction range"@"We introduce a semi-parametric estimator of the Poisson intensity parameter
of a spatial stationary Gibbs point process. Under very mild assumptions
satisfied by a large class of Gibbs models, we establish its strong consistency
and asymptotic normality. We also consider its finite-sample properties in a
simulation study."@2012
Eric J. Tchetgen Tchetgen@http://arxiv.org/abs/1210.4654v1@"Semiparametric theory for causal mediation analysis: Efficiency bounds,
  multiple robustness and sensitivity analysis"@"While estimation of the marginal (total) causal effect of a point exposure on
an outcome is arguably the most common objective of experimental and
observational studies in the health and social sciences, in recent years,
investigators have also become increasingly interested in mediation analysis.
Specifically, upon evaluating the total effect of the exposure, investigators
routinely wish to make inferences about the direct or indirect pathways of the
effect of the exposure, through a mediator variable or not, that occurs
subsequently to the exposure and prior to the outcome. Although powerful
semiparametric methodologies have been developed to analyze observational
studies that produce double robust and highly efficient estimates of the
marginal total causal effect, similar methods for mediation analysis are
currently lacking. Thus, this paper develops a general semiparametric framework
for obtaining inferences about so-called marginal natural direct and indirect
causal effects, while appropriately accounting for a large number of
pre-exposure confounding factors for the exposure and the mediator variables.
Our analytic framework is particularly appealing, because it gives new insights
on issues of efficiency and robustness in the context of mediation analysis. In
particular, we propose new multiply robust locally efficient estimators of the
marginal natural indirect and direct causal effects, and develop a novel double
robust sensitivity analysis framework for the assumption of ignorability of the
mediator variable."@2012
Ilya Shpitser@http://arxiv.org/abs/1210.4654v1@"Semiparametric theory for causal mediation analysis: Efficiency bounds,
  multiple robustness and sensitivity analysis"@"While estimation of the marginal (total) causal effect of a point exposure on
an outcome is arguably the most common objective of experimental and
observational studies in the health and social sciences, in recent years,
investigators have also become increasingly interested in mediation analysis.
Specifically, upon evaluating the total effect of the exposure, investigators
routinely wish to make inferences about the direct or indirect pathways of the
effect of the exposure, through a mediator variable or not, that occurs
subsequently to the exposure and prior to the outcome. Although powerful
semiparametric methodologies have been developed to analyze observational
studies that produce double robust and highly efficient estimates of the
marginal total causal effect, similar methods for mediation analysis are
currently lacking. Thus, this paper develops a general semiparametric framework
for obtaining inferences about so-called marginal natural direct and indirect
causal effects, while appropriately accounting for a large number of
pre-exposure confounding factors for the exposure and the mediator variables.
Our analytic framework is particularly appealing, because it gives new insights
on issues of efficiency and robustness in the context of mediation analysis. In
particular, we propose new multiply robust locally efficient estimators of the
marginal natural indirect and direct causal effects, and develop a novel double
robust sensitivity analysis framework for the assumption of ignorability of the
mediator variable."@2012
Yanqing Hu@http://arxiv.org/abs/1210.4666v1@Asymptotic properties of covariate-adaptive randomization@"Balancing treatment allocation for influential covariates is critical in
clinical trials. This has become increasingly important as more and more
biomarkers are found to be associated with different diseases in translational
research (genomics, proteomics and metabolomics). Stratified permuted block
randomization and minimization methods [Pocock and Simon Biometrics 31 (1975)
103-115, etc.] are the two most popular approaches in practice. However,
stratified permuted block randomization fails to achieve good overall balance
when the number of strata is large, whereas traditional minimization methods
also suffer from the potential drawback of large within-stratum imbalances.
Moreover, the theoretical bases of minimization methods remain largely elusive.
In this paper, we propose a new covariate-adaptive design that is able to
control various types of imbalances. We show that the joint process of
within-stratum imbalances is a positive recurrent Markov chain under certain
conditions. Therefore, this new procedure yields more balanced allocation. The
advantages of the proposed procedure are also demonstrated by extensive
simulation studies. Our work provides a theoretical tool for future research in
this area."@2012
Feifang Hu@http://arxiv.org/abs/1210.4666v1@Asymptotic properties of covariate-adaptive randomization@"Balancing treatment allocation for influential covariates is critical in
clinical trials. This has become increasingly important as more and more
biomarkers are found to be associated with different diseases in translational
research (genomics, proteomics and metabolomics). Stratified permuted block
randomization and minimization methods [Pocock and Simon Biometrics 31 (1975)
103-115, etc.] are the two most popular approaches in practice. However,
stratified permuted block randomization fails to achieve good overall balance
when the number of strata is large, whereas traditional minimization methods
also suffer from the potential drawback of large within-stratum imbalances.
Moreover, the theoretical bases of minimization methods remain largely elusive.
In this paper, we propose a new covariate-adaptive design that is able to
control various types of imbalances. We show that the joint process of
within-stratum imbalances is a positive recurrent Markov chain under certain
conditions. Therefore, this new procedure yields more balanced allocation. The
advantages of the proposed procedure are also demonstrated by extensive
simulation studies. Our work provides a theoretical tool for future research in
this area."@2012
Young K. Lee@http://arxiv.org/abs/1210.4711v1@Flexible generalized varying coefficient regression models@"This paper studies a very flexible model that can be used widely to analyze
the relation between a response and multiple covariates. The model is
nonparametric, yet renders easy interpretation for the effects of the
covariates. The model accommodates both continuous and discrete random
variables for the response and covariates. It is quite flexible to cover the
generalized varying coefficient models and the generalized additive models as
special cases. Under a weak condition we give a general theorem that the
problem of estimating the multivariate mean function is equivalent to that of
estimating its univariate component functions. We discuss implications of the
theorem for sieve and penalized least squares estimators, and then investigate
the outcomes in full details for a kernel-type estimator. The kernel estimator
is given as a solution of a system of nonlinear integral equations. We provide
an iterative algorithm to solve the system of equations and discuss the
theoretical properties of the estimator and the algorithm. Finally, we give
simulation results."@2012
Enno Mammen@http://arxiv.org/abs/1210.4711v1@Flexible generalized varying coefficient regression models@"This paper studies a very flexible model that can be used widely to analyze
the relation between a response and multiple covariates. The model is
nonparametric, yet renders easy interpretation for the effects of the
covariates. The model accommodates both continuous and discrete random
variables for the response and covariates. It is quite flexible to cover the
generalized varying coefficient models and the generalized additive models as
special cases. Under a weak condition we give a general theorem that the
problem of estimating the multivariate mean function is equivalent to that of
estimating its univariate component functions. We discuss implications of the
theorem for sieve and penalized least squares estimators, and then investigate
the outcomes in full details for a kernel-type estimator. The kernel estimator
is given as a solution of a system of nonlinear integral equations. We provide
an iterative algorithm to solve the system of equations and discuss the
theoretical properties of the estimator and the algorithm. Finally, we give
simulation results."@2012
Byeong U. Park@http://arxiv.org/abs/1210.4711v1@Flexible generalized varying coefficient regression models@"This paper studies a very flexible model that can be used widely to analyze
the relation between a response and multiple covariates. The model is
nonparametric, yet renders easy interpretation for the effects of the
covariates. The model accommodates both continuous and discrete random
variables for the response and covariates. It is quite flexible to cover the
generalized varying coefficient models and the generalized additive models as
special cases. Under a weak condition we give a general theorem that the
problem of estimating the multivariate mean function is equivalent to that of
estimating its univariate component functions. We discuss implications of the
theorem for sieve and penalized least squares estimators, and then investigate
the outcomes in full details for a kernel-type estimator. The kernel estimator
is given as a solution of a system of nonlinear integral equations. We provide
an iterative algorithm to solve the system of equations and discuss the
theoretical properties of the estimator and the algorithm. Finally, we give
simulation results."@2012
Randal Douc@http://arxiv.org/abs/1210.4739v2@"Ergodicity of observation-driven time series models and consistency of
  the maximum likelihood estimator"@"This paper deals with a general class of observation-driven time series
models with a special focus on time series of counts. We provide conditions
under which there exist strict-sense stationary and ergodic versions of such
processes. The consistency of the maximum likelihood estimators is then derived
for well- specified and misspecified models."@2012
Paul Doukhan@http://arxiv.org/abs/1210.4739v2@"Ergodicity of observation-driven time series models and consistency of
  the maximum likelihood estimator"@"This paper deals with a general class of observation-driven time series
models with a special focus on time series of counts. We provide conditions
under which there exist strict-sense stationary and ergodic versions of such
processes. The consistency of the maximum likelihood estimators is then derived
for well- specified and misspecified models."@2012
Eric Moulines@http://arxiv.org/abs/1210.4739v2@"Ergodicity of observation-driven time series models and consistency of
  the maximum likelihood estimator"@"This paper deals with a general class of observation-driven time series
models with a special focus on time series of counts. We provide conditions
under which there exist strict-sense stationary and ergodic versions of such
processes. The consistency of the maximum likelihood estimators is then derived
for well- specified and misspecified models."@2012
Angela Blanco-Fernández@http://arxiv.org/abs/1210.5881v1@"Extensions of linear regression models based on set arithmetic for
  interval data"@"Extensions of previous linear regression models for interval data are
presented. A more flexible simple linear model is formalized. The new model may
express cross-relationships between mid-points and spreads of the interval data
in a unique equation based on the interval arithmetic. Moreover, extensions to
the multiple case are addressed. The associated least-squares estimation
problem are solved. Empirical results and a real-life application are presented
in order to show the applicability and the differences among the proposed
models."@2012
Marta García-Bárzana@http://arxiv.org/abs/1210.5881v1@"Extensions of linear regression models based on set arithmetic for
  interval data"@"Extensions of previous linear regression models for interval data are
presented. A more flexible simple linear model is formalized. The new model may
express cross-relationships between mid-points and spreads of the interval data
in a unique equation based on the interval arithmetic. Moreover, extensions to
the multiple case are addressed. The associated least-squares estimation
problem are solved. Empirical results and a real-life application are presented
in order to show the applicability and the differences among the proposed
models."@2012
Ana Colubi@http://arxiv.org/abs/1210.5881v1@"Extensions of linear regression models based on set arithmetic for
  interval data"@"Extensions of previous linear regression models for interval data are
presented. A more flexible simple linear model is formalized. The new model may
express cross-relationships between mid-points and spreads of the interval data
in a unique equation based on the interval arithmetic. Moreover, extensions to
the multiple case are addressed. The associated least-squares estimation
problem are solved. Empirical results and a real-life application are presented
in order to show the applicability and the differences among the proposed
models."@2012
Erricos J. Kontoghiorghes@http://arxiv.org/abs/1210.5881v1@"Extensions of linear regression models based on set arithmetic for
  interval data"@"Extensions of previous linear regression models for interval data are
presented. A more flexible simple linear model is formalized. The new model may
express cross-relationships between mid-points and spreads of the interval data
in a unique equation based on the interval arithmetic. Moreover, extensions to
the multiple case are addressed. The associated least-squares estimation
problem are solved. Empirical results and a real-life application are presented
in order to show the applicability and the differences among the proposed
models."@2012
Loic Le Gratiet@http://arxiv.org/abs/1210.6187v2@"Kriging-based sequential design strategies using fast cross-validation
  techniques with extensions to multi-fidelity computer codes"@"Kriging-based surrogate models have become very popular during the last
decades to approximate a computer code output from few simulations. In
practical applications, it is very common to sequentially add new simulations
to obtain more accurate approximations. We propose in this paper a method of
kriging-based sequential design which combines both the error evaluation
providing by the kriging model and the observed errors of a Leave-One-Out
cross-validation procedure. This method is proposed in two versions, the first
one selects points one at-a-time. The second one allows us to parallelize the
simulations and to add several design points at-a-time. Then, we extend these
strategies to multi-fidelity co-kriging models which allow us to surrogate a
complex code using fast approximations of it. The main advantage of these
extensions is that it not only provides the new locations where to perform
simulations but also which versions of code have to be simulated (between the
complex one or one of its fast approximations). A real multi-fidelity
application is used to illustrate the efficiency of the proposed approaches. In
this example, the accurate code is a two-dimensional finite element model and
the less accurate one is a one-dimensional approximation of the system."@2012
Claire Cannamela@http://arxiv.org/abs/1210.6187v2@"Kriging-based sequential design strategies using fast cross-validation
  techniques with extensions to multi-fidelity computer codes"@"Kriging-based surrogate models have become very popular during the last
decades to approximate a computer code output from few simulations. In
practical applications, it is very common to sequentially add new simulations
to obtain more accurate approximations. We propose in this paper a method of
kriging-based sequential design which combines both the error evaluation
providing by the kriging model and the observed errors of a Leave-One-Out
cross-validation procedure. This method is proposed in two versions, the first
one selects points one at-a-time. The second one allows us to parallelize the
simulations and to add several design points at-a-time. Then, we extend these
strategies to multi-fidelity co-kriging models which allow us to surrogate a
complex code using fast approximations of it. The main advantage of these
extensions is that it not only provides the new locations where to perform
simulations but also which versions of code have to be simulated (between the
complex one or one of its fast approximations). A real multi-fidelity
application is used to illustrate the efficiency of the proposed approaches. In
this example, the accurate code is a two-dimensional finite element model and
the less accurate one is a one-dimensional approximation of the system."@2012
Bas Kleijn@http://arxiv.org/abs/1210.6204v3@Semiparametric posterior limits under local asymptotic exponentiality@"Consider semiparametric models that display local asymptotic exponentiality
(Ibragimov and Has'minskii (1981)), an asymptotic property of the likelihood
associated with discontinuities of densities. Our interest goes to estimation
of the location of such discontinuities while other aspects of the density form
a nuisance parameter. It is shown that under certain conditions on model and
prior, the posterior distribution displays Bernstein-von Mises-type asymptotic
behaviour, with exponential distributions as the limiting sequence. In contrast
to regular settings, the maximum likelihood estimator is inefficient under this
form of irregularity. However, Bayesian point estimators based on the limiting
posterior distribution attain the minimax risk. Therefore, the limiting
behaviour of the posterior is used to advocate efficiency of Bayesian point
estimation rather than compare it to frequentist estimation procedures based on
the maximum likelihood estimator. Results are applied to semiparametric LAE
location and scaling examples."@2012
Bartek Knapik@http://arxiv.org/abs/1210.6204v3@Semiparametric posterior limits under local asymptotic exponentiality@"Consider semiparametric models that display local asymptotic exponentiality
(Ibragimov and Has'minskii (1981)), an asymptotic property of the likelihood
associated with discontinuities of densities. Our interest goes to estimation
of the location of such discontinuities while other aspects of the density form
a nuisance parameter. It is shown that under certain conditions on model and
prior, the posterior distribution displays Bernstein-von Mises-type asymptotic
behaviour, with exponential distributions as the limiting sequence. In contrast
to regular settings, the maximum likelihood estimator is inefficient under this
form of irregularity. However, Bayesian point estimators based on the limiting
posterior distribution attain the minimax risk. Therefore, the limiting
behaviour of the posterior is used to advocate efficiency of Bayesian point
estimation rather than compare it to frequentist estimation procedures based on
the maximum likelihood estimator. Results are applied to semiparametric LAE
location and scaling examples."@2012
Francis Comets@http://arxiv.org/abs/1210.6328v2@"Maximum likelihood estimator consistency for ballistic random walk in a
  parametric random environment"@"We consider a one dimensional ballistic random walk evolving in an i.i.d.
parametric random environment. We provide a maximum likelihood estimation
procedure of the environment parameters based on a single observation of the
path till the time it reaches a distant site, and prove that this estimator is
consistent as the distant site tends to infinity. We also explore the numerical
performances of our estimation procedure."@2012
Mikael Falconnet@http://arxiv.org/abs/1210.6328v2@"Maximum likelihood estimator consistency for ballistic random walk in a
  parametric random environment"@"We consider a one dimensional ballistic random walk evolving in an i.i.d.
parametric random environment. We provide a maximum likelihood estimation
procedure of the environment parameters based on a single observation of the
path till the time it reaches a distant site, and prove that this estimator is
consistent as the distant site tends to infinity. We also explore the numerical
performances of our estimation procedure."@2012
Oleg Loukianov@http://arxiv.org/abs/1210.6328v2@"Maximum likelihood estimator consistency for ballistic random walk in a
  parametric random environment"@"We consider a one dimensional ballistic random walk evolving in an i.i.d.
parametric random environment. We provide a maximum likelihood estimation
procedure of the environment parameters based on a single observation of the
path till the time it reaches a distant site, and prove that this estimator is
consistent as the distant site tends to infinity. We also explore the numerical
performances of our estimation procedure."@2012
Dasha Loukianova@http://arxiv.org/abs/1210.6328v2@"Maximum likelihood estimator consistency for ballistic random walk in a
  parametric random environment"@"We consider a one dimensional ballistic random walk evolving in an i.i.d.
parametric random environment. We provide a maximum likelihood estimation
procedure of the environment parameters based on a single observation of the
path till the time it reaches a distant site, and prove that this estimator is
consistent as the distant site tends to infinity. We also explore the numerical
performances of our estimation procedure."@2012
Catherine Matias@http://arxiv.org/abs/1210.6328v2@"Maximum likelihood estimator consistency for ballistic random walk in a
  parametric random environment"@"We consider a one dimensional ballistic random walk evolving in an i.i.d.
parametric random environment. We provide a maximum likelihood estimation
procedure of the environment parameters based on a single observation of the
path till the time it reaches a distant site, and prove that this estimator is
consistent as the distant site tends to infinity. We also explore the numerical
performances of our estimation procedure."@2012
Alexander Jung@http://arxiv.org/abs/1210.6516v2@"The RKHS Approach to Minimum Variance Estimation Revisited: Variance
  Bounds, Sufficient Statistics, and Exponential Families"@"The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides
powerful tools for minimum variance estimation (MVE) problems. Here, we extend
the classical RKHS based analysis of MVE in several directions. We develop a
geometric formulation of five known lower bounds on the estimator variance
(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya
bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections
onto a subspace of the RKHS associated with a given MVE problem. We show that,
under mild conditions, the Barankin bound (the tightest possible lower bound on
the estimator variance) is a lower semicontinuous function of the parameter
vector. We also show that the RKHS associated with an MVE problem remains
unchanged if the observation is replaced by a sufficient statistic. Finally,
for MVE problems conforming to an exponential family of distributions, we
derive novel closed-form lower bound on the estimator variance and show that a
reduction of the parameter set leaves the minimum achievable variance
unchanged."@2012
Sebastian Schmutzhard@http://arxiv.org/abs/1210.6516v2@"The RKHS Approach to Minimum Variance Estimation Revisited: Variance
  Bounds, Sufficient Statistics, and Exponential Families"@"The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides
powerful tools for minimum variance estimation (MVE) problems. Here, we extend
the classical RKHS based analysis of MVE in several directions. We develop a
geometric formulation of five known lower bounds on the estimator variance
(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya
bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections
onto a subspace of the RKHS associated with a given MVE problem. We show that,
under mild conditions, the Barankin bound (the tightest possible lower bound on
the estimator variance) is a lower semicontinuous function of the parameter
vector. We also show that the RKHS associated with an MVE problem remains
unchanged if the observation is replaced by a sufficient statistic. Finally,
for MVE problems conforming to an exponential family of distributions, we
derive novel closed-form lower bound on the estimator variance and show that a
reduction of the parameter set leaves the minimum achievable variance
unchanged."@2012
Franz Hlawatsch@http://arxiv.org/abs/1210.6516v2@"The RKHS Approach to Minimum Variance Estimation Revisited: Variance
  Bounds, Sufficient Statistics, and Exponential Families"@"The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides
powerful tools for minimum variance estimation (MVE) problems. Here, we extend
the classical RKHS based analysis of MVE in several directions. We develop a
geometric formulation of five known lower bounds on the estimator variance
(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya
bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections
onto a subspace of the RKHS associated with a given MVE problem. We show that,
under mild conditions, the Barankin bound (the tightest possible lower bound on
the estimator variance) is a lower semicontinuous function of the parameter
vector. We also show that the RKHS associated with an MVE problem remains
unchanged if the observation is replaced by a sufficient statistic. Finally,
for MVE problems conforming to an exponential family of distributions, we
derive novel closed-form lower bound on the estimator variance and show that a
reduction of the parameter set leaves the minimum achievable variance
unchanged."@2012
F. Bartolucci@http://arxiv.org/abs/1210.6678v1@"Causal inference in paired two-arm experimental studies under
  non-compliance with application to prognosis of myocardial infarction"@"Motivated by a study about prompt coronary angiography in myocardial
infarction, we propose a method to estimate the causal effect of a treatment in
two-arm experimental studies with possible non-compliance in both treatment and
control arms. The method is based on a causal model for repeated binary
outcomes (before and after the treatment), which includes individual covariates
and latent variables for the unobserved heterogeneity between subjects.
Moreover, given the type of non-compliance, the model assumes the existence of
three subpopulations of subjects: compliers, never-takers, and always-takers.
The model is estimated by a two-step estimator: at the first step the
probability that a subject belongs to one of the three subpopulations is
estimated on the basis of the available covariates; at the second step the
causal effects are estimated through a conditional logistic method, the
implementation of which depends on the results from the first step. Standard
errors for this estimator are computed on the basis of a sandwich formula. The
application shows that prompt coronary angiography in patients with myocardial
infarction may significantly decrease the risk of other events within the next
two years, with a log-odds of about -2. Given that non-compliance is
significant for patients being given the treatment because of high risk
conditions, classical estimators fail to detect, or at least underestimate,
this effect."@2012
A. Farcomeni@http://arxiv.org/abs/1210.6678v1@"Causal inference in paired two-arm experimental studies under
  non-compliance with application to prognosis of myocardial infarction"@"Motivated by a study about prompt coronary angiography in myocardial
infarction, we propose a method to estimate the causal effect of a treatment in
two-arm experimental studies with possible non-compliance in both treatment and
control arms. The method is based on a causal model for repeated binary
outcomes (before and after the treatment), which includes individual covariates
and latent variables for the unobserved heterogeneity between subjects.
Moreover, given the type of non-compliance, the model assumes the existence of
three subpopulations of subjects: compliers, never-takers, and always-takers.
The model is estimated by a two-step estimator: at the first step the
probability that a subject belongs to one of the three subpopulations is
estimated on the basis of the available covariates; at the second step the
causal effects are estimated through a conditional logistic method, the
implementation of which depends on the results from the first step. Standard
errors for this estimator are computed on the basis of a sandwich formula. The
application shows that prompt coronary angiography in patients with myocardial
infarction may significantly decrease the risk of other events within the next
two years, with a log-odds of about -2. Given that non-compliance is
significant for patients being given the treatment because of high risk
conditions, classical estimators fail to detect, or at least underestimate,
this effect."@2012
Helena Ferreira@http://arxiv.org/abs/1210.7430v1@Extremal behavior of pMAX processes@"The well-known M4 processes of Smith and Weissman are very flexible models
for asymptotically dependent multivariate data. Extended M4 of Heffernan
\emph{et al.} allows to also account for asymptotic independence. In this paper
we introduce a more general multivariate model comprising asymptotic dependence
and independence, which has the extended M4 class as a particular case. We
study properties of the proposed model. In particular, we compute the
multivariate extremal index, tail dependence and extremal coefficients."@2012
Marta Ferreira@http://arxiv.org/abs/1210.7430v1@Extremal behavior of pMAX processes@"The well-known M4 processes of Smith and Weissman are very flexible models
for asymptotically dependent multivariate data. Extended M4 of Heffernan
\emph{et al.} allows to also account for asymptotic independence. In this paper
we introduce a more general multivariate model comprising asymptotic dependence
and independence, which has the extended M4 class as a particular case. We
study properties of the proposed model. In particular, we compute the
multivariate extremal index, tail dependence and extremal coefficients."@2012
Jérémie Bigot@http://arxiv.org/abs/1210.7640v2@Nonparametric adaptive time-dependent multivariate function estimation@"We consider the nonparametric estimation problem of time-dependent
multivariate functions observed in a presence of additive cylindrical Gaussian
white noise of a small intensity. We derive minimax lower bounds for the
$L^2$-risk in the proposed spatio-temporal model as the intensity goes to zero,
when the underlying unknown response function is assumed to belong to a ball of
appropriately constructed inhomogeneous time-dependent multivariate functions,
motivated by practical applications. Furthermore, we propose both non-adaptive
linear and adaptive non-linear wavelet estimators that are asymptotically
optimal (in the minimax sense) in a wide range of the so-constructed balls of
inhomogeneous time-dependent multivariate functions. The usefulness of the
suggested adaptive nonlinear wavelet estimator is illustrated with the help of
simulated and real-data examples."@2012
Theofanis Sapatinas@http://arxiv.org/abs/1210.7640v2@Nonparametric adaptive time-dependent multivariate function estimation@"We consider the nonparametric estimation problem of time-dependent
multivariate functions observed in a presence of additive cylindrical Gaussian
white noise of a small intensity. We derive minimax lower bounds for the
$L^2$-risk in the proposed spatio-temporal model as the intensity goes to zero,
when the underlying unknown response function is assumed to belong to a ball of
appropriately constructed inhomogeneous time-dependent multivariate functions,
motivated by practical applications. Furthermore, we propose both non-adaptive
linear and adaptive non-linear wavelet estimators that are asymptotically
optimal (in the minimax sense) in a wide range of the so-constructed balls of
inhomogeneous time-dependent multivariate functions. The usefulness of the
suggested adaptive nonlinear wavelet estimator is illustrated with the help of
simulated and real-data examples."@2012
R. Colombi@http://arxiv.org/abs/1210.8050v1@"A class of smooth models satisfying marginal and context specific
  conditional independencies"@"We study a class of conditional independence models for discrete data with
the property that one or more log-linear interactions are defined within two
different marginal distributions and then constrained to 0; all the conditional
independence models which are known to be non smooth belong to this class. We
introduce a new marginal log-linear parameterization and show that smoothness
may be restored by restricting one or more independence statements to hold
conditionally to a restricted subset of the configurations of the conditioning
variables. Our results are based on a specific reconstruction algorithm from
log-linear parameters to probabilities and fixed point theory. Several examples
are examined and a general rule for determining the implied conditional
independence restrictions is outlined."@2012
A. Forcina@http://arxiv.org/abs/1210.8050v1@"A class of smooth models satisfying marginal and context specific
  conditional independencies"@"We study a class of conditional independence models for discrete data with
the property that one or more log-linear interactions are defined within two
different marginal distributions and then constrained to 0; all the conditional
independence models which are known to be non smooth belong to this class. We
introduce a new marginal log-linear parameterization and show that smoothness
may be restored by restricting one or more independence statements to hold
conditionally to a restricted subset of the configurations of the conditioning
variables. Our results are based on a specific reconstruction algorithm from
log-linear parameters to probabilities and fixed point theory. Several examples
are examined and a general rule for determining the implied conditional
independence restrictions is outlined."@2012
Pengsheng Ji@http://arxiv.org/abs/1210.8162v1@Sharp adaptive nonparametric testing for Sobolev ellipsoids@"We consider testing for presence of a signal in Gaussian white noise with
intensity 1/sqrt(n), when the alternatives are given by smoothness ellipsoids
with an L2-ball of (squared) radius rho removed. It is known that, for a fixed
Sobolev type ellipsoid of smoothness beta and size M, a rho which is of order n
to the power -4 beta/(4 beta+1)} is the critical separation rate, in the sense
that the minimax error of second kind over alpha-tests stays asymptotically
between 0 and 1 strictly (Ingster, 1982). In addition, Ermakov (1990) found the
sharp asymptotics of the minimax error of second kind at the separation rate.
For adaptation over both beta and M in that context, it is known that a
loglog-penalty over the separation rate for rho is necessary for a nonzero
asymptotic power. Here, following an example in nonparametric estimation
related to the Pinsker constant, we investigate the adaptation problem over the
ellipsoid size M only, for fixed smoothness degree beta. It is established that
the sharp risk asymptotics can be replicated in that adaptive setting, if rho
tends to zero slower than the separation rate. The penalty for adaptation here
turns out to be a sequence tending to infinity arbitrarily slowly."@2012
Michael Nussbaum@http://arxiv.org/abs/1210.8162v1@Sharp adaptive nonparametric testing for Sobolev ellipsoids@"We consider testing for presence of a signal in Gaussian white noise with
intensity 1/sqrt(n), when the alternatives are given by smoothness ellipsoids
with an L2-ball of (squared) radius rho removed. It is known that, for a fixed
Sobolev type ellipsoid of smoothness beta and size M, a rho which is of order n
to the power -4 beta/(4 beta+1)} is the critical separation rate, in the sense
that the minimax error of second kind over alpha-tests stays asymptotically
between 0 and 1 strictly (Ingster, 1982). In addition, Ermakov (1990) found the
sharp asymptotics of the minimax error of second kind at the separation rate.
For adaptation over both beta and M in that context, it is known that a
loglog-penalty over the separation rate for rho is necessary for a nonzero
asymptotic power. Here, following an example in nonparametric estimation
related to the Pinsker constant, we investigate the adaptation problem over the
ellipsoid size M only, for fixed smoothness degree beta. It is established that
the sharp risk asymptotics can be replicated in that adaptive setting, if rho
tends to zero slower than the separation rate. The penalty for adaptation here
turns out to be a sequence tending to infinity arbitrarily slowly."@2012
Yingying Fan@http://arxiv.org/abs/1211.0457v1@Variable selection in linear mixed effects models@"This paper is concerned with the selection and estimation of fixed and random
effects in linear mixed effects models. We propose a class of nonconcave
penalized profile likelihood methods for selecting and estimating important
fixed effects. To overcome the difficulty of unknown covariance matrix of
random effects, we propose to use a proxy matrix in the penalized profile
likelihood. We establish conditions on the choice of the proxy matrix and show
that the proposed procedure enjoys the model selection consistency where the
number of fixed effects is allowed to grow exponentially with the sample size.
We further propose a group variable selection strategy to simultaneously select
and estimate important random effects, where the unknown covariance matrix of
random effects is replaced with a proxy matrix. We prove that, with the proxy
matrix appropriately chosen, the proposed procedure can identify all true
random effects with asymptotic probability one, where the dimension of random
effects vector is allowed to increase exponentially with the sample size. Monte
Carlo simulation studies are conducted to examine the finite-sample performance
of the proposed procedures. We further illustrate the proposed procedures via a
real data example."@2012
Runze Li@http://arxiv.org/abs/1211.0457v1@Variable selection in linear mixed effects models@"This paper is concerned with the selection and estimation of fixed and random
effects in linear mixed effects models. We propose a class of nonconcave
penalized profile likelihood methods for selecting and estimating important
fixed effects. To overcome the difficulty of unknown covariance matrix of
random effects, we propose to use a proxy matrix in the penalized profile
likelihood. We establish conditions on the choice of the proxy matrix and show
that the proposed procedure enjoys the model selection consistency where the
number of fixed effects is allowed to grow exponentially with the sample size.
We further propose a group variable selection strategy to simultaneously select
and estimate important random effects, where the unknown covariance matrix of
random effects is replaced with a proxy matrix. We prove that, with the proxy
matrix appropriately chosen, the proposed procedure can identify all true
random effects with asymptotic probability one, where the dimension of random
effects vector is allowed to increase exponentially with the sample size. Monte
Carlo simulation studies are conducted to examine the finite-sample performance
of the proposed procedures. We further illustrate the proposed procedures via a
real data example."@2012
T. Tony Cai@http://arxiv.org/abs/1211.0459v1@Adaptive covariance matrix estimation through block thresholding@"Estimation of large covariance matrices has drawn considerable recent
attention, and the theoretical focus so far has mainly been on developing a
minimax theory over a fixed parameter space. In this paper, we consider
adaptive covariance matrix estimation where the goal is to construct a single
procedure which is minimax rate optimal simultaneously over each parameter
space in a large collection. A fully data-driven block thresholding estimator
is proposed. The estimator is constructed by carefully dividing the sample
covariance matrix into blocks and then simultaneously estimating the entries in
a block by thresholding. The estimator is shown to be optimally rate adaptive
over a wide range of bandable covariance matrices. A simulation study is
carried out and shows that the block thresholding estimator performs well
numerically. Some of the technical tools developed in this paper can also be of
independent interest."@2012
Ming Yuan@http://arxiv.org/abs/1211.0459v1@Adaptive covariance matrix estimation through block thresholding@"Estimation of large covariance matrices has drawn considerable recent
attention, and the theoretical focus so far has mainly been on developing a
minimax theory over a fixed parameter space. In this paper, we consider
adaptive covariance matrix estimation where the goal is to construct a single
procedure which is minimax rate optimal simultaneously over each parameter
space in a large collection. A fully data-driven block thresholding estimator
is proposed. The estimator is constructed by carefully dividing the sample
covariance matrix into blocks and then simultaneously estimating the entries in
a block by thresholding. The estimator is shown to be optimally rate adaptive
over a wide range of bandable covariance matrices. A simulation study is
carried out and shows that the block thresholding estimator performs well
numerically. Some of the technical tools developed in this paper can also be of
independent interest."@2012
Herold Dehling@http://arxiv.org/abs/1211.0610v2@"Change Point Testing for the Drift Parameters of a Periodic Mean
  Reversion Process"@"In this paper we investigate the problem of detecting a change in the drift
parameters of a generalized Ornstein-Uhlenbeck process which is defined as the
solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in
continuous time. We derive an explicit representation of the generalized
likelihood ratio test statistic assuming that the mean reversion function
$L(t)$ is a finite linear combination of known basis functions. In the case of
a periodic mean reversion function, we determine the asymptotic distribution of
the test statistic under the null hypothesis."@2012
Brice Franke@http://arxiv.org/abs/1211.0610v2@"Change Point Testing for the Drift Parameters of a Periodic Mean
  Reversion Process"@"In this paper we investigate the problem of detecting a change in the drift
parameters of a generalized Ornstein-Uhlenbeck process which is defined as the
solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in
continuous time. We derive an explicit representation of the generalized
likelihood ratio test statistic assuming that the mean reversion function
$L(t)$ is a finite linear combination of known basis functions. In the case of
a periodic mean reversion function, we determine the asymptotic distribution of
the test statistic under the null hypothesis."@2012
Thomas Kott@http://arxiv.org/abs/1211.0610v2@"Change Point Testing for the Drift Parameters of a Periodic Mean
  Reversion Process"@"In this paper we investigate the problem of detecting a change in the drift
parameters of a generalized Ornstein-Uhlenbeck process which is defined as the
solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in
continuous time. We derive an explicit representation of the generalized
likelihood ratio test statistic assuming that the mean reversion function
$L(t)$ is a finite linear combination of known basis functions. In the case of
a periodic mean reversion function, we determine the asymptotic distribution of
the test statistic under the null hypothesis."@2012
Reg Kulperger@http://arxiv.org/abs/1211.0610v2@"Change Point Testing for the Drift Parameters of a Periodic Mean
  Reversion Process"@"In this paper we investigate the problem of detecting a change in the drift
parameters of a generalized Ornstein-Uhlenbeck process which is defined as the
solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in
continuous time. We derive an explicit representation of the generalized
likelihood ratio test statistic assuming that the mean reversion function
$L(t)$ is a finite linear combination of known basis functions. In the case of
a periodic mean reversion function, we determine the asymptotic distribution of
the test statistic under the null hypothesis."@2012
Christophe Giraud@http://arxiv.org/abs/1211.0811v1@"Discussion: Latent variable graphical model selection via convex
  optimization"@"Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290]."@2012
Alexandre Tsybakov@http://arxiv.org/abs/1211.0811v1@"Discussion: Latent variable graphical model selection via convex
  optimization"@"Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290]."@2012
Zhao Ren@http://arxiv.org/abs/1211.0813v1@"Discussion: Latent variable graphical model selection via convex
  optimization"@"Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290]."@2012
Harrison H. Zhou@http://arxiv.org/abs/1211.0813v1@"Discussion: Latent variable graphical model selection via convex
  optimization"@"Discussion of ""Latent variable graphical model selection via convex
optimization"" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky
[arXiv:1008.1290]."@2012
Ismaël Castillo@http://arxiv.org/abs/1211.1197v1@"Needles and Straw in a Haystack: Posterior concentration for possibly
  sparse sequences"@"We consider full Bayesian inference in the multivariate normal mean model in
the situation that the mean vector is sparse. The prior distribution on the
vector of means is constructed hierarchically by first choosing a collection of
nonzero means and next a prior on the nonzero values. We consider the posterior
distribution in the frequentist set-up that the observations are generated
according to a fixed mean vector, and are interested in the posterior
distribution of the number of nonzero components and the contraction of the
posterior distribution to the true mean vector. We find various combinations of
priors on the number of nonzero coefficients and on these coefficients that
give desirable performance. We also find priors that give suboptimal
convergence, for instance, Gaussian priors on the nonzero coefficients. We
illustrate the results by simulations."@2012
Aad van der Vaart@http://arxiv.org/abs/1211.1197v1@"Needles and Straw in a Haystack: Posterior concentration for possibly
  sparse sequences"@"We consider full Bayesian inference in the multivariate normal mean model in
the situation that the mean vector is sparse. The prior distribution on the
vector of means is constructed hierarchically by first choosing a collection of
nonzero means and next a prior on the nonzero values. We consider the posterior
distribution in the frequentist set-up that the observations are generated
according to a fixed mean vector, and are interested in the posterior
distribution of the number of nonzero components and the contraction of the
posterior distribution to the true mean vector. We find various combinations of
priors on the number of nonzero coefficients and on these coefficients that
give desirable performance. We also find priors that give suboptimal
convergence, for instance, Gaussian priors on the nonzero coefficients. We
illustrate the results by simulations."@2012
T. Tony Cai@http://arxiv.org/abs/1211.1309v4@Sparse PCA: Optimal rates and adaptive estimation@"Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. This paper considers
both minimax and adaptive estimation of the principal subspace in the high
dimensional setting. Under mild technical conditions, we first establish the
optimal rates of convergence for estimating the principal subspace which are
sharp with respect to all the parameters, thus providing a complete
characterization of the difficulty of the estimation problem in term of the
convergence rate. The lower bound is obtained by calculating the local metric
entropy and an application of Fano's lemma. The rate optimal estimator is
constructed using aggregation, which, however, might not be computationally
feasible. We then introduce an adaptive procedure for estimating the principal
subspace which is fully data driven and can be computed efficiently. It is
shown that the estimator attains the optimal rates of convergence
simultaneously over a large collection of the parameter spaces. A key idea in
our construction is a reduction scheme which reduces the sparse PCA problem to
a high-dimensional multivariate regression problem. This method is potentially
also useful for other related problems."@2012
Zongming Ma@http://arxiv.org/abs/1211.1309v4@Sparse PCA: Optimal rates and adaptive estimation@"Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. This paper considers
both minimax and adaptive estimation of the principal subspace in the high
dimensional setting. Under mild technical conditions, we first establish the
optimal rates of convergence for estimating the principal subspace which are
sharp with respect to all the parameters, thus providing a complete
characterization of the difficulty of the estimation problem in term of the
convergence rate. The lower bound is obtained by calculating the local metric
entropy and an application of Fano's lemma. The rate optimal estimator is
constructed using aggregation, which, however, might not be computationally
feasible. We then introduce an adaptive procedure for estimating the principal
subspace which is fully data driven and can be computed efficiently. It is
shown that the estimator attains the optimal rates of convergence
simultaneously over a large collection of the parameter spaces. A key idea in
our construction is a reduction scheme which reduces the sparse PCA problem to
a high-dimensional multivariate regression problem. This method is potentially
also useful for other related problems."@2012
Yihong Wu@http://arxiv.org/abs/1211.1309v4@Sparse PCA: Optimal rates and adaptive estimation@"Principal component analysis (PCA) is one of the most commonly used
statistical procedures with a wide range of applications. This paper considers
both minimax and adaptive estimation of the principal subspace in the high
dimensional setting. Under mild technical conditions, we first establish the
optimal rates of convergence for estimating the principal subspace which are
sharp with respect to all the parameters, thus providing a complete
characterization of the difficulty of the estimation problem in term of the
convergence rate. The lower bound is obtained by calculating the local metric
entropy and an application of Fano's lemma. The rate optimal estimator is
constructed using aggregation, which, however, might not be computationally
feasible. We then introduce an adaptive procedure for estimating the principal
subspace which is fully data driven and can be computed efficiently. It is
shown that the estimator attains the optimal rates of convergence
simultaneously over a large collection of the parameter spaces. A key idea in
our construction is a reduction scheme which reduces the sparse PCA problem to
a high-dimensional multivariate regression problem. This method is potentially
also useful for other related problems."@2012
Noureddine Berrahou@http://arxiv.org/abs/1211.1725v1@"Bahadur efficiency of nonparametric test for independence based on
  $L_1$-error"@"We introduce new test statistic to test the independence of two
multi-dimensional random variables. Based on the $L_1$-distance and the
historgram density estimation method, the test is compared via Bahadur relative
efficiency to several tests available in the literature. It arises that our
test reaches better performances than a number of usual tests among whom we
cite the Kolmogorov-Smirnov test. Beforehand, large deviation result is stated
for the associated statistic. The local asymptotic optimality relative to the
test is also studied."@2012
Lahcen Douge@http://arxiv.org/abs/1211.1725v1@"Bahadur efficiency of nonparametric test for independence based on
  $L_1$-error"@"We introduce new test statistic to test the independence of two
multi-dimensional random variables. Based on the $L_1$-distance and the
historgram density estimation method, the test is compared via Bahadur relative
efficiency to several tests available in the literature. It arises that our
test reaches better performances than a number of usual tests among whom we
cite the Kolmogorov-Smirnov test. Beforehand, large deviation result is stated
for the associated statistic. The local asymptotic optimality relative to the
test is also studied."@2012
Pierre Alquier@http://arxiv.org/abs/1211.1847v1@"Prediction of time series by statistical learning: general losses and
  fast rates"@"We establish rates of convergences in time series forecasting using the
statistical learning approach based on oracle inequalities. A series of papers
extends the oracle inequalities obtained for iid observations to time series
under weak dependence conditions. Given a family of predictors and $n$
observations, oracle inequalities state that a predictor forecasts the series
as well as the best predictor in the family up to a remainder term $\Delta_n$.
Using the PAC-Bayesian approach, we establish under weak dependence conditions
oracle inequalities with optimal rates of convergence. We extend previous
results for the absolute loss function to any Lipschitz loss function with
rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the
complexity of the model. We apply the method for quantile loss functions to
forecast the french GDP. Under additional conditions on the loss functions
(satisfied by the quadratic loss function) and on the time series, we refine
the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the
first time these fast rates for uniformly mixing processes. These rates are
known to be optimal in the iid case and for individual sequences. In
particular, we generalize the results of Dalalyan and Tsybakov on sparse
regression estimation to the case of autoregression."@2012
Xiaoyin Li@http://arxiv.org/abs/1211.1847v1@"Prediction of time series by statistical learning: general losses and
  fast rates"@"We establish rates of convergences in time series forecasting using the
statistical learning approach based on oracle inequalities. A series of papers
extends the oracle inequalities obtained for iid observations to time series
under weak dependence conditions. Given a family of predictors and $n$
observations, oracle inequalities state that a predictor forecasts the series
as well as the best predictor in the family up to a remainder term $\Delta_n$.
Using the PAC-Bayesian approach, we establish under weak dependence conditions
oracle inequalities with optimal rates of convergence. We extend previous
results for the absolute loss function to any Lipschitz loss function with
rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the
complexity of the model. We apply the method for quantile loss functions to
forecast the french GDP. Under additional conditions on the loss functions
(satisfied by the quadratic loss function) and on the time series, we refine
the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the
first time these fast rates for uniformly mixing processes. These rates are
known to be optimal in the iid case and for individual sequences. In
particular, we generalize the results of Dalalyan and Tsybakov on sparse
regression estimation to the case of autoregression."@2012
Olivier Wintenberger@http://arxiv.org/abs/1211.1847v1@"Prediction of time series by statistical learning: general losses and
  fast rates"@"We establish rates of convergences in time series forecasting using the
statistical learning approach based on oracle inequalities. A series of papers
extends the oracle inequalities obtained for iid observations to time series
under weak dependence conditions. Given a family of predictors and $n$
observations, oracle inequalities state that a predictor forecasts the series
as well as the best predictor in the family up to a remainder term $\Delta_n$.
Using the PAC-Bayesian approach, we establish under weak dependence conditions
oracle inequalities with optimal rates of convergence. We extend previous
results for the absolute loss function to any Lipschitz loss function with
rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the
complexity of the model. We apply the method for quantile loss functions to
forecast the french GDP. Under additional conditions on the loss functions
(satisfied by the quadratic loss function) and on the time series, we refine
the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the
first time these fast rates for uniformly mixing processes. These rates are
known to be optimal in the iid case and for individual sequences. In
particular, we generalize the results of Dalalyan and Tsybakov on sparse
regression estimation to the case of autoregression."@2012
Xinjia Chen@http://arxiv.org/abs/1211.1912v3@"Exact Sample Size Methods for Estimating Parameters of Discrete
  Distributions"@"In this paper, we develop an approach for the exact determination of the
minimum sample size for estimating the parameter of an integer-valued random
variable, which is parameterized by its expectation. Under some continuity and
unimodal property assumptions, the exact computation is accomplished by
reducing infinite many evaluations of coverage probability to finite many
evaluations. Such a reduction is based on our discovery that the minimum of
coverage probability with respect to the parameter bounded in an interval is
attained at a discrete set of finite many values."@2012
Zhengjia Chen@http://arxiv.org/abs/1211.1912v3@"Exact Sample Size Methods for Estimating Parameters of Discrete
  Distributions"@"In this paper, we develop an approach for the exact determination of the
minimum sample size for estimating the parameter of an integer-valued random
variable, which is parameterized by its expectation. Under some continuity and
unimodal property assumptions, the exact computation is accomplished by
reducing infinite many evaluations of coverage probability to finite many
evaluations. Such a reduction is based on our discovery that the minimum of
coverage probability with respect to the parameter bounded in an interval is
attained at a discrete set of finite many values."@2012
Christian Francq@http://arxiv.org/abs/1211.2060v2@GARCH models without positivity constraints: Exponential or Log GARCH?@"This paper provides a probabilistic and statistical comparison of the
log-GARCH and EGARCH models, which both rely on multiplicative volatility
dynamics without positivity constraints. We compare the main probabilistic
properties (strict stationarity, existence of moments, tails) of the EGARCH
model, which are already known, with those of an asymmetric version of the
log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters
is shown to be strongly consistent and asymptotically normal. Similar
estimation results are only available for particular EGARCH models, and under
much stronger assumptions. The comparison is pursued via simulation experiments
and estimation on real data."@2012
Olivier Wintenberger@http://arxiv.org/abs/1211.2060v2@GARCH models without positivity constraints: Exponential or Log GARCH?@"This paper provides a probabilistic and statistical comparison of the
log-GARCH and EGARCH models, which both rely on multiplicative volatility
dynamics without positivity constraints. We compare the main probabilistic
properties (strict stationarity, existence of moments, tails) of the EGARCH
model, which are already known, with those of an asymmetric version of the
log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters
is shown to be strongly consistent and asymptotically normal. Similar
estimation results are only available for particular EGARCH models, and under
much stronger assumptions. The comparison is pursued via simulation experiments
and estimation on real data."@2012
Jean-Michel Zakoïan@http://arxiv.org/abs/1211.2060v2@GARCH models without positivity constraints: Exponential or Log GARCH?@"This paper provides a probabilistic and statistical comparison of the
log-GARCH and EGARCH models, which both rely on multiplicative volatility
dynamics without positivity constraints. We compare the main probabilistic
properties (strict stationarity, existence of moments, tails) of the EGARCH
model, which are already known, with those of an asymmetric version of the
log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters
is shown to be strongly consistent and asymptotically normal. Similar
estimation results are only available for particular EGARCH models, and under
much stronger assumptions. The comparison is pursued via simulation experiments
and estimation on real data."@2012
Gourab Mukherjee@http://arxiv.org/abs/1211.2071v4@"Exact minimax estimation of the predictive density in sparse Gaussian
  models"@"We consider estimating the predictive density under Kullback-Leibler loss in
an $\ell_0$ sparse Gaussian sequence model. Explicit expressions of the first
order minimax risk along with its exact constant, asymptotically least
favorable priors and optimal predictive density estimates are derived. Compared
to the sparse recovery results involving point estimation of the normal mean,
new decision theoretic phenomena are seen. Suboptimal performance of the class
of plug-in density estimates reflects the predictive nature of the problem and
optimal strategies need diversification of the future risk. We find that
minimax optimal strategies lie outside the Gaussian family but can be
constructed with threshold predictive density estimates. Novel minimax
techniques involving simultaneous calibration of the sparsity adjustment and
the risk diversification mechanisms are used to design optimal predictive
density estimates."@2012
Iain M. Johnstone@http://arxiv.org/abs/1211.2071v4@"Exact minimax estimation of the predictive density in sparse Gaussian
  models"@"We consider estimating the predictive density under Kullback-Leibler loss in
an $\ell_0$ sparse Gaussian sequence model. Explicit expressions of the first
order minimax risk along with its exact constant, asymptotically least
favorable priors and optimal predictive density estimates are derived. Compared
to the sparse recovery results involving point estimation of the normal mean,
new decision theoretic phenomena are seen. Suboptimal performance of the class
of plug-in density estimates reflects the predictive nature of the problem and
optimal strategies need diversification of the future risk. We find that
minimax optimal strategies lie outside the Gaussian family but can be
constructed with threshold predictive density estimates. Novel minimax
techniques involving simultaneous calibration of the sparsity adjustment and
the risk diversification mechanisms are used to design optimal predictive
density estimates."@2012
Marc Hallin@http://arxiv.org/abs/1211.2117v1@Optimal rank-based testing for principal components@"This paper provides parametric and rank-based optimal tests for eigenvectors
and eigenvalues of covariance or scatter matrices in elliptical families. The
parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)
and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,
1983). The rank-based tests address a much broader class of problems, where
covariance matrices need not exist and principal components are associated with
more general scatter matrices. The proposed tests are shown to outperform daily
practice both from the point of view of validity as from the point of view of
efficiency. This is achieved by utilizing the Le Cam theory of locally
asymptotically normal experiments, in the nonstandard context, however, of a
curved parametrization. The results we derive for curved experiments are of
independent interest, and likely to apply in other contexts."@2012
Davy Paindaveine@http://arxiv.org/abs/1211.2117v1@Optimal rank-based testing for principal components@"This paper provides parametric and rank-based optimal tests for eigenvectors
and eigenvalues of covariance or scatter matrices in elliptical families. The
parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)
and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,
1983). The rank-based tests address a much broader class of problems, where
covariance matrices need not exist and principal components are associated with
more general scatter matrices. The proposed tests are shown to outperform daily
practice both from the point of view of validity as from the point of view of
efficiency. This is achieved by utilizing the Le Cam theory of locally
asymptotically normal experiments, in the nonstandard context, however, of a
curved parametrization. The results we derive for curved experiments are of
independent interest, and likely to apply in other contexts."@2012
Thomas Verdebout@http://arxiv.org/abs/1211.2117v1@Optimal rank-based testing for principal components@"This paper provides parametric and rank-based optimal tests for eigenvectors
and eigenvalues of covariance or scatter matrices in elliptical families. The
parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)
and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,
1983). The rank-based tests address a much broader class of problems, where
covariance matrices need not exist and principal components are associated with
more general scatter matrices. The proposed tests are shown to outperform daily
practice both from the point of view of validity as from the point of view of
efficiency. This is achieved by utilizing the Le Cam theory of locally
asymptotically normal experiments, in the nonstandard context, however, of a
curved parametrization. The results we derive for curved experiments are of
independent interest, and likely to apply in other contexts."@2012
R. de Jonge@http://arxiv.org/abs/1211.2121v1@"Adaptive nonparametric Bayesian inference using location-scale mixture
  priors"@"We study location-scale mixture priors for nonparametric statistical
problems, including multivariate regression, density estimation and
classification. We show that a rate-adaptive procedure can be obtained if the
prior is properly constructed. In particular, we show that adaptation is
achieved if a kernel mixture prior on a regression function is constructed
using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing
weights."@2012
J. H. van Zanten@http://arxiv.org/abs/1211.2121v1@"Adaptive nonparametric Bayesian inference using location-scale mixture
  priors"@"We study location-scale mixture priors for nonparametric statistical
problems, including multivariate regression, density estimation and
classification. We show that a rate-adaptive procedure can be obtained if the
prior is properly constructed. In particular, we show that adaptation is
achieved if a kernel mixture prior on a regression function is constructed
using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing
weights."@2012
Yehua Li@http://arxiv.org/abs/1211.2137v1@"Uniform convergence rates for nonparametric regression and principal
  component analysis in functional/longitudinal data"@"We consider nonparametric estimation of the mean and covariance functions for
functional/longitudinal data. Strong uniform convergence rates are developed
for estimators that are local-linear smoothers. Our results are obtained in a
unified framework in which the number of observations within each curve/cluster
can be of any rate relative to the sample size. We show that the convergence
rates for the procedures depend on both the number of sample curves and the
number of observations on each curve. For sparse functional data, these rates
are equivalent to the optimal rates in nonparametric regression. For dense
functional data, root-n rates of convergence can be achieved with proper
choices of bandwidths. We further derive almost sure rates of convergence for
principal component analysis using the estimated covariance function. The
results are illustrated with simulation studies."@2012
Tailen Hsing@http://arxiv.org/abs/1211.2137v1@"Uniform convergence rates for nonparametric regression and principal
  component analysis in functional/longitudinal data"@"We consider nonparametric estimation of the mean and covariance functions for
functional/longitudinal data. Strong uniform convergence rates are developed
for estimators that are local-linear smoothers. Our results are obtained in a
unified framework in which the number of observations within each curve/cluster
can be of any rate relative to the sample size. We show that the convergence
rates for the procedures depend on both the number of sample curves and the
number of observations on each curve. For sparse functional data, these rates
are equivalent to the optimal rates in nonparametric regression. For dense
functional data, root-n rates of convergence can be achieved with proper
choices of bandwidths. We further derive almost sure rates of convergence for
principal component analysis using the estimated covariance function. The
results are illustrated with simulation studies."@2012
Delphine Blanke@http://arxiv.org/abs/1211.2300v2@Bayesian prediction for stochastic processes. Theory and applications@"In this paper, we adopt a Bayesian point of view for predicting real
continuous-time processes. We give two equivalent definitions of a Bayesian
predictor and study some properties: admissibility, prediction sufficiency,
non-unbiasedness, comparison with efficient predictors. Prediction of Poisson
process and prediction of Ornstein-Uhlenbeck process in the continuous and
sampled situations are considered. Various simulations illustrate comparison
with non-Bayesian predictors."@2012
Denis Bosq@http://arxiv.org/abs/1211.2300v2@Bayesian prediction for stochastic processes. Theory and applications@"In this paper, we adopt a Bayesian point of view for predicting real
continuous-time processes. We give two equivalent definitions of a Bayesian
predictor and study some properties: admissibility, prediction sufficiency,
non-unbiasedness, comparison with efficient predictors. Prediction of Poisson
process and prediction of Ornstein-Uhlenbeck process in the continuous and
sampled situations are considered. Various simulations illustrate comparison
with non-Bayesian predictors."@2012
Neil Bathia@http://arxiv.org/abs/1211.2522v1@Identifying the finite dimensionality of curve time series@"The curve time series framework provides a convenient vehicle to accommodate
some nonstationary features into a stationary setup. We propose a new method to
identify the dimensionality of curve time series based on the dynamical
dependence across different curves. The practical implementation of our method
boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the
determination of the dimensionality is equivalent to the identification of the
nonzero eigenvalues of the matrix, which we carry out in terms of some
bootstrap tests. Asymptotic properties of the proposed method are investigated.
In particular, our estimators for zero-eigenvalues enjoy the fast convergence
rate n while the estimators for nonzero eigenvalues converge at the standard
$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated
and real data sets."@2012
Qiwei Yao@http://arxiv.org/abs/1211.2522v1@Identifying the finite dimensionality of curve time series@"The curve time series framework provides a convenient vehicle to accommodate
some nonstationary features into a stationary setup. We propose a new method to
identify the dimensionality of curve time series based on the dynamical
dependence across different curves. The practical implementation of our method
boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the
determination of the dimensionality is equivalent to the identification of the
nonzero eigenvalues of the matrix, which we carry out in terms of some
bootstrap tests. Asymptotic properties of the proposed method are investigated.
In particular, our estimators for zero-eigenvalues enjoy the fast convergence
rate n while the estimators for nonzero eigenvalues converge at the standard
$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated
and real data sets."@2012
Flavio Ziegelmann@http://arxiv.org/abs/1211.2522v1@Identifying the finite dimensionality of curve time series@"The curve time series framework provides a convenient vehicle to accommodate
some nonstationary features into a stationary setup. We propose a new method to
identify the dimensionality of curve time series based on the dynamical
dependence across different curves. The practical implementation of our method
boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the
determination of the dimensionality is equivalent to the identification of the
nonzero eigenvalues of the matrix, which we carry out in terms of some
bootstrap tests. Asymptotic properties of the proposed method are investigated.
In particular, our estimators for zero-eigenvalues enjoy the fast convergence
rate n while the estimators for nonzero eigenvalues converge at the standard
$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated
and real data sets."@2012
Anthony Brockwell@http://arxiv.org/abs/1211.2582v1@Sequentially interacting Markov chain Monte Carlo methods@"Sequential Monte Carlo (SMC) is a methodology for sampling approximately from
a sequence of probability distributions of increasing dimension and estimating
their normalizing constants. We propose here an alternative methodology named
Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work
by generating interacting non-Markovian sequences which behave asymptotically
like independent Metropolis-Hastings (MH) Markov chains with the desired
limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively
improve our estimates in an MCMC-like fashion. We establish convergence results
under realistic verifiable assumptions and demonstrate its performance on
several examples arising in Bayesian time series analysis."@2012
Pierre Del Moral@http://arxiv.org/abs/1211.2582v1@Sequentially interacting Markov chain Monte Carlo methods@"Sequential Monte Carlo (SMC) is a methodology for sampling approximately from
a sequence of probability distributions of increasing dimension and estimating
their normalizing constants. We propose here an alternative methodology named
Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work
by generating interacting non-Markovian sequences which behave asymptotically
like independent Metropolis-Hastings (MH) Markov chains with the desired
limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively
improve our estimates in an MCMC-like fashion. We establish convergence results
under realistic verifiable assumptions and demonstrate its performance on
several examples arising in Bayesian time series analysis."@2012
Arnaud Doucet@http://arxiv.org/abs/1211.2582v1@Sequentially interacting Markov chain Monte Carlo methods@"Sequential Monte Carlo (SMC) is a methodology for sampling approximately from
a sequence of probability distributions of increasing dimension and estimating
their normalizing constants. We propose here an alternative methodology named
Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work
by generating interacting non-Markovian sequences which behave asymptotically
like independent Metropolis-Hastings (MH) Markov chains with the desired
limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively
improve our estimates in an MCMC-like fashion. We establish convergence results
under realistic verifiable assumptions and demonstrate its performance on
several examples arising in Bayesian time series analysis."@2012
Ming Yuan@http://arxiv.org/abs/1211.2607v1@"A reproducing kernel Hilbert space approach to functional linear
  regression"@"We study in this paper a smoothness regularization method for functional
linear regression and provide a unified treatment for both the prediction and
estimation problems. By developing a tool on simultaneous diagonalization of
two positive definite kernels, we obtain shaper results on the minimax rates of
convergence and show that smoothness regularized estimators achieve the optimal
rates of convergence for both prediction and estimation under conditions weaker
than those for the functional principal components based methods developed in
the literature. Despite the generality of the method of regularization, we show
that the procedure is easily implementable. Numerical results are obtained to
illustrate the merits of the method and to demonstrate the theoretical
developments."@2012
T. Tony Cai@http://arxiv.org/abs/1211.2607v1@"A reproducing kernel Hilbert space approach to functional linear
  regression"@"We study in this paper a smoothness regularization method for functional
linear regression and provide a unified treatment for both the prediction and
estimation problems. By developing a tool on simultaneous diagonalization of
two positive definite kernels, we obtain shaper results on the minimax rates of
convergence and show that smoothness regularized estimators achieve the optimal
rates of convergence for both prediction and estimation under conditions weaker
than those for the functional principal components based methods developed in
the literature. Despite the generality of the method of regularization, we show
that the procedure is easily implementable. Numerical results are obtained to
illustrate the merits of the method and to demonstrate the theoretical
developments."@2012
Hans-Georg Müller@http://arxiv.org/abs/1211.2630v1@Empirical dynamics for longitudinal data@"We demonstrate that the processes underlying on-line auction price bids and
many other longitudinal data can be represented by an empirical first order
stochastic ordinary differential equation with time-varying coefficients and a
smooth drift process. This equation may be empirically obtained from
longitudinal observations for a sample of subjects and does not presuppose
specific knowledge of the underlying processes. For the nonparametric
estimation of the components of the differential equation, it suffices to have
available sparsely observed longitudinal measurements which may be noisy and
are generated by underlying smooth random trajectories for each subject or
experimental unit in the sample. The drift process that drives the equation
determines how closely individual process trajectories follow a deterministic
approximation of the differential equation. We provide estimates for
trajectories and especially the variance function of the drift process. At each
fixed time point, the proposed empirical dynamic model implies a decomposition
of the derivative of the process underlying the longitudinal data into a
component explained by a linear component determined by a varying coefficient
function dynamic equation and an orthogonal complement that corresponds to the
drift process. An enhanced perturbation result enables us to obtain improved
asymptotic convergence rates for eigenfunction derivative estimation and
consistency for the varying coefficient function and the components of the
drift process. We illustrate the differential equation with an application to
the dynamics of on-line auction data."@2012
Fang Yao@http://arxiv.org/abs/1211.2630v1@Empirical dynamics for longitudinal data@"We demonstrate that the processes underlying on-line auction price bids and
many other longitudinal data can be represented by an empirical first order
stochastic ordinary differential equation with time-varying coefficients and a
smooth drift process. This equation may be empirically obtained from
longitudinal observations for a sample of subjects and does not presuppose
specific knowledge of the underlying processes. For the nonparametric
estimation of the components of the differential equation, it suffices to have
available sparsely observed longitudinal measurements which may be noisy and
are generated by underlying smooth random trajectories for each subject or
experimental unit in the sample. The drift process that drives the equation
determines how closely individual process trajectories follow a deterministic
approximation of the differential equation. We provide estimates for
trajectories and especially the variance function of the drift process. At each
fixed time point, the proposed empirical dynamic model implies a decomposition
of the derivative of the process underlying the longitudinal data into a
component explained by a linear component determined by a varying coefficient
function dynamic equation and an orthogonal complement that corresponds to the
drift process. An enhanced perturbation result enables us to obtain improved
asymptotic convergence rates for eigenfunction derivative estimation and
consistency for the varying coefficient function and the components of the
drift process. We illustrate the differential equation with an application to
the dynamics of on-line auction data."@2012
Dan Shen@http://arxiv.org/abs/1211.2671v6@A General Framework For Consistency of Principal Component Analysis@"A general asymptotic framework is developed for studying consis- tency
properties of principal component analysis (PCA). Our frame- work includes
several previously studied domains of asymptotics as special cases and allows
one to investigate interesting connections and transitions among the various
domains. More importantly, it enables us to investigate asymptotic scenarios
that have not been considered before, and gain new insights into the
consistency, subspace consistency and strong inconsistency regions of PCA and
the boundaries among them. We also establish the corresponding convergence rate
within each region. Under general spike covariance models, the dimension (or
the number of variables) discourages the consistency of PCA, while the sample
size and spike information (the relative size of the population eigenvalues)
encourages PCA consistency. Our framework nicely illustrates the relationship
among these three types of information in terms of dimension, sample size and
spike size, and rigorously characterizes how their relationships affect PCA
consistency."@2012
Haipeng Shen@http://arxiv.org/abs/1211.2671v6@A General Framework For Consistency of Principal Component Analysis@"A general asymptotic framework is developed for studying consis- tency
properties of principal component analysis (PCA). Our frame- work includes
several previously studied domains of asymptotics as special cases and allows
one to investigate interesting connections and transitions among the various
domains. More importantly, it enables us to investigate asymptotic scenarios
that have not been considered before, and gain new insights into the
consistency, subspace consistency and strong inconsistency regions of PCA and
the boundaries among them. We also establish the corresponding convergence rate
within each region. Under general spike covariance models, the dimension (or
the number of variables) discourages the consistency of PCA, while the sample
size and spike information (the relative size of the population eigenvalues)
encourages PCA consistency. Our framework nicely illustrates the relationship
among these three types of information in terms of dimension, sample size and
spike size, and rigorously characterizes how their relationships affect PCA
consistency."@2012
J. S. Marron@http://arxiv.org/abs/1211.2671v6@A General Framework For Consistency of Principal Component Analysis@"A general asymptotic framework is developed for studying consis- tency
properties of principal component analysis (PCA). Our frame- work includes
several previously studied domains of asymptotics as special cases and allows
one to investigate interesting connections and transitions among the various
domains. More importantly, it enables us to investigate asymptotic scenarios
that have not been considered before, and gain new insights into the
consistency, subspace consistency and strong inconsistency regions of PCA and
the boundaries among them. We also establish the corresponding convergence rate
within each region. Under general spike covariance models, the dimension (or
the number of variables) discourages the consistency of PCA, while the sample
size and spike information (the relative size of the population eigenvalues)
encourages PCA consistency. Our framework nicely illustrates the relationship
among these three types of information in terms of dimension, sample size and
spike size, and rigorously characterizes how their relationships affect PCA
consistency."@2012
Delphine Blanke@http://arxiv.org/abs/1211.2763v2@"Global smoothness estimation of a Gaussian process from regular sequence
  designs"@"We consider a real Gaussian process $X$ having a global unknown smoothness
$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$,
$r_{\scriptscriptstyle 0}\in \mathds{N}_0$ and $\beta_{\scriptscriptstyle 0}
\in]0,1[$, with $X^{(r_{\scriptscriptstyle 0})}$ (the mean-square derivative of
$X$ if $r_{\scriptscriptstyle 0}\ge 1$) supposed to be locally stationary with
index $\beta_{\scriptscriptstyle 0}$. From the behavior of quadratic variations
built on divided differences of $X$, we derive an estimator of
$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$ based on - not
necessarily equally spaced - observations of $X$. Various numerical studies of
these estimators exhibit their properties for finite sample size and different
types of processes, and are also completed by two examples of application to
real data."@2012
Céline Vial@http://arxiv.org/abs/1211.2763v2@"Global smoothness estimation of a Gaussian process from regular sequence
  designs"@"We consider a real Gaussian process $X$ having a global unknown smoothness
$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$,
$r_{\scriptscriptstyle 0}\in \mathds{N}_0$ and $\beta_{\scriptscriptstyle 0}
\in]0,1[$, with $X^{(r_{\scriptscriptstyle 0})}$ (the mean-square derivative of
$X$ if $r_{\scriptscriptstyle 0}\ge 1$) supposed to be locally stationary with
index $\beta_{\scriptscriptstyle 0}$. From the behavior of quadratic variations
built on divided differences of $X$, we derive an estimator of
$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$ based on - not
necessarily equally spaced - observations of $X$. Various numerical studies of
these estimators exhibit their properties for finite sample size and different
types of processes, and are also completed by two examples of application to
real data."@2012
Aboubacar Amiri@http://arxiv.org/abs/1211.2780v2@"Recursive estimation of nonparametric regression with functional
  covariate"@"The main purpose is to estimate the regression function of a real random
variable with functional explanatory variable by using a recursive
nonparametric kernel approach. The mean square error and the almost sure
convergence of a family of recursive kernel estimates of the regression
function are derived. These results are established with rates and precise
evaluation of the constant terms. Also, a central limit theorem for this class
of estimators is established. The method is evaluated on simulations and real
data set studies."@2012
Christophe Crambes@http://arxiv.org/abs/1211.2780v2@"Recursive estimation of nonparametric regression with functional
  covariate"@"The main purpose is to estimate the regression function of a real random
variable with functional explanatory variable by using a recursive
nonparametric kernel approach. The mean square error and the almost sure
convergence of a family of recursive kernel estimates of the regression
function are derived. These results are established with rates and precise
evaluation of the constant terms. Also, a central limit theorem for this class
of estimators is established. The method is evaluated on simulations and real
data set studies."@2012
Baba Thiam@http://arxiv.org/abs/1211.2780v2@"Recursive estimation of nonparametric regression with functional
  covariate"@"The main purpose is to estimate the regression function of a real random
variable with functional explanatory variable by using a recursive
nonparametric kernel approach. The mean square error and the almost sure
convergence of a family of recursive kernel estimates of the regression
function are derived. These results are established with rates and precise
evaluation of the constant terms. Also, a central limit theorem for this class
of estimators is established. The method is evaluated on simulations and real
data set studies."@2012
Seunggeun Lee@http://arxiv.org/abs/1211.2970v1@"Convergence and prediction of principal component scores in
  high-dimensional settings"@"A number of settings arise in which it is of interest to predict Principal
Component (PC) scores for new observations using data from an initial sample.
In this paper, we demonstrate that naive approaches to PC score prediction can
be substantially biased toward 0 in the analysis of large matrices. This
phenomenon is largely related to known inconsistency results for sample
eigenvalues and eigenvectors as both dimensions of the matrix increase. For the
spiked eigenvalue model for random matrices, we expand the generality of these
results, and propose bias-adjusted PC score prediction. In addition, we compute
the asymptotic correlation coefficient between PC scores from sample and
population eigenvectors. Simulation and real data examples from the genetics
literature show the improved bias and numerical properties of our estimators."@2012
Fei Zou@http://arxiv.org/abs/1211.2970v1@"Convergence and prediction of principal component scores in
  high-dimensional settings"@"A number of settings arise in which it is of interest to predict Principal
Component (PC) scores for new observations using data from an initial sample.
In this paper, we demonstrate that naive approaches to PC score prediction can
be substantially biased toward 0 in the analysis of large matrices. This
phenomenon is largely related to known inconsistency results for sample
eigenvalues and eigenvectors as both dimensions of the matrix increase. For the
spiked eigenvalue model for random matrices, we expand the generality of these
results, and propose bias-adjusted PC score prediction. In addition, we compute
the asymptotic correlation coefficient between PC scores from sample and
population eigenvectors. Simulation and real data examples from the genetics
literature show the improved bias and numerical properties of our estimators."@2012
Fred A. Wright@http://arxiv.org/abs/1211.2970v1@"Convergence and prediction of principal component scores in
  high-dimensional settings"@"A number of settings arise in which it is of interest to predict Principal
Component (PC) scores for new observations using data from an initial sample.
In this paper, we demonstrate that naive approaches to PC score prediction can
be substantially biased toward 0 in the analysis of large matrices. This
phenomenon is largely related to known inconsistency results for sample
eigenvalues and eigenvectors as both dimensions of the matrix increase. For the
spiked eigenvalue model for random matrices, we expand the generality of these
results, and propose bias-adjusted PC score prediction. In addition, we compute
the asymptotic correlation coefficient between PC scores from sample and
population eigenvectors. Simulation and real data examples from the genetics
literature show the improved bias and numerical properties of our estimators."@2012
Song Xi Chen@http://arxiv.org/abs/1211.2979v1@ANOVA for longitudinal data with missing values@"We carry out ANOVA comparisons of multiple treatments for longitudinal
studies with missing values. The treatment effects are modeled
semiparametrically via a partially linear regression which is flexible in
quantifying the time effects of treatments. The empirical likelihood is
employed to formulate model-robust nonparametric ANOVA tests for treatment
effects with respect to covariates, the nonparametric time-effect functions and
interactions between covariates and time. The proposed tests can be readily
modified for a variety of data and model combinations, that encompasses
parametric, semiparametric and nonparametric regression models; cross-sectional
and longitudinal data, and with or without missing values."@2012
Ping-Shou Zhong@http://arxiv.org/abs/1211.2979v1@ANOVA for longitudinal data with missing values@"We carry out ANOVA comparisons of multiple treatments for longitudinal
studies with missing values. The treatment effects are modeled
semiparametrically via a partially linear regression which is flexible in
quantifying the time effects of treatments. The empirical likelihood is
employed to formulate model-robust nonparametric ANOVA tests for treatment
effects with respect to covariates, the nonparametric time-effect functions and
interactions between covariates and time. The proposed tests can be readily
modified for a variety of data and model combinations, that encompasses
parametric, semiparametric and nonparametric regression models; cross-sectional
and longitudinal data, and with or without missing values."@2012
Vladimir Koltchinskii@http://arxiv.org/abs/1211.2998v1@Sparsity in multiple kernel learning@"The problem of multiple kernel learning based on penalized empirical risk
minimization is discussed. The complexity penalty is determined jointly by the
empirical $L_2$ norms and the reproducing kernel Hilbert space (RKHS) norms
induced by the kernels with a data-driven choice of regularization parameters.
The main focus is on the case when the total number of kernels is large, but
only a relatively small number of them is needed to represent the target
function, so that the problem is sparse. The goal is to establish oracle
inequalities for the excess risk of the resulting prediction rule showing that
the method is adaptive both to the unknown design distribution and to the
sparsity of the problem."@2012
Ming Yuan@http://arxiv.org/abs/1211.2998v1@Sparsity in multiple kernel learning@"The problem of multiple kernel learning based on penalized empirical risk
minimization is discussed. The complexity penalty is determined jointly by the
empirical $L_2$ norms and the reproducing kernel Hilbert space (RKHS) norms
induced by the kernels with a data-driven choice of regularization parameters.
The main focus is on the case when the total number of kernels is large, but
only a relatively small number of them is needed to represent the target
function, so that the problem is sparse. The goal is to establish oracle
inequalities for the excess risk of the resulting prediction rule showing that
the method is adaptive both to the unknown design distribution and to the
sparsity of the problem."@2012
Xin Chen@http://arxiv.org/abs/1211.3215v1@"Coordinate-independent sparse sufficient dimension reduction and
  variable selection"@"Sufficient dimension reduction (SDR) in regression, which reduces the
dimension by replacing original predictors with a minimal set of their linear
combinations without loss of information, is very helpful when the number of
predictors is large. The standard SDR methods suffer because the estimated
linear combinations usually consist of all original predictors, making it
difficult to interpret. In this paper, we propose a unified method -
coordinate-independent sparse estimation (CISE) - that can simultaneously
achieve sparse sufficient dimension reduction and screen out irrelevant and
redundant variables efficiently. CISE is subspace oriented in the sense that it
incorporates a coordinate-independent penalty term with a broad series of
model-based and model-free SDR approaches. This results in a Grassmann manifold
optimization problem and a fast algorithm is suggested. Under mild conditions,
based on manifold theories and techniques, it can be shown that CISE would
perform asymptotically as well as if the true irrelevant predictors were known,
which is referred to as the oracle property. Simulation studies and a real-data
example demonstrate the effectiveness and efficiency of the proposed approach."@2012
Changliang Zou@http://arxiv.org/abs/1211.3215v1@"Coordinate-independent sparse sufficient dimension reduction and
  variable selection"@"Sufficient dimension reduction (SDR) in regression, which reduces the
dimension by replacing original predictors with a minimal set of their linear
combinations without loss of information, is very helpful when the number of
predictors is large. The standard SDR methods suffer because the estimated
linear combinations usually consist of all original predictors, making it
difficult to interpret. In this paper, we propose a unified method -
coordinate-independent sparse estimation (CISE) - that can simultaneously
achieve sparse sufficient dimension reduction and screen out irrelevant and
redundant variables efficiently. CISE is subspace oriented in the sense that it
incorporates a coordinate-independent penalty term with a broad series of
model-based and model-free SDR approaches. This results in a Grassmann manifold
optimization problem and a fast algorithm is suggested. Under mild conditions,
based on manifold theories and techniques, it can be shown that CISE would
perform asymptotically as well as if the true irrelevant predictors were known,
which is referred to as the oracle property. Simulation studies and a real-data
example demonstrate the effectiveness and efficiency of the proposed approach."@2012
R. Dennis Cook@http://arxiv.org/abs/1211.3215v1@"Coordinate-independent sparse sufficient dimension reduction and
  variable selection"@"Sufficient dimension reduction (SDR) in regression, which reduces the
dimension by replacing original predictors with a minimal set of their linear
combinations without loss of information, is very helpful when the number of
predictors is large. The standard SDR methods suffer because the estimated
linear combinations usually consist of all original predictors, making it
difficult to interpret. In this paper, we propose a unified method -
coordinate-independent sparse estimation (CISE) - that can simultaneously
achieve sparse sufficient dimension reduction and screen out irrelevant and
redundant variables efficiently. CISE is subspace oriented in the sense that it
incorporates a coordinate-independent penalty term with a broad series of
model-based and model-free SDR approaches. This results in a Grassmann manifold
optimization problem and a fast algorithm is suggested. Under mild conditions,
based on manifold theories and techniques, it can be shown that CISE would
perform asymptotically as well as if the true irrelevant predictors were known,
which is referred to as the oracle property. Simulation studies and a real-data
example demonstrate the effectiveness and efficiency of the proposed approach."@2012
Bing-Yi Jing@http://arxiv.org/abs/1211.3230v1@"Nonparametric estimate of spectral density functions of sample
  covariance matrices: A first step"@"The density function of the limiting spectral distribution of general sample
covariance matrices is usually unknown. We propose to use kernel estimators
which are proved to be consistent. A simulation study is also conducted to show
the performance of the estimators."@2012
Guangming Pan@http://arxiv.org/abs/1211.3230v1@"Nonparametric estimate of spectral density functions of sample
  covariance matrices: A first step"@"The density function of the limiting spectral distribution of general sample
covariance matrices is usually unknown. We propose to use kernel estimators
which are proved to be consistent. A simulation study is also conducted to show
the performance of the estimators."@2012
Qi-Man Shao@http://arxiv.org/abs/1211.3230v1@"Nonparametric estimate of spectral density functions of sample
  covariance matrices: A first step"@"The density function of the limiting spectral distribution of general sample
covariance matrices is usually unknown. We propose to use kernel estimators
which are proved to be consistent. A simulation study is also conducted to show
the performance of the estimators."@2012
Wang Zhou@http://arxiv.org/abs/1211.3230v1@"Nonparametric estimate of spectral density functions of sample
  covariance matrices: A first step"@"The density function of the limiting spectral distribution of general sample
covariance matrices is usually unknown. We propose to use kernel estimators
which are proved to be consistent. A simulation study is also conducted to show
the performance of the estimators."@2012
Jelle J. Goeman@http://arxiv.org/abs/1211.3313v1@The sequential rejection principle of familywise error control@"Closed testing and partitioning are recognized as fundamental principles of
familywise error control. In this paper, we argue that sequential rejection can
be considered equally fundamental as a general principle of multiple testing.
We present a general sequentially rejective multiple testing procedure and show
that many well-known familywise error controlling methods can be constructed as
special cases of this procedure, among which are the procedures of Holm,
Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern
procedures for multiple testing in graphs, resampling-based multiple testing
procedures and even the closed testing and partitioning procedures themselves.
We also give a general proof that sequentially rejective multiple testing
procedures strongly control the familywise error if they fulfill simple
criteria of monotonicity of the critical values and a limited form of weak
familywise error control in each single step. The sequential rejection
principle gives a novel theoretical perspective on many well-known multiple
testing procedures, emphasizing the sequential aspect. Its main practical
usefulness is for the development of multiple testing procedures for null
hypotheses, possibly logically related, that are structured in a graph. We
illustrate this by presenting a uniform improvement of a recently published
procedure."@2012
Aldo Solari@http://arxiv.org/abs/1211.3313v1@The sequential rejection principle of familywise error control@"Closed testing and partitioning are recognized as fundamental principles of
familywise error control. In this paper, we argue that sequential rejection can
be considered equally fundamental as a general principle of multiple testing.
We present a general sequentially rejective multiple testing procedure and show
that many well-known familywise error controlling methods can be constructed as
special cases of this procedure, among which are the procedures of Holm,
Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern
procedures for multiple testing in graphs, resampling-based multiple testing
procedures and even the closed testing and partitioning procedures themselves.
We also give a general proof that sequentially rejective multiple testing
procedures strongly control the familywise error if they fulfill simple
criteria of monotonicity of the critical values and a limited form of weak
familywise error control in each single step. The sequential rejection
principle gives a novel theoretical perspective on many well-known multiple
testing procedures, emphasizing the sequential aspect. Its main practical
usefulness is for the development of multiple testing procedures for null
hypotheses, possibly logically related, that are structured in a graph. We
illustrate this by presenting a uniform improvement of a recently published
procedure."@2012
Olga Klopp@http://arxiv.org/abs/1211.3394v2@Non-asymptotic approach to varying coefficient model@"In the present paper we consider the varying coefficient model which
represents a useful tool for exploring dynamic patterns in many applications.
Existing methods typically provide asymptotic evaluation of precision of
estimation procedures under the assumption that the number of observations
tends to infinity. In practical applications, however, only a finite number of
measurements are available. In the present paper we focus on a non-asymptotic
approach to the problem. We propose a novel estimation procedure which is based
on recent developments in matrix estimation. In particular, for our estimator,
we obtain upper bounds for the mean squared and the pointwise estimation
errors. The obtained oracle inequalities are non-asymptotic and hold for finite
sample size."@2012
Marianna Pensky@http://arxiv.org/abs/1211.3394v2@Non-asymptotic approach to varying coefficient model@"In the present paper we consider the varying coefficient model which
represents a useful tool for exploring dynamic patterns in many applications.
Existing methods typically provide asymptotic evaluation of precision of
estimation procedures under the assumption that the number of observations
tends to infinity. In practical applications, however, only a finite number of
measurements are available. In the present paper we focus on a non-asymptotic
approach to the problem. We propose a novel estimation procedure which is based
on recent developments in matrix estimation. In particular, for our estimator,
we obtain upper bounds for the mean squared and the pointwise estimation
errors. The obtained oracle inequalities are non-asymptotic and hold for finite
sample size."@2012
Natalia A. Bochkina@http://arxiv.org/abs/1211.3434v5@The Bernstein-von Mises theorem and nonregular models@"We study the asymptotic behaviour of the posterior distribution in a broad
class of statistical models where the ""true"" solution occurs on the boundary of
the parameter space. We show that in this case Bayesian inference is
consistent, and that the posterior distribution has not only Gaussian
components as in the case of regular models (the Bernstein-von Mises theorem)
but also has Gamma distribution components whose form depends on the behaviour
of the prior distribution near the boundary and have a faster rate of
convergence. We also demonstrate a remarkable property of Bayesian inference,
that for some models, there appears to be no bound on efficiency of estimating
the unknown parameter if it is on the boundary of the parameter space. We
illustrate the results on a problem from emission tomography."@2012
Peter J. Green@http://arxiv.org/abs/1211.3434v5@The Bernstein-von Mises theorem and nonregular models@"We study the asymptotic behaviour of the posterior distribution in a broad
class of statistical models where the ""true"" solution occurs on the boundary of
the parameter space. We show that in this case Bayesian inference is
consistent, and that the posterior distribution has not only Gaussian
components as in the case of regular models (the Bernstein-von Mises theorem)
but also has Gamma distribution components whose form depends on the behaviour
of the prior distribution near the boundary and have a faster rate of
convergence. We also demonstrate a remarkable property of Bayesian inference,
that for some models, there appears to be no bound on efficiency of estimating
the unknown parameter if it is on the boundary of the parameter space. We
illustrate the results on a problem from emission tomography."@2012
Hua Liang@http://arxiv.org/abs/1211.3509v1@Estimation and testing for partially linear single-index models@"In partially linear single-index models, we obtain the semiparametrically
efficient profile least-squares estimators of regression coefficients. We also
employ the smoothly clipped absolute deviation penalty (SCAD) approach to
simultaneously select variables and estimate regression coefficients. We show
that the resulting SCAD estimators are consistent and possess the oracle
property. Subsequently, we demonstrate that a proposed tuning parameter
selector, BIC, identifies the true model consistently. Finally, we develop a
linear hypothesis test for the parametric coefficients and a goodness-of-fit
test for the nonparametric component, respectively. Monte Carlo studies are
also presented."@2012
Xiang Liu@http://arxiv.org/abs/1211.3509v1@Estimation and testing for partially linear single-index models@"In partially linear single-index models, we obtain the semiparametrically
efficient profile least-squares estimators of regression coefficients. We also
employ the smoothly clipped absolute deviation penalty (SCAD) approach to
simultaneously select variables and estimate regression coefficients. We show
that the resulting SCAD estimators are consistent and possess the oracle
property. Subsequently, we demonstrate that a proposed tuning parameter
selector, BIC, identifies the true model consistently. Finally, we develop a
linear hypothesis test for the parametric coefficients and a goodness-of-fit
test for the nonparametric component, respectively. Monte Carlo studies are
also presented."@2012
Runze Li@http://arxiv.org/abs/1211.3509v1@Estimation and testing for partially linear single-index models@"In partially linear single-index models, we obtain the semiparametrically
efficient profile least-squares estimators of regression coefficients. We also
employ the smoothly clipped absolute deviation penalty (SCAD) approach to
simultaneously select variables and estimate regression coefficients. We show
that the resulting SCAD estimators are consistent and possess the oracle
property. Subsequently, we demonstrate that a proposed tuning parameter
selector, BIC, identifies the true model consistently. Finally, we develop a
linear hypothesis test for the parametric coefficients and a goodness-of-fit
test for the nonparametric component, respectively. Monte Carlo studies are
also presented."@2012
Chih-Ling Tsai@http://arxiv.org/abs/1211.3509v1@Estimation and testing for partially linear single-index models@"In partially linear single-index models, we obtain the semiparametrically
efficient profile least-squares estimators of regression coefficients. We also
employ the smoothly clipped absolute deviation penalty (SCAD) approach to
simultaneously select variables and estimate regression coefficients. We show
that the resulting SCAD estimators are consistent and possess the oracle
property. Subsequently, we demonstrate that a proposed tuning parameter
selector, BIC, identifies the true model consistently. Finally, we develop a
linear hypothesis test for the parametric coefficients and a goodness-of-fit
test for the nonparametric component, respectively. Monte Carlo studies are
also presented."@2012
Chris Lloyd@http://arxiv.org/abs/1211.3594v1@"Letter to the Editor: Some comments on: On construction of the smallest
  one-sided confidence interval for the difference of two proportions"@"Letter to the Editor: Some comments on ""On construction of the smallest
one-sided confidence interval for the difference of two proportions"" by Weizhen
Wang [arXiv:1002.4945]."@2012
Paul Kabaila@http://arxiv.org/abs/1211.3594v1@"Letter to the Editor: Some comments on: On construction of the smallest
  one-sided confidence interval for the difference of two proportions"@"Letter to the Editor: Some comments on ""On construction of the smallest
one-sided confidence interval for the difference of two proportions"" by Weizhen
Wang [arXiv:1002.4945]."@2012
Arnaud Guyader@http://arxiv.org/abs/1211.3930v2@A Geometrical Approach to Iterative Isotone Regression@"In the present paper, we propose and analyze a novel method for estimating a
univariate regression function of bounded variation. The underpinning idea is
to combine two classical tools in nonparametric statistics, namely isotonic
regression and the estimation of additive models. A geometrical interpretation
enables us to link this iterative method with Von Neumann's algorithm.
Moreover, making a connection with the general property of isotonicity of
projection onto convex cones, we derive another equivalent algorithm and go
further in the analysis. As iterating the algorithm leads to overfitting,
several practical stopping criteria are also presented and discussed."@2012
Nicolas Jégou@http://arxiv.org/abs/1211.3930v2@A Geometrical Approach to Iterative Isotone Regression@"In the present paper, we propose and analyze a novel method for estimating a
univariate regression function of bounded variation. The underpinning idea is
to combine two classical tools in nonparametric statistics, namely isotonic
regression and the estimation of additive models. A geometrical interpretation
enables us to link this iterative method with Von Neumann's algorithm.
Moreover, making a connection with the general property of isotonicity of
projection onto convex cones, we derive another equivalent algorithm and go
further in the analysis. As iterating the algorithm leads to overfitting,
several practical stopping criteria are also presented and discussed."@2012
Alexander B. Németh@http://arxiv.org/abs/1211.3930v2@A Geometrical Approach to Iterative Isotone Regression@"In the present paper, we propose and analyze a novel method for estimating a
univariate regression function of bounded variation. The underpinning idea is
to combine two classical tools in nonparametric statistics, namely isotonic
regression and the estimation of additive models. A geometrical interpretation
enables us to link this iterative method with Von Neumann's algorithm.
Moreover, making a connection with the general property of isotonicity of
projection onto convex cones, we derive another equivalent algorithm and go
further in the analysis. As iterating the algorithm leads to overfitting,
several practical stopping criteria are also presented and discussed."@2012
Sándor Z. Németh@http://arxiv.org/abs/1211.3930v2@A Geometrical Approach to Iterative Isotone Regression@"In the present paper, we propose and analyze a novel method for estimating a
univariate regression function of bounded variation. The underpinning idea is
to combine two classical tools in nonparametric statistics, namely isotonic
regression and the estimation of additive models. A geometrical interpretation
enables us to link this iterative method with Von Neumann's algorithm.
Moreover, making a connection with the general property of isotonicity of
projection onto convex cones, we derive another equivalent algorithm and go
further in the analysis. As iterating the algorithm leads to overfitting,
several practical stopping criteria are also presented and discussed."@2012
Elena Chernousova@http://arxiv.org/abs/1211.4207v1@Ordered Smoothers With Exponential Weighting@"The main goal in this paper is to propose a new method for deriving oracle
inequalities related to the exponential weighting method. For the sake of
simplicity we focus on recovering an unknown vector from noisy data with the
help of a family of ordered smoothers. The estimators withing this family are
aggregated using the exponential weighting and the aim is to control the risk
of the aggregated estimate. Based on simple probabilistic properties of the
unbiased risk estimate, we derive new oracle inequalities and show that the
exponential weighting permits to improve Kneip's oracle inequality."@2012
Yuri Golubev@http://arxiv.org/abs/1211.4207v1@Ordered Smoothers With Exponential Weighting@"The main goal in this paper is to propose a new method for deriving oracle
inequalities related to the exponential weighting method. For the sake of
simplicity we focus on recovering an unknown vector from noisy data with the
help of a family of ordered smoothers. The estimators withing this family are
aggregated using the exponential weighting and the aim is to control the risk
of the aggregated estimate. Based on simple probabilistic properties of the
unbiased risk estimate, we derive new oracle inequalities and show that the
exponential weighting permits to improve Kneip's oracle inequality."@2012
Katerina Krymova@http://arxiv.org/abs/1211.4207v1@Ordered Smoothers With Exponential Weighting@"The main goal in this paper is to propose a new method for deriving oracle
inequalities related to the exponential weighting method. For the sake of
simplicity we focus on recovering an unknown vector from noisy data with the
help of a family of ordered smoothers. The estimators withing this family are
aggregated using the exponential weighting and the aim is to control the risk
of the aggregated estimate. Based on simple probabilistic properties of the
unbiased risk estimate, we derive new oracle inequalities and show that the
exponential weighting permits to improve Kneip's oracle inequality."@2012
Claudia Kirch@http://arxiv.org/abs/1211.4732v1@"TFT-bootstrap: Resampling time series in the frequency domain to obtain
  replicates in the time domain"@"A new time series bootstrap scheme, the time frequency toggle
(TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier
coefficients of the observed time series, and then to back-transform them to
obtain a bootstrap sample in the time domain. Related previous proposals, such
as the ""surrogate data"" approach, resampled only the phase of the Fourier
coefficients and thus had only limited validity. By contrast, we show that the
appropriate resampling of phase and magnitude, in addition to some smoothing of
Fourier coefficients, yields a bootstrap scheme that mimics the correct
second-order moment structure for a large class of time series processes. As a
main result we obtain a functional limit theorem for the TFT-bootstrap under a
variety of popular ways of frequency domain bootstrapping. Possible
applications of the TFT-bootstrap naturally arise in change-point analysis and
unit-root testing where statistics are frequently based on functionals of
partial sums. Finally, a small simulation study explores the potential of the
TFT-bootstrap for small samples showing that for the discussed tests in
change-point analysis as well as unit-root testing, it yields better results
than the corresponding asymptotic tests if measured by size and power."@2012
Dimitris N. Politis@http://arxiv.org/abs/1211.4732v1@"TFT-bootstrap: Resampling time series in the frequency domain to obtain
  replicates in the time domain"@"A new time series bootstrap scheme, the time frequency toggle
(TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier
coefficients of the observed time series, and then to back-transform them to
obtain a bootstrap sample in the time domain. Related previous proposals, such
as the ""surrogate data"" approach, resampled only the phase of the Fourier
coefficients and thus had only limited validity. By contrast, we show that the
appropriate resampling of phase and magnitude, in addition to some smoothing of
Fourier coefficients, yields a bootstrap scheme that mimics the correct
second-order moment structure for a large class of time series processes. As a
main result we obtain a functional limit theorem for the TFT-bootstrap under a
variety of popular ways of frequency domain bootstrapping. Possible
applications of the TFT-bootstrap naturally arise in change-point analysis and
unit-root testing where statistics are frequently based on functionals of
partial sums. Finally, a small simulation study explores the potential of the
TFT-bootstrap for small samples showing that for the discussed tests in
change-point analysis as well as unit-root testing, it yields better results
than the corresponding asymptotic tests if measured by size and power."@2012
Ngai Hang Chan@http://arxiv.org/abs/1211.4962v1@"Uniform moment bounds of Fisher's information with applications to time
  series"@"In this paper, a uniform (over some parameter space) moment bound for the
inverse of Fisher's information matrix is established. This result is then
applied to develop moment bounds for the normalized least squares estimate in
(nonlinear) stochastic regression models. The usefulness of these results is
illustrated using time series models. In particular, an asymptotic expression
for the mean squared prediction error of the least squares predictor in
autoregressive moving average models is obtained. This asymptotic expression
provides a solid theoretical foundation for some model selection criteria."@2012
Ching-Kang Ing@http://arxiv.org/abs/1211.4962v1@"Uniform moment bounds of Fisher's information with applications to time
  series"@"In this paper, a uniform (over some parameter space) moment bound for the
inverse of Fisher's information matrix is established. This result is then
applied to develop moment bounds for the normalized least squares estimate in
(nonlinear) stochastic regression models. The usefulness of these results is
illustrated using time series models. In particular, an asymptotic expression
for the mean squared prediction error of the least squares predictor in
autoregressive moving average models is obtained. This asymptotic expression
provides a solid theoretical foundation for some model selection criteria."@2012
Dong Chen@http://arxiv.org/abs/1211.5018v1@"Single and multiple index functional regression models with
  nonparametric link"@"Fully nonparametric methods for regression from functional data have poor
accuracy from a statistical viewpoint, reflecting the fact that their
convergence rates are slower than nonparametric rates for the estimation of
high-dimensional functions. This difficulty has led to an emphasis on the
so-called functional linear model, which is much more flexible than common
linear models in finite dimension, but nevertheless imposes structural
constraints on the relationship between predictors and responses. Recent
advances have extended the linear approach by using it in conjunction with link
functions, and by considering multiple indices, but the flexibility of this
technique is still limited. For example, the link may be modeled parametrically
or on a grid only, or may be constrained by an assumption such as monotonicity;
multiple indices have been modeled by making finite-dimensional assumptions. In
this paper we introduce a new technique for estimating the link function
nonparametrically, and we suggest an approach to multi-index modeling using
adaptively defined linear projections of functional data. We show that our
methods enable prediction with polynomial convergence rates. The finite sample
performance of our methods is studied in simulations, and is illustrated by an
application to a functional regression problem."@2012
Peter Hall@http://arxiv.org/abs/1211.5018v1@"Single and multiple index functional regression models with
  nonparametric link"@"Fully nonparametric methods for regression from functional data have poor
accuracy from a statistical viewpoint, reflecting the fact that their
convergence rates are slower than nonparametric rates for the estimation of
high-dimensional functions. This difficulty has led to an emphasis on the
so-called functional linear model, which is much more flexible than common
linear models in finite dimension, but nevertheless imposes structural
constraints on the relationship between predictors and responses. Recent
advances have extended the linear approach by using it in conjunction with link
functions, and by considering multiple indices, but the flexibility of this
technique is still limited. For example, the link may be modeled parametrically
or on a grid only, or may be constrained by an assumption such as monotonicity;
multiple indices have been modeled by making finite-dimensional assumptions. In
this paper we introduce a new technique for estimating the link function
nonparametrically, and we suggest an approach to multi-index modeling using
adaptively defined linear projections of functional data. We show that our
methods enable prediction with polynomial convergence rates. The finite sample
performance of our methods is studied in simulations, and is illustrated by an
application to a functional regression problem."@2012
Hans-Georg Müller@http://arxiv.org/abs/1211.5018v1@"Single and multiple index functional regression models with
  nonparametric link"@"Fully nonparametric methods for regression from functional data have poor
accuracy from a statistical viewpoint, reflecting the fact that their
convergence rates are slower than nonparametric rates for the estimation of
high-dimensional functions. This difficulty has led to an emphasis on the
so-called functional linear model, which is much more flexible than common
linear models in finite dimension, but nevertheless imposes structural
constraints on the relationship between predictors and responses. Recent
advances have extended the linear approach by using it in conjunction with link
functions, and by considering multiple indices, but the flexibility of this
technique is still limited. For example, the link may be modeled parametrically
or on a grid only, or may be constrained by an assumption such as monotonicity;
multiple indices have been modeled by making finite-dimensional assumptions. In
this paper we introduce a new technique for estimating the link function
nonparametrically, and we suggest an approach to multi-index modeling using
adaptively defined linear projections of functional data. We show that our
methods enable prediction with polynomial convergence rates. The finite sample
performance of our methods is studied in simulations, and is illustrated by an
application to a functional regression problem."@2012
Yacine Aït-Sahalia@http://arxiv.org/abs/1211.5219v1@Testing whether jumps have finite or infinite activity@"We propose statistical tests to discriminate between the finite and infinite
activity of jumps in a semimartingale discretely observed at high frequency.
The two statistics allow for a symmetric treatment of the problem: we can
either take the null hypothesis to be finite activity, or infinite activity.
When implemented on high-frequency stock returns, both tests point toward the
presence of infinite-activity jumps in the data."@2012
Jean Jacod@http://arxiv.org/abs/1211.5219v1@Testing whether jumps have finite or infinite activity@"We propose statistical tests to discriminate between the finite and infinite
activity of jumps in a semimartingale discretely observed at high frequency.
The two statistics allow for a symmetric treatment of the problem: we can
either take the null hypothesis to be finite activity, or infinite activity.
When implemented on high-frequency stock returns, both tests point toward the
presence of infinite-activity jumps in the data."@2012
Xia Cui@http://arxiv.org/abs/1211.5220v1@The EFM approach for single-index models@"Single-index models are natural extensions of linear models and circumvent
the so-called curse of dimensionality. They are becoming increasingly popular
in many scientific fields including biostatistics, medicine, economics and
financial econometrics. Estimating and testing the model index coefficients
$\bolds{\beta}$ is one of the most important objectives in the statistical
analysis. However, the commonly used assumption on the index coefficients,
$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on
the boundary of the unit ball. In this paper we introduce the EFM approach, a
method of estimating functions, to study the single-index model. The procedure
is to first relax the equality constraint to one with (d-1) components of
$\bolds{\beta}$ lying in an open unit ball, and then to construct the
associated (d-1) estimating functions by projecting the score function to the
linear space spanned by the residuals with the unknown link being estimated by
kernel estimating functions. The root-n consistency and asymptotic normality
for the estimator obtained from solving the resulting estimating equations are
achieved, and a Wilks type theorem for testing the index is demonstrated. A
noticeable result we obtain is that our estimator for $\bolds{\beta}$ has
smaller or equal limiting variance than the estimator of Carroll et al. [J.
Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for
computing this estimator is proposed. This algorithm only involves
one-dimensional nonparametric smoothers, thereby avoiding the data sparsity
problem caused by high model dimensionality. Numerical studies based on
simulation and on applications suggest that this new estimating system is quite
powerful and easy to implement."@2012
Wolfgang Karl Härdle@http://arxiv.org/abs/1211.5220v1@The EFM approach for single-index models@"Single-index models are natural extensions of linear models and circumvent
the so-called curse of dimensionality. They are becoming increasingly popular
in many scientific fields including biostatistics, medicine, economics and
financial econometrics. Estimating and testing the model index coefficients
$\bolds{\beta}$ is one of the most important objectives in the statistical
analysis. However, the commonly used assumption on the index coefficients,
$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on
the boundary of the unit ball. In this paper we introduce the EFM approach, a
method of estimating functions, to study the single-index model. The procedure
is to first relax the equality constraint to one with (d-1) components of
$\bolds{\beta}$ lying in an open unit ball, and then to construct the
associated (d-1) estimating functions by projecting the score function to the
linear space spanned by the residuals with the unknown link being estimated by
kernel estimating functions. The root-n consistency and asymptotic normality
for the estimator obtained from solving the resulting estimating equations are
achieved, and a Wilks type theorem for testing the index is demonstrated. A
noticeable result we obtain is that our estimator for $\bolds{\beta}$ has
smaller or equal limiting variance than the estimator of Carroll et al. [J.
Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for
computing this estimator is proposed. This algorithm only involves
one-dimensional nonparametric smoothers, thereby avoiding the data sparsity
problem caused by high model dimensionality. Numerical studies based on
simulation and on applications suggest that this new estimating system is quite
powerful and easy to implement."@2012
Lixing Zhu@http://arxiv.org/abs/1211.5220v1@The EFM approach for single-index models@"Single-index models are natural extensions of linear models and circumvent
the so-called curse of dimensionality. They are becoming increasingly popular
in many scientific fields including biostatistics, medicine, economics and
financial econometrics. Estimating and testing the model index coefficients
$\bolds{\beta}$ is one of the most important objectives in the statistical
analysis. However, the commonly used assumption on the index coefficients,
$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on
the boundary of the unit ball. In this paper we introduce the EFM approach, a
method of estimating functions, to study the single-index model. The procedure
is to first relax the equality constraint to one with (d-1) components of
$\bolds{\beta}$ lying in an open unit ball, and then to construct the
associated (d-1) estimating functions by projecting the score function to the
linear space spanned by the residuals with the unknown link being estimated by
kernel estimating functions. The root-n consistency and asymptotic normality
for the estimator obtained from solving the resulting estimating equations are
achieved, and a Wilks type theorem for testing the index is demonstrated. A
noticeable result we obtain is that our estimator for $\bolds{\beta}$ has
smaller or equal limiting variance than the estimator of Carroll et al. [J.
Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for
computing this estimator is proposed. This algorithm only involves
one-dimensional nonparametric smoothers, thereby avoiding the data sparsity
problem caused by high model dimensionality. Numerical studies based on
simulation and on applications suggest that this new estimating system is quite
powerful and easy to implement."@2012
Juan-Juan Cai@http://arxiv.org/abs/1211.5239v1@Estimation of extreme risk regions under multivariate regular variation@"When considering d possibly dependent random variables, one is often
interested in extreme risk regions, with very small probability p. We consider
risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,
where f is the joint density and $\beta$ a small number. Estimation of such an
extreme risk region is difficult since it contains hardly any or no data. Using
extreme value theory, we construct a natural estimator of an extreme risk
region and prove a refined form of consistency, given a random sample of
multivariate regularly varying random vectors. In a detailed simulation and
comparison study, the good performance of the procedure is demonstrated. We
also apply our estimator to financial data."@2012
John H. J. Einmahl@http://arxiv.org/abs/1211.5239v1@Estimation of extreme risk regions under multivariate regular variation@"When considering d possibly dependent random variables, one is often
interested in extreme risk regions, with very small probability p. We consider
risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,
where f is the joint density and $\beta$ a small number. Estimation of such an
extreme risk region is difficult since it contains hardly any or no data. Using
extreme value theory, we construct a natural estimator of an extreme risk
region and prove a refined form of consistency, given a random sample of
multivariate regularly varying random vectors. In a detailed simulation and
comparison study, the good performance of the procedure is demonstrated. We
also apply our estimator to financial data."@2012
Laurens de Haan@http://arxiv.org/abs/1211.5239v1@Estimation of extreme risk regions under multivariate regular variation@"When considering d possibly dependent random variables, one is often
interested in extreme risk regions, with very small probability p. We consider
risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,
where f is the joint density and $\beta$ a small number. Estimation of such an
extreme risk region is difficult since it contains hardly any or no data. Using
extreme value theory, we construct a natural estimator of an extreme risk
region and prove a refined form of consistency, given a random sample of
multivariate regularly varying random vectors. In a detailed simulation and
comparison study, the good performance of the procedure is demonstrated. We
also apply our estimator to financial data."@2012
Anita Behme@http://arxiv.org/abs/1211.5281v1@"Distributions of exponential integrals of independent increment
  processes related to generalized gamma convolutions"@"It is known that in many cases distributions of exponential integrals of Levy
processes are infinitely divisible and in some cases they are also
selfdecomposable. In this paper, we give some sufficient conditions under which
distributions of exponential integrals are not only selfdecomposable but
furthermore are generalized gamma convolution. We also study exponential
integrals of more general independent increment processes. Several examples are
given for illustration."@2012
Makoto Maejima@http://arxiv.org/abs/1211.5281v1@"Distributions of exponential integrals of independent increment
  processes related to generalized gamma convolutions"@"It is known that in many cases distributions of exponential integrals of Levy
processes are infinitely divisible and in some cases they are also
selfdecomposable. In this paper, we give some sufficient conditions under which
distributions of exponential integrals are not only selfdecomposable but
furthermore are generalized gamma convolution. We also study exponential
integrals of more general independent increment processes. Several examples are
given for illustration."@2012
Muneya Matsui@http://arxiv.org/abs/1211.5281v1@"Distributions of exponential integrals of independent increment
  processes related to generalized gamma convolutions"@"It is known that in many cases distributions of exponential integrals of Levy
processes are infinitely divisible and in some cases they are also
selfdecomposable. In this paper, we give some sufficient conditions under which
distributions of exponential integrals are not only selfdecomposable but
furthermore are generalized gamma convolution. We also study exponential
integrals of more general independent increment processes. Several examples are
given for illustration."@2012
Noriyoshi Sakuma@http://arxiv.org/abs/1211.5281v1@"Distributions of exponential integrals of independent increment
  processes related to generalized gamma convolutions"@"It is known that in many cases distributions of exponential integrals of Levy
processes are infinitely divisible and in some cases they are also
selfdecomposable. In this paper, we give some sufficient conditions under which
distributions of exponential integrals are not only selfdecomposable but
furthermore are generalized gamma convolution. We also study exponential
integrals of more general independent increment processes. Several examples are
given for illustration."@2012
Wen Cao@http://arxiv.org/abs/1211.5372v2@Drift in Transaction-Level Asset Price Models@"We study the effect of drift in pure-jump transaction-level models for asset
prices in continuous time, driven by point processes. The drift is as-sumed to
arise from a nonzero mean in the efficient shock series. It follows that the
drift is proportional to the driving point process itself, i.e. the cumulative
number of transactions. This link reveals a mechanism by which properties of
intertrade durations (such as heavy tails and long memory) can have a strong
impact on properties of average returns, thereby poten-tially making it
extremely difficult to determine long-term growth rates or to reliably detect
an equity premium. We focus on a basic univariate model for log price, coupled
with general assumptions on the point process that are satisfied by several
existing flexible models, allowing for both long mem-ory and heavy tails in
durations. Under our pure-jump model, we obtain the limiting distribution for
the suitably normalized log price. This limiting distribution need not be
Gaussian, and may have either finite variance or infinite variance. We show
that the drift can affect not only the limiting dis-tribution for the
normalized log price, but also the rate in the corresponding normalization.
Therefore, the drift (or equivalently, the properties of dura-tions) affects
the rate of convergence of estimators of the growth rate, and can invalidate
standard hypothesis tests for that growth rate. As a rem-edy to these problems,
we propose a new ratio statistic which behaves more"@2012
Clifford Hurvich@http://arxiv.org/abs/1211.5372v2@Drift in Transaction-Level Asset Price Models@"We study the effect of drift in pure-jump transaction-level models for asset
prices in continuous time, driven by point processes. The drift is as-sumed to
arise from a nonzero mean in the efficient shock series. It follows that the
drift is proportional to the driving point process itself, i.e. the cumulative
number of transactions. This link reveals a mechanism by which properties of
intertrade durations (such as heavy tails and long memory) can have a strong
impact on properties of average returns, thereby poten-tially making it
extremely difficult to determine long-term growth rates or to reliably detect
an equity premium. We focus on a basic univariate model for log price, coupled
with general assumptions on the point process that are satisfied by several
existing flexible models, allowing for both long mem-ory and heavy tails in
durations. Under our pure-jump model, we obtain the limiting distribution for
the suitably normalized log price. This limiting distribution need not be
Gaussian, and may have either finite variance or infinite variance. We show
that the drift can affect not only the limiting dis-tribution for the
normalized log price, but also the rate in the corresponding normalization.
Therefore, the drift (or equivalently, the properties of dura-tions) affects
the rate of convergence of estimators of the growth rate, and can invalidate
standard hypothesis tests for that growth rate. As a rem-edy to these problems,
we propose a new ratio statistic which behaves more"@2012
Philippe Soulier@http://arxiv.org/abs/1211.5372v2@Drift in Transaction-Level Asset Price Models@"We study the effect of drift in pure-jump transaction-level models for asset
prices in continuous time, driven by point processes. The drift is as-sumed to
arise from a nonzero mean in the efficient shock series. It follows that the
drift is proportional to the driving point process itself, i.e. the cumulative
number of transactions. This link reveals a mechanism by which properties of
intertrade durations (such as heavy tails and long memory) can have a strong
impact on properties of average returns, thereby poten-tially making it
extremely difficult to determine long-term growth rates or to reliably detect
an equity premium. We focus on a basic univariate model for log price, coupled
with general assumptions on the point process that are satisfied by several
existing flexible models, allowing for both long mem-ory and heavy tails in
durations. Under our pure-jump model, we obtain the limiting distribution for
the suitably normalized log price. This limiting distribution need not be
Gaussian, and may have either finite variance or infinite variance. We show
that the drift can affect not only the limiting dis-tribution for the
normalized log price, but also the rate in the corresponding normalization.
Therefore, the drift (or equivalently, the properties of dura-tions) affects
the rate of convergence of estimators of the growth rate, and can invalidate
standard hypothesis tests for that growth rate. As a rem-edy to these problems,
we propose a new ratio statistic which behaves more"@2012
Bodhisattva Sen@http://arxiv.org/abs/1211.5420v1@"Bootstrap confidence intervals for isotonic estimators in a
  stereological problem"@"Let $\mathbf{X}=(X_1,X_2,X_3)$ be a spherically symmetric random vector of
which only $(X_1,X_2)$ can be observed. We focus attention on estimating F, the
distribution function of the squared radius $Z:=X_1^2+X_2^2+X_3^2$, from a
random sample of $(X_1,X_2)$. Such a problem arises in astronomy where
$(X_1,X_2,X_3)$ denotes the three dimensional position of a star in a galaxy
but we can only observe the projected stellar positions $(X_1,X_2)$. We
consider isotonic estimators of F and derive their limit distributions. The
results are nonstandard with a rate of convergence $\sqrt{n/{\log n}}$. The
isotonized estimators of F have exactly half the limiting variance when
compared to naive estimators, which do not incorporate the shape constraint. We
consider the problem of constructing point-wise confidence intervals for F,
state sufficient conditions for the consistency of a bootstrap procedure, and
show that the conditions are met by the conventional bootstrap method
(generating samples from the empirical distribution function)."@2012
Michael Woodroofe@http://arxiv.org/abs/1211.5420v1@"Bootstrap confidence intervals for isotonic estimators in a
  stereological problem"@"Let $\mathbf{X}=(X_1,X_2,X_3)$ be a spherically symmetric random vector of
which only $(X_1,X_2)$ can be observed. We focus attention on estimating F, the
distribution function of the squared radius $Z:=X_1^2+X_2^2+X_3^2$, from a
random sample of $(X_1,X_2)$. Such a problem arises in astronomy where
$(X_1,X_2,X_3)$ denotes the three dimensional position of a star in a galaxy
but we can only observe the projected stellar positions $(X_1,X_2)$. We
consider isotonic estimators of F and derive their limit distributions. The
results are nonstandard with a rate of convergence $\sqrt{n/{\log n}}$. The
isotonized estimators of F have exactly half the limiting variance when
compared to naive estimators, which do not incorporate the shape constraint. We
consider the problem of constructing point-wise confidence intervals for F,
state sufficient conditions for the consistency of a bootstrap procedure, and
show that the conditions are met by the conventional bootstrap method
(generating samples from the empirical distribution function)."@2012
Stefano Favaro@http://arxiv.org/abs/1211.5422v1@Asymptotics for a Bayesian nonparametric estimator of species variety@"In Bayesian nonparametric inference, random discrete probability measures are
commonly used as priors within hierarchical mixture models for density
estimation and for inference on the clustering of the data. Recently, it has
been shown that they can also be exploited in species sampling problems: indeed
they are natural tools for modeling the random proportions of species within a
population thus allowing for inference on various quantities of statistical
interest. For applications that involve large samples, the exact evaluation of
the corresponding estimators becomes impracticable and, therefore, asymptotic
approximations are sought. In the present paper, we study the limiting
behaviour of the number of new species to be observed from further sampling,
conditional on observed data, assuming the observations are exchangeable and
directed by a normalized generalized gamma process prior. Such an asymptotic
study highlights a connection between the normalized generalized gamma process
and the two-parameter Poisson-Dirichlet process that was previously known only
in the unconditional case."@2012
Antonio Lijoi@http://arxiv.org/abs/1211.5422v1@Asymptotics for a Bayesian nonparametric estimator of species variety@"In Bayesian nonparametric inference, random discrete probability measures are
commonly used as priors within hierarchical mixture models for density
estimation and for inference on the clustering of the data. Recently, it has
been shown that they can also be exploited in species sampling problems: indeed
they are natural tools for modeling the random proportions of species within a
population thus allowing for inference on various quantities of statistical
interest. For applications that involve large samples, the exact evaluation of
the corresponding estimators becomes impracticable and, therefore, asymptotic
approximations are sought. In the present paper, we study the limiting
behaviour of the number of new species to be observed from further sampling,
conditional on observed data, assuming the observations are exchangeable and
directed by a normalized generalized gamma process prior. Such an asymptotic
study highlights a connection between the normalized generalized gamma process
and the two-parameter Poisson-Dirichlet process that was previously known only
in the unconditional case."@2012
Igor Prünster@http://arxiv.org/abs/1211.5422v1@Asymptotics for a Bayesian nonparametric estimator of species variety@"In Bayesian nonparametric inference, random discrete probability measures are
commonly used as priors within hierarchical mixture models for density
estimation and for inference on the clustering of the data. Recently, it has
been shown that they can also be exploited in species sampling problems: indeed
they are natural tools for modeling the random proportions of species within a
population thus allowing for inference on various quantities of statistical
interest. For applications that involve large samples, the exact evaluation of
the corresponding estimators becomes impracticable and, therefore, asymptotic
approximations are sought. In the present paper, we study the limiting
behaviour of the number of new species to be observed from further sampling,
conditional on observed data, assuming the observations are exchangeable and
directed by a normalized generalized gamma process prior. Such an asymptotic
study highlights a connection between the normalized generalized gamma process
and the two-parameter Poisson-Dirichlet process that was previously known only
in the unconditional case."@2012
Daniel Bonnéry@http://arxiv.org/abs/1211.5468v1@"Uniform convergence of the empirical cumulative distribution function
  under informative selection from a finite population"@"Consider informative selection of a sample from a finite population.
Responses are realized as independent and identically distributed (i.i.d.)
random variables with a probability density function (p.d.f.) f, referred to as
the superpopulation model. The selection is informative in the sense that the
sample responses, given that they were selected, are not i.i.d. f. In general,
the informative selection mechanism may induce dependence among the selected
observations. The impact of such dependence on the empirical cumulative
distribution function (c.d.f.) is studied. An asymptotic framework and weak
conditions on the informative selection mechanism are developed under which the
(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,
to a weighted version of the superpopulation c.d.f. This yields an analogue of
the Glivenko-Cantelli theorem. A series of examples, motivated by real problems
in surveys and other observational studies, shows that the conditions are
verifiable for specified designs."@2012
F. Jay Breidt@http://arxiv.org/abs/1211.5468v1@"Uniform convergence of the empirical cumulative distribution function
  under informative selection from a finite population"@"Consider informative selection of a sample from a finite population.
Responses are realized as independent and identically distributed (i.i.d.)
random variables with a probability density function (p.d.f.) f, referred to as
the superpopulation model. The selection is informative in the sense that the
sample responses, given that they were selected, are not i.i.d. f. In general,
the informative selection mechanism may induce dependence among the selected
observations. The impact of such dependence on the empirical cumulative
distribution function (c.d.f.) is studied. An asymptotic framework and weak
conditions on the informative selection mechanism are developed under which the
(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,
to a weighted version of the superpopulation c.d.f. This yields an analogue of
the Glivenko-Cantelli theorem. A series of examples, motivated by real problems
in surveys and other observational studies, shows that the conditions are
verifiable for specified designs."@2012
François Coquet@http://arxiv.org/abs/1211.5468v1@"Uniform convergence of the empirical cumulative distribution function
  under informative selection from a finite population"@"Consider informative selection of a sample from a finite population.
Responses are realized as independent and identically distributed (i.i.d.)
random variables with a probability density function (p.d.f.) f, referred to as
the superpopulation model. The selection is informative in the sense that the
sample responses, given that they were selected, are not i.i.d. f. In general,
the informative selection mechanism may induce dependence among the selected
observations. The impact of such dependence on the empirical cumulative
distribution function (c.d.f.) is studied. An asymptotic framework and weak
conditions on the informative selection mechanism are developed under which the
(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,
to a weighted version of the superpopulation c.d.f. This yields an analogue of
the Glivenko-Cantelli theorem. A series of examples, motivated by real problems
in surveys and other observational studies, shows that the conditions are
verifiable for specified designs."@2012
B. B. Chen@http://arxiv.org/abs/1211.5479v1@"Convergence of the largest eigenvalue of normalized sample covariance
  matrices when p and n both tend to infinity with their ratio converging to
  zero"@"Let $\mathbf{X}_p=(\mathbf{s}_1,...,\mathbf{s}_n)=(X_{ij})_{p \times n}$
where $X_{ij}$'s are independent and identically distributed (i.i.d.) random
variables with $EX_{11}=0,EX_{11}^2=1$ and $EX_{11}^4<\infty$. It is showed
that the largest eigenvalue of the random matrix
$\mathbf{A}_p=\frac{1}{2\sqrt{np}}(\mathbf{X}_p\mathbf{X}_p^{\prime}-n\mathbf{I}_p)$
tends to 1 almost surely as $p\rightarrow\infty,n\rightarrow\infty$ with
$p/n\rightarrow0$."@2012
G. M. Pan@http://arxiv.org/abs/1211.5479v1@"Convergence of the largest eigenvalue of normalized sample covariance
  matrices when p and n both tend to infinity with their ratio converging to
  zero"@"Let $\mathbf{X}_p=(\mathbf{s}_1,...,\mathbf{s}_n)=(X_{ij})_{p \times n}$
where $X_{ij}$'s are independent and identically distributed (i.i.d.) random
variables with $EX_{11}=0,EX_{11}^2=1$ and $EX_{11}^4<\infty$. It is showed
that the largest eigenvalue of the random matrix
$\mathbf{A}_p=\frac{1}{2\sqrt{np}}(\mathbf{X}_p\mathbf{X}_p^{\prime}-n\mathbf{I}_p)$
tends to 1 almost surely as $p\rightarrow\infty,n\rightarrow\infty$ with
$p/n\rightarrow0$."@2012
Mathias Vetter@http://arxiv.org/abs/1211.5507v1@Model checks for the volatility under microstructure noise@"We consider the problem of testing the parametric form of the volatility for
high frequency data. It is demonstrated that in the presence of microstructure
noise commonly used tests do not keep the preassigned level and are
inconsistent. The concept of preaveraging is used to construct new tests, which
do not suffer from these drawbacks. These tests are based on a
Kolmogorov-Smirnov or Cramer-von-Mises functional of an integrated stochastic
process, for which weak convergence to a (conditional) Gaussian process is
established. The finite sample properties of a bootstrap version of the test
are illustrated by means of a simulation study."@2012
Holger Dette@http://arxiv.org/abs/1211.5507v1@Model checks for the volatility under microstructure noise@"We consider the problem of testing the parametric form of the volatility for
high frequency data. It is demonstrated that in the presence of microstructure
noise commonly used tests do not keep the preassigned level and are
inconsistent. The concept of preaveraging is used to construct new tests, which
do not suffer from these drawbacks. These tests are based on a
Kolmogorov-Smirnov or Cramer-von-Mises functional of an integrated stochastic
process, for which weak convergence to a (conditional) Gaussian process is
established. The finite sample properties of a bootstrap version of the test
are illustrated by means of a simulation study."@2012
Kung-Sik Chan@http://arxiv.org/abs/1211.5513v1@Inference of seasonal long-memory aggregate time series@"Time-series data with regular and/or seasonal long-memory are often
aggregated before analysis. Often, the aggregation scale is large enough to
remove any short-memory components of the underlying process but too short to
eliminate seasonal patterns of much longer periods. In this paper, we
investigate the limiting correlation structure of aggregate time series within
an intermediate asymptotic framework that attempts to capture the
aforementioned sampling scheme. In particular, we study the autocorrelation
structure and the spectral density function of aggregates from a discrete-time
process. The underlying discrete-time process is assumed to be a stationary
Seasonal AutoRegressive Fractionally Integrated Moving-Average (SARFIMA)
process, after suitable number of differencing if necessary, and the seasonal
periods of the underlying process are multiples of the aggregation size. We
derive the limit of the normalized spectral density function of the aggregates,
with increasing aggregation. The limiting aggregate (seasonal) long-memory
model may then be useful for analyzing aggregate time-series data, which can be
estimated by maximizing the Whittle likelihood. We prove that the maximum
Whittle likelihood estimator (spectral maximum likelihood estimator) is
consistent and asymptotically normal, and study its finite-sample properties
through simulation. The efficacy of the proposed approach is illustrated by a
real-life internet traffic example."@2012
Henghsiu Tsai@http://arxiv.org/abs/1211.5513v1@Inference of seasonal long-memory aggregate time series@"Time-series data with regular and/or seasonal long-memory are often
aggregated before analysis. Often, the aggregation scale is large enough to
remove any short-memory components of the underlying process but too short to
eliminate seasonal patterns of much longer periods. In this paper, we
investigate the limiting correlation structure of aggregate time series within
an intermediate asymptotic framework that attempts to capture the
aforementioned sampling scheme. In particular, we study the autocorrelation
structure and the spectral density function of aggregates from a discrete-time
process. The underlying discrete-time process is assumed to be a stationary
Seasonal AutoRegressive Fractionally Integrated Moving-Average (SARFIMA)
process, after suitable number of differencing if necessary, and the seasonal
periods of the underlying process are multiples of the aggregation size. We
derive the limit of the normalized spectral density function of the aggregates,
with increasing aggregation. The limiting aggregate (seasonal) long-memory
model may then be useful for analyzing aggregate time-series data, which can be
estimated by maximizing the Whittle likelihood. We prove that the maximum
Whittle likelihood estimator (spectral maximum likelihood estimator) is
consistent and asymptotically normal, and study its finite-sample properties
through simulation. The efficacy of the proposed approach is illustrated by a
real-life internet traffic example."@2012
Alessandro Soranzo@http://arxiv.org/abs/1211.6403v1@"Practical Explicitly Invertible Approximation to 4 Decimals of Normal
  Cumulative Distribution Function Modifying Winitzki's Approximation of erf"@"We give a new explicitly invertible approximation of the normal cumulative
distribution function: $\Phi(x) \simeq 1/2 + 1/2
\sqrt{1-{e}^{-x^2\frac{17+{x}^{2}}{26.694+2x^2}}}$, $\forall x \ge 0$, with
absolute error $<4.00\cdot 10^{-5}$, absolute value of the relative error
$<4.53\cdot 10^{-5}$, which, beeing designed essentially for practical use, is
much simpler than a previously published formula and, though less precise,
still reaches 4 decimals of precision, and has a complexity essentially
comparable with that of the approximation of the normal cumulative distribution
function $\Phi(x)$ immediatly derived from Winitzki's approximation of
erf$(x)$, reducing about 36% the absolute error and about 28% the relative
error with respect to that, overcoming the threshold of 4 decimals of
precision."@2012
Emanuela Epure@http://arxiv.org/abs/1211.6403v1@"Practical Explicitly Invertible Approximation to 4 Decimals of Normal
  Cumulative Distribution Function Modifying Winitzki's Approximation of erf"@"We give a new explicitly invertible approximation of the normal cumulative
distribution function: $\Phi(x) \simeq 1/2 + 1/2
\sqrt{1-{e}^{-x^2\frac{17+{x}^{2}}{26.694+2x^2}}}$, $\forall x \ge 0$, with
absolute error $<4.00\cdot 10^{-5}$, absolute value of the relative error
$<4.53\cdot 10^{-5}$, which, beeing designed essentially for practical use, is
much simpler than a previously published formula and, though less precise,
still reaches 4 decimals of precision, and has a complexity essentially
comparable with that of the approximation of the normal cumulative distribution
function $\Phi(x)$ immediatly derived from Winitzki's approximation of
erf$(x)$, reducing about 36% the absolute error and about 28% the relative
error with respect to that, overcoming the threshold of 4 decimals of
precision."@2012
Rida Benhaddou@http://arxiv.org/abs/1211.7114v2@"Anisotropic Denoising in Functional Deconvolution Model with
  Dimension-free Convergence Rates"@"In the present paper we consider the problem of estimating a periodic
$(r+1)$-dimensional function $f$ based on observations from its noisy
convolution. We construct a wavelet estimator of $f$, derive minimax lower
bounds for the $L^2$-risk when $f$ belongs to a Besov ball of mixed smoothness
and demonstrate that the wavelet estimator is adaptive and asymptotically
near-optimal within a logarithmic factor, in a wide range of Besov balls. We
prove in particular that choosing this type of mixed smoothness leads to rates
of convergence which are free of the ""curse of dimensionality"" and, hence, are
higher than usual convergence rates when $r$ is large. The problem studied in
the paper is motivated by seismic inversion which can be reduced to solution of
noisy two-dimensional convolution equations that allow to draw inference on
underground layer structures along the chosen profiles. The common practice in
seismology is to recover layer structures separately for each profile and then
to combine the derived estimates into a two-dimensional function. By studying
the two-dimensional version of the model, we demonstrate that this strategy
usually leads to estimators which are less accurate than the ones obtained as
two-dimensional functional deconvolutions. Indeed, we show that unless the
function $f$ is very smooth in the direction of the profiles, very spatially
inhomogeneous along the other direction and the number of profiles is very
limited, the functional deconvolution solution has a much better precision
compared to a combination of $M$ solutions of separate convolution equations. A
limited simulation study in the case of $r=1$ confirms theoretical claims of
the paper."@2012
Marianna Pensky@http://arxiv.org/abs/1211.7114v2@"Anisotropic Denoising in Functional Deconvolution Model with
  Dimension-free Convergence Rates"@"In the present paper we consider the problem of estimating a periodic
$(r+1)$-dimensional function $f$ based on observations from its noisy
convolution. We construct a wavelet estimator of $f$, derive minimax lower
bounds for the $L^2$-risk when $f$ belongs to a Besov ball of mixed smoothness
and demonstrate that the wavelet estimator is adaptive and asymptotically
near-optimal within a logarithmic factor, in a wide range of Besov balls. We
prove in particular that choosing this type of mixed smoothness leads to rates
of convergence which are free of the ""curse of dimensionality"" and, hence, are
higher than usual convergence rates when $r$ is large. The problem studied in
the paper is motivated by seismic inversion which can be reduced to solution of
noisy two-dimensional convolution equations that allow to draw inference on
underground layer structures along the chosen profiles. The common practice in
seismology is to recover layer structures separately for each profile and then
to combine the derived estimates into a two-dimensional function. By studying
the two-dimensional version of the model, we demonstrate that this strategy
usually leads to estimators which are less accurate than the ones obtained as
two-dimensional functional deconvolutions. Indeed, we show that unless the
function $f$ is very smooth in the direction of the profiles, very spatially
inhomogeneous along the other direction and the number of profiles is very
limited, the functional deconvolution solution has a much better precision
compared to a combination of $M$ solutions of separate convolution equations. A
limited simulation study in the case of $r=1$ confirms theoretical claims of
the paper."@2012
Dominique Picard@http://arxiv.org/abs/1211.7114v2@"Anisotropic Denoising in Functional Deconvolution Model with
  Dimension-free Convergence Rates"@"In the present paper we consider the problem of estimating a periodic
$(r+1)$-dimensional function $f$ based on observations from its noisy
convolution. We construct a wavelet estimator of $f$, derive minimax lower
bounds for the $L^2$-risk when $f$ belongs to a Besov ball of mixed smoothness
and demonstrate that the wavelet estimator is adaptive and asymptotically
near-optimal within a logarithmic factor, in a wide range of Besov balls. We
prove in particular that choosing this type of mixed smoothness leads to rates
of convergence which are free of the ""curse of dimensionality"" and, hence, are
higher than usual convergence rates when $r$ is large. The problem studied in
the paper is motivated by seismic inversion which can be reduced to solution of
noisy two-dimensional convolution equations that allow to draw inference on
underground layer structures along the chosen profiles. The common practice in
seismology is to recover layer structures separately for each profile and then
to combine the derived estimates into a two-dimensional function. By studying
the two-dimensional version of the model, we demonstrate that this strategy
usually leads to estimators which are less accurate than the ones obtained as
two-dimensional functional deconvolutions. Indeed, we show that unless the
function $f$ is very smooth in the direction of the profiles, very spatially
inhomogeneous along the other direction and the number of profiles is very
limited, the functional deconvolution solution has a much better precision
compared to a combination of $M$ solutions of separate convolution equations. A
limited simulation study in the case of $r=1$ confirms theoretical claims of
the paper."@2012
Mor Ndongo@http://arxiv.org/abs/1211.7262v3@"Estimation for seasonal fractional ARIMA with stable innovations via the
  empirical characteristic function method"@"Maximum likelihood methods, while widely used, may be non-robust due to
disagreement between the assumptions upon which the models are based and the
true density probability distribution of observed data. Because the Empirical
Characteristic Function (ECF) is the Fourier transform of the empirical
distribution function, it retains all the information in the sample but can
overcome difficulties arising from the likelihood. This paper discusses an
estimation method via the ECF for stable seasonal fractional ARIMA processes.
Under some assumptions, we show that the resulting estimators are consistent
and asymptotically normally distributed. For comparison purpose, we consider
also the MCMC Whittle method developed by Ndongo et al. (2010). The performance
of the two methods is discussed using a Monte Carlo simulation."@2012
Abdou Kâ Diongue@http://arxiv.org/abs/1211.7262v3@"Estimation for seasonal fractional ARIMA with stable innovations via the
  empirical characteristic function method"@"Maximum likelihood methods, while widely used, may be non-robust due to
disagreement between the assumptions upon which the models are based and the
true density probability distribution of observed data. Because the Empirical
Characteristic Function (ECF) is the Fourier transform of the empirical
distribution function, it retains all the information in the sample but can
overcome difficulties arising from the likelihood. This paper discusses an
estimation method via the ECF for stable seasonal fractional ARIMA processes.
Under some assumptions, we show that the resulting estimators are consistent
and asymptotically normally distributed. For comparison purpose, we consider
also the MCMC Whittle method developed by Ndongo et al. (2010). The performance
of the two methods is discussed using a Monte Carlo simulation."@2012
Aliou Diop@http://arxiv.org/abs/1211.7262v3@"Estimation for seasonal fractional ARIMA with stable innovations via the
  empirical characteristic function method"@"Maximum likelihood methods, while widely used, may be non-robust due to
disagreement between the assumptions upon which the models are based and the
true density probability distribution of observed data. Because the Empirical
Characteristic Function (ECF) is the Fourier transform of the empirical
distribution function, it retains all the information in the sample but can
overcome difficulties arising from the likelihood. This paper discusses an
estimation method via the ECF for stable seasonal fractional ARIMA processes.
Under some assumptions, we show that the resulting estimators are consistent
and asymptotically normally distributed. For comparison purpose, we consider
also the MCMC Whittle method developed by Ndongo et al. (2010). The performance
of the two methods is discussed using a Monte Carlo simulation."@2012
Simplice Dossou-Gbété@http://arxiv.org/abs/1211.7262v3@"Estimation for seasonal fractional ARIMA with stable innovations via the
  empirical characteristic function method"@"Maximum likelihood methods, while widely used, may be non-robust due to
disagreement between the assumptions upon which the models are based and the
true density probability distribution of observed data. Because the Empirical
Characteristic Function (ECF) is the Fourier transform of the empirical
distribution function, it retains all the information in the sample but can
overcome difficulties arising from the likelihood. This paper discusses an
estimation method via the ECF for stable seasonal fractional ARIMA processes.
Under some assumptions, we show that the resulting estimators are consistent
and asymptotically normally distributed. For comparison purpose, we consider
also the MCMC Whittle method developed by Ndongo et al. (2010). The performance
of the two methods is discussed using a Monte Carlo simulation."@2012
Gourab Mukherjee@http://arxiv.org/abs/1212.0325v1@On the within-family Kullback-Leibler risk in Gaussian Predictive models@"We consider estimating the predictive density under Kullback-Leibler loss in
a high-dimensional Gaussian model. Decision theoretic properties of the
within-family prediction error -- the minimal risk among estimates in the class
$\mathcal{G}$ of all Gaussian densities are discussed. We show that in sparse
models, the class $\mathcal{G}$ is minimax sub-optimal. We produce
asymptotically sharp upper and lower bounds on the within-family prediction
errors for various subfamilies of $\mathcal{G}$. Under mild regularity
conditions, in the sub-family where the covariance structure is represented by
a single data dependent parameter $\Shat=\dhat \cdot I$, the Kullback-Leiber
risk has a tractable decomposition which can be subsequently minimized to yield
optimally flattened predictive density estimates. The optimal predictive risk
can be explicitly expressed in terms of the corresponding mean square error of
the location estimate, and so, the role of shrinkage in the predictive regime
can be determined based on point estimation theory results. Our results
demonstrate that some of the decision theoretic parallels between predictive
density estimation and point estimation regimes can be explained by second
moment based concentration properties of the quadratic loss."@2012
Iain M. Johnstone@http://arxiv.org/abs/1212.0325v1@On the within-family Kullback-Leibler risk in Gaussian Predictive models@"We consider estimating the predictive density under Kullback-Leibler loss in
a high-dimensional Gaussian model. Decision theoretic properties of the
within-family prediction error -- the minimal risk among estimates in the class
$\mathcal{G}$ of all Gaussian densities are discussed. We show that in sparse
models, the class $\mathcal{G}$ is minimax sub-optimal. We produce
asymptotically sharp upper and lower bounds on the within-family prediction
errors for various subfamilies of $\mathcal{G}$. Under mild regularity
conditions, in the sub-family where the covariance structure is represented by
a single data dependent parameter $\Shat=\dhat \cdot I$, the Kullback-Leiber
risk has a tractable decomposition which can be subsequently minimized to yield
optimally flattened predictive density estimates. The optimal predictive risk
can be explicitly expressed in terms of the corresponding mean square error of
the location estimate, and so, the role of shrinkage in the predictive regime
can be determined based on point estimation theory results. Our results
demonstrate that some of the decision theoretic parallels between predictive
density estimation and point estimation regimes can be explained by second
moment based concentration properties of the quadratic loss."@2012
Jean-Marc Azais@http://arxiv.org/abs/1212.0757v1@"Remark on the finite-dimensional character of certain results of
  functional statistics"@"This note shows that some assumption on small balls probability, frequently
used in the domain of functional statistics, implies that the considered
functional space is of finite dimension. To complete this result an example of
L2 process is given that does not fulfill this assumption."@2012
Jean-Claude Fort@http://arxiv.org/abs/1212.0757v1@"Remark on the finite-dimensional character of certain results of
  functional statistics"@"This note shows that some assumption on small balls probability, frequently
used in the domain of functional statistics, implies that the considered
functional space is of finite dimension. To complete this result an example of
L2 process is given that does not fulfill this assumption."@2012
L. Gardes@http://arxiv.org/abs/1212.1076v1@Functional kernel estimators of conditional extreme quantiles@"We address the estimation of ""extreme"" conditional quantiles i.e. when their
order converges to one as the sample size increases. Conditions on the rate of
convergence of their order to one are provided to obtain asymptotically
Gaussian distributed kernel estimators. A Weissman-type estimator and kernel
estimators of the conditional tail-index are derived, permitting to estimate
extreme conditional quantiles of arbitrary order."@2012
S. Girard@http://arxiv.org/abs/1212.1076v1@Functional kernel estimators of conditional extreme quantiles@"We address the estimation of ""extreme"" conditional quantiles i.e. when their
order converges to one as the sample size increases. Conditions on the rate of
convergence of their order to one are provided to obtain asymptotically
Gaussian distributed kernel estimators. A Weissman-type estimator and kernel
estimators of the conditional tail-index are derived, permitting to estimate
extreme conditional quantiles of arbitrary order."@2012
Marta Ferreira@http://arxiv.org/abs/1212.1885v1@Extremes of multivariate ARMAX processes@"We define a new multivariate time series model by generalizing the ARMAX
process in a multivariate way. We give conditions on stationarity and analyze
local dependence and domains of attraction. As a consequence of the obtained
result, we derive a new method of construction of multivariate extreme value
copulas. We characterize the extremal dependence by computing the multivariate
extremal index and bivariate upper tail dependence coefficients. An estimation
procedure for the multivariate extremal index shall be presented. We also
address the marginal estimation and propose a new estimator for the ARMAX
autoregressive parameter."@2012
Helena Ferreira@http://arxiv.org/abs/1212.1885v1@Extremes of multivariate ARMAX processes@"We define a new multivariate time series model by generalizing the ARMAX
process in a multivariate way. We give conditions on stationarity and analyze
local dependence and domains of attraction. As a consequence of the obtained
result, we derive a new method of construction of multivariate extreme value
copulas. We characterize the extremal dependence by computing the multivariate
extremal index and bivariate upper tail dependence coefficients. An estimation
procedure for the multivariate extremal index shall be presented. We also
address the marginal estimation and propose a new estimator for the ARMAX
autoregressive parameter."@2012
Jérémie Bigot@http://arxiv.org/abs/1212.2562v6@"Characterization of barycenters in the Wasserstein space by averaging
  optimal transport maps"@"This paper is concerned by the study of barycenters for random probability
measures in the Wasserstein space. Using a duality argument, we give a precise
characterization of the population barycenter for various parametric classes of
random probability measures with compact support. In particular, we make a
connection between averaging in the Wasserstein space as introduced in Agueh
and Carlier (2011), and taking the expectation of optimal transport maps with
respect to a fixed reference measure. We also discuss the usefulness of this
approach in statistics for the analysis of deformable models in signal and
image processing. In this setting, the problem of estimating a population
barycenter from n independent and identically distributed random probability
measures is also considered."@2012
Thierry Klein@http://arxiv.org/abs/1212.2562v6@"Characterization of barycenters in the Wasserstein space by averaging
  optimal transport maps"@"This paper is concerned by the study of barycenters for random probability
measures in the Wasserstein space. Using a duality argument, we give a precise
characterization of the population barycenter for various parametric classes of
random probability measures with compact support. In particular, we make a
connection between averaging in the Wasserstein space as introduced in Agueh
and Carlier (2011), and taking the expectation of optimal transport maps with
respect to a fixed reference measure. We also discuss the usefulness of this
approach in statistics for the analysis of deformable models in signal and
image processing. In this setting, the problem of estimating a population
barycenter from n independent and identically distributed random probability
measures is also considered."@2012
Stephane Girard@http://arxiv.org/abs/1212.3111v2@"Uniform strong consistency of a frontier estimator using kernel
  regression on high order moments"@"We consider the high order moments estimator of the frontier of a random pair
introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier
estimation with kernel regression on high order moments}. In the present paper,
we show that this estimator is strongly uniformly consistent on compact sets
and its rate of convergence is given when the conditional cumulative
distribution function belongs to the Hall class of distribution functions."@2012
Armelle Guillou@http://arxiv.org/abs/1212.3111v2@"Uniform strong consistency of a frontier estimator using kernel
  regression on high order moments"@"We consider the high order moments estimator of the frontier of a random pair
introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier
estimation with kernel regression on high order moments}. In the present paper,
we show that this estimator is strongly uniformly consistent on compact sets
and its rate of convergence is given when the conditional cumulative
distribution function belongs to the Hall class of distribution functions."@2012
Gilles Stupfler@http://arxiv.org/abs/1212.3111v2@"Uniform strong consistency of a frontier estimator using kernel
  regression on high order moments"@"We consider the high order moments estimator of the frontier of a random pair
introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier
estimation with kernel regression on high order moments}. In the present paper,
we show that this estimator is strongly uniformly consistent on compact sets
and its rate of convergence is given when the conditional cumulative
distribution function belongs to the Hall class of distribution functions."@2012
Ana Karina Fermin@http://arxiv.org/abs/1212.4457v2@Probability bounds for active learning in the regression problem@"In this article we consider the problem of choosing an optimal sampling
scheme for the regression problem simultaneously with that of model selection.
We consider a batch type approach and an on-line approach following algorithms
recently developed for the classification problem. Our main tools are
concentration-type inequalities which allow us to bound the supremum of the
deviations of the sampling scheme corrected by an appropriate weight function."@2012
Carenne Ludeña@http://arxiv.org/abs/1212.4457v2@Probability bounds for active learning in the regression problem@"In this article we consider the problem of choosing an optimal sampling
scheme for the regression problem simultaneously with that of model selection.
We consider a batch type approach and an on-line approach following algorithms
recently developed for the classification problem. Our main tools are
concentration-type inequalities which allow us to bound the supremum of the
deviations of the sampling scheme corrected by an appropriate weight function."@2012
Teppei Ogihara@http://arxiv.org/abs/1212.4911v1@"Quasi-Likelihood Analysis for Stochastic Regression Models with
  Nonsynchronous Observations"@"We consider nonsynchronous sampling of parameterized stochastic regression
models, which contain stochastic differential equations. Constructing a
quasi-likelihood function, we prove that the quasi-maximum likelihood estimator
and the Bayes type estimator are consistent and asymptotically mixed normal
when the sampling frequency of the nonsynchronous data becomes large."@2012
Nakahiro Yoshida@http://arxiv.org/abs/1212.4911v1@"Quasi-Likelihood Analysis for Stochastic Regression Models with
  Nonsynchronous Observations"@"We consider nonsynchronous sampling of parameterized stochastic regression
models, which contain stochastic differential equations. Constructing a
quasi-likelihood function, we prove that the quasi-maximum likelihood estimator
and the Bayes type estimator are consistent and asymptotically mixed normal
when the sampling frequency of the nonsynchronous data becomes large."@2012
Vladimir Vovk@http://arxiv.org/abs/1212.4966v4@Combining p-values via averaging@"This paper proposes general methods for the problem of multiple testing of a
single hypothesis, with a standard goal of combining a number of p-values
without making any assumptions about their dependence structure. An old result
by R\""uschendorf and, independently, Meng implies that the p-values can be
combined by scaling up their arithmetic mean by a factor of 2 (and no smaller
factor is sufficient in general). Based on more recent developments in
mathematical finance, specifically, robust risk aggregation techniques, we show
that $K$ p-values can be combined by scaling up their geometric mean by a
factor of $e$ (for all $K$) and by scaling up their harmonic mean by a factor
of $\ln K$ (asymptotically as $K\to\infty$). These and other results lead to a
generalized version of the Bonferroni-Holm method. A simulation study compares
the performance of various averaging methods."@2012
Ruodu Wang@http://arxiv.org/abs/1212.4966v4@Combining p-values via averaging@"This paper proposes general methods for the problem of multiple testing of a
single hypothesis, with a standard goal of combining a number of p-values
without making any assumptions about their dependence structure. An old result
by R\""uschendorf and, independently, Meng implies that the p-values can be
combined by scaling up their arithmetic mean by a factor of 2 (and no smaller
factor is sufficient in general). Based on more recent developments in
mathematical finance, specifically, robust risk aggregation techniques, we show
that $K$ p-values can be combined by scaling up their geometric mean by a
factor of $e$ (for all $K$) and by scaling up their harmonic mean by a factor
of $\ln K$ (asymptotically as $K\to\infty$). These and other results lead to a
generalized version of the Bonferroni-Holm method. A simulation study compares
the performance of various averaging methods."@2012
Monica Billio@http://arxiv.org/abs/1212.5397v1@Efficient Gibbs Sampling for Markov Switching GARCH Models@"We develop efficient simulation techniques for Bayesian inference on
switching GARCH models. Our contribution to existing literature is manifold.
First, we discuss different multi-move sampling techniques for Markov Switching
(MS) state space models with particular attention to MS-GARCH models. Our
multi-move sampling strategy is based on the Forward Filtering Backward
Sampling (FFBS) applied to an approximation of MS-GARCH. Another important
contribution is the use of multi-point samplers, such as the Multiple-Try
Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in
combination with FFBS for the MS-GARCH process. In this sense we ex- tend to
the MS state space models the work of So [2006] on efficient MTM sampler for
continuous state space models. Finally, we suggest to further improve the
sampler efficiency by introducing the antithetic sampling of Craiu and Meng
[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments
on MS-GARCH model show that our multi-point and multi-move strategies allow the
sampler to gain efficiency when compared with single-move Gibbs sampling."@2012
Roberto Casarin@http://arxiv.org/abs/1212.5397v1@Efficient Gibbs Sampling for Markov Switching GARCH Models@"We develop efficient simulation techniques for Bayesian inference on
switching GARCH models. Our contribution to existing literature is manifold.
First, we discuss different multi-move sampling techniques for Markov Switching
(MS) state space models with particular attention to MS-GARCH models. Our
multi-move sampling strategy is based on the Forward Filtering Backward
Sampling (FFBS) applied to an approximation of MS-GARCH. Another important
contribution is the use of multi-point samplers, such as the Multiple-Try
Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in
combination with FFBS for the MS-GARCH process. In this sense we ex- tend to
the MS state space models the work of So [2006] on efficient MTM sampler for
continuous state space models. Finally, we suggest to further improve the
sampler efficiency by introducing the antithetic sampling of Craiu and Meng
[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments
on MS-GARCH model show that our multi-point and multi-move strategies allow the
sampler to gain efficiency when compared with single-move Gibbs sampling."@2012
Anthony Osuntuyi@http://arxiv.org/abs/1212.5397v1@Efficient Gibbs Sampling for Markov Switching GARCH Models@"We develop efficient simulation techniques for Bayesian inference on
switching GARCH models. Our contribution to existing literature is manifold.
First, we discuss different multi-move sampling techniques for Markov Switching
(MS) state space models with particular attention to MS-GARCH models. Our
multi-move sampling strategy is based on the Forward Filtering Backward
Sampling (FFBS) applied to an approximation of MS-GARCH. Another important
contribution is the use of multi-point samplers, such as the Multiple-Try
Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in
combination with FFBS for the MS-GARCH process. In this sense we ex- tend to
the MS state space models the work of So [2006] on efficient MTM sampler for
continuous state space models. Finally, we suggest to further improve the
sampler efficiency by introducing the antithetic sampling of Craiu and Meng
[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments
on MS-GARCH model show that our multi-point and multi-move strategies allow the
sampler to gain efficiency when compared with single-move Gibbs sampling."@2012
Dominique Bontemps@http://arxiv.org/abs/1212.5429v2@"Bayesian posterior consistency in the functional randomly shifted curves
  model"@"In this paper, we consider the so-called Shape Invariant Model which stands
for the estimation of a function $f^0$ submitted to a random translation of law
$g^0$ in a white noise model. We are interested in such a model when the law of
the deformations is unknown. We aim to recover the law of the process
$\PP_{f^0,g^0}$ as well as $f^0$ and $g^0$. In this perspective, we adopt a
Bayesian point of view and find prior on $f$ and $g$ such that the posterior
distribution concentrates around $\PP_{f^0,g^0}$ at a polynomial rate when $n$
goes to $+\infty$. We obtain a logarithmic posterior contraction rate for the
shape $f^0$ and the distribution $g^0$. We also derive logarithmic lower bounds
for the estimation of $f^0$ and $g^0$ in a frequentist paradigm."@2012
Sébastien Gadat@http://arxiv.org/abs/1212.5429v2@"Bayesian posterior consistency in the functional randomly shifted curves
  model"@"In this paper, we consider the so-called Shape Invariant Model which stands
for the estimation of a function $f^0$ submitted to a random translation of law
$g^0$ in a white noise model. We are interested in such a model when the law of
the deformations is unknown. We aim to recover the law of the process
$\PP_{f^0,g^0}$ as well as $f^0$ and $g^0$. In this perspective, we adopt a
Bayesian point of view and find prior on $f$ and $g$ such that the posterior
distribution concentrates around $\PP_{f^0,g^0}$ at a polynomial rate when $n$
goes to $+\infty$. We obtain a logarithmic posterior contraction rate for the
shape $f^0$ and the distribution $g^0$. We also derive logarithmic lower bounds
for the estimation of $f^0$ and $g^0$ in a frequentist paradigm."@2012
Jean Jacod@http://arxiv.org/abs/1212.5490v1@"A test for the rank of the volatility process: the random perturbation
  approach"@"In this paper we present a test for the maximal rank of the matrix-valued
volatility process in the continuous Ito semimartingale framework. Our idea is
based upon a random perturbation of the original high frequency observations of
an Ito semimartingale, which opens the way for rank testing. We develop the
complete limit theory for the test statistic and apply it to various null and
alternative hypotheses. Finally, we demonstrate a homoscedasticity test for the
rank process."@2012
Mark Podolskij@http://arxiv.org/abs/1212.5490v1@"A test for the rank of the volatility process: the random perturbation
  approach"@"In this paper we present a test for the maximal rank of the matrix-valued
volatility process in the continuous Ito semimartingale framework. Our idea is
based upon a random perturbation of the original high frequency observations of
an Ito semimartingale, which opens the way for rank testing. We develop the
complete limit theory for the test statistic and apply it to various null and
alternative hypotheses. Finally, we demonstrate a homoscedasticity test for the
rank process."@2012
Arun Chandrasekhar@http://arxiv.org/abs/1212.5627v1@Inference for best linear approximations to set identified functions@"This paper provides inference methods for best linear approximations to
functions which are known to lie within a band. It extends the partial
identification literature by allowing the upper and lower functions defining
the band to be any functions, including ones carrying an index, which can be
estimated parametrically or non-parametrically. The identification region of
the parameters of the best linear approximation is characterized via its
support function, and limit theory is developed for the latter. We prove that
the support function approximately converges to a Gaussian process and
establish validity of the Bayesian bootstrap. The paper nests as special cases
the canonical examples in the literature: mean regression with interval valued
outcome data and interval valued regressor data. Because the bounds may carry
an index, the paper covers problems beyond mean regression; the framework is
extremely versatile. Applications include quantile and distribution regression
with interval valued data, sample selection problems, as well as mean,
quantile, and distribution treatment effects. Moreover, the framework can
account for the availability of instruments. An application is carried out,
studying female labor force participation along the lines of Mulligan and
Rubinstein (2008)."@2012
Victor Chernozhukov@http://arxiv.org/abs/1212.5627v1@Inference for best linear approximations to set identified functions@"This paper provides inference methods for best linear approximations to
functions which are known to lie within a band. It extends the partial
identification literature by allowing the upper and lower functions defining
the band to be any functions, including ones carrying an index, which can be
estimated parametrically or non-parametrically. The identification region of
the parameters of the best linear approximation is characterized via its
support function, and limit theory is developed for the latter. We prove that
the support function approximately converges to a Gaussian process and
establish validity of the Bayesian bootstrap. The paper nests as special cases
the canonical examples in the literature: mean regression with interval valued
outcome data and interval valued regressor data. Because the bounds may carry
an index, the paper covers problems beyond mean regression; the framework is
extremely versatile. Applications include quantile and distribution regression
with interval valued data, sample selection problems, as well as mean,
quantile, and distribution treatment effects. Moreover, the framework can
account for the availability of instruments. An application is carried out,
studying female labor force participation along the lines of Mulligan and
Rubinstein (2008)."@2012
Francesca Molinari@http://arxiv.org/abs/1212.5627v1@Inference for best linear approximations to set identified functions@"This paper provides inference methods for best linear approximations to
functions which are known to lie within a band. It extends the partial
identification literature by allowing the upper and lower functions defining
the band to be any functions, including ones carrying an index, which can be
estimated parametrically or non-parametrically. The identification region of
the parameters of the best linear approximation is characterized via its
support function, and limit theory is developed for the latter. We prove that
the support function approximately converges to a Gaussian process and
establish validity of the Bayesian bootstrap. The paper nests as special cases
the canonical examples in the literature: mean regression with interval valued
outcome data and interval valued regressor data. Because the bounds may carry
an index, the paper covers problems beyond mean regression; the framework is
extremely versatile. Applications include quantile and distribution regression
with interval valued data, sample selection problems, as well as mean,
quantile, and distribution treatment effects. Moreover, the framework can
account for the availability of instruments. An application is carried out,
studying female labor force participation along the lines of Mulligan and
Rubinstein (2008)."@2012
Paul Schrimpf@http://arxiv.org/abs/1212.5627v1@Inference for best linear approximations to set identified functions@"This paper provides inference methods for best linear approximations to
functions which are known to lie within a band. It extends the partial
identification literature by allowing the upper and lower functions defining
the band to be any functions, including ones carrying an index, which can be
estimated parametrically or non-parametrically. The identification region of
the parameters of the best linear approximation is characterized via its
support function, and limit theory is developed for the latter. We prove that
the support function approximately converges to a Gaussian process and
establish validity of the Bayesian bootstrap. The paper nests as special cases
the canonical examples in the literature: mean regression with interval valued
outcome data and interval valued regressor data. Because the bounds may carry
an index, the paper covers problems beyond mean regression; the framework is
extremely versatile. Applications include quantile and distribution regression
with interval valued data, sample selection problems, as well as mean,
quantile, and distribution treatment effects. Moreover, the framework can
account for the availability of instruments. An application is carried out,
studying female labor force participation along the lines of Mulligan and
Rubinstein (2008)."@2012
Anirban Bhattacharya@http://arxiv.org/abs/1212.6088v1@Bayesian shrinkage@"Penalized regression methods, such as $L_1$ regularization, are routinely
used in high-dimensional applications, and there is a rich literature on
optimality properties under sparsity assumptions. In the Bayesian paradigm,
sparsity is routinely induced through two-component mixture priors having a
probability mass at zero, but such priors encounter daunting computational
problems in high dimensions. This has motivated an amazing variety of
continuous shrinkage priors, which can be expressed as global-local scale
mixtures of Gaussians, facilitating computation. In sharp contrast to the
corresponding frequentist literature, very little is known about the properties
of such priors. Focusing on a broad class of shrinkage priors, we provide
precise results on prior and posterior concentration. Interestingly, we
demonstrate that most commonly used shrinkage priors, including the Bayesian
Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet
Laplace (DL) priors are proposed, which are optimal and lead to efficient
posterior computation exploiting results from normalized random measure theory.
Finite sample performance of Dirichlet Laplace priors relative to alternatives
is assessed in simulations."@2012
Debdeep Pati@http://arxiv.org/abs/1212.6088v1@Bayesian shrinkage@"Penalized regression methods, such as $L_1$ regularization, are routinely
used in high-dimensional applications, and there is a rich literature on
optimality properties under sparsity assumptions. In the Bayesian paradigm,
sparsity is routinely induced through two-component mixture priors having a
probability mass at zero, but such priors encounter daunting computational
problems in high dimensions. This has motivated an amazing variety of
continuous shrinkage priors, which can be expressed as global-local scale
mixtures of Gaussians, facilitating computation. In sharp contrast to the
corresponding frequentist literature, very little is known about the properties
of such priors. Focusing on a broad class of shrinkage priors, we provide
precise results on prior and posterior concentration. Interestingly, we
demonstrate that most commonly used shrinkage priors, including the Bayesian
Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet
Laplace (DL) priors are proposed, which are optimal and lead to efficient
posterior computation exploiting results from normalized random measure theory.
Finite sample performance of Dirichlet Laplace priors relative to alternatives
is assessed in simulations."@2012
Natesh S. Pillai@http://arxiv.org/abs/1212.6088v1@Bayesian shrinkage@"Penalized regression methods, such as $L_1$ regularization, are routinely
used in high-dimensional applications, and there is a rich literature on
optimality properties under sparsity assumptions. In the Bayesian paradigm,
sparsity is routinely induced through two-component mixture priors having a
probability mass at zero, but such priors encounter daunting computational
problems in high dimensions. This has motivated an amazing variety of
continuous shrinkage priors, which can be expressed as global-local scale
mixtures of Gaussians, facilitating computation. In sharp contrast to the
corresponding frequentist literature, very little is known about the properties
of such priors. Focusing on a broad class of shrinkage priors, we provide
precise results on prior and posterior concentration. Interestingly, we
demonstrate that most commonly used shrinkage priors, including the Bayesian
Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet
Laplace (DL) priors are proposed, which are optimal and lead to efficient
posterior computation exploiting results from normalized random measure theory.
Finite sample performance of Dirichlet Laplace priors relative to alternatives
is assessed in simulations."@2012
David B. Dunson@http://arxiv.org/abs/1212.6088v1@Bayesian shrinkage@"Penalized regression methods, such as $L_1$ regularization, are routinely
used in high-dimensional applications, and there is a rich literature on
optimality properties under sparsity assumptions. In the Bayesian paradigm,
sparsity is routinely induced through two-component mixture priors having a
probability mass at zero, but such priors encounter daunting computational
problems in high dimensions. This has motivated an amazing variety of
continuous shrinkage priors, which can be expressed as global-local scale
mixtures of Gaussians, facilitating computation. In sharp contrast to the
corresponding frequentist literature, very little is known about the properties
of such priors. Focusing on a broad class of shrinkage priors, we provide
precise results on prior and posterior concentration. Interestingly, we
demonstrate that most commonly used shrinkage priors, including the Bayesian
Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet
Laplace (DL) priors are proposed, which are optimal and lead to efficient
posterior computation exploiting results from normalized random measure theory.
Finite sample performance of Dirichlet Laplace priors relative to alternatives
is assessed in simulations."@2012
