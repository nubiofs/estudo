author@id@title@summary@published_year
Azaïs Romain@http://arxiv.org/abs/1101.2121v2@Optimal quantization applied to Sliced Inverse Regression@"In this paper we consider a semiparametric regression model involving a
$d$-dimensional quantitative explanatory variable $X$ and including a dimension
reduction of $X$ via an index $\beta'X$. In this model, the main goal is to
estimate the euclidean parameter $\beta$ and to predict the real response
variable $Y$ conditionally to $X$. Our approach is based on sliced inverse
regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We
obtain the convergence of the proposed estimators of $\beta$ and of the
conditional distribution. Simulation studies show the good numerical behavior
of the proposed estimators for finite sample size."@2011
Gégout-Petit Anne@http://arxiv.org/abs/1101.2121v2@Optimal quantization applied to Sliced Inverse Regression@"In this paper we consider a semiparametric regression model involving a
$d$-dimensional quantitative explanatory variable $X$ and including a dimension
reduction of $X$ via an index $\beta'X$. In this model, the main goal is to
estimate the euclidean parameter $\beta$ and to predict the real response
variable $Y$ conditionally to $X$. Our approach is based on sliced inverse
regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We
obtain the convergence of the proposed estimators of $\beta$ and of the
conditional distribution. Simulation studies show the good numerical behavior
of the proposed estimators for finite sample size."@2011
Saracco Jérôme@http://arxiv.org/abs/1101.2121v2@Optimal quantization applied to Sliced Inverse Regression@"In this paper we consider a semiparametric regression model involving a
$d$-dimensional quantitative explanatory variable $X$ and including a dimension
reduction of $X$ via an index $\beta'X$. In this model, the main goal is to
estimate the euclidean parameter $\beta$ and to predict the real response
variable $Y$ conditionally to $X$. Our approach is based on sliced inverse
regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We
obtain the convergence of the proposed estimators of $\beta$ and of the
conditional distribution. Simulation studies show the good numerical behavior
of the proposed estimators for finite sample size."@2011
Pierre Alquier@http://arxiv.org/abs/1101.3229v2@Sparse single-index model@"Let $(\bX, Y)$ be a random pair taking values in $\mathbb R^p \times \mathbb
R$. In the so-called single-index model, one has $Y=f^{\star}(\theta^{\star
T}\bX)+\bW$, where $f^{\star}$ is an unknown univariate measurable function,
$\theta^{\star}$ is an unknown vector in $\mathbb R^d$, and $W$ denotes a
random noise satisfying $\mathbb E[\bW|\bX]=0$. The single-index model is known
to offer a flexible way to model a variety of high-dimensional real-world
phenomena. However, despite its relative simplicity, this dimension reduction
scheme is faced with severe complications as soon as the underlying dimension
becomes larger than the number of observations (""$p$ larger than $n$""
paradigm). To circumvent this difficulty, we consider the single-index model
estimation problem from a sparsity perspective using a PAC-Bayesian approach.
On the theoretical side, we offer a sharp oracle inequality, which is more
powerful than the best known oracle inequalities for other common procedures of
single-index recovery. The proposed method is implemented by means of the
reversible jump Markov chain Monte Carlo technique and its performance is
compared with that of standard procedures."@2011
Gérard Biau@http://arxiv.org/abs/1101.3229v2@Sparse single-index model@"Let $(\bX, Y)$ be a random pair taking values in $\mathbb R^p \times \mathbb
R$. In the so-called single-index model, one has $Y=f^{\star}(\theta^{\star
T}\bX)+\bW$, where $f^{\star}$ is an unknown univariate measurable function,
$\theta^{\star}$ is an unknown vector in $\mathbb R^d$, and $W$ denotes a
random noise satisfying $\mathbb E[\bW|\bX]=0$. The single-index model is known
to offer a flexible way to model a variety of high-dimensional real-world
phenomena. However, despite its relative simplicity, this dimension reduction
scheme is faced with severe complications as soon as the underlying dimension
becomes larger than the number of observations (""$p$ larger than $n$""
paradigm). To circumvent this difficulty, we consider the single-index model
estimation problem from a sparsity perspective using a PAC-Bayesian approach.
On the theoretical side, we offer a sharp oracle inequality, which is more
powerful than the best known oracle inequalities for other common procedures of
single-index recovery. The proposed method is implemented by means of the
reversible jump Markov chain Monte Carlo technique and its performance is
compared with that of standard procedures."@2011
Piet Groeneboom@http://arxiv.org/abs/1101.3306v1@Isotonic L_2-projection test for local monotonicity of a hazard@"We introduce a new test statistic for testing the null hypothesis that the
sampling distribution has an increasing hazard rate on a specified interval
[0,a]. It is based on a comparison of the empirical distribution function with
an isotonic estimate, using the restriction that the hazard is increasing, and
measures the excursions of the empirical distribution above the isotonic
estimate, due to local non-monotonicity. It is proved in the companion paper
Groeneboom and Jongbloed (2011a) that the test statistic is asymptotically
normal if the hazard is strictly increasing on the interval [0,a] and certain
regularity conditions are satisfied. We discuss a bootstrap method for
computing the critical values and compare the test, thus obtained, with other
proposals in a simulation study."@2011
Geurt Jongbloed@http://arxiv.org/abs/1101.3306v1@Isotonic L_2-projection test for local monotonicity of a hazard@"We introduce a new test statistic for testing the null hypothesis that the
sampling distribution has an increasing hazard rate on a specified interval
[0,a]. It is based on a comparison of the empirical distribution function with
an isotonic estimate, using the restriction that the hazard is increasing, and
measures the excursions of the empirical distribution above the isotonic
estimate, due to local non-monotonicity. It is proved in the companion paper
Groeneboom and Jongbloed (2011a) that the test statistic is asymptotically
normal if the hazard is strictly increasing on the interval [0,a] and certain
regularity conditions are satisfied. We discuss a bootstrap method for
computing the critical values and compare the test, thus obtained, with other
proposals in a simulation study."@2011
Piet Groeneboom@http://arxiv.org/abs/1101.3333v2@Testing monotonicity of a hazard: asymptotic distribution theory@"Two new test statistics are introduced to test the null hypotheses that the
sampling distribution has an increasing hazard rate on a specified interval
[0,a]. These statistics are empirical L_1-type distances between the isotonic
estimates, which use the monotonicity constraint, and either the empirical
distribution function or the empirical cumulative hazard. They measure the
excursions of the empirical estimates with respect to the isotonic estimates,
due to local non-monotonicity. Asymptotic normality of the test statistics, if
the hazard is strictly increasing on [0,a], is established under mild
conditions. This is done by first approximating the global empirical distance
by an distance with respect to the underlying distribution function. The
resulting integral is treated as sum of increasingly many local integrals to
which a CLT can be applied. The behavior of the local integrals is determined
by a canonical process: the difference between the stochastic process x ->
W(x)+x^2 where W is standard two-sided Brownian Motion, and its greatest convex
minorant."@2011
Geurt Jongbloed@http://arxiv.org/abs/1101.3333v2@Testing monotonicity of a hazard: asymptotic distribution theory@"Two new test statistics are introduced to test the null hypotheses that the
sampling distribution has an increasing hazard rate on a specified interval
[0,a]. These statistics are empirical L_1-type distances between the isotonic
estimates, which use the monotonicity constraint, and either the empirical
distribution function or the empirical cumulative hazard. They measure the
excursions of the empirical estimates with respect to the isotonic estimates,
due to local non-monotonicity. Asymptotic normality of the test statistics, if
the hazard is strictly increasing on [0,a], is established under mild
conditions. This is done by first approximating the global empirical distance
by an distance with respect to the underlying distribution function. The
resulting integral is treated as sum of increasingly many local integrals to
which a CLT can be applied. The behavior of the local integrals is determined
by a canonical process: the difference between the stochastic process x ->
W(x)+x^2 where W is standard two-sided Brownian Motion, and its greatest convex
minorant."@2011
Reman Abu-Shanab@http://arxiv.org/abs/1101.3412v1@Shrinkage estimation with a matrix loss function@"Consider estimating the n by p matrix of means of an n by p matrix of
independent normally distributed observations with constant variance, where the
performance of an estimator is judged using a p by p matrix quadratic error
loss function. A matrix version of the James-Stein estimator is proposed,
depending on a tuning constant. It is shown to dominate the usual maximum
likelihood estimator for some choices of of the tuning constant when n is
greater than or equal to 3. This result also extends to other shrinkage
estimators and settings."@2011
John T. Kent@http://arxiv.org/abs/1101.3412v1@Shrinkage estimation with a matrix loss function@"Consider estimating the n by p matrix of means of an n by p matrix of
independent normally distributed observations with constant variance, where the
performance of an estimator is judged using a p by p matrix quadratic error
loss function. A matrix version of the James-Stein estimator is proposed,
depending on a tuning constant. It is shown to dominate the usual maximum
likelihood estimator for some choices of of the tuning constant when n is
greater than or equal to 3. This result also extends to other shrinkage
estimators and settings."@2011
William E. Strawderman@http://arxiv.org/abs/1101.3412v1@Shrinkage estimation with a matrix loss function@"Consider estimating the n by p matrix of means of an n by p matrix of
independent normally distributed observations with constant variance, where the
performance of an estimator is judged using a p by p matrix quadratic error
loss function. A matrix version of the James-Stein estimator is proposed,
depending on a tuning constant. It is shown to dominate the usual maximum
likelihood estimator for some choices of of the tuning constant when n is
greater than or equal to 3. This result also extends to other shrinkage
estimators and settings."@2011
Helene Gehrmann@http://arxiv.org/abs/1101.3709v3@Estimation of means in graphical Gaussian models with symmetries@"We study the problem of estimability of means in undirected graphical
Gaussian models with symmetry restrictions represented by a colored graph.
Following on from previous studies, we partition the variables into sets of
vertices whose corresponding means are restricted to being identical. We find a
necessary and sufficient condition on the partition to ensure equality between
the maximum likelihood and least-squares estimators of the mean."@2011
Steffen L. Lauritzen@http://arxiv.org/abs/1101.3709v3@Estimation of means in graphical Gaussian models with symmetries@"We study the problem of estimability of means in undirected graphical
Gaussian models with symmetry restrictions represented by a colored graph.
Following on from previous studies, we partition the variables into sets of
vertices whose corresponding means are restricted to being identical. We find a
necessary and sufficient condition on the partition to ensure equality between
the maximum likelihood and least-squares estimators of the mean."@2011
Michel Broniatowski@http://arxiv.org/abs/1101.4352v1@"Upper bounds for the error in some interpolation and extrapolation
  designs"@"This paper deals with probabilistic upper bounds for the error in functional
estimation defined on some interpolation and extrapolation designs, when the
function to estimate is supposed to be analytic. The error pertaining to the
estimate may depend on various factors: the frequency of observations on the
knots, the position and number of the knots, and also on the error committed
when approximating the function through its Taylor expansion. When the number
of observations is fixed, then all these parameters are determined by the
choice of the design and by the choice estimator of the unknown function. The
scope of the paper is therefore to determine a rule for the minimal number of
observation required to achieve an upper bound of the error on the estimate
with a given maximal probability."@2011
Giorgio Celant@http://arxiv.org/abs/1101.4352v1@"Upper bounds for the error in some interpolation and extrapolation
  designs"@"This paper deals with probabilistic upper bounds for the error in functional
estimation defined on some interpolation and extrapolation designs, when the
function to estimate is supposed to be analytic. The error pertaining to the
estimate may depend on various factors: the frequency of observations on the
knots, the position and number of the knots, and also on the error committed
when approximating the function through its Taylor expansion. When the number
of observations is fixed, then all these parameters are determined by the
choice of the design and by the choice estimator of the unknown function. The
scope of the paper is therefore to determine a rule for the minimal number of
observation required to achieve an upper bound of the error on the estimate
with a given maximal probability."@2011
Marco Di Battista@http://arxiv.org/abs/1101.4352v1@"Upper bounds for the error in some interpolation and extrapolation
  designs"@"This paper deals with probabilistic upper bounds for the error in functional
estimation defined on some interpolation and extrapolation designs, when the
function to estimate is supposed to be analytic. The error pertaining to the
estimate may depend on various factors: the frequency of observations on the
knots, the position and number of the knots, and also on the error committed
when approximating the function through its Taylor expansion. When the number
of observations is fixed, then all these parameters are determined by the
choice of the design and by the choice estimator of the unknown function. The
scope of the paper is therefore to determine a rule for the minimal number of
observation required to achieve an upper bound of the error on the estimate
with a given maximal probability."@2011
Samuela Leoni-Aubin@http://arxiv.org/abs/1101.4352v1@"Upper bounds for the error in some interpolation and extrapolation
  designs"@"This paper deals with probabilistic upper bounds for the error in functional
estimation defined on some interpolation and extrapolation designs, when the
function to estimate is supposed to be analytic. The error pertaining to the
estimate may depend on various factors: the frequency of observations on the
knots, the position and number of the knots, and also on the error committed
when approximating the function through its Taylor expansion. When the number
of observations is fixed, then all these parameters are determined by the
choice of the design and by the choice estimator of the unknown function. The
scope of the paper is therefore to determine a rule for the minimal number of
observation required to achieve an upper bound of the error on the estimate
with a given maximal probability."@2011
Michel Broniatowski@http://arxiv.org/abs/1101.4353v1@"An estimation method for the chi-square divergence with application to
  test of hypotheses"@"We propose a new definition of the chi-square divergence between
distributions. Based on convexity properties and duality, this version of the
{\chi}^2 is well suited both for the classical applications of the {\chi}^2 for
the analysis of contingency tables and for the statistical tests for parametric
models, for which it has been advocated to be robust against inliers. We
present two applications in testing. In the first one we deal with tests for
finite and infinite numbers of linear constraints, while, in the second one, we
apply {\chi}^2-methodology for parametric testing against contamination."@2011
Samantha Leorato@http://arxiv.org/abs/1101.4353v1@"An estimation method for the chi-square divergence with application to
  test of hypotheses"@"We propose a new definition of the chi-square divergence between
distributions. Based on convexity properties and duality, this version of the
{\chi}^2 is well suited both for the classical applications of the {\chi}^2 for
the analysis of contingency tables and for the statistical tests for parametric
models, for which it has been advocated to be robust against inliers. We
present two applications in testing. In the first one we deal with tests for
finite and infinite numbers of linear constraints, while, in the second one, we
apply {\chi}^2-methodology for parametric testing against contamination."@2011
Matthew Parry@http://arxiv.org/abs/1101.5011v4@Proper local scoring rules@"We investigate proper scoring rules for continuous distributions on the real
line. It is known that the log score is the only such rule that depends on the
quoted density only through its value at the outcome that materializes. Here we
allow further dependence on a finite number $m$ of derivatives of the density
at the outcome, and describe a large class of such $m$-local proper scoring
rules: these exist for all even $m$ but no odd $m$. We further show that for
$m\geq2$ all such $m$-local rules can be computed without knowledge of the
normalizing constant of the distribution."@2011
A. Philip Dawid@http://arxiv.org/abs/1101.5011v4@Proper local scoring rules@"We investigate proper scoring rules for continuous distributions on the real
line. It is known that the log score is the only such rule that depends on the
quoted density only through its value at the outcome that materializes. Here we
allow further dependence on a finite number $m$ of derivatives of the density
at the outcome, and describe a large class of such $m$-local proper scoring
rules: these exist for all even $m$ but no odd $m$. We further show that for
$m\geq2$ all such $m$-local rules can be computed without knowledge of the
normalizing constant of the distribution."@2011
Steffen Lauritzen@http://arxiv.org/abs/1101.5011v4@Proper local scoring rules@"We investigate proper scoring rules for continuous distributions on the real
line. It is known that the log score is the only such rule that depends on the
quoted density only through its value at the outcome that materializes. Here we
allow further dependence on a finite number $m$ of derivatives of the density
at the outcome, and describe a large class of such $m$-local proper scoring
rules: these exist for all even $m$ but no odd $m$. We further show that for
$m\geq2$ all such $m$-local rules can be computed without knowledge of the
normalizing constant of the distribution."@2011
Alexander Meister@http://arxiv.org/abs/1101.5248v1@"Asymptotic Equivalence for Nonparametric Regression with Non-Regular
  Errors"@"Asymptotic equivalence in Le Cam's sense for nonparametric regression
experiments is extended to the case of non-regular error densities, which have
jump discontinuities at their endpoints. We prove asymptotic equivalence of
such regression models and the observation of two independent Poisson point
processes which contain the target curve as the support boundary of its
intensity function. The intensity of the point processes is of order of the
sample size $n$ and involves the jump sizes as well as the design density. The
statistical model significantly differs from regression problems with Gaussian
or regular errors, which are known to be asymptotically equivalent to Gaussian
white noise models."@2011
Markus Reiß@http://arxiv.org/abs/1101.5248v1@"Asymptotic Equivalence for Nonparametric Regression with Non-Regular
  Errors"@"Asymptotic equivalence in Le Cam's sense for nonparametric regression
experiments is extended to the case of non-regular error densities, which have
jump discontinuities at their endpoints. We prove asymptotic equivalence of
such regression models and the observation of two independent Poisson point
processes which contain the target curve as the support boundary of its
intensity function. The intensity of the point processes is of order of the
sample size $n$ and involves the jump sizes as well as the design density. The
statistical model significantly differs from regression problems with Gaussian
or regular errors, which are known to be asymptotically equivalent to Gaussian
white noise models."@2011
Paul Kabaila@http://arxiv.org/abs/1101.5461v2@The performance of a two-stage analysis of ABAB/BABA crossover trials@"Freeman has considered the following two-stage procedure for finding a
confidence interval for the treatment difference theta, using data from an
AB/BA crossover trial. In the first stage, a preliminary test of the null
hypothesis that the differential carryover is zero, is carried out. If this
hypothesis is accepted then the confidence interval for theta is constructed
assuming that the differential carryover is zero. If, on the other hand, this
hypothesis is rejected then this confidence interval is constructed using only
data from the first period. Freeman has shown that this confidence interval has
minimum coverage probability far below nominal. He therefore concludes that
this confidence interval should not be used. In the present paper, we analyse
the performance of a similar two-stage procedure for an ABAB/BABA crossover
trial. This trial differs in very significant ways from an AB/BA crossover
trial, including the fact that for an ABAB/BABA crossover trial there is an
unbiased estimator of the differential carryover that is unaffected by
between-subject variation. Despite these great differences, we arrive at the
same conclusion as Freeman. Namely, that the confidence interval resulting from
the two-stage procedure should not be used."@2011
Matthew Vicendese@http://arxiv.org/abs/1101.5461v2@The performance of a two-stage analysis of ABAB/BABA crossover trials@"Freeman has considered the following two-stage procedure for finding a
confidence interval for the treatment difference theta, using data from an
AB/BA crossover trial. In the first stage, a preliminary test of the null
hypothesis that the differential carryover is zero, is carried out. If this
hypothesis is accepted then the confidence interval for theta is constructed
assuming that the differential carryover is zero. If, on the other hand, this
hypothesis is rejected then this confidence interval is constructed using only
data from the first period. Freeman has shown that this confidence interval has
minimum coverage probability far below nominal. He therefore concludes that
this confidence interval should not be used. In the present paper, we analyse
the performance of a similar two-stage procedure for an ABAB/BABA crossover
trial. This trial differs in very significant ways from an AB/BA crossover
trial, including the fact that for an ABAB/BABA crossover trial there is an
unbiased estimator of the differential carryover that is unaffected by
between-subject variation. Despite these great differences, we arrive at the
same conclusion as Freeman. Namely, that the confidence interval resulting from
the two-stage procedure should not be used."@2011
Axel Bücher@http://arxiv.org/abs/1102.0110v3@Multiplier bootstrap of tail copulas with applications@"For the problem of estimating lower tail and upper tail copulas, we propose
two bootstrap procedures for approximating the distribution of the
corresponding empirical tail copulas. The first method uses a multiplier
bootstrap of the empirical tail copula process and requires estimation of the
partial derivatives of the tail copula. The second method avoids this
estimation problem and uses multipliers in the two-dimensional empirical
distribution function and in the estimates of the marginal distributions. For
both multiplier bootstrap procedures, we prove consistency. For these
investigations, we demonstrate that the common assumption of the existence of
continuous partial derivatives in the the literature on tail copula estimation
is so restrictive, such that the tail copula corresponding to tail independence
is the only tail copula with this property. Moreover, we are able to solve this
problem and prove weak convergence of the empirical tail copula process under
nonrestrictive smoothness assumptions that are satisfied for many commonly used
models. These results are applied in several statistical problems, including
minimum distance estimation and goodness-of-fit testing."@2011
Holger Dette@http://arxiv.org/abs/1102.0110v3@Multiplier bootstrap of tail copulas with applications@"For the problem of estimating lower tail and upper tail copulas, we propose
two bootstrap procedures for approximating the distribution of the
corresponding empirical tail copulas. The first method uses a multiplier
bootstrap of the empirical tail copula process and requires estimation of the
partial derivatives of the tail copula. The second method avoids this
estimation problem and uses multipliers in the two-dimensional empirical
distribution function and in the estimates of the marginal distributions. For
both multiplier bootstrap procedures, we prove consistency. For these
investigations, we demonstrate that the common assumption of the existence of
continuous partial derivatives in the the literature on tail copula estimation
is so restrictive, such that the tail copula corresponding to tail independence
is the only tail copula with this property. Moreover, we are able to solve this
problem and prove weak convergence of the empirical tail copula process under
nonrestrictive smoothness assumptions that are satisfied for many commonly used
models. These results are applied in several statistical problems, including
minimum distance estimation and goodness-of-fit testing."@2011
Masoumeh Dashti@http://arxiv.org/abs/1102.0143v1@"Uncertainty quantification and weak approximation of an elliptic inverse
  problem"@"We consider the inverse problem of determining the permeability from the
pressure in a Darcy model of flow in a porous medium. Mathematically the
problem is to find the diffusion coefficient for a linear uniformly elliptic
partial differential equation in divergence form, in a bounded domain in
dimension $d \le 3$, from measurements of the solution in the interior. We
adopt a Bayesian approach to the problem. We place a prior random field measure
on the log permeability, specified through the Karhunen-Lo\`eve expansion of
its draws. We consider Gaussian measures constructed this way, and study the
regularity of functions drawn from them. We also study the Lipschitz properties
of the observation operator mapping the log permeability to the observations.
Combining these regularity and continuity estimates, we show that the posterior
measure is well-defined on a suitable Banach space. Furthermore the posterior
measure is shown to be Lipschitz with respect to the data in the Hellinger
metric, giving rise to a form of well-posedness of the inverse problem.
Determining the posterior measure, given the data, solves the problem of
uncertainty quantification for this inverse problem. In practice the posterior
measure must be approximated in a finite dimensional space. We quantify the
errors incurred by employing a truncated Karhunen-Lo\`eve expansion to
represent this meausure. In particular we study weak convergence of a general
class of locally Lipschitz functions of the log permeability, and apply this
general theory to estimate errors in the posterior mean of the pressure and the
pressure covariance, under refinement of the finite dimensional
Karhunen-Lo\`eve truncation."@2011
Andrew M. Stuart@http://arxiv.org/abs/1102.0143v1@"Uncertainty quantification and weak approximation of an elliptic inverse
  problem"@"We consider the inverse problem of determining the permeability from the
pressure in a Darcy model of flow in a porous medium. Mathematically the
problem is to find the diffusion coefficient for a linear uniformly elliptic
partial differential equation in divergence form, in a bounded domain in
dimension $d \le 3$, from measurements of the solution in the interior. We
adopt a Bayesian approach to the problem. We place a prior random field measure
on the log permeability, specified through the Karhunen-Lo\`eve expansion of
its draws. We consider Gaussian measures constructed this way, and study the
regularity of functions drawn from them. We also study the Lipschitz properties
of the observation operator mapping the log permeability to the observations.
Combining these regularity and continuity estimates, we show that the posterior
measure is well-defined on a suitable Banach space. Furthermore the posterior
measure is shown to be Lipschitz with respect to the data in the Hellinger
metric, giving rise to a form of well-posedness of the inverse problem.
Determining the posterior measure, given the data, solves the problem of
uncertainty quantification for this inverse problem. In practice the posterior
measure must be approximated in a finite dimensional space. We quantify the
errors incurred by employing a truncated Karhunen-Lo\`eve expansion to
represent this meausure. In particular we study weak convergence of a general
class of locally Lipschitz functions of the log permeability, and apply this
general theory to estimate errors in the posterior mean of the pressure and the
pressure covariance, under refinement of the finite dimensional
Karhunen-Lo\`eve truncation."@2011
Axel Bücher@http://arxiv.org/abs/1102.0405v2@"New estimators of the Pickands dependence function and a test for
  extreme-value dependence"@"We propose a new class of estimators for Pickands dependence function which
is based on the concept of minimum distance estimation. An explicit integral
representation of the function $A^*(t)$, which minimizes a weighted
$L^2$-distance between the logarithm of the copula $C(y^{1-t},y^t)$ and
functions of the form $A(t)\log(y)$ is derived. If the unknown copula is an
extreme-value copula, the function $A^*(t)$ coincides with Pickands dependence
function. Moreover, even if this is not the case, the function $A^*(t)$ always
satisfies the boundary conditions of a Pickands dependence function. The
estimators are obtained by replacing the unknown copula by its empirical
counterpart and weak convergence of the corresponding process is shown. A
comparison with the commonly used estimators is performed from a theoretical
point of view and by means of a simulation study. Our asymptotic and numerical
results indicate that some of the new estimators outperform the estimators,
which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009)
2990--3022]. As a by-product of our results, we obtain a simple test for the
hypothesis of an extreme-value copula, which is consistent against all positive
quadrant dependent alternatives satisfying weak differentiability assumptions
of first order."@2011
Holger Dette@http://arxiv.org/abs/1102.0405v2@"New estimators of the Pickands dependence function and a test for
  extreme-value dependence"@"We propose a new class of estimators for Pickands dependence function which
is based on the concept of minimum distance estimation. An explicit integral
representation of the function $A^*(t)$, which minimizes a weighted
$L^2$-distance between the logarithm of the copula $C(y^{1-t},y^t)$ and
functions of the form $A(t)\log(y)$ is derived. If the unknown copula is an
extreme-value copula, the function $A^*(t)$ coincides with Pickands dependence
function. Moreover, even if this is not the case, the function $A^*(t)$ always
satisfies the boundary conditions of a Pickands dependence function. The
estimators are obtained by replacing the unknown copula by its empirical
counterpart and weak convergence of the corresponding process is shown. A
comparison with the commonly used estimators is performed from a theoretical
point of view and by means of a simulation study. Our asymptotic and numerical
results indicate that some of the new estimators outperform the estimators,
which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009)
2990--3022]. As a by-product of our results, we obtain a simple test for the
hypothesis of an extreme-value copula, which is consistent against all positive
quadrant dependent alternatives satisfying weak differentiability assumptions
of first order."@2011
Stanislav Volgushev@http://arxiv.org/abs/1102.0405v2@"New estimators of the Pickands dependence function and a test for
  extreme-value dependence"@"We propose a new class of estimators for Pickands dependence function which
is based on the concept of minimum distance estimation. An explicit integral
representation of the function $A^*(t)$, which minimizes a weighted
$L^2$-distance between the logarithm of the copula $C(y^{1-t},y^t)$ and
functions of the form $A(t)\log(y)$ is derived. If the unknown copula is an
extreme-value copula, the function $A^*(t)$ coincides with Pickands dependence
function. Moreover, even if this is not the case, the function $A^*(t)$ always
satisfies the boundary conditions of a Pickands dependence function. The
estimators are obtained by replacing the unknown copula by its empirical
counterpart and weak convergence of the corresponding process is shown. A
comparison with the commonly used estimators is performed from a theoretical
point of view and by means of a simulation study. Our asymptotic and numerical
results indicate that some of the new estimators outperform the estimators,
which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009)
2990--3022]. As a by-product of our results, we obtain a simple test for the
hypothesis of an extreme-value copula, which is consistent against all positive
quadrant dependent alternatives satisfying weak differentiability assumptions
of first order."@2011
Alessandro Felluga@http://arxiv.org/abs/1102.0625v3@"Intensive natural distribution as Bernoulli success ratio extension to
  continuous: enhanced Gaussian, continuous Poisson, and phenomena explanation"@"A new distribution named intensive natural distribution is introduced with
the intent of consolidating statistics and empirical data. Based on the
probability derived from the Bernoulli distribution, this method extended also
Poisson distribution to continuous, preserving its skewness. Using this model,
the Horwitz curve has been explained. The theoretical derivation of our method,
which applies to every kind of measurements collected through sampling, is here
supported by a mathematical demonstration and illustrated with several
applications to real data collected from chemical and geotechnical fields. We
compared the proposed intensive natural distribution to other widely used
frequency functions to test the robustness of the proposed method in fitting
the histograms and the probability charts obtained from various intensive
variables."@2011
Stefano Tiziani@http://arxiv.org/abs/1102.0625v3@"Intensive natural distribution as Bernoulli success ratio extension to
  continuous: enhanced Gaussian, continuous Poisson, and phenomena explanation"@"A new distribution named intensive natural distribution is introduced with
the intent of consolidating statistics and empirical data. Based on the
probability derived from the Bernoulli distribution, this method extended also
Poisson distribution to continuous, preserving its skewness. Using this model,
the Horwitz curve has been explained. The theoretical derivation of our method,
which applies to every kind of measurements collected through sampling, is here
supported by a mathematical demonstration and illustrated with several
applications to real data collected from chemical and geotechnical fields. We
compared the proposed intensive natural distribution to other widely used
frequency functions to test the robustness of the proposed method in fitting
the histograms and the probability charts obtained from various intensive
variables."@2011
Antonio Galves@http://arxiv.org/abs/1102.0673v3@Joint estimation of intersecting context tree models@"We study a problem of model selection for data produced by two different
context tree sources. Motivated by linguistic questions, we consider the case
where the probabilistic context trees corresponding to the two sources are
finite and share many of their contexts. In order to understand the differences
between the two sources, it is important to identify which contexts and which
transition probabilities are specific to each source.
  We consider a class of probabilistic context tree models with three types of
contexts: those which appear in one, the other, or both sources. We use a BIC
penalized maximum likelihood procedure that jointly estimates the two sources.
  We propose a new algorithm which efficiently computes the estimated context
trees. We prove that the procedure is strongly consistent. We also present a
simulation study showing the practical advantage of our procedure over a
procedure that works separately on each dataset."@2011
Aurélien Garivier@http://arxiv.org/abs/1102.0673v3@Joint estimation of intersecting context tree models@"We study a problem of model selection for data produced by two different
context tree sources. Motivated by linguistic questions, we consider the case
where the probabilistic context trees corresponding to the two sources are
finite and share many of their contexts. In order to understand the differences
between the two sources, it is important to identify which contexts and which
transition probabilities are specific to each source.
  We consider a class of probabilistic context tree models with three types of
contexts: those which appear in one, the other, or both sources. We use a BIC
penalized maximum likelihood procedure that jointly estimates the two sources.
  We propose a new algorithm which efficiently computes the estimated context
trees. We prove that the procedure is strongly consistent. We also present a
simulation study showing the practical advantage of our procedure over a
procedure that works separately on each dataset."@2011
Elisabeth Gassiat@http://arxiv.org/abs/1102.0673v3@Joint estimation of intersecting context tree models@"We study a problem of model selection for data produced by two different
context tree sources. Motivated by linguistic questions, we consider the case
where the probabilistic context trees corresponding to the two sources are
finite and share many of their contexts. In order to understand the differences
between the two sources, it is important to identify which contexts and which
transition probabilities are specific to each source.
  We consider a class of probabilistic context tree models with three types of
contexts: those which appear in one, the other, or both sources. We use a BIC
penalized maximum likelihood procedure that jointly estimates the two sources.
  We propose a new algorithm which efficiently computes the estimated context
trees. We prove that the procedure is strongly consistent. We also present a
simulation study showing the practical advantage of our procedure over a
procedure that works separately on each dataset."@2011
Zuofeng Shang@http://arxiv.org/abs/1102.0826v2@"Consistency of Bayesian Linear Model Selection With a Growing Number of
  Parameters"@"Linear models with a growing number of parameters have been widely used in
modern statistics. One important problem about this kind of model is the
variable selection issue. Bayesian approaches, which provide a stochastic
search of informative variables, have gained popularity. In this paper, we will
study the asymptotic properties related to Bayesian model selection when the
model dimension $p$ is growing with the sample size $n$. We consider $p\le n$
and provide sufficient conditions under which: (1) with large probability, the
posterior probability of the true model (from which samples are drawn)
uniformly dominates the posterior probability of any incorrect models; and (2)
with large probability, the posterior probability of the true model converges
to one. Both (1) and (2) guarantee that the true model will be selected under a
Bayesian framework. We also demonstrate several situations when (1) holds but
(2) fails, which illustrates the difference between these two properties.
Simulated examples are provided to illustrate the main results."@2011
Murray K. Clayton@http://arxiv.org/abs/1102.0826v2@"Consistency of Bayesian Linear Model Selection With a Growing Number of
  Parameters"@"Linear models with a growing number of parameters have been widely used in
modern statistics. One important problem about this kind of model is the
variable selection issue. Bayesian approaches, which provide a stochastic
search of informative variables, have gained popularity. In this paper, we will
study the asymptotic properties related to Bayesian model selection when the
model dimension $p$ is growing with the sample size $n$. We consider $p\le n$
and provide sufficient conditions under which: (1) with large probability, the
posterior probability of the true model (from which samples are drawn)
uniformly dominates the posterior probability of any incorrect models; and (2)
with large probability, the posterior probability of the true model converges
to one. Both (1) and (2) guarantee that the true model will be selected under a
Bayesian framework. We also demonstrate several situations when (1) holds but
(2) fails, which illustrates the difference between these two properties.
Simulated examples are provided to illustrate the main results."@2011
Pierre Alquier@http://arxiv.org/abs/1102.1615v5@Sparsity considerations for dependent observations@"The aim of this paper is to provide a comprehensive introduction for the
study of L1-penalized estimators in the context of dependent observations. We
define a general $\ell_{1}$-penalized estimator for solving problems of
stochastic optimization. This estimator turns out to be the LASSO in the
regression estimation setting. Powerful theoretical guarantees on the
statistical performances of the LASSO were provided in recent papers, however,
they usually only deal with the iid case. Here, we study our estimator under
various dependence assumptions."@2011
Paul Doukhan@http://arxiv.org/abs/1102.1615v5@Sparsity considerations for dependent observations@"The aim of this paper is to provide a comprehensive introduction for the
study of L1-penalized estimators in the context of dependent observations. We
define a general $\ell_{1}$-penalized estimator for solving problems of
stochastic optimization. This estimator turns out to be the LASSO in the
regression estimation setting. Powerful theoretical guarantees on the
statistical performances of the LASSO were provided in recent papers, however,
they usually only deal with the iid case. Here, we study our estimator under
various dependence assumptions."@2011
Gustavo Didier@http://arxiv.org/abs/1102.1822v1@"Integral representations and properties of operator fractional Brownian
  motions"@"Operator fractional Brownian motions (OFBMs) are (i) Gaussian, (ii) operator
self-similar and (iii) stationary increment processes. They are the natural
multivariate generalizations of the well-studied fractional Brownian motions.
Because of the possible lack of time-reversibility, the defining properties
(i)--(iii) do not, in general, characterize the covariance structure of OFBMs.
To circumvent this problem, the class of OFBMs is characterized here by means
of their integral representations in the spectral and time domains. For the
spectral domain representations, this involves showing how the operator
self-similarity shapes the spectral density in the general representation of
stationary increment processes. The time domain representations are derived by
using primary matrix functions and taking the Fourier transforms of the
deterministic spectral domain kernels. Necessary and sufficient conditions for
OFBMs to be time-reversible are established in terms of their spectral and time
domain representations. It is also shown that the spectral density of the
stationary increments of an OFBM has a rigid structure, here called the
dichotomy principle. The notion of operator Brownian motions is also explored."@2011
Vladas Pipiras@http://arxiv.org/abs/1102.1822v1@"Integral representations and properties of operator fractional Brownian
  motions"@"Operator fractional Brownian motions (OFBMs) are (i) Gaussian, (ii) operator
self-similar and (iii) stationary increment processes. They are the natural
multivariate generalizations of the well-studied fractional Brownian motions.
Because of the possible lack of time-reversibility, the defining properties
(i)--(iii) do not, in general, characterize the covariance structure of OFBMs.
To circumvent this problem, the class of OFBMs is characterized here by means
of their integral representations in the spectral and time domains. For the
spectral domain representations, this involves showing how the operator
self-similarity shapes the spectral density in the general representation of
stationary increment processes. The time domain representations are derived by
using primary matrix functions and taking the Fourier transforms of the
deterministic spectral domain kernels. Necessary and sufficient conditions for
OFBMs to be time-reversible are established in terms of their spectral and time
domain representations. It is also shown that the spectral density of the
stationary increments of an OFBM has a rigid structure, here called the
dichotomy principle. The notion of operator Brownian motions is also explored."@2011
Holger Fink@http://arxiv.org/abs/1102.1830v1@"Fractional Lévy-driven Ornstein--Uhlenbeck processes and stochastic
  differential equations"@"Using Riemann-Stieltjes methods for integrators of bounded $p$-variation we
define a pathwise integral driven by a fractional L\'{e}vy process (FLP). To
explicitly solve general fractional stochastic differential equations (SDEs) we
introduce an Ornstein-Uhlenbeck model by a stochastic integral representation,
where the driving stochastic process is an FLP. To achieve the convergence of
improper integrals, the long-time behavior of FLPs is derived. This is
sufficient to define the fractional L\'{e}vy-Ornstein-Uhlenbeck process (FLOUP)
pathwise as an improper Riemann-Stieltjes integral. We show further that the
FLOUP is the unique stationary solution of the corresponding Langevin equation.
Furthermore, we calculate the autocovariance function and prove that its
increments exhibit long-range dependence. Exploiting the Langevin equation, we
consider SDEs driven by FLPs of bounded $p$-variation for $p<2$ and construct
solutions using the corresponding FLOUP. Finally, we consider examples of such
SDEs, including various state space transforms of the FLOUP and also fractional
L\'{e}vy-driven Cox-Ingersoll-Ross (CIR) models."@2011
Claudia Klüppelberg@http://arxiv.org/abs/1102.1830v1@"Fractional Lévy-driven Ornstein--Uhlenbeck processes and stochastic
  differential equations"@"Using Riemann-Stieltjes methods for integrators of bounded $p$-variation we
define a pathwise integral driven by a fractional L\'{e}vy process (FLP). To
explicitly solve general fractional stochastic differential equations (SDEs) we
introduce an Ornstein-Uhlenbeck model by a stochastic integral representation,
where the driving stochastic process is an FLP. To achieve the convergence of
improper integrals, the long-time behavior of FLPs is derived. This is
sufficient to define the fractional L\'{e}vy-Ornstein-Uhlenbeck process (FLOUP)
pathwise as an improper Riemann-Stieltjes integral. We show further that the
FLOUP is the unique stationary solution of the corresponding Langevin equation.
Furthermore, we calculate the autocovariance function and prove that its
increments exhibit long-range dependence. Exploiting the Langevin equation, we
consider SDEs driven by FLPs of bounded $p$-variation for $p<2$ and construct
solutions using the corresponding FLOUP. Finally, we consider examples of such
SDEs, including various state space transforms of the FLOUP and also fractional
L\'{e}vy-driven Cox-Ingersoll-Ross (CIR) models."@2011
Oliver Linton@http://arxiv.org/abs/1102.1857v1@Nonparametric regression with filtered data@"We present a general principle for estimating a regression function
nonparametrically, allowing for a wide variety of data filtering, for example,
repeated left truncation and right censoring. Both the mean and the median
regression cases are considered. The method works by first estimating the
conditional hazard function or conditional survivor function and then
integrating. We also investigate improved methods that take account of model
structure such as independent errors and show that such methods can improve
performance when the model structure is true. We establish the pointwise
asymptotic normality of our estimators."@2011
Enno Mammen@http://arxiv.org/abs/1102.1857v1@Nonparametric regression with filtered data@"We present a general principle for estimating a regression function
nonparametrically, allowing for a wide variety of data filtering, for example,
repeated left truncation and right censoring. Both the mean and the median
regression cases are considered. The method works by first estimating the
conditional hazard function or conditional survivor function and then
integrating. We also investigate improved methods that take account of model
structure such as independent errors and show that such methods can improve
performance when the model structure is true. We establish the pointwise
asymptotic normality of our estimators."@2011
Jens Perch Nielsen@http://arxiv.org/abs/1102.1857v1@Nonparametric regression with filtered data@"We present a general principle for estimating a regression function
nonparametrically, allowing for a wide variety of data filtering, for example,
repeated left truncation and right censoring. Both the mean and the median
regression cases are considered. The method works by first estimating the
conditional hazard function or conditional survivor function and then
integrating. We also investigate improved methods that take account of model
structure such as independent errors and show that such methods can improve
performance when the model structure is true. We establish the pointwise
asymptotic normality of our estimators."@2011
Ingrid Van Keilegom@http://arxiv.org/abs/1102.1857v1@Nonparametric regression with filtered data@"We present a general principle for estimating a regression function
nonparametrically, allowing for a wide variety of data filtering, for example,
repeated left truncation and right censoring. Both the mean and the median
regression cases are considered. The method works by first estimating the
conditional hazard function or conditional survivor function and then
integrating. We also investigate improved methods that take account of model
structure such as independent errors and show that such methods can improve
performance when the model structure is true. We establish the pointwise
asymptotic normality of our estimators."@2011
Piet Groeneboom@http://arxiv.org/abs/1102.1875v1@"Smooth plug-in inverse estimators in the current status continuous mark
  model"@"We consider the problem of estimating the joint distribution function of the
event time and a continuous mark variable when the event time is subject to
interval censoring case 1 and the continuous mark variable is only observed in
case the event occurred before the time of inspection. The nonparametric
maximum likelihood estimator in this model is known to be inconsistent. We
study two alternative smooth estimators, based on the explicit (inverse)
expression of the distribution function of interest in terms of the density of
the observable vector. We derive the pointwise asymptotic distribution of both
estimators."@2011
Geurt Jongbloed@http://arxiv.org/abs/1102.1875v1@"Smooth plug-in inverse estimators in the current status continuous mark
  model"@"We consider the problem of estimating the joint distribution function of the
event time and a continuous mark variable when the event time is subject to
interval censoring case 1 and the continuous mark variable is only observed in
case the event occurred before the time of inspection. The nonparametric
maximum likelihood estimator in this model is known to be inconsistent. We
study two alternative smooth estimators, based on the explicit (inverse)
expression of the distribution function of interest in terms of the density of
the observable vector. We derive the pointwise asymptotic distribution of both
estimators."@2011
Birgit Witte@http://arxiv.org/abs/1102.1875v1@"Smooth plug-in inverse estimators in the current status continuous mark
  model"@"We consider the problem of estimating the joint distribution function of the
event time and a continuous mark variable when the event time is subject to
interval censoring case 1 and the continuous mark variable is only observed in
case the event occurred before the time of inspection. The nonparametric
maximum likelihood estimator in this model is known to be inconsistent. We
study two alternative smooth estimators, based on the explicit (inverse)
expression of the distribution function of interest in terms of the density of
the observable vector. We derive the pointwise asymptotic distribution of both
estimators."@2011
Hongyuan Cao@http://arxiv.org/abs/1102.2046v2@Simultaneous critical values for $t$-tests in very high dimensions@"This article considers the problem of multiple hypothesis testing using
$t$-tests. The observed data are assumed to be independently generated
conditional on an underlying and unknown two-state hidden model. We propose an
asymptotically valid data-driven procedure to find critical values for
rejection regions controlling the $k$-familywise error rate ($k$-FWER), false
discovery rate (FDR) and the tail probability of false discovery proportion
(FDTP) by using one-sample and two-sample $t$-statistics. We only require a
finite fourth moment plus some very general conditions on the mean and variance
of the population by virtue of the moderate deviations properties of
$t$-statistics. A new consistent estimator for the proportion of alternative
hypotheses is developed. Simulation studies support our theoretical results and
demonstrate that the power of a multiple testing procedure can be substantially
improved by using critical values directly, as opposed to the conventional
$p$-value approach. Our method is applied in an analysis of the microarray data
from a leukemia cancer study that involves testing a large number of hypotheses
simultaneously."@2011
Michael R. Kosorok@http://arxiv.org/abs/1102.2046v2@Simultaneous critical values for $t$-tests in very high dimensions@"This article considers the problem of multiple hypothesis testing using
$t$-tests. The observed data are assumed to be independently generated
conditional on an underlying and unknown two-state hidden model. We propose an
asymptotically valid data-driven procedure to find critical values for
rejection regions controlling the $k$-familywise error rate ($k$-FWER), false
discovery rate (FDR) and the tail probability of false discovery proportion
(FDTP) by using one-sample and two-sample $t$-statistics. We only require a
finite fourth moment plus some very general conditions on the mean and variance
of the population by virtue of the moderate deviations properties of
$t$-statistics. A new consistent estimator for the proportion of alternative
hypotheses is developed. Simulation studies support our theoretical results and
demonstrate that the power of a multiple testing procedure can be substantially
improved by using critical values directly, as opposed to the conventional
$p$-value approach. Our method is applied in an analysis of the microarray data
from a leukemia cancer study that involves testing a large number of hypotheses
simultaneously."@2011
Piotr Fryzlewicz@http://arxiv.org/abs/1102.2053v1@Mixing properties of ARCH and time-varying ARCH processes@"There exist very few results on mixing for non-stationary processes. However,
mixing is often required in statistical inference for non-stationary processes
such as time-varying ARCH (tvARCH) models. In this paper, bounds for the mixing
rates of a stochastic process are derived in terms of the conditional densities
of the process. These bounds are used to obtain the $\alpha$, 2-mixing and
$\beta$-mixing rates of the non-stationary time-varying $\operatorname
{ARCH}(p)$ process and $\operatorname {ARCH}(\infty)$ process. It is shown that
the mixing rate of the time-varying $\operatorname {ARCH}(p)$ process is
geometric, whereas the bound on the mixing rate of the $\operatorname
{ARCH}(\infty)$ process depends on the rate of decay of the $\operatorname
{ARCH}(\infty)$ parameters. We note that the methodology given in this paper is
applicable to other processes."@2011
Suhasini Subba Rao@http://arxiv.org/abs/1102.2053v1@Mixing properties of ARCH and time-varying ARCH processes@"There exist very few results on mixing for non-stationary processes. However,
mixing is often required in statistical inference for non-stationary processes
such as time-varying ARCH (tvARCH) models. In this paper, bounds for the mixing
rates of a stochastic process are derived in terms of the conditional densities
of the process. These bounds are used to obtain the $\alpha$, 2-mixing and
$\beta$-mixing rates of the non-stationary time-varying $\operatorname
{ARCH}(p)$ process and $\operatorname {ARCH}(\infty)$ process. It is shown that
the mixing rate of the time-varying $\operatorname {ARCH}(p)$ process is
geometric, whereas the bound on the mixing rate of the $\operatorname
{ARCH}(\infty)$ process depends on the rate of decay of the $\operatorname
{ARCH}(\infty)$ parameters. We note that the methodology given in this paper is
applicable to other processes."@2011
Christian Genest@http://arxiv.org/abs/1102.2078v1@A goodness-of-fit test for bivariate extreme-value copulas@"It is often reasonable to assume that the dependence structure of a bivariate
continuous distribution belongs to the class of extreme-value copulas. The
latter are characterized by their Pickands dependence function. In this paper,
a procedure is proposed for testing whether this function belongs to a given
parametric family. The test is based on a Cram\'{e}r--von Mises statistic
measuring the distance between an estimate of the parametric Pickands
dependence function and either one of two nonparametric estimators thereof
studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the
limiting distribution of the test statistic depends on unknown parameters, it
must be estimated via a parametric bootstrap procedure, the validity of which
is established. Monte Carlo simulations are used to assess the power of the
test and an extension to dependence structures that are left-tail decreasing in
both variables is considered."@2011
Ivan Kojadinovic@http://arxiv.org/abs/1102.2078v1@A goodness-of-fit test for bivariate extreme-value copulas@"It is often reasonable to assume that the dependence structure of a bivariate
continuous distribution belongs to the class of extreme-value copulas. The
latter are characterized by their Pickands dependence function. In this paper,
a procedure is proposed for testing whether this function belongs to a given
parametric family. The test is based on a Cram\'{e}r--von Mises statistic
measuring the distance between an estimate of the parametric Pickands
dependence function and either one of two nonparametric estimators thereof
studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the
limiting distribution of the test statistic depends on unknown parameters, it
must be estimated via a parametric bootstrap procedure, the validity of which
is established. Monte Carlo simulations are used to assess the power of the
test and an extension to dependence structures that are left-tail decreasing in
both variables is considered."@2011
Johanna Nešlehová@http://arxiv.org/abs/1102.2078v1@A goodness-of-fit test for bivariate extreme-value copulas@"It is often reasonable to assume that the dependence structure of a bivariate
continuous distribution belongs to the class of extreme-value copulas. The
latter are characterized by their Pickands dependence function. In this paper,
a procedure is proposed for testing whether this function belongs to a given
parametric family. The test is based on a Cram\'{e}r--von Mises statistic
measuring the distance between an estimate of the parametric Pickands
dependence function and either one of two nonparametric estimators thereof
studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the
limiting distribution of the test statistic depends on unknown parameters, it
must be estimated via a parametric bootstrap procedure, the validity of which
is established. Monte Carlo simulations are used to assess the power of the
test and an extension to dependence structures that are left-tail decreasing in
both variables is considered."@2011
Jun Yan@http://arxiv.org/abs/1102.2078v1@A goodness-of-fit test for bivariate extreme-value copulas@"It is often reasonable to assume that the dependence structure of a bivariate
continuous distribution belongs to the class of extreme-value copulas. The
latter are characterized by their Pickands dependence function. In this paper,
a procedure is proposed for testing whether this function belongs to a given
parametric family. The test is based on a Cram\'{e}r--von Mises statistic
measuring the distance between an estimate of the parametric Pickands
dependence function and either one of two nonparametric estimators thereof
studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the
limiting distribution of the test statistic depends on unknown parameters, it
must be estimated via a parametric bootstrap procedure, the validity of which
is established. Monte Carlo simulations are used to assess the power of the
test and an extension to dependence structures that are left-tail decreasing in
both variables is considered."@2011
Ingo Steinwart@http://arxiv.org/abs/1102.2101v1@Estimating conditional quantiles with the help of the pinball loss@"The so-called pinball loss for estimating conditional quantiles is a
well-known tool in both statistics and machine learning. So far, however, only
little work has been done to quantify the efficiency of this tool for
nonparametric approaches. We fill this gap by establishing inequalities that
describe how close approximate pinball risk minimizers are to the corresponding
conditional quantile. These inequalities, which hold under mild assumptions on
the data-generating distribution, are then used to establish so-called variance
bounds, which recently turned out to play an important role in the statistical
analysis of (regularized) empirical risk minimization approaches. Finally, we
use both types of inequalities to establish an oracle inequality for support
vector machines that use the pinball loss. The resulting learning rates are
min--max optimal under some standard regularity assumptions on the conditional
quantile."@2011
Andreas Christmann@http://arxiv.org/abs/1102.2101v1@Estimating conditional quantiles with the help of the pinball loss@"The so-called pinball loss for estimating conditional quantiles is a
well-known tool in both statistics and machine learning. So far, however, only
little work has been done to quantify the efficiency of this tool for
nonparametric approaches. We fill this gap by establishing inequalities that
describe how close approximate pinball risk minimizers are to the corresponding
conditional quantile. These inequalities, which hold under mild assumptions on
the data-generating distribution, are then used to establish so-called variance
bounds, which recently turned out to play an important role in the statistical
analysis of (regularized) empirical risk minimization approaches. Finally, we
use both types of inequalities to establish an oracle inequality for support
vector machines that use the pinball loss. The resulting learning rates are
min--max optimal under some standard regularity assumptions on the conditional
quantile."@2011
Daniel Bruynooghe@http://arxiv.org/abs/1102.2118v1@Differential cumulants, hierachical models and monomial ideals@"For a joint probability density function f(x) of a random vector X the mixed
partial derivatives of log f(x) can be interpreted as limiting cumulants in an
infinitesimally small open neighborhood around x. Moreover, setting them to
zero everywhere gives independence and conditional independence conditions. The
latter conditions can be mapped, using an algebraic differential duality, into
monomial ideal conditions. This provides an isomorphism between hierarchical
models and monomial ideals. It is thus shown that certain monomial ideals are
associated with particular classes of hierarchical models."@2011
Henry P. Wynn@http://arxiv.org/abs/1102.2118v1@Differential cumulants, hierachical models and monomial ideals@"For a joint probability density function f(x) of a random vector X the mixed
partial derivatives of log f(x) can be interpreted as limiting cumulants in an
infinitesimally small open neighborhood around x. Moreover, setting them to
zero everywhere gives independence and conditional independence conditions. The
latter conditions can be mapped, using an algebraic differential duality, into
monomial ideal conditions. This provides an isomorphism between hierarchical
models and monomial ideals. It is thus shown that certain monomial ideals are
associated with particular classes of hierarchical models."@2011
Yutao Ma@http://arxiv.org/abs/1102.2297v1@Transportation inequalities: From Poisson to Gibbs measures@"We establish an optimal transportation inequality for the Poisson measure on
the configuration space. Furthermore, under the Dobrushin uniqueness condition,
we obtain a sharp transportation inequality for the Gibbs measure on
$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration
space."@2011
Shi Shen@http://arxiv.org/abs/1102.2297v1@Transportation inequalities: From Poisson to Gibbs measures@"We establish an optimal transportation inequality for the Poisson measure on
the configuration space. Furthermore, under the Dobrushin uniqueness condition,
we obtain a sharp transportation inequality for the Gibbs measure on
$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration
space."@2011
Xinyu Wang@http://arxiv.org/abs/1102.2297v1@Transportation inequalities: From Poisson to Gibbs measures@"We establish an optimal transportation inequality for the Poisson measure on
the configuration space. Furthermore, under the Dobrushin uniqueness condition,
we obtain a sharp transportation inequality for the Gibbs measure on
$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration
space."@2011
Liming Wu@http://arxiv.org/abs/1102.2297v1@Transportation inequalities: From Poisson to Gibbs measures@"We establish an optimal transportation inequality for the Poisson measure on
the configuration space. Furthermore, under the Dobrushin uniqueness condition,
we obtain a sharp transportation inequality for the Gibbs measure on
$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration
space."@2011
Marianna Pensky@http://arxiv.org/abs/1102.2298v1@Multichannel Boxcar Deconvolution with Growing Number of Channels@"We consider the problem of estimating the unknown response function in the
multichannel deconvolution model with a boxcar-like kernel which is of
particular interest in signal processing. It is known that, when the number of
channels is finite, the precision of reconstruction of the response function
increases as the number of channels $M$ grow (even when the total number of
observations $n$ for all channels $M$ remains constant) and this requires that
the parameter of the channels form a Badly Approximable $M$-tuple.
  Recent advances in data collection and recording techniques made it of urgent
interest to study the case when the number of channels $M=M_n$ grow with the
total number of observations $n$. However, in real-life situations, the number
of channels $M = M_n$ usually refers to the number of physical devices and,
consequently, may grow to infinity only at a slow rate as $n \rightarrow
\infty$. When $M=M_n$ grows slowly as $n$ increases, we develop a procedure for
the construction of a Badly Approximable $M$-tuple on a specified interval, of
a non-asymptotic length, together with a lower bound associated with this
$M$-tuple, which explicitly shows its dependence on $M$ as $M$ is growing. This
result is further used for the evaluation of the $L^2$-risk of the suggested
adaptive wavelet thresholding estimator of the unknown response function and,
furthermore, for the choice of the optimal number of channels $M$ which
minimizes the $L^2$-risk."@2011
Theofanis Sapatinas@http://arxiv.org/abs/1102.2298v1@Multichannel Boxcar Deconvolution with Growing Number of Channels@"We consider the problem of estimating the unknown response function in the
multichannel deconvolution model with a boxcar-like kernel which is of
particular interest in signal processing. It is known that, when the number of
channels is finite, the precision of reconstruction of the response function
increases as the number of channels $M$ grow (even when the total number of
observations $n$ for all channels $M$ remains constant) and this requires that
the parameter of the channels form a Badly Approximable $M$-tuple.
  Recent advances in data collection and recording techniques made it of urgent
interest to study the case when the number of channels $M=M_n$ grow with the
total number of observations $n$. However, in real-life situations, the number
of channels $M = M_n$ usually refers to the number of physical devices and,
consequently, may grow to infinity only at a slow rate as $n \rightarrow
\infty$. When $M=M_n$ grows slowly as $n$ increases, we develop a procedure for
the construction of a Badly Approximable $M$-tuple on a specified interval, of
a non-asymptotic length, together with a lower bound associated with this
$M$-tuple, which explicitly shows its dependence on $M$ as $M$ is growing. This
result is further used for the evaluation of the $L^2$-risk of the suggested
adaptive wavelet thresholding estimator of the unknown response function and,
furthermore, for the choice of the optimal number of channels $M$ which
minimizes the $L^2$-risk."@2011
Gerard Kerkyacharian@http://arxiv.org/abs/1102.2450v1@"Concentration Inequalities and Confidence Bands for Needlet Density
  Estimators on Compact Homogeneous Manifolds"@"Let $X_1,...,X_n$ be a random sample from some unknown probability density
$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge
1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised
projection onto the space of eigenfunctions of the Laplace operator on $\mathbf
M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in
\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform
deviations of the linear needlet density estimator $f_n(j)$ obtained from an
empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f
\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive
estimators and nonasymptotic confidence bands for the unknown density $f$. The
confidence bands are adaptive over classes of differentiable and
H\""{older}-continuous functions on $\mathbf M$ that attain their H\""{o}lder
exponents."@2011
Richard Nickl@http://arxiv.org/abs/1102.2450v1@"Concentration Inequalities and Confidence Bands for Needlet Density
  Estimators on Compact Homogeneous Manifolds"@"Let $X_1,...,X_n$ be a random sample from some unknown probability density
$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge
1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised
projection onto the space of eigenfunctions of the Laplace operator on $\mathbf
M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in
\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform
deviations of the linear needlet density estimator $f_n(j)$ obtained from an
empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f
\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive
estimators and nonasymptotic confidence bands for the unknown density $f$. The
confidence bands are adaptive over classes of differentiable and
H\""{older}-continuous functions on $\mathbf M$ that attain their H\""{o}lder
exponents."@2011
Dominique Picard@http://arxiv.org/abs/1102.2450v1@"Concentration Inequalities and Confidence Bands for Needlet Density
  Estimators on Compact Homogeneous Manifolds"@"Let $X_1,...,X_n$ be a random sample from some unknown probability density
$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge
1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised
projection onto the space of eigenfunctions of the Laplace operator on $\mathbf
M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in
\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform
deviations of the linear needlet density estimator $f_n(j)$ obtained from an
empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f
\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive
estimators and nonasymptotic confidence bands for the unknown density $f$. The
confidence bands are adaptive over classes of differentiable and
H\""{older}-continuous functions on $\mathbf M$ that attain their H\""{o}lder
exponents."@2011
Yannick Baraud@http://arxiv.org/abs/1102.2818v2@Estimating composite functions by model selection@"We consider the problem of estimating a function $s$ on $[-1,1]^{k}$ for
large values of $k$ by looking for some best approximation by composite
functions of the form $g\circ u$. Our solution is based on model selection and
leads to a very general approach to solve this problem with respect to many
different types of functions $g,u$ and statistical frameworks. In particular,
we handle the problems of approximating $s$ by additive functions, single and
multiple index models, neural networks, mixtures of Gaussian densities (when
$s$ is a density) among other examples. We also investigate the situation where
$s=g\circ u$ for functions $g$ and $u$ belonging to possibly anisotropic
smoothness classes. In this case, our approach leads to a completely adaptive
estimator with respect to the regularity of $s$."@2011
Lucien Birgé@http://arxiv.org/abs/1102.2818v2@Estimating composite functions by model selection@"We consider the problem of estimating a function $s$ on $[-1,1]^{k}$ for
large values of $k$ by looking for some best approximation by composite
functions of the form $g\circ u$. Our solution is based on model selection and
leads to a very general approach to solve this problem with respect to many
different types of functions $g,u$ and statistical frameworks. In particular,
we handle the problems of approximating $s$ by additive functions, single and
multiple index models, neural networks, mixtures of Gaussian densities (when
$s$ is a density) among other examples. We also investigate the situation where
$s=g\circ u$ for functions $g$ and $u$ belonging to possibly anisotropic
smoothness classes. In this case, our approach leads to a completely adaptive
estimator with respect to the regularity of $s$."@2011
Pierre-Olivier Amblard@http://arxiv.org/abs/1102.2872v1@Identification of the Multivariate Fractional Brownian Motion@"This paper deals with the identification of the multivariate fractional
Brownian motion, a recently developed extension of the fractional Brownian
motion to the multivariate case. This process is a $p$-multivariate
self-similar Gaussian process parameterized by $p$ different Hurst exponents
$H_i$, $p$ scaling coefficients $\sigma_i$ (of each component) and also by
$p(p-1)$ coefficients $\rho_{ij},\eta_{ij}$ (for $i,j=1,...,p$ with $j>i$)
allowing two components to be more or less strongly correlated and allowing the
process to be time reversible or not. We investigate the use of discrete
filtering techniques to estimate jointly or separately the different parameters
and prove the efficiency of the methodology with a simulation study and the
derivation of asymptotic results."@2011
Jean-François Coeurjolly@http://arxiv.org/abs/1102.2872v1@Identification of the Multivariate Fractional Brownian Motion@"This paper deals with the identification of the multivariate fractional
Brownian motion, a recently developed extension of the fractional Brownian
motion to the multivariate case. This process is a $p$-multivariate
self-similar Gaussian process parameterized by $p$ different Hurst exponents
$H_i$, $p$ scaling coefficients $\sigma_i$ (of each component) and also by
$p(p-1)$ coefficients $\rho_{ij},\eta_{ij}$ (for $i,j=1,...,p$ with $j>i$)
allowing two components to be more or less strongly correlated and allowing the
process to be time reversible or not. We investigate the use of discrete
filtering techniques to estimate jointly or separately the different parameters
and prove the efficiency of the methodology with a simulation study and the
derivation of asymptotic results."@2011
Tony Cai@http://arxiv.org/abs/1102.2925v1@"Limiting Laws of Coherence of Random Matrices with Applications to
  Testing Covariance Structure and Construction of Compressed Sensing Matrices"@"Testing covariance structure is of significant interest in many areas of
statistical analysis and construction of compressed sensing matrices is an
important problem in signal processing. Motivated by these applications, we
study in this paper the limiting laws of the coherence of an $n\times p$ random
matrix in the high-dimensional setting where $p$ can be much larger than $n$.
Both the law of large numbers and the limiting distribution are derived. We
then consider testing the bandedness of the covariance matrix of a high
dimensional Gaussian distribution which includes testing for independence as a
special case. The limiting laws of the coherence of the data matrix play a
critical role in the construction of the test. We also apply the asymptotic
results to the construction of compressed sensing matrices."@2011
Tiefeng Jiang@http://arxiv.org/abs/1102.2925v1@"Limiting Laws of Coherence of Random Matrices with Applications to
  Testing Covariance Structure and Construction of Compressed Sensing Matrices"@"Testing covariance structure is of significant interest in many areas of
statistical analysis and construction of compressed sensing matrices is an
important problem in signal processing. Motivated by these applications, we
study in this paper the limiting laws of the coherence of an $n\times p$ random
matrix in the high-dimensional setting where $p$ can be much larger than $n$.
Both the law of large numbers and the limiting distribution are derived. We
then consider testing the bandedness of the covariance matrix of a high
dimensional Gaussian distribution which includes testing for independence as a
special case. The limiting laws of the coherence of the data matrix play a
critical role in the construction of the test. We also apply the asymptotic
results to the construction of compressed sensing matrices."@2011
Takuya Kashimura@http://arxiv.org/abs/1102.2927v4@Standard imsets for undirected and chain graphical models@"We derive standard imsets for undirected graphical models and chain graphical
models. Standard imsets for undirected graphical models are described in terms
of minimal triangulations for maximal prime subgraphs of the undirected graphs.
For describing standard imsets for chain graphical models, we first define a
triangulation of a chain graph. We then use the triangulation to generalize our
results for the undirected graphs to chain graphs."@2011
Akimichi Takemura@http://arxiv.org/abs/1102.2927v4@Standard imsets for undirected and chain graphical models@"We derive standard imsets for undirected graphical models and chain graphical
models. Standard imsets for undirected graphical models are described in terms
of minimal triangulations for maximal prime subgraphs of the undirected graphs.
For describing standard imsets for chain graphical models, we first define a
triangulation of a chain graph. We then use the triangulation to generalize our
results for the undirected graphs to chain graphs."@2011
Sándor Baran@http://arxiv.org/abs/1102.3318v3@Parameter estimation in a spatial unit root autoregressive model@"Spatial unilateral autoregressive model $X_{k,\ell}=\alpha X_{k-1,\ell}+\beta
X_{k,\ell-1}+\gamma X_{k-1,\ell-1}+\epsilon_{k,\ell}$ is investigated in the
unit root case, that is when the parameters are on the boundary of the domain
of stability that forms a tetrahedron with vertices $(1,1,-1), \ (1,-1,1),\
(-1,1,1)$ and $(-1,-1,-1)$. It is shown that the limiting distribution of the
least squares estimator of the parameters is normal and the rate of convergence
is $n$ when the parameters are in the faces or on the edges of the tetrahedron,
while on the vertices the rate is $n^{3/2}$."@2011
Gyula Pap@http://arxiv.org/abs/1102.3318v3@Parameter estimation in a spatial unit root autoregressive model@"Spatial unilateral autoregressive model $X_{k,\ell}=\alpha X_{k-1,\ell}+\beta
X_{k,\ell-1}+\gamma X_{k-1,\ell-1}+\epsilon_{k,\ell}$ is investigated in the
unit root case, that is when the parameters are on the boundary of the domain
of stability that forms a tetrahedron with vertices $(1,1,-1), \ (1,-1,1),\
(-1,1,1)$ and $(-1,-1,-1)$. It is shown that the limiting distribution of the
least squares estimator of the parameters is normal and the rate of convergence
is $n$ when the parameters are in the faces or on the edges of the tetrahedron,
while on the vertices the rate is $n^{3/2}$."@2011
Masayuki Kumon@http://arxiv.org/abs/1102.3471v1@"Conformal geometry of statistical manifold with application to
  sequential estimation"@"We present a geometrical method for analyzing sequential estimating
procedures. It is based on the design principle of the second-order efficient
sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By
introducing a dual conformal curvature quantity, we clarify the conditions for
the covariance minimization of sequential estimators. These conditions are
further elabolated for the multidimensional curved exponential family. The
theoretical results are then numerically examined by using typical statistical
models, von Mises-Fisher and hyperboloid models."@2011
Akimichi Takemura@http://arxiv.org/abs/1102.3471v1@"Conformal geometry of statistical manifold with application to
  sequential estimation"@"We present a geometrical method for analyzing sequential estimating
procedures. It is based on the design principle of the second-order efficient
sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By
introducing a dual conformal curvature quantity, we clarify the conditions for
the covariance minimization of sequential estimators. These conditions are
further elabolated for the multidimensional curved exponential family. The
theoretical results are then numerically examined by using typical statistical
models, von Mises-Fisher and hyperboloid models."@2011
Kei Takeuchi@http://arxiv.org/abs/1102.3471v1@"Conformal geometry of statistical manifold with application to
  sequential estimation"@"We present a geometrical method for analyzing sequential estimating
procedures. It is based on the design principle of the second-order efficient
sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By
introducing a dual conformal curvature quantity, we clarify the conditions for
the covariance minimization of sequential estimators. These conditions are
further elabolated for the multidimensional curved exponential family. The
theoretical results are then numerically examined by using typical statistical
models, von Mises-Fisher and hyperboloid models."@2011
Laëtitia Comminges@http://arxiv.org/abs/1102.3616v1@"Tight conditions for consistent variable selection in high dimensional
  nonparametric regression"@"We address the issue of variable selection in the regression model with very
high ambient dimension, i.e., when the number of covariates is very large. The
main focus is on the situation where the number of relevant covariates, called
intrinsic dimension, is much smaller than the ambient dimension. Without
assuming any parametric form of the underlying regression function, we get
tight conditions making it possible to consistently estimate the set of
relevant variables. These conditions relate the intrinsic dimension to the
ambient dimension and to the sample size. The procedure that is provably
consistent under these tight conditions is simple and is based on comparing the
empirical Fourier coefficients with an appropriately chosen threshold value."@2011
Arnak Dalalyan@http://arxiv.org/abs/1102.3616v1@"Tight conditions for consistent variable selection in high dimensional
  nonparametric regression"@"We address the issue of variable selection in the regression model with very
high ambient dimension, i.e., when the number of covariates is very large. The
main focus is on the situation where the number of relevant covariates, called
intrinsic dimension, is much smaller than the ambient dimension. Without
assuming any parametric form of the underlying regression function, we get
tight conditions making it possible to consistently estimate the set of
relevant variables. These conditions relate the intrinsic dimension to the
ambient dimension and to the sample size. The procedure that is provably
consistent under these tight conditions is simple and is based on comparing the
empirical Fourier coefficients with an appropriately chosen threshold value."@2011
Piet Groeneboom@http://arxiv.org/abs/1102.3999v1@Smooth and non-smooth estimates of a monotone hazard@"We discuss a number of estimates of the hazard under the assumption that the
hazard is monotone on an interval [0,a]. The usual isotonic least squares
estimators of the hazard are inconsistent at the boundary points 0 and a. We
use penalization to obtain uniformly consistent estimators. Moreover, we
determine the optimal penalization constants, extending related work in this
direction by Woodroofe and Sun (1993) and Woodroofe and Sun (1999). Two methods
of obtaining smooth monotone estimates based on a non-smooth monotone estimator
are discussed. One is based on kernel smoothing, the other on penalization."@2011
Geurt Jongbloed@http://arxiv.org/abs/1102.3999v1@Smooth and non-smooth estimates of a monotone hazard@"We discuss a number of estimates of the hazard under the assumption that the
hazard is monotone on an interval [0,a]. The usual isotonic least squares
estimators of the hazard are inconsistent at the boundary points 0 and a. We
use penalization to obtain uniformly consistent estimators. Moreover, we
determine the optimal penalization constants, extending related work in this
direction by Woodroofe and Sun (1993) and Woodroofe and Sun (1999). Two methods
of obtaining smooth monotone estimates based on a non-smooth monotone estimator
are discussed. One is based on kernel smoothing, the other on penalization."@2011
Xavier Brossat@http://arxiv.org/abs/1102.4351v1@"Estimating and forecasting partially linear models with non stationary
  exogeneous variables"@"This paper presents a backfitting-type method for estimating and forecasting
a periodically correlated partially linear model with exogeneous variables and
heteroskedastic input noise. A rate of convergence of the estimator is given.
The results are valid even if the period is unknown."@2011
Georges Oppenheim@http://arxiv.org/abs/1102.4351v1@"Estimating and forecasting partially linear models with non stationary
  exogeneous variables"@"This paper presents a backfitting-type method for estimating and forecasting
a periodically correlated partially linear model with exogeneous variables and
heteroskedastic input noise. A rate of convergence of the estimator is given.
The results are valid even if the period is unknown."@2011
Marie-Claude Viano@http://arxiv.org/abs/1102.4351v1@"Estimating and forecasting partially linear models with non stationary
  exogeneous variables"@"This paper presents a backfitting-type method for estimating and forecasting
a periodically correlated partially linear model with exogeneous variables and
heteroskedastic input noise. A rate of convergence of the estimator is given.
The results are valid even if the period is unknown."@2011
Pawel Lorek@http://arxiv.org/abs/1102.4368v1@"Empirical process of residuals for regression models with long memory
  errors"@"We consider the residual empirical process in random design regression with
long memory errors. We establish its limiting behaviour, showing that its rates
of convergence are different from the rates of convergence for to the empirical
process based on (unobserved) errors. Also, we study a residual empirical
process with estimated parameters. Its asymptotic distribution can be used to
construct Kolmogorov-Smirnov, Cram\'{e}r-Smirnov-von Mises, or other
goodness-of-fit tests. Theoretical results are justified by simulation studies."@2011
Rafal Kulik@http://arxiv.org/abs/1102.4368v1@"Empirical process of residuals for regression models with long memory
  errors"@"We consider the residual empirical process in random design regression with
long memory errors. We establish its limiting behaviour, showing that its rates
of convergence are different from the rates of convergence for to the empirical
process based on (unobserved) errors. Also, we study a residual empirical
process with estimated parameters. Its asymptotic distribution can be used to
construct Kolmogorov-Smirnov, Cram\'{e}r-Smirnov-von Mises, or other
goodness-of-fit tests. Theoretical results are justified by simulation studies."@2011
Artur J. Lemonte@http://arxiv.org/abs/1102.4371v1@"Local power of the LR, Wald, score and gradient tests in dispersion
  models"@"We derive asymptotic expansions up to order $n^{-1/2}$ for the nonnull
distribution functions of the likelihood ratio, Wald, score and gradient test
statistics in the class of dispersion models, under a sequence of Pitman
alternatives. The asymptotic distributions of these statistics are obtained for
testing a subset of regression parameters and for testing the precision
parameter. Based on these nonnull asymptotic expansions it is shown that there
is no uniform superiority of one test with respect to the others for testing a
subset of regression parameters. Furthermore, in order to compare the
finite-sample performance of these tests in this class of models, Monte Carlo
simulations are presented. An empirical application to a real data set is
considered for illustrative purposes."@2011
Silvia L. P. Ferrari@http://arxiv.org/abs/1102.4371v1@"Local power of the LR, Wald, score and gradient tests in dispersion
  models"@"We derive asymptotic expansions up to order $n^{-1/2}$ for the nonnull
distribution functions of the likelihood ratio, Wald, score and gradient test
statistics in the class of dispersion models, under a sequence of Pitman
alternatives. The asymptotic distributions of these statistics are obtained for
testing a subset of regression parameters and for testing the precision
parameter. Based on these nonnull asymptotic expansions it is shown that there
is no uniform superiority of one test with respect to the others for testing a
subset of regression parameters. Furthermore, in order to compare the
finite-sample performance of these tests in this class of models, Monte Carlo
simulations are presented. An empirical application to a real data set is
considered for illustrative purposes."@2011
Rafal Kulik@http://arxiv.org/abs/1102.4372v1@"Some results on random design regression with long memory errors and
  predictors"@"This paper studies nonparametric regression with long memory (LRD) errors and
predictors. First, we formulate general conditions which guarantee the standard
rate of convergence for a nonparametric kernel estimator. Second, we calculate
the Mean Integrated Squared Error (MISE). In particular, we show that LRD of
errors may influence MISE. On the other hand, an estimator for a shape function
is typically not influenced by LRD in errors. Finally, we investigate
properties of a data-driven bandwidth choice. We show that Averaged Squared
Error (ASE) is a good approximation of MISE, however, this is not the case for
a cross-validation criterion."@2011
Pawel Lorek@http://arxiv.org/abs/1102.4372v1@"Some results on random design regression with long memory errors and
  predictors"@"This paper studies nonparametric regression with long memory (LRD) errors and
predictors. First, we formulate general conditions which guarantee the standard
rate of convergence for a nonparametric kernel estimator. Second, we calculate
the Mean Integrated Squared Error (MISE). In particular, we show that LRD of
errors may influence MISE. On the other hand, an estimator for a shape function
is typically not influenced by LRD in errors. Finally, we investigate
properties of a data-driven bandwidth choice. We show that Averaged Squared
Error (ASE) is a good approximation of MISE, however, this is not the case for
a cross-validation criterion."@2011
Taisei Kudo@http://arxiv.org/abs/1102.4674v1@"A lower bound for the Graver complexity of the incidence matrix of a
  complete bipartite graph"@"We give an exponential lower bound for the Graver complexity of the incidence
matrix of a complete bipartite graph of arbitrary size. Our result is a
generalization of the result by Berstein and Onn (2009) for 3xr complete
bipartite graphs, r \ge 3."@2011
Akimichi Takemura@http://arxiv.org/abs/1102.4674v1@"A lower bound for the Graver complexity of the incidence matrix of a
  complete bipartite graph"@"We give an exponential lower bound for the Graver complexity of the incidence
matrix of a complete bipartite graph of arbitrary size. Our result is a
generalization of the result by Berstein and Onn (2009) for 3xr complete
bipartite graphs, r \ge 3."@2011
David Azriel@http://arxiv.org/abs/1102.4915v3@"Are adaptive allocation designs beneficial for improving power in binary
  response trials?"@"We consider the classical problem of selecting the best of two treatments in
clinical trials with binary response. The target is to find the design that
maximizes the power of the relevant test. Many papers use a normal
approximation to the power function and claim that Neyman allocation that
assigns subjects to treatment groups according to the ratio of the responses'
standard deviations, should be used. As the standard deviations are unknown, an
adaptive design is often recommended. The asymptotic justification of this
approach is arguable, since it uses the normal approximation in tails where the
error in the approximation is larger than the estimated quantity. We consider
two different approaches for optimality of designs that are related to Pitman
and Bahadur definitions of relative efficiency of tests. We prove that the
optimal allocation according to the Pitman criterion is the balanced allocation
and that the optimal allocation according to the Bahadur approach depends on
the unknown parameters. Exact calculations reveal that the optimal allocation
according to Bahadur is often close to the balanced design, and the powers of
both are comparable to the Neyman allocation for small sample sizes and are
generally better for large experiments. Our findings have important
implications to the design of experiments, as the balanced design is proved to
be optimal or close to optimal and the need for the complications involved in
following an adaptive design for the purpose of increasing the power of tests
is therefore questionable."@2011
Micha Mandel@http://arxiv.org/abs/1102.4915v3@"Are adaptive allocation designs beneficial for improving power in binary
  response trials?"@"We consider the classical problem of selecting the best of two treatments in
clinical trials with binary response. The target is to find the design that
maximizes the power of the relevant test. Many papers use a normal
approximation to the power function and claim that Neyman allocation that
assigns subjects to treatment groups according to the ratio of the responses'
standard deviations, should be used. As the standard deviations are unknown, an
adaptive design is often recommended. The asymptotic justification of this
approach is arguable, since it uses the normal approximation in tails where the
error in the approximation is larger than the estimated quantity. We consider
two different approaches for optimality of designs that are related to Pitman
and Bahadur definitions of relative efficiency of tests. We prove that the
optimal allocation according to the Pitman criterion is the balanced allocation
and that the optimal allocation according to the Bahadur approach depends on
the unknown parameters. Exact calculations reveal that the optimal allocation
according to Bahadur is often close to the balanced design, and the powers of
both are comparable to the Neyman allocation for small sample sizes and are
generally better for large experiments. Our findings have important
implications to the design of experiments, as the balanced design is proved to
be optimal or close to optimal and the need for the complications involved in
following an adaptive design for the purpose of increasing the power of tests
is therefore questionable."@2011
Yosef Rinott@http://arxiv.org/abs/1102.4915v3@"Are adaptive allocation designs beneficial for improving power in binary
  response trials?"@"We consider the classical problem of selecting the best of two treatments in
clinical trials with binary response. The target is to find the design that
maximizes the power of the relevant test. Many papers use a normal
approximation to the power function and claim that Neyman allocation that
assigns subjects to treatment groups according to the ratio of the responses'
standard deviations, should be used. As the standard deviations are unknown, an
adaptive design is often recommended. The asymptotic justification of this
approach is arguable, since it uses the normal approximation in tails where the
error in the approximation is larger than the estimated quantity. We consider
two different approaches for optimality of designs that are related to Pitman
and Bahadur definitions of relative efficiency of tests. We prove that the
optimal allocation according to the Pitman criterion is the balanced allocation
and that the optimal allocation according to the Bahadur approach depends on
the unknown parameters. Exact calculations reveal that the optimal allocation
according to Bahadur is often close to the balanced design, and the powers of
both are comparable to the Neyman allocation for small sample sizes and are
generally better for large experiments. Our findings have important
implications to the design of experiments, as the balanced design is proved to
be optimal or close to optimal and the need for the complications involved in
following an adaptive design for the purpose of increasing the power of tests
is therefore questionable."@2011
Guillaume Lecué@http://arxiv.org/abs/1102.4983v1@"Sharper lower bounds on the performance of the empirical risk
  minimization algorithm"@"We present an argument based on the multidimensional and the uniform central
limit theorems, proving that, under some geometrical assumptions between the
target function $T$ and the learning class $F$, the excess risk of the
empirical risk minimization algorithm is lower bounded by
\[\frac{\mathbb{E}\sup_{q\in Q}G_q}{\sqrt{n}}\delta,\] where $(G_q)_{q\in Q}$
is a canonical Gaussian process associated with $Q$ (a well chosen subset of
$F$) and $\delta$ is a parameter governing the oscillations of the empirical
excess risk function over a small ball in $F$."@2011
Shahar Mendelson@http://arxiv.org/abs/1102.4983v1@"Sharper lower bounds on the performance of the empirical risk
  minimization algorithm"@"We present an argument based on the multidimensional and the uniform central
limit theorems, proving that, under some geometrical assumptions between the
target function $T$ and the learning class $F$, the excess risk of the
empirical risk minimization algorithm is lower bounded by
\[\frac{\mathbb{E}\sup_{q\in Q}G_q}{\sqrt{n}}\delta,\] where $(G_q)_{q\in Q}$
is a canonical Gaussian process associated with $Q$ (a well chosen subset of
$F$) and $\delta$ is a parameter governing the oscillations of the empirical
excess risk function over a small ball in $F$."@2011
Pierpaolo De Blasi@http://arxiv.org/abs/1102.5008v1@"Bayesian nonparametric estimation and consistency of mixed multinomial
  logit choice models"@"This paper develops nonparametric estimation for discrete choice models based
on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models
encompass all discrete choice models derived under the assumption of random
utility maximization, subject to the identification of an unknown distribution
$G$. Noting the mixture model description of the MMNL, we employ a Bayesian
nonparametric approach, using nonparametric priors on the unknown mixing
distribution $G$, to estimate choice probabilities. We provide an important
theoretical support for the use of the proposed methodology by investigating
consistency of the posterior distribution for a general nonparametric prior on
the mixing distribution. Consistency is defined according to an $L_1$-type
distance on the space of choice probabilities and is achieved by extending to a
regression model framework a recent approach to strong consistency based on the
summability of square roots of prior probabilities. Moving to estimation,
slightly different techniques for non-panel and panel data models are
discussed. For practical implementation, we describe efficient and relatively
easy-to-use blocked Gibbs sampling procedures. These procedures are based on
approximations of the random probability measure by classes of finite
stick-breaking processes. A simulation study is also performed to investigate
the performance of the proposed methods."@2011
Lancelot F. James@http://arxiv.org/abs/1102.5008v1@"Bayesian nonparametric estimation and consistency of mixed multinomial
  logit choice models"@"This paper develops nonparametric estimation for discrete choice models based
on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models
encompass all discrete choice models derived under the assumption of random
utility maximization, subject to the identification of an unknown distribution
$G$. Noting the mixture model description of the MMNL, we employ a Bayesian
nonparametric approach, using nonparametric priors on the unknown mixing
distribution $G$, to estimate choice probabilities. We provide an important
theoretical support for the use of the proposed methodology by investigating
consistency of the posterior distribution for a general nonparametric prior on
the mixing distribution. Consistency is defined according to an $L_1$-type
distance on the space of choice probabilities and is achieved by extending to a
regression model framework a recent approach to strong consistency based on the
summability of square roots of prior probabilities. Moving to estimation,
slightly different techniques for non-panel and panel data models are
discussed. For practical implementation, we describe efficient and relatively
easy-to-use blocked Gibbs sampling procedures. These procedures are based on
approximations of the random probability measure by classes of finite
stick-breaking processes. A simulation study is also performed to investigate
the performance of the proposed methods."@2011
John W. Lau@http://arxiv.org/abs/1102.5008v1@"Bayesian nonparametric estimation and consistency of mixed multinomial
  logit choice models"@"This paper develops nonparametric estimation for discrete choice models based
on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models
encompass all discrete choice models derived under the assumption of random
utility maximization, subject to the identification of an unknown distribution
$G$. Noting the mixture model description of the MMNL, we employ a Bayesian
nonparametric approach, using nonparametric priors on the unknown mixing
distribution $G$, to estimate choice probabilities. We provide an important
theoretical support for the use of the proposed methodology by investigating
consistency of the posterior distribution for a general nonparametric prior on
the mixing distribution. Consistency is defined according to an $L_1$-type
distance on the space of choice probabilities and is achieved by extending to a
regression model framework a recent approach to strong consistency based on the
summability of square roots of prior probabilities. Moving to estimation,
slightly different techniques for non-panel and panel data models are
discussed. For practical implementation, we describe efficient and relatively
easy-to-use blocked Gibbs sampling procedures. These procedures are based on
approximations of the random probability measure by classes of finite
stick-breaking processes. A simulation study is also performed to investigate
the performance of the proposed methods."@2011
Werner Ehm@http://arxiv.org/abs/1102.5031v2@Local proper scoring rules of order two@"Scoring rules assess the quality of probabilistic forecasts, by assigning a
numerical score based on the predictive distribution and on the event or value
that materializes. A scoring rule is proper if it encourages truthful
reporting. It is local of order $k$ if the score depends on the predictive
density only through its value and the values of its derivatives of order up to
$k$ at the realizing event. Complementing fundamental recent work by Parry,
Dawid and Lauritzen, we characterize the local proper scoring rules of order 2
relative to a broad class of Lebesgue densities on the real line, using a
different approach. In a data example, we use local and nonlocal proper scoring
rules to assess statistically postprocessed ensemble weather forecasts."@2011
Tilmann Gneiting@http://arxiv.org/abs/1102.5031v2@Local proper scoring rules of order two@"Scoring rules assess the quality of probabilistic forecasts, by assigning a
numerical score based on the predictive distribution and on the event or value
that materializes. A scoring rule is proper if it encourages truthful
reporting. It is local of order $k$ if the score depends on the predictive
density only through its value and the values of its derivatives of order up to
$k$ at the realizing event. Complementing fundamental recent work by Parry,
Dawid and Lauritzen, we characterize the local proper scoring rules of order 2
relative to a broad class of Lebesgue densities on the real line, using a
different approach. In a data example, we use local and nonlocal proper scoring
rules to assess statistically postprocessed ensemble weather forecasts."@2011
Carenne Ludeña@http://arxiv.org/abs/1102.5176v3@"Estimating the scaling function of multifractal measures and
  multifractal random walks using ratios"@"In this paper, we prove central limit theorems for bias reduced estimators of
the structure function of several multifractal processes, namely mutiplicative
cascades, multifractal random measures, multifractal random walk and
multifractal fractional random walk as defined by Lude\~{n}a [Ann. Appl.
Probab. 18 (2008) 1138-1163]. Previous estimators of the structure functions
considered in the literature were severely biased with a logarithmic rate of
convergence, whereas the estimators considered here have a polynomial rate of
convergence."@2011
Philippe Soulier@http://arxiv.org/abs/1102.5176v3@"Estimating the scaling function of multifractal measures and
  multifractal random walks using ratios"@"In this paper, we prove central limit theorems for bias reduced estimators of
the structure function of several multifractal processes, namely mutiplicative
cascades, multifractal random measures, multifractal random walk and
multifractal fractional random walk as defined by Lude\~{n}a [Ann. Appl.
Probab. 18 (2008) 1138-1163]. Previous estimators of the structure functions
considered in the literature were severely biased with a logarithmic rate of
convergence, whereas the estimators considered here have a polynomial rate of
convergence."@2011
Guozhong He@http://arxiv.org/abs/1102.5212v1@Functional linear regression via canonical analysis@"We study regression models for the situation where both dependent and
independent variables are square-integrable stochastic processes. Questions
concerning the definition and existence of the corresponding functional linear
regression models and some basic properties are explored for this situation. We
derive a representation of the regression parameter function in terms of the
canonical components of the processes involved. This representation establishes
a connection between functional regression and functional canonical analysis
and suggests alternative approaches for the implementation of functional linear
regression analysis. A specific procedure for the estimation of the regression
parameter function using canonical expansions is proposed and compared with an
established functional principal component regression approach. As an example
of an application, we present an analysis of mortality data for cohorts of
medflies, obtained in experimental studies of aging and longevity."@2011
Hans-Georg Müller@http://arxiv.org/abs/1102.5212v1@Functional linear regression via canonical analysis@"We study regression models for the situation where both dependent and
independent variables are square-integrable stochastic processes. Questions
concerning the definition and existence of the corresponding functional linear
regression models and some basic properties are explored for this situation. We
derive a representation of the regression parameter function in terms of the
canonical components of the processes involved. This representation establishes
a connection between functional regression and functional canonical analysis
and suggests alternative approaches for the implementation of functional linear
regression analysis. A specific procedure for the estimation of the regression
parameter function using canonical expansions is proposed and compared with an
established functional principal component regression approach. As an example
of an application, we present an analysis of mortality data for cohorts of
medflies, obtained in experimental studies of aging and longevity."@2011
Jane-Ling Wang@http://arxiv.org/abs/1102.5212v1@Functional linear regression via canonical analysis@"We study regression models for the situation where both dependent and
independent variables are square-integrable stochastic processes. Questions
concerning the definition and existence of the corresponding functional linear
regression models and some basic properties are explored for this situation. We
derive a representation of the regression parameter function in terms of the
canonical components of the processes involved. This representation establishes
a connection between functional regression and functional canonical analysis
and suggests alternative approaches for the implementation of functional linear
regression analysis. A specific procedure for the estimation of the regression
parameter function using canonical expansions is proposed and compared with an
established functional principal component regression approach. As an example
of an application, we present an analysis of mortality data for cohorts of
medflies, obtained in experimental studies of aging and longevity."@2011
Wenjing Yang@http://arxiv.org/abs/1102.5212v1@Functional linear regression via canonical analysis@"We study regression models for the situation where both dependent and
independent variables are square-integrable stochastic processes. Questions
concerning the definition and existence of the corresponding functional linear
regression models and some basic properties are explored for this situation. We
derive a representation of the regression parameter function in terms of the
canonical components of the processes involved. This representation establishes
a connection between functional regression and functional canonical analysis
and suggests alternative approaches for the implementation of functional linear
regression analysis. A specific procedure for the estimation of the regression
parameter function using canonical expansions is proposed and compared with an
established functional principal component regression approach. As an example
of an application, we present an analysis of mortality data for cohorts of
medflies, obtained in experimental studies of aging and longevity."@2011
Yichao Wu@http://arxiv.org/abs/1102.5217v1@Varying-coefficient functional linear regression@"Functional linear regression analysis aims to model regression relations
which include a functional predictor. The analog of the regression parameter
vector or matrix in conventional multivariate or multiple-response linear
regression models is a regression parameter function in one or two arguments.
If, in addition, one has scalar predictors, as is often the case in
applications to longitudinal studies, the question arises how to incorporate
these into a functional regression model. We study a varying-coefficient
approach where the scalar covariates are modeled as additional arguments of the
regression parameter function. This extension of the functional linear
regression model is analogous to the extension of conventional linear
regression models to varying-coefficient models and shares its advantages, such
as increased flexibility; however, the details of this extension are more
challenging in the functional case. Our methodology combines smoothing methods
with regularization by truncation at a finite number of functional principal
components. A practical version is developed and is shown to perform better
than functional linear regression for longitudinal data. We investigate the
asymptotic properties of varying-coefficient functional linear regression and
establish consistency properties."@2011
Jianqing Fan@http://arxiv.org/abs/1102.5217v1@Varying-coefficient functional linear regression@"Functional linear regression analysis aims to model regression relations
which include a functional predictor. The analog of the regression parameter
vector or matrix in conventional multivariate or multiple-response linear
regression models is a regression parameter function in one or two arguments.
If, in addition, one has scalar predictors, as is often the case in
applications to longitudinal studies, the question arises how to incorporate
these into a functional regression model. We study a varying-coefficient
approach where the scalar covariates are modeled as additional arguments of the
regression parameter function. This extension of the functional linear
regression model is analogous to the extension of conventional linear
regression models to varying-coefficient models and shares its advantages, such
as increased flexibility; however, the details of this extension are more
challenging in the functional case. Our methodology combines smoothing methods
with regularization by truncation at a finite number of functional principal
components. A practical version is developed and is shown to perform better
than functional linear regression for longitudinal data. We investigate the
asymptotic properties of varying-coefficient functional linear regression and
establish consistency properties."@2011
Hans-Georg Müller@http://arxiv.org/abs/1102.5217v1@Varying-coefficient functional linear regression@"Functional linear regression analysis aims to model regression relations
which include a functional predictor. The analog of the regression parameter
vector or matrix in conventional multivariate or multiple-response linear
regression models is a regression parameter function in one or two arguments.
If, in addition, one has scalar predictors, as is often the case in
applications to longitudinal studies, the question arises how to incorporate
these into a functional regression model. We study a varying-coefficient
approach where the scalar covariates are modeled as additional arguments of the
regression parameter function. This extension of the functional linear
regression model is analogous to the extension of conventional linear
regression models to varying-coefficient models and shares its advantages, such
as increased flexibility; however, the details of this extension are more
challenging in the functional case. Our methodology combines smoothing methods
with regularization by truncation at a finite number of functional principal
components. A practical version is developed and is shown to perform better
than functional linear regression for longitudinal data. We investigate the
asymptotic properties of varying-coefficient functional linear regression and
establish consistency properties."@2011
Heping He@http://arxiv.org/abs/1102.5224v1@"Asymptotic properties of maximum likelihood estimators in models with
  multiple change points"@"Models with multiple change points are used in many fields; however, the
theoretical properties of maximum likelihood estimators of such models have
received relatively little attention. The goal of this paper is to establish
the asymptotic properties of maximum likelihood estimators of the parameters of
a multiple change-point model for a general class of models in which the form
of the distribution can change from segment to segment and in which, possibly,
there are parameters that are common to all segments. Consistency of the
maximum likelihood estimators of the change points is established and the rate
of convergence is determined; the asymptotic distribution of the maximum
likelihood estimators of the parameters of the within-segment distributions is
also derived. Since the approach used in single change-point models is not
easily extended to multiple change-point models, these results require the
introduction of those tools for analyzing the likelihood function in a multiple
change-point model."@2011
Thomas A. Severini@http://arxiv.org/abs/1102.5224v1@"Asymptotic properties of maximum likelihood estimators in models with
  multiple change points"@"Models with multiple change points are used in many fields; however, the
theoretical properties of maximum likelihood estimators of such models have
received relatively little attention. The goal of this paper is to establish
the asymptotic properties of maximum likelihood estimators of the parameters of
a multiple change-point model for a general class of models in which the form
of the distribution can change from segment to segment and in which, possibly,
there are parameters that are common to all segments. Consistency of the
maximum likelihood estimators of the change points is established and the rate
of convergence is determined; the asymptotic distribution of the maximum
likelihood estimators of the parameters of the within-segment distributions is
also derived. Since the approach used in single change-point models is not
easily extended to multiple change-point models, these results require the
introduction of those tools for analyzing the likelihood function in a multiple
change-point model."@2011
Brice Franke@http://arxiv.org/abs/1102.5241v1@"A self-similar process arising from a random walk with random
  environment in random scenery"@"In this article, we merge celebrated results of Kesten and Spitzer [Z.
Wahrsch. Verw. Gebiete 50 (1979) 5-25] and Kawazu and Kesten [J. Stat. Phys. 37
(1984) 561-575]. A random walk performs a motion in an i.i.d. environment and
observes an i.i.d. scenery along its path. We assume that the scenery is in the
domain of attraction of a stable distribution and prove that the resulting
observations satisfy a limit theorem. The resulting limit process is a
self-similar stochastic process with non-trivial dependencies."@2011
Tatsuhiko Saigo@http://arxiv.org/abs/1102.5241v1@"A self-similar process arising from a random walk with random
  environment in random scenery"@"In this article, we merge celebrated results of Kesten and Spitzer [Z.
Wahrsch. Verw. Gebiete 50 (1979) 5-25] and Kawazu and Kesten [J. Stat. Phys. 37
(1984) 561-575]. A random walk performs a motion in an i.i.d. environment and
observes an i.i.d. scenery along its path. We assume that the scenery is in the
domain of attraction of a stable distribution and prove that the resulting
observations satisfy a limit theorem. The resulting limit process is a
self-similar stochastic process with non-trivial dependencies."@2011
Neal Madras@http://arxiv.org/abs/1102.5245v1@"Quantitative bounds for Markov chain convergence: Wasserstein and total
  variation distances"@"We present a framework for obtaining explicit bounds on the rate of
convergence to equilibrium of a Markov chain on a general state space, with
respect to both total variation and Wasserstein distances. For Wasserstein
bounds, our main tool is Steinsaltz's convergence theorem for locally
contractive random dynamical systems. We describe practical methods for finding
Steinsaltz's ""drift functions"" that prove local contractivity. We then use the
idea of ""one-shot coupling"" to derive criteria that give bounds for total
variation distances in terms of Wasserstein distances. Our methods are applied
to two examples: a two-component Gibbs sampler for the Normal distribution and
a random logistic dynamical system."@2011
Deniz Sezer@http://arxiv.org/abs/1102.5245v1@"Quantitative bounds for Markov chain convergence: Wasserstein and total
  variation distances"@"We present a framework for obtaining explicit bounds on the rate of
convergence to equilibrium of a Markov chain on a general state space, with
respect to both total variation and Wasserstein distances. For Wasserstein
bounds, our main tool is Steinsaltz's convergence theorem for locally
contractive random dynamical systems. We describe practical methods for finding
Steinsaltz's ""drift functions"" that prove local contractivity. We then use the
idea of ""one-shot coupling"" to derive criteria that give bounds for total
variation distances in terms of Wasserstein distances. Our methods are applied
to two examples: a two-component Gibbs sampler for the Normal distribution and
a random logistic dynamical system."@2011
Anil Aswani@http://arxiv.org/abs/1103.1457v1@Regression on manifolds: Estimation of the exterior derivative@"Collinearity and near-collinearity of predictors cause difficulties when
doing regression. In these cases, variable selection becomes untenable because
of mathematical issues concerning the existence and numerical stability of the
regression coefficients, and interpretation of the coefficients is ambiguous
because gradients are not defined. Using a differential geometric
interpretation, in which the regression coefficients are interpreted as
estimates of the exterior derivative of a function, we develop a new method to
do regression in the presence of collinearities. Our regularization scheme can
improve estimation error, and it can be easily modified to include lasso-type
regularization. These estimators also have simple extensions to the ""large $p$,
small $n$"" context."@2011
Peter Bickel@http://arxiv.org/abs/1103.1457v1@Regression on manifolds: Estimation of the exterior derivative@"Collinearity and near-collinearity of predictors cause difficulties when
doing regression. In these cases, variable selection becomes untenable because
of mathematical issues concerning the existence and numerical stability of the
regression coefficients, and interpretation of the coefficients is ambiguous
because gradients are not defined. Using a differential geometric
interpretation, in which the regression coefficients are interpreted as
estimates of the exterior derivative of a function, we develop a new method to
do regression in the presence of collinearities. Our regularization scheme can
improve estimation error, and it can be easily modified to include lasso-type
regularization. These estimators also have simple extensions to the ""large $p$,
small $n$"" context."@2011
Claire Tomlin@http://arxiv.org/abs/1103.1457v1@Regression on manifolds: Estimation of the exterior derivative@"Collinearity and near-collinearity of predictors cause difficulties when
doing regression. In these cases, variable selection becomes untenable because
of mathematical issues concerning the existence and numerical stability of the
regression coefficients, and interpretation of the coefficients is ambiguous
because gradients are not defined. Using a differential geometric
interpretation, in which the regression coefficients are interpreted as
estimates of the exterior derivative of a function, we develop a new method to
do regression in the presence of collinearities. Our regularization scheme can
improve estimation error, and it can be easily modified to include lasso-type
regularization. These estimators also have simple extensions to the ""large $p$,
small $n$"" context."@2011
Mingyuan Zhang@http://arxiv.org/abs/1103.1472v1@"Causal inference for continuous-time processes when covariates are
  observed only at discrete times"@"Most of the work on the structural nested model and g-estimation for causal
inference in longitudinal data assumes a discrete-time underlying data
generating process. However, in some observational studies, it is more
reasonable to assume that the data are generated from a continuous-time process
and are only observable at discrete time points. When these circumstances
arise, the sequential randomization assumption in the observed discrete-time
data, which is essential in justifying discrete-time g-estimation, may not be
reasonable. Under a deterministic model, we discuss other useful assumptions
that guarantee the consistency of discrete-time g-estimation. In more general
cases, when those assumptions are violated, we propose a controlling-the-future
method that performs at least as well as g-estimation in most scenarios and
which provides consistent estimation in some cases where g-estimation is
severely inconsistent. We apply the methods discussed in this paper to
simulated data, as well as to a data set collected following a massive flood in
Bangladesh, estimating the effect of diarrhea on children's height. Results
from different methods are compared in both simulation and the real
application."@2011
Marshall M. Joffe@http://arxiv.org/abs/1103.1472v1@"Causal inference for continuous-time processes when covariates are
  observed only at discrete times"@"Most of the work on the structural nested model and g-estimation for causal
inference in longitudinal data assumes a discrete-time underlying data
generating process. However, in some observational studies, it is more
reasonable to assume that the data are generated from a continuous-time process
and are only observable at discrete time points. When these circumstances
arise, the sequential randomization assumption in the observed discrete-time
data, which is essential in justifying discrete-time g-estimation, may not be
reasonable. Under a deterministic model, we discuss other useful assumptions
that guarantee the consistency of discrete-time g-estimation. In more general
cases, when those assumptions are violated, we propose a controlling-the-future
method that performs at least as well as g-estimation in most scenarios and
which provides consistent estimation in some cases where g-estimation is
severely inconsistent. We apply the methods discussed in this paper to
simulated data, as well as to a data set collected following a massive flood in
Bangladesh, estimating the effect of diarrhea on children's height. Results
from different methods are compared in both simulation and the real
application."@2011
Dylan S. Small@http://arxiv.org/abs/1103.1472v1@"Causal inference for continuous-time processes when covariates are
  observed only at discrete times"@"Most of the work on the structural nested model and g-estimation for causal
inference in longitudinal data assumes a discrete-time underlying data
generating process. However, in some observational studies, it is more
reasonable to assume that the data are generated from a continuous-time process
and are only observable at discrete time points. When these circumstances
arise, the sequential randomization assumption in the observed discrete-time
data, which is essential in justifying discrete-time g-estimation, may not be
reasonable. Under a deterministic model, we discuss other useful assumptions
that guarantee the consistency of discrete-time g-estimation. In more general
cases, when those assumptions are violated, we propose a controlling-the-future
method that performs at least as well as g-estimation in most scenarios and
which provides consistent estimation in some cases where g-estimation is
severely inconsistent. We apply the methods discussed in this paper to
simulated data, as well as to a data set collected following a massive flood in
Bangladesh, estimating the effect of diarrhea on children's height. Results
from different methods are compared in both simulation and the real
application."@2011
Xinyu Zhang@http://arxiv.org/abs/1103.1480v1@"Focused information criterion and model averaging for generalized
  additive partial linear models"@"We study model selection and model averaging in generalized additive partial
linear models (GAPLMs). Polynomial spline is used to approximate nonparametric
functions. The corresponding estimators of the linear parameters are shown to
be asymptotically normal. We then develop a focused information criterion (FIC)
and a frequentist model average (FMA) estimator on the basis of the
quasi-likelihood principle and examine theoretical properties of the FIC and
FMA. The major advantages of the proposed procedures over the existing ones are
their computational expediency and theoretical reliability. Simulation
experiments have provided evidence of the superiority of the proposed
procedures. The approach is further applied to a real-world data example."@2011
Hua Liang@http://arxiv.org/abs/1103.1480v1@"Focused information criterion and model averaging for generalized
  additive partial linear models"@"We study model selection and model averaging in generalized additive partial
linear models (GAPLMs). Polynomial spline is used to approximate nonparametric
functions. The corresponding estimators of the linear parameters are shown to
be asymptotically normal. We then develop a focused information criterion (FIC)
and a frequentist model average (FMA) estimator on the basis of the
quasi-likelihood principle and examine theoretical properties of the FIC and
FMA. The major advantages of the proposed procedures over the existing ones are
their computational expediency and theoretical reliability. Simulation
experiments have provided evidence of the superiority of the proposed
procedures. The approach is further applied to a real-world data example."@2011
Karim Lounici@http://arxiv.org/abs/1103.1489v1@Global uniform risk bounds for wavelet deconvolution estimators@"We consider the statistical deconvolution problem where one observes $n$
replications from the model $Y=X+\epsilon$, where $X$ is the unobserved random
signal of interest and $\epsilon$ is an independent random error with
distribution $\phi$. Under weak assumptions on the decay of the Fourier
transform of $\phi,$ we derive upper bounds for the finite-sample sup-norm risk
of wavelet deconvolution density estimators $f_n$ for the density $f$ of $X$,
where $f:\mathbb{R}\to \mathbb{R}$ is assumed to be bounded. We then derive
lower bounds for the minimax sup-norm risk over Besov balls in this estimation
problem and show that wavelet deconvolution density estimators attain these
bounds. We further show that linear estimators adapt to the unknown smoothness
of $f$ if the Fourier transform of $\phi$ decays exponentially and that a
corresponding result holds true for the hard thresholding wavelet estimator if
$\phi$ decays polynomially. We also analyze the case where $f$ is a
""supersmooth""/analytic density. We finally show how our results and recent
techniques from Rademacher processes can be applied to construct global
confidence bands for the density $f$."@2011
Richard Nickl@http://arxiv.org/abs/1103.1489v1@Global uniform risk bounds for wavelet deconvolution estimators@"We consider the statistical deconvolution problem where one observes $n$
replications from the model $Y=X+\epsilon$, where $X$ is the unobserved random
signal of interest and $\epsilon$ is an independent random error with
distribution $\phi$. Under weak assumptions on the decay of the Fourier
transform of $\phi,$ we derive upper bounds for the finite-sample sup-norm risk
of wavelet deconvolution density estimators $f_n$ for the density $f$ of $X$,
where $f:\mathbb{R}\to \mathbb{R}$ is assumed to be bounded. We then derive
lower bounds for the minimax sup-norm risk over Besov balls in this estimation
problem and show that wavelet deconvolution density estimators attain these
bounds. We further show that linear estimators adapt to the unknown smoothness
of $f$ if the Fourier transform of $\phi$ decays exponentially and that a
corresponding result holds true for the hard thresholding wavelet estimator if
$\phi$ decays polynomially. We also analyze the case where $f$ is a
""supersmooth""/analytic density. We finally show how our results and recent
techniques from Rademacher processes can be applied to construct global
confidence bands for the density $f$."@2011
Raúl Jiménez@http://arxiv.org/abs/1103.1492v1@Nonparametric estimation of surface integrals@"The estimation of surface integrals on the boundary of an unknown body is a
challenge for nonparametric methods in statistics, with powerful applications
to physics and image analysis, among other fields. Provided that one can
determine whether random shots hit the body, Cuevas et al. [Ann. Statist. 35
(2007) 1031--1051] estimate the boundary measure (the boundary length for
planar sets and the surface area for 3-dimensional objects) via the
consideration of shots at a box containing the body. The statistics considered
by these authors, as well as those in subsequent papers, are based on the
estimation of Minkowski content and depend on a smoothing parameter which must
be carefully chosen. For the same sampling scheme, we introduce a new approach
which bypasses this issue, providing strongly consistent estimators of both the
boundary measure and the surface integrals of scalar functions, provided one
can collect the function values at the sample points. Examples arise in
experiments in which the density of the body can be measured by physical
properties of the impacts, or in situations where such quantities as
temperature and humidity are observed by randomly distributed sensors. Our
method is based on random Delaunay triangulations and involves a simple
procedure for surface reconstruction from a dense cloud of points inside and
outside the body. We obtain basic asymptotics of the estimator, perform
simulations and discuss, via Google Earth's data, an application to the image
analysis of the Aral Sea coast and its cliffs."@2011
J. E. Yukich@http://arxiv.org/abs/1103.1492v1@Nonparametric estimation of surface integrals@"The estimation of surface integrals on the boundary of an unknown body is a
challenge for nonparametric methods in statistics, with powerful applications
to physics and image analysis, among other fields. Provided that one can
determine whether random shots hit the body, Cuevas et al. [Ann. Statist. 35
(2007) 1031--1051] estimate the boundary measure (the boundary length for
planar sets and the surface area for 3-dimensional objects) via the
consideration of shots at a box containing the body. The statistics considered
by these authors, as well as those in subsequent papers, are based on the
estimation of Minkowski content and depend on a smoothing parameter which must
be carefully chosen. For the same sampling scheme, we introduce a new approach
which bypasses this issue, providing strongly consistent estimators of both the
boundary measure and the surface integrals of scalar functions, provided one
can collect the function values at the sample points. Examples arise in
experiments in which the density of the body can be measured by physical
properties of the impacts, or in situations where such quantities as
temperature and humidity are observed by randomly distributed sensors. Our
method is based on random Delaunay triangulations and involves a simple
procedure for surface reconstruction from a dense cloud of points inside and
outside the body. We obtain basic asymptotics of the estimator, perform
simulations and discuss, via Google Earth's data, an application to the image
analysis of the Aral Sea coast and its cliffs."@2011
Bo Kai@http://arxiv.org/abs/1103.1525v1@"New efficient estimation and variable selection methods for
  semiparametric varying-coefficient partially linear models"@"The complexity of semiparametric models poses new challenges to statistical
inference and model selection that frequently arise from real applications. In
this work, we propose new estimation and variable selection procedures for the
semiparametric varying-coefficient partially linear model. We first study
quantile regression estimates for the nonparametric varying-coefficient
functions and the parametric regression coefficients. To achieve nice
efficiency properties, we further develop a semiparametric composite quantile
regression procedure. We establish the asymptotic normality of proposed
estimators for both the parametric and nonparametric parts and show that the
estimators achieve the best convergence rate. Moreover, we show that the
proposed method is much more efficient than the least-squares-based method for
many non-normal errors and that it only loses a small amount of efficiency for
normal errors. In addition, it is shown that the loss in efficiency is at most
11.1% for estimating varying coefficient functions and is no greater than 13.6%
for estimating parametric components. To achieve sparsity with high-dimensional
covariates, we propose adaptive penalization methods for variable selection in
the semiparametric varying-coefficient partially linear model and prove that
the methods possess the oracle property. Extensive Monte Carlo simulation
studies are conducted to examine the finite-sample performance of the proposed
procedures. Finally, we apply the new methods to analyze the plasma
beta-carotene level data."@2011
Runze Li@http://arxiv.org/abs/1103.1525v1@"New efficient estimation and variable selection methods for
  semiparametric varying-coefficient partially linear models"@"The complexity of semiparametric models poses new challenges to statistical
inference and model selection that frequently arise from real applications. In
this work, we propose new estimation and variable selection procedures for the
semiparametric varying-coefficient partially linear model. We first study
quantile regression estimates for the nonparametric varying-coefficient
functions and the parametric regression coefficients. To achieve nice
efficiency properties, we further develop a semiparametric composite quantile
regression procedure. We establish the asymptotic normality of proposed
estimators for both the parametric and nonparametric parts and show that the
estimators achieve the best convergence rate. Moreover, we show that the
proposed method is much more efficient than the least-squares-based method for
many non-normal errors and that it only loses a small amount of efficiency for
normal errors. In addition, it is shown that the loss in efficiency is at most
11.1% for estimating varying coefficient functions and is no greater than 13.6%
for estimating parametric components. To achieve sparsity with high-dimensional
covariates, we propose adaptive penalization methods for variable selection in
the semiparametric varying-coefficient partially linear model and prove that
the methods possess the oracle property. Extensive Monte Carlo simulation
studies are conducted to examine the finite-sample performance of the proposed
procedures. Finally, we apply the new methods to analyze the plasma
beta-carotene level data."@2011
Hui Zou@http://arxiv.org/abs/1103.1525v1@"New efficient estimation and variable selection methods for
  semiparametric varying-coefficient partially linear models"@"The complexity of semiparametric models poses new challenges to statistical
inference and model selection that frequently arise from real applications. In
this work, we propose new estimation and variable selection procedures for the
semiparametric varying-coefficient partially linear model. We first study
quantile regression estimates for the nonparametric varying-coefficient
functions and the parametric regression coefficients. To achieve nice
efficiency properties, we further develop a semiparametric composite quantile
regression procedure. We establish the asymptotic normality of proposed
estimators for both the parametric and nonparametric parts and show that the
estimators achieve the best convergence rate. Moreover, we show that the
proposed method is much more efficient than the least-squares-based method for
many non-normal errors and that it only loses a small amount of efficiency for
normal errors. In addition, it is shown that the loss in efficiency is at most
11.1% for estimating varying coefficient functions and is no greater than 13.6%
for estimating parametric components. To achieve sparsity with high-dimensional
covariates, we propose adaptive penalization methods for variable selection in
the semiparametric varying-coefficient partially linear model and prove that
the methods possess the oracle property. Extensive Monte Carlo simulation
studies are conducted to examine the finite-sample performance of the proposed
procedures. Finally, we apply the new methods to analyze the plasma
beta-carotene level data."@2011
Ci-Ren Jiang@http://arxiv.org/abs/1103.1726v1@Functional single index models for longitudinal data@"A new single-index model that reflects the time-dynamic effects of the single
index is proposed for longitudinal and functional response data, possibly
measured with errors, for both longitudinal and time-invariant covariates. With
appropriate initial estimates of the parametric index, the proposed estimator
is shown to be $\sqrt{n}$-consistent and asymptotically normally distributed.
We also address the nonparametric estimation of regression functions and
provide estimates with optimal convergence rates. One advantage of the new
approach is that the same bandwidth is used to estimate both the nonparametric
mean function and the parameter in the index. The finite-sample performance for
the proposed procedure is studied numerically."@2011
Jane-Ling Wang@http://arxiv.org/abs/1103.1726v1@Functional single index models for longitudinal data@"A new single-index model that reflects the time-dynamic effects of the single
index is proposed for longitudinal and functional response data, possibly
measured with errors, for both longitudinal and time-invariant covariates. With
appropriate initial estimates of the parametric index, the proposed estimator
is shown to be $\sqrt{n}$-consistent and asymptotically normally distributed.
We also address the nonparametric estimation of regression functions and
provide estimates with optimal convergence rates. One advantage of the new
approach is that the same bandwidth is used to estimate both the nonparametric
mean function and the parameter in the index. The finite-sample performance for
the proposed procedure is studied numerically."@2011
Kshitij Khare@http://arxiv.org/abs/1103.1768v1@Wishart distributions for decomposable covariance graph models@"Gaussian covariance graph models encode marginal independence among the
components of a multivariate random vector by means of a graph $G$. These
models are distinctly different from the traditional concentration graph models
(often also referred to as Gaussian graphical models or covariance selection
models) since the zeros in the parameter are now reflected in the covariance
matrix $\Sigma$, as compared to the concentration matrix $\Omega =\Sigma^{-1}$.
The parameter space of interest for covariance graph models is the cone $P_G$
of positive definite matrices with fixed zeros corresponding to the missing
edges of $G$. As in Letac and Massam [Ann. Statist. 35 (2007) 1278--1323], we
consider the case where $G$ is decomposable. In this paper, we construct on the
cone $P_G$ a family of Wishart distributions which serve a similar purpose in
the covariance graph setting as those constructed by Letac and Massam [Ann.
Statist. 35 (2007) 1278--1323] and Dawid and Lauritzen [Ann. Statist. 21 (1993)
1272--1317] do in the concentration graph setting. We proceed to undertake a
rigorous study of these ""covariance"" Wishart distributions and derive several
deep and useful properties of this class."@2011
Bala Rajaratnam@http://arxiv.org/abs/1103.1768v1@Wishart distributions for decomposable covariance graph models@"Gaussian covariance graph models encode marginal independence among the
components of a multivariate random vector by means of a graph $G$. These
models are distinctly different from the traditional concentration graph models
(often also referred to as Gaussian graphical models or covariance selection
models) since the zeros in the parameter are now reflected in the covariance
matrix $\Sigma$, as compared to the concentration matrix $\Omega =\Sigma^{-1}$.
The parameter space of interest for covariance graph models is the cone $P_G$
of positive definite matrices with fixed zeros corresponding to the missing
edges of $G$. As in Letac and Massam [Ann. Statist. 35 (2007) 1278--1323], we
consider the case where $G$ is decomposable. In this paper, we construct on the
cone $P_G$ a family of Wishart distributions which serve a similar purpose in
the covariance graph setting as those constructed by Letac and Massam [Ann.
Statist. 35 (2007) 1278--1323] and Dawid and Lauritzen [Ann. Statist. 21 (1993)
1272--1317] do in the concentration graph setting. We proceed to undertake a
rigorous study of these ""covariance"" Wishart distributions and derive several
deep and useful properties of this class."@2011
Chunming Zhang@http://arxiv.org/abs/1103.1966v1@Multiple testing via $FDR_L$ for large-scale imaging data@"The multiple testing procedure plays an important role in detecting the
presence of spatial signals for large-scale imaging data. Typically, the
spatial signals are sparse but clustered. This paper provides empirical
evidence that for a range of commonly used control levels, the conventional
$\operatorname {FDR}$ procedure can lack the ability to detect statistical
significance, even if the $p$-values under the true null hypotheses are
independent and uniformly distributed; more generally, ignoring the neighboring
information of spatially structured data will tend to diminish the detection
effectiveness of the $\operatorname {FDR}$ procedure. This paper first
introduces a scalar quantity to characterize the extent to which the ""lack of
identification phenomenon"" ($\operatorname {LIP}$) of the $\operatorname {FDR}$
procedure occurs. Second, we propose a new multiple comparison procedure,
called $\operatorname {FDR}_L$, to accommodate the spatial information of
neighboring $p$-values, via a local aggregation of $p$-values. Theoretical
properties of the $\operatorname {FDR}_L$ procedure are investigated under weak
dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$
procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$
procedure, thus substantially facilitating the selection of more stringent
control levels. Simulation evaluations indicate that the $\operatorname
{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname
{FDR}$ procedure with little loss in detection specificity. The computational
simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure
are illustrated through a real brain fMRI dataset."@2011
Jianqing Fan@http://arxiv.org/abs/1103.1966v1@Multiple testing via $FDR_L$ for large-scale imaging data@"The multiple testing procedure plays an important role in detecting the
presence of spatial signals for large-scale imaging data. Typically, the
spatial signals are sparse but clustered. This paper provides empirical
evidence that for a range of commonly used control levels, the conventional
$\operatorname {FDR}$ procedure can lack the ability to detect statistical
significance, even if the $p$-values under the true null hypotheses are
independent and uniformly distributed; more generally, ignoring the neighboring
information of spatially structured data will tend to diminish the detection
effectiveness of the $\operatorname {FDR}$ procedure. This paper first
introduces a scalar quantity to characterize the extent to which the ""lack of
identification phenomenon"" ($\operatorname {LIP}$) of the $\operatorname {FDR}$
procedure occurs. Second, we propose a new multiple comparison procedure,
called $\operatorname {FDR}_L$, to accommodate the spatial information of
neighboring $p$-values, via a local aggregation of $p$-values. Theoretical
properties of the $\operatorname {FDR}_L$ procedure are investigated under weak
dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$
procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$
procedure, thus substantially facilitating the selection of more stringent
control levels. Simulation evaluations indicate that the $\operatorname
{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname
{FDR}$ procedure with little loss in detection specificity. The computational
simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure
are illustrated through a real brain fMRI dataset."@2011
Tao Yu@http://arxiv.org/abs/1103.1966v1@Multiple testing via $FDR_L$ for large-scale imaging data@"The multiple testing procedure plays an important role in detecting the
presence of spatial signals for large-scale imaging data. Typically, the
spatial signals are sparse but clustered. This paper provides empirical
evidence that for a range of commonly used control levels, the conventional
$\operatorname {FDR}$ procedure can lack the ability to detect statistical
significance, even if the $p$-values under the true null hypotheses are
independent and uniformly distributed; more generally, ignoring the neighboring
information of spatially structured data will tend to diminish the detection
effectiveness of the $\operatorname {FDR}$ procedure. This paper first
introduces a scalar quantity to characterize the extent to which the ""lack of
identification phenomenon"" ($\operatorname {LIP}$) of the $\operatorname {FDR}$
procedure occurs. Second, we propose a new multiple comparison procedure,
called $\operatorname {FDR}_L$, to accommodate the spatial information of
neighboring $p$-values, via a local aggregation of $p$-values. Theoretical
properties of the $\operatorname {FDR}_L$ procedure are investigated under weak
dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$
procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$
procedure, thus substantially facilitating the selection of more stringent
control levels. Simulation evaluations indicate that the $\operatorname
{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname
{FDR}$ procedure with little loss in detection specificity. The computational
simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure
are illustrated through a real brain fMRI dataset."@2011
Serge Cohen@http://arxiv.org/abs/1103.2021v5@"Conditional Density Estimation by Penalized Likelihood Model Selection
  and Applications"@"In this technical report, we consider conditional density estimation with a
maximum likelihood approach. Under weak assumptions, we obtain a theoretical
bound for a Kullback-Leibler type loss for a single model maximum likelihood
estimate. We use a penalized model selection technique to select a best model
within a collection. We give a general condition on penalty choice that leads
to oracle type inequality for the resulting estimate. This construction is
applied to two examples of partition-based conditional density models, models
in which the conditional density depends only in a piecewise manner from the
covariate. The first example relies on classical piecewise polynomial densities
while the second uses Gaussian mixtures with varying mixing proportion but same
mixture components. We show how this last case is related to an unsupervised
segmentation application that has been the source of our motivation to this
study."@2011
Erwan Le Pennec@http://arxiv.org/abs/1103.2021v5@"Conditional Density Estimation by Penalized Likelihood Model Selection
  and Applications"@"In this technical report, we consider conditional density estimation with a
maximum likelihood approach. Under weak assumptions, we obtain a theoretical
bound for a Kullback-Leibler type loss for a single model maximum likelihood
estimate. We use a penalized model selection technique to select a best model
within a collection. We give a general condition on penalty choice that leads
to oracle type inequality for the resulting estimate. This construction is
applied to two examples of partition-based conditional density models, models
in which the conditional density depends only in a piecewise manner from the
covariate. The first example relies on classical piecewise polynomial densities
while the second uses Gaussian mixtures with varying mixing proportion but same
mixture components. We show how this last case is related to an unsupervised
segmentation application that has been the source of our motivation to this
study."@2011
B. T. Knapik@http://arxiv.org/abs/1103.2692v2@Bayesian inverse problems with Gaussian priors@"The posterior distribution in a nonparametric inverse problem is shown to
contract to the true parameter at a rate that depends on the smoothness of the
parameter, and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the minimax rate. The frequentist coverage of
credible sets is shown to depend on the combination of prior and true
parameter, with smoother priors leading to zero coverage and rougher priors to
conservative coverage. In the latter case credible sets are of the correct
order of magnitude. The results are numerically illustrated by the problem of
recovering a function from observation of a noisy version of its primitive."@2011
A. W. van der Vaart@http://arxiv.org/abs/1103.2692v2@Bayesian inverse problems with Gaussian priors@"The posterior distribution in a nonparametric inverse problem is shown to
contract to the true parameter at a rate that depends on the smoothness of the
parameter, and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the minimax rate. The frequentist coverage of
credible sets is shown to depend on the combination of prior and true
parameter, with smoother priors leading to zero coverage and rougher priors to
conservative coverage. In the latter case credible sets are of the correct
order of magnitude. The results are numerically illustrated by the problem of
recovering a function from observation of a noisy version of its primitive."@2011
J. H. van Zanten@http://arxiv.org/abs/1103.2692v2@Bayesian inverse problems with Gaussian priors@"The posterior distribution in a nonparametric inverse problem is shown to
contract to the true parameter at a rate that depends on the smoothness of the
parameter, and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the minimax rate. The frequentist coverage of
credible sets is shown to depend on the combination of prior and true
parameter, with smoother priors leading to zero coverage and rougher priors to
conservative coverage. In the latter case credible sets are of the correct
order of magnitude. The results are numerically illustrated by the problem of
recovering a function from observation of a noisy version of its primitive."@2011
Yuri I. Ingster@http://arxiv.org/abs/1103.3442v3@"Minimax nonparametric testing in a problem related to the Radon
  transform"@"We consider the detection problem of a two-dimensional function from noisy
observations of its integrals over lines. We study both rate and sharp
asymptotics for the error probabilities in the minimax setup. By construction,
the derived tests are non-adaptive. We also construct a minimax rate-optimal
adaptive test of rather simple structure."@2011
Theofanis Sapatinas@http://arxiv.org/abs/1103.3442v3@"Minimax nonparametric testing in a problem related to the Radon
  transform"@"We consider the detection problem of a two-dimensional function from noisy
observations of its integrals over lines. We study both rate and sharp
asymptotics for the error probabilities in the minimax setup. By construction,
the derived tests are non-adaptive. We also construct a minimax rate-optimal
adaptive test of rather simple structure."@2011
Irina A. Suslina@http://arxiv.org/abs/1103.3442v3@"Minimax nonparametric testing in a problem related to the Radon
  transform"@"We consider the detection problem of a two-dimensional function from noisy
observations of its integrals over lines. We study both rate and sharp
asymptotics for the error probabilities in the minimax setup. By construction,
the derived tests are non-adaptive. We also construct a minimax rate-optimal
adaptive test of rather simple structure."@2011
Mathilde Mougeot@http://arxiv.org/abs/1103.3967v1@"A new selection method for high-dimensionial instrumental setting:
  application to the Growth Rate convergence hypothesis"@"This paper investigates the problem of selecting variables in regression-type
models for an ""instrumental"" setting. Our study is motivated by empirically
verifying the conditional convergence hypothesis used in the economical
literature concerning the growth rate. To avoid unnecessary discussion about
the choice and the pertinence of instrumental variables, we embed the model in
a very high dimensional setting. We propose a selection procedure with no
optimization step called LOLA, for Learning Out of Leaders with Adaptation.
LOLA is an auto-driven algorithm with two thresholding steps. The consistency
of the procedure is proved under sparsity conditions and simulations are
conducted to illustrate the practical good performances of LOLA. The behavior
of the algorithm is studied when instrumental variables are artificially added
without a priori significant connection to the model. Using our algorithm, we
provide a solution for modeling the link between the growth rate and the
initial level of the gross domestic product and empirically prove the
convergence hypothesis."@2011
Dominique Picard@http://arxiv.org/abs/1103.3967v1@"A new selection method for high-dimensionial instrumental setting:
  application to the Growth Rate convergence hypothesis"@"This paper investigates the problem of selecting variables in regression-type
models for an ""instrumental"" setting. Our study is motivated by empirically
verifying the conditional convergence hypothesis used in the economical
literature concerning the growth rate. To avoid unnecessary discussion about
the choice and the pertinence of instrumental variables, we embed the model in
a very high dimensional setting. We propose a selection procedure with no
optimization step called LOLA, for Learning Out of Leaders with Adaptation.
LOLA is an auto-driven algorithm with two thresholding steps. The consistency
of the procedure is proved under sparsity conditions and simulations are
conducted to illustrate the practical good performances of LOLA. The behavior
of the algorithm is studied when instrumental variables are artificially added
without a priori significant connection to the model. Using our algorithm, we
provide a solution for modeling the link between the growth rate and the
initial level of the gross domestic product and empirically prove the
convergence hypothesis."@2011
Karine Tribouley@http://arxiv.org/abs/1103.3967v1@"A new selection method for high-dimensionial instrumental setting:
  application to the Growth Rate convergence hypothesis"@"This paper investigates the problem of selecting variables in regression-type
models for an ""instrumental"" setting. Our study is motivated by empirically
verifying the conditional convergence hypothesis used in the economical
literature concerning the growth rate. To avoid unnecessary discussion about
the choice and the pertinence of instrumental variables, we embed the model in
a very high dimensional setting. We propose a selection procedure with no
optimization step called LOLA, for Learning Out of Leaders with Adaptation.
LOLA is an auto-driven algorithm with two thresholding steps. The consistency
of the procedure is proved under sparsity conditions and simulations are
conducted to illustrate the practical good performances of LOLA. The behavior
of the algorithm is studied when instrumental variables are artificially added
without a priori significant connection to the model. Using our algorithm, we
provide a solution for modeling the link between the growth rate and the
initial level of the gross domestic product and empirically prove the
convergence hypothesis."@2011
Mehdi Fhima@http://arxiv.org/abs/1103.4029v1@"Fast change point analysis on the Hurst index of piecewise fractional
  Brownian motion"@"In this presentation, we introduce a new method for change point analysis on
the Hurst index for a piecewise fractional Brownian motion. We first set the
model and the statistical problem. The proposed method is a transposition of
the FDpV (Filtered Derivative with p-value) method introduced for the detection
of change points on the mean in Bertrand et al. (2011) to the case of changes
on the Hurst index. The underlying statistics of the FDpV technology is a new
statistic estimator for Hurst index, so-called Increment Bernoulli Statistic
(IBS). Both FDpV and IBS are methods with linear time and memory complexity,
with respect to the size of the series. Thus the resulting method for change
point analysis on Hurst index reaches also a linear complexity."@2011
Arnaud Guillin@http://arxiv.org/abs/1103.4029v1@"Fast change point analysis on the Hurst index of piecewise fractional
  Brownian motion"@"In this presentation, we introduce a new method for change point analysis on
the Hurst index for a piecewise fractional Brownian motion. We first set the
model and the statistical problem. The proposed method is a transposition of
the FDpV (Filtered Derivative with p-value) method introduced for the detection
of change points on the mean in Bertrand et al. (2011) to the case of changes
on the Hurst index. The underlying statistics of the FDpV technology is a new
statistic estimator for Hurst index, so-called Increment Bernoulli Statistic
(IBS). Both FDpV and IBS are methods with linear time and memory complexity,
with respect to the size of the series. Thus the resulting method for change
point analysis on Hurst index reaches also a linear complexity."@2011
Pierre R. Bertrand@http://arxiv.org/abs/1103.4029v1@"Fast change point analysis on the Hurst index of piecewise fractional
  Brownian motion"@"In this presentation, we introduce a new method for change point analysis on
the Hurst index for a piecewise fractional Brownian motion. We first set the
model and the statistical problem. The proposed method is a transposition of
the FDpV (Filtered Derivative with p-value) method introduced for the detection
of change points on the mean in Bertrand et al. (2011) to the case of changes
on the Hurst index. The underlying statistics of the FDpV technology is a new
statistic estimator for Hurst index, so-called Increment Bernoulli Statistic
(IBS). Both FDpV and IBS are methods with linear time and memory complexity,
with respect to the size of the series. Thus the resulting method for change
point analysis on Hurst index reaches also a linear complexity."@2011
Maugis Cathy@http://arxiv.org/abs/1103.4253v2@Adaptive density estimation for clustering with Gaussian mixtures@"Gaussian mixture models are widely used to study clustering problems. These
model-based clustering methods require an accurate estimation of the unknown
data density by Gaussian mixtures. In Maugis and Michel (2009), a penalized
maximum likelihood estimator is proposed for automatically selecting the number
of mixture components. In the present paper, a collection of univariate
densities whose logarithm is locally {\beta}-H\""older with moment and tail
conditions are considered. We show that this penalized estimator is minimax
adaptive to the {\beta} regularity of such densities in the Hellinger sense."@2011
Michel Bertrand@http://arxiv.org/abs/1103.4253v2@Adaptive density estimation for clustering with Gaussian mixtures@"Gaussian mixture models are widely used to study clustering problems. These
model-based clustering methods require an accurate estimation of the unknown
data density by Gaussian mixtures. In Maugis and Michel (2009), a penalized
maximum likelihood estimator is proposed for automatically selecting the number
of mixture components. In the present paper, a collection of univariate
densities whose logarithm is locally {\beta}-H\""older with moment and tail
conditions are considered. We show that this penalized estimator is minimax
adaptive to the {\beta} regularity of such densities in the Hellinger sense."@2011
I. Bairamov@http://arxiv.org/abs/1103.4464v1@Baker- Lin-Huang type Bivariate distributions based on order statistics@"Baker (2008) introduced a new class of bivariate distributions based on
distributions of order statistics from two independent samples of size n.
Lin-Huang (2010) discovered an important property of Baker's distribution and
showed that the Pearson's correlation coefficient for this distribution
converges to maximum attainable value, i.e. the correlation coefficient of the
Frech\'et upper bound, as n increases to infinity. Bairamov and Bayramoglu
(2011) investigated a new class of bivariate distributions constructed by using
Baker's model and distributions of order statistics from dependent random
variables, allowing high correlation than that of Baker's distribution. In this
paper a new class of Baker's type bivariate distributions with high correlation
are constructed on the base of distributions of order statistics by using an
arbitrary continuous copula instead of the product copula.
  Keywords: Bivariate distribution function, FGM distributions, copula,
positive quadrant dependent, negative quadrant dependent, order statistics,
Pearson's correlation coefficient."@2011
K. Bayramoglu@http://arxiv.org/abs/1103.4464v1@Baker- Lin-Huang type Bivariate distributions based on order statistics@"Baker (2008) introduced a new class of bivariate distributions based on
distributions of order statistics from two independent samples of size n.
Lin-Huang (2010) discovered an important property of Baker's distribution and
showed that the Pearson's correlation coefficient for this distribution
converges to maximum attainable value, i.e. the correlation coefficient of the
Frech\'et upper bound, as n increases to infinity. Bairamov and Bayramoglu
(2011) investigated a new class of bivariate distributions constructed by using
Baker's model and distributions of order statistics from dependent random
variables, allowing high correlation than that of Baker's distribution. In this
paper a new class of Baker's type bivariate distributions with high correlation
are constructed on the base of distributions of order statistics by using an
arbitrary continuous copula instead of the product copula.
  Keywords: Bivariate distribution function, FGM distributions, copula,
positive quadrant dependent, negative quadrant dependent, order statistics,
Pearson's correlation coefficient."@2011
Tabea Rebafka@http://arxiv.org/abs/1103.5158v1@"OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar
  Signals"@"A transmitted, unknown radar signal is observed at the receiver through more
than one path in additive noise. The aim is to recover the waveform of the
intercepted signal and to simultaneously estimate the direction of arrival
(DOA). We propose an approach exploiting the parsimonious time-frequency
representation of the signal by applying a new OMP-type algorithm for
structured sparsity patterns. An important issue is the scalability of the
proposed algorithm since high-dimensional models shall be used for radar
signals. Monte-Carlo simulations for modulated signals illustrate the good
performance of the method even for low signal-to-noise ratios and a gain of 20
dB for the DOA estimation compared to some elementary method."@2011
Céline Lévy-Leduc@http://arxiv.org/abs/1103.5158v1@"OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar
  Signals"@"A transmitted, unknown radar signal is observed at the receiver through more
than one path in additive noise. The aim is to recover the waveform of the
intercepted signal and to simultaneously estimate the direction of arrival
(DOA). We propose an approach exploiting the parsimonious time-frequency
representation of the signal by applying a new OMP-type algorithm for
structured sparsity patterns. An important issue is the scalability of the
proposed algorithm since high-dimensional models shall be used for radar
signals. Monte-Carlo simulations for modulated signals illustrate the good
performance of the method even for low signal-to-noise ratios and a gain of 20
dB for the DOA estimation compared to some elementary method."@2011
Maurice Charbit@http://arxiv.org/abs/1103.5158v1@"OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar
  Signals"@"A transmitted, unknown radar signal is observed at the receiver through more
than one path in additive noise. The aim is to recover the waveform of the
intercepted signal and to simultaneously estimate the direction of arrival
(DOA). We propose an approach exploiting the parsimonious time-frequency
representation of the signal by applying a new OMP-type algorithm for
structured sparsity patterns. An important issue is the scalability of the
proposed algorithm since high-dimensional models shall be used for radar
signals. Monte-Carlo simulations for modulated signals illustrate the good
performance of the method even for low signal-to-noise ratios and a gain of 20
dB for the DOA estimation compared to some elementary method."@2011
A. Murillo-Salas@http://arxiv.org/abs/1103.5220v3@"A note on the infinite divisibility of a class of transformations of
  normal variables"@"This note examines the infinite divisibility of density-based transformations
of normal random variables. We characterize a class of density-based
transformations of normal variables which produces non-infinitely divisible
distributions. We relate our result with some known skewing mechanisms."@2011
F. J. Rubio@http://arxiv.org/abs/1103.5220v3@"A note on the infinite divisibility of a class of transformations of
  normal variables"@"This note examines the infinite divisibility of density-based transformations
of normal random variables. We characterize a class of density-based
transformations of normal variables which produces non-infinitely divisible
distributions. We relate our result with some known skewing mechanisms."@2011
Gerard Letac@http://arxiv.org/abs/1103.5381v3@Bayes factors and the geometry of discrete hierarchical loglinear models@"A standard tool for model selection in a Bayesian framework is the Bayes
factor which compares the marginal likelihood of the data under two given
different models. In this paper, we consider the class of hierarchical
loglinear models for discrete data given under the form of a contingency table
with multinomial sampling. We assume that the Diaconis-Ylvisaker conjugate
prior is the prior distribution on the loglinear parameters and the uniform is
the prior distribution on the space of models. Under these conditions, the
Bayes factor between two models is a function of their prior and posterior
normalizing constants. These constants are functions of the hyperparameters
$(m,\alpha)$ which can be interpreted respectively as marginal counts and the
total count of a fictive contingency table.
  We study the behaviour of the Bayes factor when $\alpha$ tends to zero. In
this study two mathematical objects play a most important role. They are,
first, the interior $C$ of the convex hull $\bar{C}$ of the support of the
multinomial distribution for a given hierarchical loglinear model together with
its faces and second, the characteristic function $\mathbb{J}_C$ of this convex
set $C$.
  We show that, when $\alpha$ tends to 0, if the data lies on a face $F_i$ of
$\bar{C_i},i=1,2$ of dimension $k_i$, the Bayes factor behaves like
$\alpha^{k_1-k_2}$. This implies in particular that when the data is in $C_1$
and in $C_2$, i.e. when $k_i$ equals the dimension of model $J_i$, the sparser
model is favored, thus confirming the idea of Bayesian regularization."@2011
Helene Massam@http://arxiv.org/abs/1103.5381v3@Bayes factors and the geometry of discrete hierarchical loglinear models@"A standard tool for model selection in a Bayesian framework is the Bayes
factor which compares the marginal likelihood of the data under two given
different models. In this paper, we consider the class of hierarchical
loglinear models for discrete data given under the form of a contingency table
with multinomial sampling. We assume that the Diaconis-Ylvisaker conjugate
prior is the prior distribution on the loglinear parameters and the uniform is
the prior distribution on the space of models. Under these conditions, the
Bayes factor between two models is a function of their prior and posterior
normalizing constants. These constants are functions of the hyperparameters
$(m,\alpha)$ which can be interpreted respectively as marginal counts and the
total count of a fictive contingency table.
  We study the behaviour of the Bayes factor when $\alpha$ tends to zero. In
this study two mathematical objects play a most important role. They are,
first, the interior $C$ of the convex hull $\bar{C}$ of the support of the
multinomial distribution for a given hierarchical loglinear model together with
its faces and second, the characteristic function $\mathbb{J}_C$ of this convex
set $C$.
  We show that, when $\alpha$ tends to 0, if the data lies on a face $F_i$ of
$\bar{C_i},i=1,2$ of dimension $k_i$, the Bayes factor behaves like
$\alpha^{k_1-k_2}$. This implies in particular that when the data is in $C_1$
and in $C_2$, i.e. when $k_i$ equals the dimension of model $J_i$, the sparser
model is favored, thus confirming the idea of Bayesian regularization."@2011
Stéphane Girard@http://arxiv.org/abs/1103.5884v1@"Central limit theorems for smoothed extreme value estimates of Poisson
  point processes boundaries"@"In this paper, we give sufficient conditions to establish central limit
theorems for boundary estimates of Poisson point processes. The considered
estimates are obtained by smoothing some bias corrected extreme values of the
point process. We show how the smoothing leads Gaussian asymptotic
distributions and therefore pointwise confidence intervals. Some new
unidimensional and multidimensional examples are provided."@2011
Ludovic Menneteau@http://arxiv.org/abs/1103.5884v1@"Central limit theorems for smoothed extreme value estimates of Poisson
  point processes boundaries"@"In this paper, we give sufficient conditions to establish central limit
theorems for boundary estimates of Poisson point processes. The considered
estimates are obtained by smoothing some bias corrected extreme values of the
point process. We show how the smoothing leads Gaussian asymptotic
distributions and therefore pointwise confidence intervals. Some new
unidimensional and multidimensional examples are provided."@2011
Laurent Gardes@http://arxiv.org/abs/1103.5894v1@"Estimation of the Weibull tail-coefficient with linear combination of
  upper order statistics"@"We present a new family of estimators of the Weibull tail-coefficient. The
Weibull tail-coefficient is defined as the regular variation coefficient of the
inverse failure rate function. Our estimators are based on a linear combination
of log-spacings of the upper order statistics. Their asymptotic normality is
established and illustrated for two particular cases of estimators in this
family. Their finite sample performances are presented on a simulation study."@2011
Stéphane Girard@http://arxiv.org/abs/1103.5894v1@"Estimation of the Weibull tail-coefficient with linear combination of
  upper order statistics"@"We present a new family of estimators of the Weibull tail-coefficient. The
Weibull tail-coefficient is defined as the regular variation coefficient of the
inverse failure rate function. Our estimators are based on a linear combination
of log-spacings of the upper order statistics. Their asymptotic normality is
established and illustrated for two particular cases of estimators in this
family. Their finite sample performances are presented on a simulation study."@2011
Cécile Amblard@http://arxiv.org/abs/1103.5921v1@A new bivariate extension of FGM copulas@"We propose a new family of copulas generalizing the
  Farlie-Gumbel-Morgenstern family and generated by two univariate functions.
The main feature of this family is to permit the modeling of high positive
dependence. In particular, it is established that the range of the Spearman's
Rho is [-3/4,1] and that the upper tail dependence coefficient can reach any
value in [0,1]. Necessary and sufficient conditions are given on the generating
functions in order to obtain various dependence properties. Some examples of
parametric subfamilies are provided."@2011
Stéphane Girard@http://arxiv.org/abs/1103.5921v1@A new bivariate extension of FGM copulas@"We propose a new family of copulas generalizing the
  Farlie-Gumbel-Morgenstern family and generated by two univariate functions.
The main feature of this family is to permit the modeling of high positive
dependence. In particular, it is established that the range of the Spearman's
Rho is [-3/4,1] and that the upper tail dependence coefficient can reach any
value in [0,1]. Necessary and sufficient conditions are given on the generating
functions in order to obtain various dependence properties. Some examples of
parametric subfamilies are provided."@2011
Cécile Amblard@http://arxiv.org/abs/1103.5953v1@"Symmetry and dependence properties within a semiparametric family of
  bivariate copulas"@"In this paper, we study a semiparametric family of bivariate copulas. The
family is generated by an univariate function, determining the symmetry (radial
symmetry, joint symmetry) and dependence property (quadrant dependence, total
positivity, ...) of the copulas. We provide bounds on different measures of
association (such as Kendall's Tau, Spearman's Rho) for this family and several
choices of generating functions allowing to reach these bounds."@2011
Stéphane Girard@http://arxiv.org/abs/1103.5953v1@"Symmetry and dependence properties within a semiparametric family of
  bivariate copulas"@"In this paper, we study a semiparametric family of bivariate copulas. The
family is generated by an univariate function, determining the symmetry (radial
symmetry, joint symmetry) and dependence property (quadrant dependence, total
positivity, ...) of the copulas. We provide bounds on different measures of
association (such as Kendall's Tau, Spearman's Rho) for this family and several
choices of generating functions allowing to reach these bounds."@2011
C. Bernard-Michel@http://arxiv.org/abs/1103.6118v1@Gaussian Regularized Sliced Inverse Regression@"Sliced Inverse Regression (SIR) is an effective method for dimension
reduction in high-dimensional regression problems. The original method,
however, requires the inversion of the predictors covariance matrix. In case of
collinearity between these predictors or small sample sizes compared to the
dimension, the inversion is not possible and a regularization technique has to
be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it
is shown that SIR axes can be interpreted as solutions of an inverse regression
problem. In this paper, a Gaussian prior distribution is introduced on the
unknown parameters of the inverse regression problem in order to regularize
their estimation. We show that some existing SIR regularizations can enter our
framework, which permits a global understanding of these methods. Three new
priors are proposed leading to new regularizations of the SIR method. A
comparison on simulated data is provided."@2011
L. Gardes@http://arxiv.org/abs/1103.6118v1@Gaussian Regularized Sliced Inverse Regression@"Sliced Inverse Regression (SIR) is an effective method for dimension
reduction in high-dimensional regression problems. The original method,
however, requires the inversion of the predictors covariance matrix. In case of
collinearity between these predictors or small sample sizes compared to the
dimension, the inversion is not possible and a regularization technique has to
be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it
is shown that SIR axes can be interpreted as solutions of an inverse regression
problem. In this paper, a Gaussian prior distribution is introduced on the
unknown parameters of the inverse regression problem in order to regularize
their estimation. We show that some existing SIR regularizations can enter our
framework, which permits a global understanding of these methods. Three new
priors are proposed leading to new regularizations of the SIR method. A
comparison on simulated data is provided."@2011
S. Girard@http://arxiv.org/abs/1103.6118v1@Gaussian Regularized Sliced Inverse Regression@"Sliced Inverse Regression (SIR) is an effective method for dimension
reduction in high-dimensional regression problems. The original method,
however, requires the inversion of the predictors covariance matrix. In case of
collinearity between these predictors or small sample sizes compared to the
dimension, the inversion is not possible and a regularization technique has to
be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it
is shown that SIR axes can be interpreted as solutions of an inverse regression
problem. In this paper, a Gaussian prior distribution is introduced on the
unknown parameters of the inverse regression problem in order to regularize
their estimation. We show that some existing SIR regularizations can enter our
framework, which permits a global understanding of these methods. Three new
priors are proposed leading to new regularizations of the SIR method. A
comparison on simulated data is provided."@2011
J. Diebolt@http://arxiv.org/abs/1103.6172v1@Bias-reduced estimators of the Weibull tail-coefficient@"In this paper, we consider the problem of the estimation of a Weibull
tail-coefficient. In particular, we propose a regression model, from which we
derive a bias-reduced estimator. This estimator is based on a least-squares
approach. The asymptotic normality of this estimator is also established. A
small simulation study is provided in order to prove its efficiency."@2011
L. Gardes@http://arxiv.org/abs/1103.6172v1@Bias-reduced estimators of the Weibull tail-coefficient@"In this paper, we consider the problem of the estimation of a Weibull
tail-coefficient. In particular, we propose a regression model, from which we
derive a bias-reduced estimator. This estimator is based on a least-squares
approach. The asymptotic normality of this estimator is also established. A
small simulation study is provided in order to prove its efficiency."@2011
S. Girard@http://arxiv.org/abs/1103.6172v1@Bias-reduced estimators of the Weibull tail-coefficient@"In this paper, we consider the problem of the estimation of a Weibull
tail-coefficient. In particular, we propose a regression model, from which we
derive a bias-reduced estimator. This estimator is based on a least-squares
approach. The asymptotic normality of this estimator is also established. A
small simulation study is provided in order to prove its efficiency."@2011
A. Guillou@http://arxiv.org/abs/1103.6172v1@Bias-reduced estimators of the Weibull tail-coefficient@"In this paper, we consider the problem of the estimation of a Weibull
tail-coefficient. In particular, we propose a regression model, from which we
derive a bias-reduced estimator. This estimator is based on a least-squares
approach. The asymptotic normality of this estimator is also established. A
small simulation study is provided in order to prove its efficiency."@2011
Caroline Bernard-Michel@http://arxiv.org/abs/1104.0098v1@A Note on Sliced Inverse Regression with Regularizations@"In ""Li, L. and Yin, X. (2008). Sliced Inverse Regression with
Regularizations. Biometrics, 64(1):124--131"" a ridge SIR estimator is
introduced as the solution of a minimization problem and computed thanks to an
alternating least-squares algorithm. This methodology reveals good performance
in practice. In this note, we focus on the theoretical properties of the
estimator. Is it shown that the minimization problem is degenerated in the
sense that only two situations can occur: Either the ridge SIR estimator does
not exist or it is zero."@2011
Laurent Gardes@http://arxiv.org/abs/1104.0098v1@A Note on Sliced Inverse Regression with Regularizations@"In ""Li, L. and Yin, X. (2008). Sliced Inverse Regression with
Regularizations. Biometrics, 64(1):124--131"" a ridge SIR estimator is
introduced as the solution of a minimization problem and computed thanks to an
alternating least-squares algorithm. This methodology reveals good performance
in practice. In this note, we focus on the theoretical properties of the
estimator. Is it shown that the minimization problem is degenerated in the
sense that only two situations can occur: Either the ridge SIR estimator does
not exist or it is zero."@2011
Stéphane Girard@http://arxiv.org/abs/1104.0098v1@A Note on Sliced Inverse Regression with Regularizations@"In ""Li, L. and Yin, X. (2008). Sliced Inverse Regression with
Regularizations. Biometrics, 64(1):124--131"" a ridge SIR estimator is
introduced as the solution of a minimization problem and computed thanks to an
alternating least-squares algorithm. This methodology reveals good performance
in practice. In this note, we focus on the theoretical properties of the
estimator. Is it shown that the minimization problem is degenerated in the
sense that only two situations can occur: Either the ridge SIR estimator does
not exist or it is zero."@2011
Ery Arias-Castro@http://arxiv.org/abs/1104.0338v2@Cluster detection in networks using percolation@"We consider the task of detecting a salient cluster in a sensor network, that
is, an undirected graph with a random variable attached to each node. Motivated
by recent research in environmental statistics and the drive to compete with
the reigning scan statistic, we explore alternatives based on the percolative
properties of the network. The first method is based on the size of the largest
connected component after removing the nodes in the network with a value below
a given threshold. The second method is the upper level set scan test
introduced by Patil and Taillie [Statist. Sci. 18 (2003) 457-465]. We establish
the performance of these methods in an asymptotic decision- theoretic framework
in which the network size increases. These tests have two advantages over the
more conventional scan statistic: they do not require previous information
about cluster shape, and they are computationally more feasible. We make
abundant use of percolation theory to derive our theoretical results, and
complement our theory with some numerical experiments."@2011
Geoffrey R. Grimmett@http://arxiv.org/abs/1104.0338v2@Cluster detection in networks using percolation@"We consider the task of detecting a salient cluster in a sensor network, that
is, an undirected graph with a random variable attached to each node. Motivated
by recent research in environmental statistics and the drive to compete with
the reigning scan statistic, we explore alternatives based on the percolative
properties of the network. The first method is based on the size of the largest
connected component after removing the nodes in the network with a value below
a given threshold. The second method is the upper level set scan test
introduced by Patil and Taillie [Statist. Sci. 18 (2003) 457-465]. We establish
the performance of these methods in an asymptotic decision- theoretic framework
in which the network size increases. These tests have two advantages over the
more conventional scan statistic: they do not require previous information
about cluster shape, and they are computationally more feasible. We make
abundant use of percolation theory to derive our theoretical results, and
complement our theory with some numerical experiments."@2011
Alexander Aue@http://arxiv.org/abs/1104.0841v2@Limit Laws in Transaction-Level Asset Price Models@"We consider pure-jump transaction-level models for asset prices in continuous
time, driven by point processes. In a bivariate model that admits
cointegration, we allow for time deformations to account for such effects as
intraday seasonal patterns in volatility, and non-trading periods that may be
different for the two assets. We also allow for asymmetries (leverage effects).
We obtain the asymptotic distribution of the log-price process. We also obtain
the asymptotic distribution of the ordinary least-squares estimator of the
cointegrating parameter based on data sampled from an equally-spaced
discretization of calendar time, in the case of weak fractional cointegration.
For this same case, we obtain the asymptotic distribution for a tapered
estimator under more"@2011
Lajos Horváth@http://arxiv.org/abs/1104.0841v2@Limit Laws in Transaction-Level Asset Price Models@"We consider pure-jump transaction-level models for asset prices in continuous
time, driven by point processes. In a bivariate model that admits
cointegration, we allow for time deformations to account for such effects as
intraday seasonal patterns in volatility, and non-trading periods that may be
different for the two assets. We also allow for asymmetries (leverage effects).
We obtain the asymptotic distribution of the log-price process. We also obtain
the asymptotic distribution of the ordinary least-squares estimator of the
cointegrating parameter based on data sampled from an equally-spaced
discretization of calendar time, in the case of weak fractional cointegration.
For this same case, we obtain the asymptotic distribution for a tapered
estimator under more"@2011
Clifford M. Hurvich@http://arxiv.org/abs/1104.0841v2@Limit Laws in Transaction-Level Asset Price Models@"We consider pure-jump transaction-level models for asset prices in continuous
time, driven by point processes. In a bivariate model that admits
cointegration, we allow for time deformations to account for such effects as
intraday seasonal patterns in volatility, and non-trading periods that may be
different for the two assets. We also allow for asymmetries (leverage effects).
We obtain the asymptotic distribution of the log-price process. We also obtain
the asymptotic distribution of the ordinary least-squares estimator of the
cointegrating parameter based on data sampled from an equally-spaced
discretization of calendar time, in the case of weak fractional cointegration.
For this same case, we obtain the asymptotic distribution for a tapered
estimator under more"@2011
Philippe Soulier@http://arxiv.org/abs/1104.0841v2@Limit Laws in Transaction-Level Asset Price Models@"We consider pure-jump transaction-level models for asset prices in continuous
time, driven by point processes. In a bivariate model that admits
cointegration, we allow for time deformations to account for such effects as
intraday seasonal patterns in volatility, and non-trading periods that may be
different for the two assets. We also allow for asymmetries (leverage effects).
We obtain the asymptotic distribution of the log-price process. We also obtain
the asymptotic distribution of the ordinary least-squares estimator of the
cointegrating parameter based on data sampled from an equally-spaced
discretization of calendar time, in the case of weak fractional cointegration.
For this same case, we obtain the asymptotic distribution for a tapered
estimator under more"@2011
Giorgio Dall'Aglio@http://arxiv.org/abs/1104.1536v1@"The Minimum of the Entropy of a Two-Dimensional Distribution with Given
  Marginals"@"The paper search for the minimum of the entropy of a two- dimensional
distribution in the Fr\'echet class, the class of distributions with given
marginals. The main result for discrete distributions is an algorithm for
building the minimizing distribution, which is given by the maximum
distribution function of the Fr\'echet class after a suitable rearrangement of
the rows and of the columns. For absolutely continuous distributions a minimum
does not exists, and the infimum is equal to -\infty."@2011
Elisabetta Bona@http://arxiv.org/abs/1104.1536v1@"The Minimum of the Entropy of a Two-Dimensional Distribution with Given
  Marginals"@"The paper search for the minimum of the entropy of a two- dimensional
distribution in the Fr\'echet class, the class of distributions with given
marginals. The main result for discrete distributions is an algorithm for
building the minimizing distribution, which is given by the maximum
distribution function of the Fr\'echet class after a suitable rearrangement of
the rows and of the columns. For absolutely continuous distributions a minimum
does not exists, and the infimum is equal to -\infty."@2011
Michel Broniatowski@http://arxiv.org/abs/1104.1541v1@Decomposable Pseudodistances and Applications in Statistical Estimation@"The aim of this paper is to introduce new statistical criterions for
estimation, suitable for inference in models with common continuous support.
This proposal is in the direct line of a renewed interest for divergence based
inference tools imbedding the most classical ones, such as maximum likelihood,
Chi-square or Kullback Leibler. General pseudodistances with decomposable
structure are considered, they allowing to define minimum pseudodistance
estimators, without using nonparametric density estimators. A special class of
pseudodistances indexed by {\alpha}>0, leading for {\alpha}\downarrow0 to the
Kulback Leibler divergence, is presented in detail. Corresponding estimation
criteria are developed and asymptotic properties are studied. The estimation
method is then extended to regression models. Finally, some examples based on
Monte Carlo simulations are discussed."@2011
Aida Toma@http://arxiv.org/abs/1104.1541v1@Decomposable Pseudodistances and Applications in Statistical Estimation@"The aim of this paper is to introduce new statistical criterions for
estimation, suitable for inference in models with common continuous support.
This proposal is in the direct line of a renewed interest for divergence based
inference tools imbedding the most classical ones, such as maximum likelihood,
Chi-square or Kullback Leibler. General pseudodistances with decomposable
structure are considered, they allowing to define minimum pseudodistance
estimators, without using nonparametric density estimators. A special class of
pseudodistances indexed by {\alpha}>0, leading for {\alpha}\downarrow0 to the
Kulback Leibler divergence, is presented in detail. Corresponding estimation
criteria are developed and asymptotic properties are studied. The estimation
method is then extended to regression models. Finally, some examples based on
Monte Carlo simulations are discussed."@2011
Igor Vajda@http://arxiv.org/abs/1104.1541v1@Decomposable Pseudodistances and Applications in Statistical Estimation@"The aim of this paper is to introduce new statistical criterions for
estimation, suitable for inference in models with common continuous support.
This proposal is in the direct line of a renewed interest for divergence based
inference tools imbedding the most classical ones, such as maximum likelihood,
Chi-square or Kullback Leibler. General pseudodistances with decomposable
structure are considered, they allowing to define minimum pseudodistance
estimators, without using nonparametric density estimators. A special class of
pseudodistances indexed by {\alpha}>0, leading for {\alpha}\downarrow0 to the
Kulback Leibler divergence, is presented in detail. Corresponding estimation
criteria are developed and asymptotic properties are studied. The estimation
method is then extended to regression models. Finally, some examples based on
Monte Carlo simulations are discussed."@2011
Felix Abramovich@http://arxiv.org/abs/1104.1771v2@Estimation of a sparse group of sparse vectors@"We consider a problem of estimating a sparse group of sparse normal mean
vectors. The proposed approach is based on penalized likelihood estimation with
complexity penalties on the number of nonzero mean vectors and the numbers of
their ""significant"" components, and can be performed by a computationally fast
algorithm. The resulting estimators are developed within Bayesian framework and
can be viewed as MAP estimators. We establish their adaptive minimaxity over a
wide range of sparse and dense settings. The presented short simulation study
demonstrates the efficiency of the proposed approach that successfully competes
with the recently developed sparse group lasso estimator."@2011
Vadim Grinshtein@http://arxiv.org/abs/1104.1771v2@Estimation of a sparse group of sparse vectors@"We consider a problem of estimating a sparse group of sparse normal mean
vectors. The proposed approach is based on penalized likelihood estimation with
complexity penalties on the number of nonzero mean vectors and the numbers of
their ""significant"" components, and can be performed by a computationally fast
algorithm. The resulting estimators are developed within Bayesian framework and
can be viewed as MAP estimators. We establish their adaptive minimaxity over a
wide range of sparse and dense settings. The presented short simulation study
demonstrates the efficiency of the proposed approach that successfully competes
with the recently developed sparse group lasso estimator."@2011
A. Philip Dawid@http://arxiv.org/abs/1104.2224v3@Proper local scoring rules on discrete sample spaces@"A scoring rule is a loss function measuring the quality of a quoted
probability distribution $Q$ for a random variable $X$, in the light of the
realized outcome $x$ of $X$; it is proper if the expected score, under any
distribution $P$ for $X$, is minimized by quoting $Q=P$. Using the fact that
any differentiable proper scoring rule on a finite sample space ${\mathcal{X}}$
is the gradient of a concave homogeneous function, we consider when such a rule
can be local in the sense of depending only on the probabilities quoted for
points in a nominated neighborhood of $x$. Under mild conditions, we
characterize such a proper local scoring rule in terms of a collection of
homogeneous functions on the cliques of an undirected graph on the space
${\mathcal{X}}$. A useful property of such rules is that the quoted
distribution $Q$ need only be known up to a scale factor. Examples of the use
of such scoring rules include Besag's pseudo-likelihood and Hyv\""{a}rinen's
method of ratio matching."@2011
Steffen Lauritzen@http://arxiv.org/abs/1104.2224v3@Proper local scoring rules on discrete sample spaces@"A scoring rule is a loss function measuring the quality of a quoted
probability distribution $Q$ for a random variable $X$, in the light of the
realized outcome $x$ of $X$; it is proper if the expected score, under any
distribution $P$ for $X$, is minimized by quoting $Q=P$. Using the fact that
any differentiable proper scoring rule on a finite sample space ${\mathcal{X}}$
is the gradient of a concave homogeneous function, we consider when such a rule
can be local in the sense of depending only on the probabilities quoted for
points in a nominated neighborhood of $x$. Under mild conditions, we
characterize such a proper local scoring rule in terms of a collection of
homogeneous functions on the cliques of an undirected graph on the space
${\mathcal{X}}$. A useful property of such rules is that the quoted
distribution $Q$ need only be known up to a scale factor. Examples of the use
of such scoring rules include Besag's pseudo-likelihood and Hyv\""{a}rinen's
method of ratio matching."@2011
Matthew Parry@http://arxiv.org/abs/1104.2224v3@Proper local scoring rules on discrete sample spaces@"A scoring rule is a loss function measuring the quality of a quoted
probability distribution $Q$ for a random variable $X$, in the light of the
realized outcome $x$ of $X$; it is proper if the expected score, under any
distribution $P$ for $X$, is minimized by quoting $Q=P$. Using the fact that
any differentiable proper scoring rule on a finite sample space ${\mathcal{X}}$
is the gradient of a concave homogeneous function, we consider when such a rule
can be local in the sense of depending only on the probabilities quoted for
points in a nominated neighborhood of $x$. Under mild conditions, we
characterize such a proper local scoring rule in terms of a collection of
homogeneous functions on the cliques of an undirected graph on the space
${\mathcal{X}}$. A useful property of such rules is that the quoted
distribution $Q$ need only be known up to a scale factor. Examples of the use
of such scoring rules include Besag's pseudo-likelihood and Hyv\""{a}rinen's
method of ratio matching."@2011
Cecília Fonseca@http://arxiv.org/abs/1104.2637v2@"Generalized madogram and pairwise dependence of maxima over two regions
  of a random field"@"Spatial environmental processes often exhibit dependence in their large
values. In order to model such processes their dependence properties must be
characterized and quantified. In this paper we introduce a measure that
evaluates the dependence among extreme observations located in two separated
regions of locations of R^2. We compute the range of this new dependence
measure, which extends the existing {\lambda}-madogram concept, and compare it
with extremal coefficients, finding generalizations of the known relations in
pairwise approach. Estimators for this measure are introduced and asymptotic
normality and strong consistency are shown. An application to the annual maxima
precipitation in Portuguese regions is presented."@2011
Luísa Pereira@http://arxiv.org/abs/1104.2637v2@"Generalized madogram and pairwise dependence of maxima over two regions
  of a random field"@"Spatial environmental processes often exhibit dependence in their large
values. In order to model such processes their dependence properties must be
characterized and quantified. In this paper we introduce a measure that
evaluates the dependence among extreme observations located in two separated
regions of locations of R^2. We compute the range of this new dependence
measure, which extends the existing {\lambda}-madogram concept, and compare it
with extremal coefficients, finding generalizations of the known relations in
pairwise approach. Estimators for this measure are introduced and asymptotic
normality and strong consistency are shown. An application to the annual maxima
precipitation in Portuguese regions is presented."@2011
Helena Ferreira@http://arxiv.org/abs/1104.2637v2@"Generalized madogram and pairwise dependence of maxima over two regions
  of a random field"@"Spatial environmental processes often exhibit dependence in their large
values. In order to model such processes their dependence properties must be
characterized and quantified. In this paper we introduce a measure that
evaluates the dependence among extreme observations located in two separated
regions of locations of R^2. We compute the range of this new dependence
measure, which extends the existing {\lambda}-madogram concept, and compare it
with extremal coefficients, finding generalizations of the known relations in
pairwise approach. Estimators for this measure are introduced and asymptotic
normality and strong consistency are shown. An application to the annual maxima
precipitation in Portuguese regions is presented."@2011
Ana Paula Martins@http://arxiv.org/abs/1104.2637v2@"Generalized madogram and pairwise dependence of maxima over two regions
  of a random field"@"Spatial environmental processes often exhibit dependence in their large
values. In order to model such processes their dependence properties must be
characterized and quantified. In this paper we introduce a measure that
evaluates the dependence among extreme observations located in two separated
regions of locations of R^2. We compute the range of this new dependence
measure, which extends the existing {\lambda}-madogram concept, and compare it
with extremal coefficients, finding generalizations of the known relations in
pairwise approach. Estimators for this measure are introduced and asymptotic
normality and strong consistency are shown. An application to the annual maxima
precipitation in Portuguese regions is presented."@2011
Damien Passemier@http://arxiv.org/abs/1104.2677v1@"On determining the number of spikes in a high-dimensional spiked
  population model"@"In a spiked population model, the population covariance matrix has all its
eigenvalues equal to units except for a few fixed eigenvalues (spikes).
Determining the number of spikes is a fundamental problem which appears in many
scientific fields, including signal processing (linear mixture model) or
economics (factor model). Several recent papers studied the asymptotic behavior
of the eigenvalues of the sample covariance matrix (sample eigenvalues) when
the dimension of the observations and the sample size both grow to infinity so
that their ratio converges to a positive constant. Using these results, we
propose a new estimator based on the difference between two consecutive sample
eigenvalues."@2011
Jian-Feng Yao@http://arxiv.org/abs/1104.2677v1@"On determining the number of spikes in a high-dimensional spiked
  population model"@"In a spiked population model, the population covariance matrix has all its
eigenvalues equal to units except for a few fixed eigenvalues (spikes).
Determining the number of spikes is a fundamental problem which appears in many
scientific fields, including signal processing (linear mixture model) or
economics (factor model). Several recent papers studied the asymptotic behavior
of the eigenvalues of the sample covariance matrix (sample eigenvalues) when
the dimension of the observations and the sample size both grow to infinity so
that their ratio converges to a positive constant. Using these results, we
propose a new estimator based on the difference between two consecutive sample
eigenvalues."@2011
Siegfried Hörmann@http://arxiv.org/abs/1104.3074v2@"Consistency of the mean and the principal components of spatially
  distributed functional data"@"This paper develops a framework for the estimation of the functional mean and
the functional principal components when the functions form a random field.
More specifically, the data we study consist of curves
$X(\mathbf{s}_k;t),t\in[0,T]$, observed at spatial points
$\mathbf{s}_1,\mathbf{s}_2,\ldots,\mathbf{s}_N$. We establish conditions for
the sample average (in space) of the $X(\mathbf{s}_k)$ to be a consistent
estimator of the population mean function, and for the usual empirical
covariance operator to be a consistent estimator of the population covariance
operator. These conditions involve an interplay of the assumptions on an
appropriately defined dependence between the functions $X(\mathbf{s}_k)$ and
the assumptions on the spatial distribution of the points $\mathbf{s}_k$. The
rates of convergence may be the same as for i.i.d. functional samples, but
generally depend on the strength of dependence and appropriately quantified
distances between the points $\mathbf{s}_k$. We also formulate conditions for
the lack of consistency."@2011
Piotr Kokoszka@http://arxiv.org/abs/1104.3074v2@"Consistency of the mean and the principal components of spatially
  distributed functional data"@"This paper develops a framework for the estimation of the functional mean and
the functional principal components when the functions form a random field.
More specifically, the data we study consist of curves
$X(\mathbf{s}_k;t),t\in[0,T]$, observed at spatial points
$\mathbf{s}_1,\mathbf{s}_2,\ldots,\mathbf{s}_N$. We establish conditions for
the sample average (in space) of the $X(\mathbf{s}_k)$ to be a consistent
estimator of the population mean function, and for the usual empirical
covariance operator to be a consistent estimator of the population covariance
operator. These conditions involve an interplay of the assumptions on an
appropriately defined dependence between the functions $X(\mathbf{s}_k)$ and
the assumptions on the spatial distribution of the points $\mathbf{s}_k$. The
rates of convergence may be the same as for i.i.d. functional samples, but
generally depend on the strength of dependence and appropriately quantified
distances between the points $\mathbf{s}_k$. We also formulate conditions for
the lack of consistency."@2011
Michael Evans@http://arxiv.org/abs/1104.3258v1@Inferences from prior-based loss functions@"Inferences that arise from loss functions determined by the prior are
considered and it is shown that these lead to limiting Bayes rules that are
closely connected with likelihood. The procedures obtained via these loss
functions are invariant under reparameterizations and are Bayesian unbiased or
limits of Bayesian unbiased inferences. These inferences serve as
well-supported alternatives to MAP-based inferences."@2011
Gun Ho Jang@http://arxiv.org/abs/1104.3258v1@Inferences from prior-based loss functions@"Inferences that arise from loss functions determined by the prior are
considered and it is shown that these lead to limiting Bayes rules that are
closely connected with likelihood. The procedures obtained via these loss
functions are invariant under reparameterizations and are Bayesian unbiased or
limits of Bayesian unbiased inferences. These inferences serve as
well-supported alternatives to MAP-based inferences."@2011
Zhengyan Lin@http://arxiv.org/abs/1104.3402v3@"Weak Convergence to Stochastic Integrals Driven by $α-$Stable
  Lévy Processes"@"We use the martingale convergence method to get the weak convergence theorem
on general functionals of partial sums of independent heavy-tailed random
variables. The limiting process is the stochastic integral driven by
$\alpha-$stable L\'evy process. Our method is very powerful to obtain the limit
behavior of heavy-tailed random variables."@2011
Hanchao Wang@http://arxiv.org/abs/1104.3402v3@"Weak Convergence to Stochastic Integrals Driven by $α-$Stable
  Lévy Processes"@"We use the martingale convergence method to get the weak convergence theorem
on general functionals of partial sums of independent heavy-tailed random
variables. The limiting process is the stochastic integral driven by
$\alpha-$stable L\'evy process. Our method is very powerful to obtain the limit
behavior of heavy-tailed random variables."@2011
Stephen E. Fienberg@http://arxiv.org/abs/1104.3618v2@Maximum likelihood estimation in log-linear models@"We study maximum likelihood estimation in log-linear models under conditional
Poisson sampling schemes. We derive necessary and sufficient conditions for
existence of the maximum likelihood estimator (MLE) of the model parameters and
investigate estimability of the natural and mean-value parameters under a
nonexistent MLE. Our conditions focus on the role of sampling zeros in the
observed table. We situate our results within the framework of extended
exponential families, and we exploit the geometric properties of log-linear
models. We propose algorithms for extended maximum likelihood estimation that
improve and correct the existing algorithms for log-linear model analysis."@2011
Alessandro Rinaldo@http://arxiv.org/abs/1104.3618v2@Maximum likelihood estimation in log-linear models@"We study maximum likelihood estimation in log-linear models under conditional
Poisson sampling schemes. We derive necessary and sufficient conditions for
existence of the maximum likelihood estimator (MLE) of the model parameters and
investigate estimability of the natural and mean-value parameters under a
nonexistent MLE. Our conditions focus on the role of sampling zeros in the
observed table. We situate our results within the framework of extended
exponential families, and we exploit the geometric properties of log-linear
models. We propose algorithms for extended maximum likelihood estimation that
improve and correct the existing algorithms for log-linear model analysis."@2011
Thibault Espinasse@http://arxiv.org/abs/1104.3664v3@"Gaussian stationary processes over graphs, general frame and maximum
  likelihood identification"@"In this paper, using spectral theory of Hilbertian operators, we study ARMA
Gaussian processes indexed by graphs. We extend Whittle maximum likelihood
estimation of the parameters for the corresponding spectral density and show
their asymptotic optimality."@2011
Fabrice Gamboa@http://arxiv.org/abs/1104.3664v3@"Gaussian stationary processes over graphs, general frame and maximum
  likelihood identification"@"In this paper, using spectral theory of Hilbertian operators, we study ARMA
Gaussian processes indexed by graphs. We extend Whittle maximum likelihood
estimation of the parameters for the corresponding spectral density and show
their asymptotic optimality."@2011
Jean-Michel Loubes@http://arxiv.org/abs/1104.3664v3@"Gaussian stationary processes over graphs, general frame and maximum
  likelihood identification"@"In this paper, using spectral theory of Hilbertian operators, we study ARMA
Gaussian processes indexed by graphs. We extend Whittle maximum likelihood
estimation of the parameters for the corresponding spectral density and show
their asymptotic optimality."@2011
Arnak Dalalyan@http://arxiv.org/abs/1104.3969v4@Sharp Oracle Inequalities for Aggregation of Affine Estimators@"We consider the problem of combining a (possibly uncountably infinite) set of
affine estimators in non-parametric regression model with heteroscedastic
Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a
PAC-Bayesian type inequality that leads to sharp oracle inequalities in
discrete but also in continuous settings. The framework is general enough to
cover the combinations of various procedures such as least square regression,
kernel ridge regression, shrinking estimators and many other estimators used in
the literature on statistical inverse problems. As a consequence, we show that
the proposed aggregate provides an adaptive estimator in the exact minimax
sense without neither discretizing the range of tuning parameters nor splitting
the set of observations. We also illustrate numerically the good performance
achieved by the exponentially weighted aggregate."@2011
Joseph Salmon@http://arxiv.org/abs/1104.3969v4@Sharp Oracle Inequalities for Aggregation of Affine Estimators@"We consider the problem of combining a (possibly uncountably infinite) set of
affine estimators in non-parametric regression model with heteroscedastic
Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a
PAC-Bayesian type inequality that leads to sharp oracle inequalities in
discrete but also in continuous settings. The framework is general enough to
cover the combinations of various procedures such as least square regression,
kernel ridge regression, shrinking estimators and many other estimators used in
the literature on statistical inverse problems. As a consequence, we show that
the proposed aggregate provides an adaptive estimator in the exact minimax
sense without neither discretizing the range of tuning parameters nor splitting
the set of observations. We also illustrate numerically the good performance
achieved by the exponentially weighted aggregate."@2011
Olivier Collier@http://arxiv.org/abs/1104.4210v6@Curve registration by nonparametric goodness-of-fit testing@"The problem of curve registration appears in many different areas of
applications ranging from neuroscience to road traffic modeling. In the present
work, we propose a nonparametric testing framework in which we develop a
generalized likelihood ratio test to perform curve registration. We first prove
that, under the null hypothesis, the resulting test statistic is asymptotically
distributed as a chi-squared random variable. This result, often referred to as
Wilks' phenomenon, provides a natural threshold for the test of a prescribed
asymptotic significance level and a natural measure of lack-of-fit in terms of
the $p$-value of the $\chi^2$-test. We also prove that the proposed test is
consistent, \textit{i.e.}, its power is asymptotically equal to $1$. Finite
sample properties of the proposed methodology are demonstrated by numerical
simulations. As an application, a new local descriptor for digital images is
introduced and an experimental evaluation of its discriminative power is
conducted."@2011
Arnak S. Dalalyan@http://arxiv.org/abs/1104.4210v6@Curve registration by nonparametric goodness-of-fit testing@"The problem of curve registration appears in many different areas of
applications ranging from neuroscience to road traffic modeling. In the present
work, we propose a nonparametric testing framework in which we develop a
generalized likelihood ratio test to perform curve registration. We first prove
that, under the null hypothesis, the resulting test statistic is asymptotically
distributed as a chi-squared random variable. This result, often referred to as
Wilks' phenomenon, provides a natural threshold for the test of a prescribed
asymptotic significance level and a natural measure of lack-of-fit in terms of
the $p$-value of the $\chi^2$-test. We also prove that the proposed test is
consistent, \textit{i.e.}, its power is asymptotically equal to $1$. Finite
sample properties of the proposed methodology are demonstrated by numerical
simulations. As an application, a new local descriptor for digital images is
introduced and an experimental evaluation of its discriminative power is
conducted."@2011
John H. J. Einmahl@http://arxiv.org/abs/1104.4220v1@"Central limit theorems for local empirical processes near boundaries of
  sets"@"We define the local empirical process, based on $n$ i.i.d. random vectors in
dimension $d$, in the neighborhood of the boundary of a fixed set. Under
natural conditions on the shrinking neighborhood, we show that, for these local
empirical processes, indexed by classes of sets that vary with $n$ and satisfy
certain conditions, an appropriately defined uniform central limit theorem
holds. The concept of differentiation of sets in measure is very convenient for
developing the results. Some examples and statistical applications are also
presented."@2011
Estáte V. Khmaladze@http://arxiv.org/abs/1104.4220v1@"Central limit theorems for local empirical processes near boundaries of
  sets"@"We define the local empirical process, based on $n$ i.i.d. random vectors in
dimension $d$, in the neighborhood of the boundary of a fixed set. Under
natural conditions on the shrinking neighborhood, we show that, for these local
empirical processes, indexed by classes of sets that vary with $n$ and satisfy
certain conditions, an appropriately defined uniform central limit theorem
holds. The concept of differentiation of sets in measure is very convenient for
developing the results. Some examples and statistical applications are also
presented."@2011
Dan Shen@http://arxiv.org/abs/1104.4289v1@Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts@"Sparse Principal Component Analysis (PCA) methods are efficient tools to
reduce the dimension (or the number of variables) of complex data. Sparse
principal components (PCs) are easier to interpret than conventional PCs,
because most loadings are zero. We study the asymptotic properties of these
sparse PC directions for scenarios with fixed sample size and increasing
dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously
studied spike covariance assumption, we show that Sparse PCA remains consistent
under the same large spike condition that was previously established for
conventional PCA. Under a broad range of small spike conditions, we find a
large set of sparsity assumptions where Sparse PCA is consistent, but PCA is
strongly inconsistent. The boundaries of the consistent region are clarified
using an oracle result."@2011
Haipeng Shen@http://arxiv.org/abs/1104.4289v1@Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts@"Sparse Principal Component Analysis (PCA) methods are efficient tools to
reduce the dimension (or the number of variables) of complex data. Sparse
principal components (PCs) are easier to interpret than conventional PCs,
because most loadings are zero. We study the asymptotic properties of these
sparse PC directions for scenarios with fixed sample size and increasing
dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously
studied spike covariance assumption, we show that Sparse PCA remains consistent
under the same large spike condition that was previously established for
conventional PCA. Under a broad range of small spike conditions, we find a
large set of sparsity assumptions where Sparse PCA is consistent, but PCA is
strongly inconsistent. The boundaries of the consistent region are clarified
using an oracle result."@2011
J. S. Marron@http://arxiv.org/abs/1104.4289v1@Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts@"Sparse Principal Component Analysis (PCA) methods are efficient tools to
reduce the dimension (or the number of variables) of complex data. Sparse
principal components (PCs) are easier to interpret than conventional PCs,
because most loadings are zero. We study the asymptotic properties of these
sparse PC directions for scenarios with fixed sample size and increasing
dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously
studied spike covariance assumption, we show that Sparse PCA remains consistent
under the same large spike condition that was previously established for
conventional PCA. Under a broad range of small spike conditions, we find a
large set of sparsity assumptions where Sparse PCA is consistent, but PCA is
strongly inconsistent. The boundaries of the consistent region are clarified
using an oracle result."@2011
G. Jogesh Babu@http://arxiv.org/abs/1104.4396v1@Limit theorems for functions of marginal quantiles@"Multivariate distributions are explored using the joint distributions of
marginal sample quantiles. Limit theory for the mean of a function of order
statistics is presented. The results include a multivariate central limit
theorem and a strong law of large numbers. A result similar to Bahadur's
representation of quantiles is established for the mean of a function of the
marginal quantiles. In particular, it is shown that
\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]
as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are
i.i.d. random variables for each $n$. This leads to the central limit theorem.
Weak convergence to a Gaussian process using equicontinuity of functions is
indicated. The results are established under very general conditions. These
conditions are shown to be satisfied in many commonly occurring situations."@2011
Zhidong Bai@http://arxiv.org/abs/1104.4396v1@Limit theorems for functions of marginal quantiles@"Multivariate distributions are explored using the joint distributions of
marginal sample quantiles. Limit theory for the mean of a function of order
statistics is presented. The results include a multivariate central limit
theorem and a strong law of large numbers. A result similar to Bahadur's
representation of quantiles is established for the mean of a function of the
marginal quantiles. In particular, it is shown that
\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]
as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are
i.i.d. random variables for each $n$. This leads to the central limit theorem.
Weak convergence to a Gaussian process using equicontinuity of functions is
indicated. The results are established under very general conditions. These
conditions are shown to be satisfied in many commonly occurring situations."@2011
Kwok Pui Choi@http://arxiv.org/abs/1104.4396v1@Limit theorems for functions of marginal quantiles@"Multivariate distributions are explored using the joint distributions of
marginal sample quantiles. Limit theory for the mean of a function of order
statistics is presented. The results include a multivariate central limit
theorem and a strong law of large numbers. A result similar to Bahadur's
representation of quantiles is established for the mean of a function of the
marginal quantiles. In particular, it is shown that
\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]
as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are
i.i.d. random variables for each $n$. This leads to the central limit theorem.
Weak convergence to a Gaussian process using equicontinuity of functions is
indicated. The results are established under very general conditions. These
conditions are shown to be satisfied in many commonly occurring situations."@2011
Vasudevan Mangalam@http://arxiv.org/abs/1104.4396v1@Limit theorems for functions of marginal quantiles@"Multivariate distributions are explored using the joint distributions of
marginal sample quantiles. Limit theory for the mean of a function of order
statistics is presented. The results include a multivariate central limit
theorem and a strong law of large numbers. A result similar to Bahadur's
representation of quantiles is established for the mean of a function of the
marginal quantiles. In particular, it is shown that
\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]
as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are
i.i.d. random variables for each $n$. This leads to the central limit theorem.
Weak convergence to a Gaussian process using equicontinuity of functions is
indicated. The results are established under very general conditions. These
conditions are shown to be satisfied in many commonly occurring situations."@2011
Kyusang Yu@http://arxiv.org/abs/1104.4410v1@"Semi-parametric regression: Efficiency gains from modeling the
  nonparametric part"@"It is widely admitted that structured nonparametric modeling that circumvents
the curse of dimensionality is important in nonparametric estimation. In this
paper we show that the same holds for semi-parametric estimation. We argue that
estimation of the parametric component of a semi-parametric model can be
improved essentially when more structure is put into the nonparametric part of
the model. We illustrate this for the partially linear model, and investigate
efficiency gains when the nonparametric part of the model has an additive
structure. We present the semi-parametric Fisher information bound for
estimating the parametric part of the partially linear additive model and
provide semi-parametric efficient estimators for which we use a smooth
backfitting technique to deal with the additive nonparametric part. We also
present the finite sample performances of the proposed estimators and analyze
Boston housing data as an illustration."@2011
Enno Mammen@http://arxiv.org/abs/1104.4410v1@"Semi-parametric regression: Efficiency gains from modeling the
  nonparametric part"@"It is widely admitted that structured nonparametric modeling that circumvents
the curse of dimensionality is important in nonparametric estimation. In this
paper we show that the same holds for semi-parametric estimation. We argue that
estimation of the parametric component of a semi-parametric model can be
improved essentially when more structure is put into the nonparametric part of
the model. We illustrate this for the partially linear model, and investigate
efficiency gains when the nonparametric part of the model has an additive
structure. We present the semi-parametric Fisher information bound for
estimating the parametric part of the partially linear additive model and
provide semi-parametric efficient estimators for which we use a smooth
backfitting technique to deal with the additive nonparametric part. We also
present the finite sample performances of the proposed estimators and analyze
Boston housing data as an illustration."@2011
Byeong U. Park@http://arxiv.org/abs/1104.4410v1@"Semi-parametric regression: Efficiency gains from modeling the
  nonparametric part"@"It is widely admitted that structured nonparametric modeling that circumvents
the curse of dimensionality is important in nonparametric estimation. In this
paper we show that the same holds for semi-parametric estimation. We argue that
estimation of the parametric component of a semi-parametric model can be
improved essentially when more structure is put into the nonparametric part of
the model. We illustrate this for the partially linear model, and investigate
efficiency gains when the nonparametric part of the model has an additive
structure. We present the semi-parametric Fisher information bound for
estimating the parametric part of the partially linear additive model and
provide semi-parametric efficient estimators for which we use a smooth
backfitting technique to deal with the additive nonparametric part. We also
present the finite sample performances of the proposed estimators and analyze
Boston housing data as an illustration."@2011
Rama Cont@http://arxiv.org/abs/1104.4429v1@Nonparametric tests for pathwise properties of semimartingales@"We propose two nonparametric tests for investigating the pathwise properties
of a signal modeled as the sum of a L\'{e}vy process and a Brownian
semimartingale. Using a nonparametric threshold estimator for the continuous
component of the quadratic variation, we design a test for the presence of a
continuous martingale component in the process and a test for establishing
whether the jumps have finite or infinite variation, based on observations on a
discrete-time grid. We evaluate the performance of our tests using simulations
of various stochastic models and use the tests to investigate the fine
structure of the DM/USD exchange rate fluctuations and SPX futures prices. In
both cases, our tests reveal the presence of a non-zero Brownian component and
a finite variation jump component."@2011
Cecilia Mancini@http://arxiv.org/abs/1104.4429v1@Nonparametric tests for pathwise properties of semimartingales@"We propose two nonparametric tests for investigating the pathwise properties
of a signal modeled as the sum of a L\'{e}vy process and a Brownian
semimartingale. Using a nonparametric threshold estimator for the continuous
component of the quadratic variation, we design a test for the presence of a
continuous martingale component in the process and a test for establishing
whether the jumps have finite or infinite variation, based on observations on a
discrete-time grid. We evaluate the performance of our tests using simulations
of various stochastic models and use the tests to investigate the fine
structure of the DM/USD exchange rate fluctuations and SPX futures prices. In
both cases, our tests reveal the presence of a non-zero Brownian component and
a finite variation jump component."@2011
Jean-Marc Bardet@http://arxiv.org/abs/1104.4732v2@"Moment bounds and central limit theorems for Gaussian subordinated
  arrays"@"A general moment bound for sums of products of Gaussian vector's functions
extending the moment bound in Taqqu (1977, Lemma 4.5) is established. A general
central limit theorem for triangular arrays of nonlinear functionals of
multidimensional non-stationary Gaussian sequences is proved. This theorem
extends the previous results of Breuer and Major (1981), Arcones (1994) and
others. A Berry-Esseen-type bound in the above-mentioned central limit theorem
is derived following Nourdin, Peccati and Podolskij (2011). Two applications of
the above results are discussed. The first one refers to the asymptotic
behavior of a roughness statistic for continuous-time Gaussian processes and
the second one is a central limit theorem satisfied by long memory locally
stationary process."@2011
Donatas Surgailis@http://arxiv.org/abs/1104.4732v2@"Moment bounds and central limit theorems for Gaussian subordinated
  arrays"@"A general moment bound for sums of products of Gaussian vector's functions
extending the moment bound in Taqqu (1977, Lemma 4.5) is established. A general
central limit theorem for triangular arrays of nonlinear functionals of
multidimensional non-stationary Gaussian sequences is proved. This theorem
extends the previous results of Breuer and Major (1981), Arcones (1994) and
others. A Berry-Esseen-type bound in the above-mentioned central limit theorem
is derived following Nourdin, Peccati and Podolskij (2011). Two applications of
the above results are discussed. The first one refers to the asymptotic
behavior of a roughness statistic for continuous-time Gaussian processes and
the second one is a central limit theorem satisfied by long memory locally
stationary process."@2011
T. Yoshida@http://arxiv.org/abs/1104.5136v1@Asymptotics for penalized additive B-spline regression@"This paper is concerned with asymptotic theory for penalized spline estimator
in bivariate additive model. The focus of this paper is put upon the penalized
spline estimator obtained by the backfitting algorithm. The convergence of the
algorithm as well as the uniqueness of its solution are shown. The asymptotic
bias and variance of penalized spline estimator are derived by an efficient use
of the asymptotic results for the penalized spline estimator in marginal
univariate model. Asymptotic normality of estimator is also developed, by which
an approximate confidence interval can be obtained. Some numerical experiments
confirming theoretical results are provided."@2011
K. Naito@http://arxiv.org/abs/1104.5136v1@Asymptotics for penalized additive B-spline regression@"This paper is concerned with asymptotic theory for penalized spline estimator
in bivariate additive model. The focus of this paper is put upon the penalized
spline estimator obtained by the backfitting algorithm. The convergence of the
algorithm as well as the uniqueness of its solution are shown. The asymptotic
bias and variance of penalized spline estimator are derived by an efficient use
of the asymptotic results for the penalized spline estimator in marginal
univariate model. Asymptotic normality of estimator is also developed, by which
an approximate confidence interval can be obtained. Some numerical experiments
confirming theoretical results are provided."@2011
Lajos Horváth@http://arxiv.org/abs/1105.0014v2@A test of significance in functional quadratic regression@"We consider a quadratic functional regression model in which a scalar
response depends on a functional predictor; the common functional linear model
is a special case. We wish to test the significance of the nonlinear term in
the model. We develop a testing method which is based on projecting the
observations onto a suitably chosen finite dimensional space using functional
principal component analysis. The asymptotic behavior of our testing procedure
is established. A simulation study shows that the testing procedure has good
size and power with finite sample sizes. We then apply our test to a data set
provided by Tecator, which consists of near-infrared absorbance spectra and fat
content of meat."@2011
Ron Reeder@http://arxiv.org/abs/1105.0014v2@A test of significance in functional quadratic regression@"We consider a quadratic functional regression model in which a scalar
response depends on a functional predictor; the common functional linear model
is a special case. We wish to test the significance of the nonlinear term in
the model. We develop a testing method which is based on projecting the
observations onto a suitably chosen finite dimensional space using functional
principal component analysis. The asymptotic behavior of our testing procedure
is established. A simulation study shows that the testing procedure has good
size and power with finite sample sizes. We then apply our test to a data set
provided by Tecator, which consists of near-infrared absorbance spectra and fat
content of meat."@2011
Lajos Horvath@http://arxiv.org/abs/1105.0015v2@Detecting changes in functional linear models@"We observe two sequences of curve which are connected via an integral
operator. Our model includes linear models as well as autoregressive models in
Hilbert spaces. We wish to test the null hypothesis that the operator did not
change during the observation period. Our method is based on projecting the
observations onto a suitably chosen finite dimensional space. The testing
procedure is based on functionals of the weighted residuals of the projections.
Since the quadratic form is based on estimating the long-term covariance matrix
of the residuals, we also provide some results on Bartlett-type estimators."@2011
Ron Reeder@http://arxiv.org/abs/1105.0015v2@Detecting changes in functional linear models@"We observe two sequences of curve which are connected via an integral
operator. Our model includes linear models as well as autoregressive models in
Hilbert spaces. We wish to test the null hypothesis that the operator did not
change during the observation period. Our method is based on projecting the
observations onto a suitably chosen finite dimensional space. The testing
procedure is based on functionals of the weighted residuals of the projections.
Since the quadratic form is based on estimating the long-term covariance matrix
of the residuals, we also provide some results on Bartlett-type estimators."@2011
Lajos Horvath@http://arxiv.org/abs/1105.0019v1@"Estimation of the mean of functional time series and a two sample
  problem"@"This paper is concerned with inference based on the mean function of a
functional time series, which is defined as a collection of curves obtained by
splitting a continuous time record, e.g. into daily or annual curves. We
develop a normal approximation for the functional sample mean, and then focus
on the estimation of the asymptotic variance kernel. Using these results, we
develop and asymptotically justify a testing procedure for the equality of
means in two functional samples exhibiting temporal dependence. Evaluated by
means of a simulations study and application to real data sets, this two sample
procedure enjoys good size and power in finite samples. We provide the details
of its numerical implementation."@2011
Piotr Kokoszka@http://arxiv.org/abs/1105.0019v1@"Estimation of the mean of functional time series and a two sample
  problem"@"This paper is concerned with inference based on the mean function of a
functional time series, which is defined as a collection of curves obtained by
splitting a continuous time record, e.g. into daily or annual curves. We
develop a normal approximation for the functional sample mean, and then focus
on the estimation of the asymptotic variance kernel. Using these results, we
develop and asymptotically justify a testing procedure for the equality of
means in two functional samples exhibiting temporal dependence. Evaluated by
means of a simulations study and application to real data sets, this two sample
procedure enjoys good size and power in finite samples. We provide the details
of its numerical implementation."@2011
Ron Reeder@http://arxiv.org/abs/1105.0019v1@"Estimation of the mean of functional time series and a two sample
  problem"@"This paper is concerned with inference based on the mean function of a
functional time series, which is defined as a collection of curves obtained by
splitting a continuous time record, e.g. into daily or annual curves. We
develop a normal approximation for the functional sample mean, and then focus
on the estimation of the asymptotic variance kernel. Using these results, we
develop and asymptotically justify a testing procedure for the equality of
means in two functional samples exhibiting temporal dependence. Evaluated by
means of a simulations study and application to real data sets, this two sample
procedure enjoys good size and power in finite samples. We provide the details
of its numerical implementation."@2011
Fabrice Rossi@http://arxiv.org/abs/1105.0204v1@Consistency of functional learning methods based on derivatives@"In some real world applications, such as spectrometry, functional models
achieve better predictive performances if they work on the derivatives of order
m of their inputs rather than on the original functions. As a consequence, the
use of derivatives is a common practice in Functional Data Analysis, despite a
lack of theoretical guarantees on the asymptotically achievable performances of
a derivative based model. In this paper, we show that a smoothing spline
approach can be used to preprocess multivariate observations obtained by
sampling functions on a discrete and finite sampling grid in a way that leads
to a consistent scheme on the original infinite dimensional functional problem.
This work extends (Mas and Pumo, 2009) to nonparametric approaches and
incomplete knowledge. To be more precise, the paper tackles two difficulties in
a nonparametric framework: the information loss due to the use of the
derivatives instead of the original functions and the information loss due to
the fact that the functions are observed through a discrete sampling and are
thus also unperfectly known: the use of a smoothing spline based approach
solves these two problems. Finally, the proposed approach is tested on two real
world datasets and the approach is experimentaly proven to be a good solution
in the case of noisy functional predictors."@2011
Nathalie Villa-Vialaneix@http://arxiv.org/abs/1105.0204v1@Consistency of functional learning methods based on derivatives@"In some real world applications, such as spectrometry, functional models
achieve better predictive performances if they work on the derivatives of order
m of their inputs rather than on the original functions. As a consequence, the
use of derivatives is a common practice in Functional Data Analysis, despite a
lack of theoretical guarantees on the asymptotically achievable performances of
a derivative based model. In this paper, we show that a smoothing spline
approach can be used to preprocess multivariate observations obtained by
sampling functions on a discrete and finite sampling grid in a way that leads
to a consistent scheme on the original infinite dimensional functional problem.
This work extends (Mas and Pumo, 2009) to nonparametric approaches and
incomplete knowledge. To be more precise, the paper tackles two difficulties in
a nonparametric framework: the information loss due to the use of the
derivatives instead of the original functions and the information loss due to
the fact that the functions are observed through a discrete sampling and are
thus also unperfectly known: the use of a smoothing spline based approach
solves these two problems. Finally, the proposed approach is tested on two real
world datasets and the approach is experimentaly proven to be a good solution
in the case of noisy functional predictors."@2011
Denys Pommeret@http://arxiv.org/abs/1105.0205v1@Nonparametric test for detecting change in distribution with panel data@"This paper considers the problem of comparing two processes with panel data.
A nonparametric test is proposed for detecting a monotone change in the link
between the two process distributions. The test statistic is of CUSUM type,
based on the empirical distribution functions. The asymptotic distribution of
the proposed statistic is derived and its finite sample property is examined by
bootstrap procedures through Monte Carlo simulations."@2011
Mohamed Boutahar@http://arxiv.org/abs/1105.0205v1@Nonparametric test for detecting change in distribution with panel data@"This paper considers the problem of comparing two processes with panel data.
A nonparametric test is proposed for detecting a monotone change in the link
between the two process distributions. The test statistic is of CUSUM type,
based on the empirical distribution functions. The asymptotic distribution of
the proposed statistic is derived and its finite sample property is examined by
bootstrap procedures through Monte Carlo simulations."@2011
Badih Ghattas@http://arxiv.org/abs/1105.0205v1@Nonparametric test for detecting change in distribution with panel data@"This paper considers the problem of comparing two processes with panel data.
A nonparametric test is proposed for detecting a monotone change in the link
between the two process distributions. The test statistic is of CUSUM type,
based on the empirical distribution functions. The asymptotic distribution of
the proposed statistic is derived and its finite sample property is examined by
bootstrap procedures through Monte Carlo simulations."@2011
Siegfried Hormann@http://arxiv.org/abs/1105.0343v1@A Functional Version of the ARCH Model@"Improvements in data acquisition and processing techniques have lead to an
almost continuous flow of information for financial data. High resolution tick
data are available and can be quite conveniently described by a continuous time
process. It is therefore natural to ask for possible extensions of financial
time series models to a functional setup. In this paper we propose a functional
version of the popular ARCH model. We will establish conditions for the
existence of a strictly stationary solution, derive weak dependence and moment
conditions, show consistency of the estimators and perform a small empirical
study demonstrating how our model matches with real data."@2011
Lajos Horvath@http://arxiv.org/abs/1105.0343v1@A Functional Version of the ARCH Model@"Improvements in data acquisition and processing techniques have lead to an
almost continuous flow of information for financial data. High resolution tick
data are available and can be quite conveniently described by a continuous time
process. It is therefore natural to ask for possible extensions of financial
time series models to a functional setup. In this paper we propose a functional
version of the popular ARCH model. We will establish conditions for the
existence of a strictly stationary solution, derive weak dependence and moment
conditions, show consistency of the estimators and perform a small empirical
study demonstrating how our model matches with real data."@2011
Ron Reeder@http://arxiv.org/abs/1105.0343v1@A Functional Version of the ARCH Model@"Improvements in data acquisition and processing techniques have lead to an
almost continuous flow of information for financial data. High resolution tick
data are available and can be quite conveniently described by a continuous time
process. It is therefore natural to ask for possible extensions of financial
time series models to a functional setup. In this paper we propose a functional
version of the popular ARCH model. We will establish conditions for the
existence of a strictly stationary solution, derive weak dependence and moment
conditions, show consistency of the estimators and perform a small empirical
study demonstrating how our model matches with real data."@2011
Angelika Franke@http://arxiv.org/abs/1105.0852v2@"The Asymptotic Covariance Matrix of the Odds Ratio Parameter Estimator
  in Semiparametric Log-bilinear Odds Ratio Models"@"The association between two random variables is often of primary interest in
statistical research. In this paper semiparametric models for the association
between random vectors X and Y are considered which leave the marginal
distributions arbitrary. Given that the odds ratio function comprises the whole
information about the association the focus is on bilinear log-odds ratio
models and in particular on the odds ratio parameter vector {\theta}. The
covariance structure of the maximum likelihood estimator {\theta}^ of {\theta}
is of major importance for asymptotic inference. To this end different
representations of the estimated covariance matrix are derived for conditional
and unconditional sampling schemes and different asymptotic approaches
depending on whether X and/or Y has finite or arbitrary support. The main
result is the invariance of the estimated asymptotic covariance matrix of
{\theta}^ with respect to all above approaches. As applications we compute the
asymptotic power for tests of linear hypotheses about {\theta} - with emphasis
to logistic and linear regression models - which allows to determine the
necessary sample size to achieve a wanted power."@2011
Gerhard Osius@http://arxiv.org/abs/1105.0852v2@"The Asymptotic Covariance Matrix of the Odds Ratio Parameter Estimator
  in Semiparametric Log-bilinear Odds Ratio Models"@"The association between two random variables is often of primary interest in
statistical research. In this paper semiparametric models for the association
between random vectors X and Y are considered which leave the marginal
distributions arbitrary. Given that the odds ratio function comprises the whole
information about the association the focus is on bilinear log-odds ratio
models and in particular on the odds ratio parameter vector {\theta}. The
covariance structure of the maximum likelihood estimator {\theta}^ of {\theta}
is of major importance for asymptotic inference. To this end different
representations of the estimated covariance matrix are derived for conditional
and unconditional sampling schemes and different asymptotic approaches
depending on whether X and/or Y has finite or arbitrary support. The main
result is the invariance of the estimated asymptotic covariance matrix of
{\theta}^ with respect to all above approaches. As applications we compute the
asymptotic power for tests of linear hypotheses about {\theta} - with emphasis
to logistic and linear regression models - which allows to determine the
necessary sample size to achieve a wanted power."@2011
Masoumeh Dashti@http://arxiv.org/abs/1105.0889v2@Besov priors for Bayesian inverse problems@"We consider the inverse problem of estimating a function $u$ from noisy,
possibly nonlinear, observations. We adopt a Bayesian approach to the problem.
This approach has a long history for inversion, dating back to 1970, and has,
over the last decade, gained importance as a practical tool. However most of
the existing theory has been developed for Gaussian prior measures. Recently
Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct
Besov prior measures, based on wavelet expansions with random coefficients, and
used these prior measures to study linear inverse problems. In this paper we
build on this development of Besov priors to include the case of nonlinear
measurements. In doing so a key technical tool, established here, is a
Fernique-like theorem for Besov measures. This theorem enables us to identify
appropriate conditions on the forward solution operator which, when matched to
properties of the prior Besov measure, imply the well-definedness and
well-posedness of the posterior measure. We then consider the application of
these results to the inverse problem of finding the diffusion coefficient of an
elliptic partial differential equation, given noisy measurements of its
solution."@2011
Stephen Harris@http://arxiv.org/abs/1105.0889v2@Besov priors for Bayesian inverse problems@"We consider the inverse problem of estimating a function $u$ from noisy,
possibly nonlinear, observations. We adopt a Bayesian approach to the problem.
This approach has a long history for inversion, dating back to 1970, and has,
over the last decade, gained importance as a practical tool. However most of
the existing theory has been developed for Gaussian prior measures. Recently
Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct
Besov prior measures, based on wavelet expansions with random coefficients, and
used these prior measures to study linear inverse problems. In this paper we
build on this development of Besov priors to include the case of nonlinear
measurements. In doing so a key technical tool, established here, is a
Fernique-like theorem for Besov measures. This theorem enables us to identify
appropriate conditions on the forward solution operator which, when matched to
properties of the prior Besov measure, imply the well-definedness and
well-posedness of the posterior measure. We then consider the application of
these results to the inverse problem of finding the diffusion coefficient of an
elliptic partial differential equation, given noisy measurements of its
solution."@2011
Andrew Stuart@http://arxiv.org/abs/1105.0889v2@Besov priors for Bayesian inverse problems@"We consider the inverse problem of estimating a function $u$ from noisy,
possibly nonlinear, observations. We adopt a Bayesian approach to the problem.
This approach has a long history for inversion, dating back to 1970, and has,
over the last decade, gained importance as a practical tool. However most of
the existing theory has been developed for Gaussian prior measures. Recently
Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct
Besov prior measures, based on wavelet expansions with random coefficients, and
used these prior measures to study linear inverse problems. In this paper we
build on this development of Besov priors to include the case of nonlinear
measurements. In doing so a key technical tool, established here, is a
Fernique-like theorem for Besov measures. This theorem enables us to identify
appropriate conditions on the forward solution operator which, when matched to
properties of the prior Besov measure, imply the well-definedness and
well-posedness of the posterior measure. We then consider the application of
these results to the inverse problem of finding the diffusion coefficient of an
elliptic partial differential equation, given noisy measurements of its
solution."@2011
Marianne Clausel@http://arxiv.org/abs/1105.1011v3@"Wavelet estimation of the long memory parameter for Hermite polynomial
  of Gaussian processes"@"We consider stationary processes with long memory which are non-Gaussian and
represented as Hermite polynomials of a Gaussian process. We focus on the
corresponding wavelet coefficients and study the asymptotic behavior of the sum
of their squares since this sum is often used for estimating the long-memory
parameter. We show that the limit is not Gaussian but can be expressed using
the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order
2. This happens even if the original process is defined through a Hermite
polynomial of order higher than 2."@2011
François Roueff@http://arxiv.org/abs/1105.1011v3@"Wavelet estimation of the long memory parameter for Hermite polynomial
  of Gaussian processes"@"We consider stationary processes with long memory which are non-Gaussian and
represented as Hermite polynomials of a Gaussian process. We focus on the
corresponding wavelet coefficients and study the asymptotic behavior of the sum
of their squares since this sum is often used for estimating the long-memory
parameter. We show that the limit is not Gaussian but can be expressed using
the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order
2. This happens even if the original process is defined through a Hermite
polynomial of order higher than 2."@2011
Murad S. Taqqu@http://arxiv.org/abs/1105.1011v3@"Wavelet estimation of the long memory parameter for Hermite polynomial
  of Gaussian processes"@"We consider stationary processes with long memory which are non-Gaussian and
represented as Hermite polynomials of a Gaussian process. We focus on the
corresponding wavelet coefficients and study the asymptotic behavior of the sum
of their squares since this sum is often used for estimating the long-memory
parameter. We show that the limit is not Gaussian but can be expressed using
the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order
2. This happens even if the original process is defined through a Hermite
polynomial of order higher than 2."@2011
Ciprian A. Tudor@http://arxiv.org/abs/1105.1011v3@"Wavelet estimation of the long memory parameter for Hermite polynomial
  of Gaussian processes"@"We consider stationary processes with long memory which are non-Gaussian and
represented as Hermite polynomials of a Gaussian process. We focus on the
corresponding wavelet coefficients and study the asymptotic behavior of the sum
of their squares since this sum is often used for estimating the long-memory
parameter. We show that the limit is not Gaussian but can be expressed using
the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order
2. This happens even if the original process is defined through a Hermite
polynomial of order higher than 2."@2011
S. H. Alizadeh@http://arxiv.org/abs/1105.1212v2@Hidden Markov Mixture Autoregressive Models: Stability and Moments@"This paper introduces a new parsimonious structure for mixture of
autoregressive models. the weighting coefficients are determined through latent
random variables, following a hidden Markov model. We propose a dynamic
programming algorithm for the application of forecasting. We also derive the
limiting behavior of unconditional first moment of the process and an
appropriate upper bound for the limiting value of the variance. This can be
considered as long run behavior of the process. Finally we show convergence and
stability of the second moment. Further, we illustrate the efficacy of the
proposed model by simulation and forecasting."@2011
S. Rezakhah@http://arxiv.org/abs/1105.1212v2@Hidden Markov Mixture Autoregressive Models: Stability and Moments@"This paper introduces a new parsimonious structure for mixture of
autoregressive models. the weighting coefficients are determined through latent
random variables, following a hidden Markov model. We propose a dynamic
programming algorithm for the application of forecasting. We also derive the
limiting behavior of unconditional first moment of the process and an
appropriate upper bound for the limiting value of the variance. This can be
considered as long run behavior of the process. Finally we show convergence and
stability of the second moment. Further, we illustrate the efficacy of the
proposed model by simulation and forecasting."@2011
Guang Cheng@http://arxiv.org/abs/1105.1304v1@Semiparametric Additive Transformation Model under Current Status Data@"We consider the efficient estimation of the semiparametric additive
transformation model with current status data. A wide range of survival models
and econometric models can be incorporated into this general transformation
framework. We apply the B-spline approach to simultaneously estimate the linear
regression vector, the nondecreasing transformation function, and a set of
nonparametric regression functions. We show that the parametric estimate is
semiparametric efficient in the presence of multiple nonparametric nuisance
functions. An explicit consistent B-spline estimate of the asymptotic variance
is also provided. All nonparametric estimates are smooth, and shown to be
uniformly consistent and have faster than cubic rate of convergence.
Interestingly, we observe the convergence rate interfere phenomenon, i.e., the
convergence rates of B-spline estimators are all slowed down to equal the
slowest one. The constrained optimization is not required in our
implementation. Numerical results are used to illustrate the finite sample
performance of the proposed estimators."@2011
Xiao Wang@http://arxiv.org/abs/1105.1304v1@Semiparametric Additive Transformation Model under Current Status Data@"We consider the efficient estimation of the semiparametric additive
transformation model with current status data. A wide range of survival models
and econometric models can be incorporated into this general transformation
framework. We apply the B-spline approach to simultaneously estimate the linear
regression vector, the nondecreasing transformation function, and a set of
nonparametric regression functions. We show that the parametric estimate is
semiparametric efficient in the presence of multiple nonparametric nuisance
functions. An explicit consistent B-spline estimate of the asymptotic variance
is also provided. All nonparametric estimates are smooth, and shown to be
uniformly consistent and have faster than cubic rate of convergence.
Interestingly, we observe the convergence rate interfere phenomenon, i.e., the
convergence rates of B-spline estimators are all slowed down to equal the
slowest one. The constrained optimization is not required in our
implementation. Numerical results are used to illustrate the finite sample
performance of the proposed estimators."@2011
Jérôme Dedecker@http://arxiv.org/abs/1105.1310v2@Estimation in autoregressive model with measurement error@"Consider an autoregressive model with measurement error: we observe
$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation
$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is
known up to a finite dimensional parameter $\theta^0$. The distributions of
$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is
completely known. We want to estimate the parameter $\theta^0$ by using the
observations $Z_0,..,Z_n$. We propose an estimation procedure based on a
modified least square criterion involving a weight function $w$, to be suitably
chosen. We give upper bounds for the risk of the estimator, which depend on the
smoothness of the errors density $f_\epsilon$ and on the smoothness properties
of $w f_\theta$."@2011
Adeline Samson@http://arxiv.org/abs/1105.1310v2@Estimation in autoregressive model with measurement error@"Consider an autoregressive model with measurement error: we observe
$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation
$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is
known up to a finite dimensional parameter $\theta^0$. The distributions of
$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is
completely known. We want to estimate the parameter $\theta^0$ by using the
observations $Z_0,..,Z_n$. We propose an estimation procedure based on a
modified least square criterion involving a weight function $w$, to be suitably
chosen. We give upper bounds for the risk of the estimator, which depend on the
smoothness of the errors density $f_\epsilon$ and on the smoothness properties
of $w f_\theta$."@2011
Marie-Luce Taupin@http://arxiv.org/abs/1105.1310v2@Estimation in autoregressive model with measurement error@"Consider an autoregressive model with measurement error: we observe
$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation
$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is
known up to a finite dimensional parameter $\theta^0$. The distributions of
$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is
completely known. We want to estimate the parameter $\theta^0$ by using the
observations $Z_0,..,Z_n$. We propose an estimation procedure based on a
modified least square criterion involving a weight function $w$, to be suitably
chosen. We give upper bounds for the risk of the estimator, which depend on the
smoothness of the errors density $f_\epsilon$ and on the smoothness properties
of $w f_\theta$."@2011
Emilio Seijo@http://arxiv.org/abs/1105.1320v1@A continuous mapping theorem for the smallest argmax functional@"This paper introduces a version of the argmax continuous mapping theorem that
applies to M-estimation problems in which the objective functions converge to a
limiting process with multiple maximizers. The concept of the smallest
maximizer of a function in the d-dimensional Skorohod space is introduced and
its main properties are studied. The resulting continuous mapping theorem is
applied to three problems arising in change-point regression analysis. Some of
the results proved in connection to the d-dimensional Skorohod space are also
of independent interest."@2011
Bodhisattva Sen@http://arxiv.org/abs/1105.1320v1@A continuous mapping theorem for the smallest argmax functional@"This paper introduces a version of the argmax continuous mapping theorem that
applies to M-estimation problems in which the objective functions converge to a
limiting process with multiple maximizers. The concept of the smallest
maximizer of a function in the d-dimensional Skorohod space is introduced and
its main properties are studied. The resulting continuous mapping theorem is
applied to three problems arising in change-point regression analysis. Some of
the results proved in connection to the d-dimensional Skorohod space are also
of independent interest."@2011
Noureddine El Karoui@http://arxiv.org/abs/1105.1404v1@"Geometric sensitivity of random matrix results: consequences for
  shrinkage estimators of covariance and related statistical methods"@"Shrinkage estimators of covariance are an important tool in modern applied
and theoretical statistics. They play a key role in regularized estimation
problems, such as ridge regression (aka Tykhonov regularization), regularized
discriminant analysis and a variety of optimization problems.
  In this paper, we bring to bear the tools of random matrix theory to
understand their behavior, and in particular, that of quadratic forms involving
inverses of those estimators, which are important in practice.
  We use very mild assumptions compared to the usual assumptions made in random
matrix theory, requiring only mild conditions on the moments of linear and
quadratic forms in our random vectors. In particular, we show that our results
apply for instance to log-normal data, which are of interest in financial
applications.
  Our study highlights the relative sensitivity of random matrix results (and
their practical consequences) to geometric assumptions which are often
implicitly made by random matrix theorists and may not be relevant in data
analytic practice."@2011
Holger Koesters@http://arxiv.org/abs/1105.1404v1@"Geometric sensitivity of random matrix results: consequences for
  shrinkage estimators of covariance and related statistical methods"@"Shrinkage estimators of covariance are an important tool in modern applied
and theoretical statistics. They play a key role in regularized estimation
problems, such as ridge regression (aka Tykhonov regularization), regularized
discriminant analysis and a variety of optimization problems.
  In this paper, we bring to bear the tools of random matrix theory to
understand their behavior, and in particular, that of quadratic forms involving
inverses of those estimators, which are important in practice.
  We use very mild assumptions compared to the usual assumptions made in random
matrix theory, requiring only mild conditions on the moments of linear and
quadratic forms in our random vectors. In particular, we show that our results
apply for instance to log-normal data, which are of interest in financial
applications.
  Our study highlights the relative sensitivity of random matrix results (and
their practical consequences) to geometric assumptions which are often
implicitly made by random matrix theorists and may not be relevant in data
analytic practice."@2011
S. Chen@http://arxiv.org/abs/1105.1896v1@Consistency of Markov chain quasi-Monte Carlo on continuous state spaces@"The random numbers driving Markov chain Monte Carlo (MCMC) simulation are
usually modeled as independent U(0,1) random variables. Tribble [Markov chain
Monte Carlo algorithms using completely uniformly distributed driving sequences
(2007) Stanford Univ.] reports substantial improvements when those random
numbers are replaced by carefully balanced inputs from completely uniformly
distributed sequences. The previous theoretical justification for using
anything other than i.i.d. U(0,1) points shows consistency for estimated means,
but only applies for discrete stationary distributions. We extend those results
to some MCMC algorithms for continuous stationary distributions. The main
motivation is the search for quasi-Monte Carlo versions of MCMC. As a side
benefit, the results also establish consistency for the usual method of using
pseudo-random numbers in place of random ones."@2011
J. Dick@http://arxiv.org/abs/1105.1896v1@Consistency of Markov chain quasi-Monte Carlo on continuous state spaces@"The random numbers driving Markov chain Monte Carlo (MCMC) simulation are
usually modeled as independent U(0,1) random variables. Tribble [Markov chain
Monte Carlo algorithms using completely uniformly distributed driving sequences
(2007) Stanford Univ.] reports substantial improvements when those random
numbers are replaced by carefully balanced inputs from completely uniformly
distributed sequences. The previous theoretical justification for using
anything other than i.i.d. U(0,1) points shows consistency for estimated means,
but only applies for discrete stationary distributions. We extend those results
to some MCMC algorithms for continuous stationary distributions. The main
motivation is the search for quasi-Monte Carlo versions of MCMC. As a side
benefit, the results also establish consistency for the usual method of using
pseudo-random numbers in place of random ones."@2011
A. B. Owen@http://arxiv.org/abs/1105.1896v1@Consistency of Markov chain quasi-Monte Carlo on continuous state spaces@"The random numbers driving Markov chain Monte Carlo (MCMC) simulation are
usually modeled as independent U(0,1) random variables. Tribble [Markov chain
Monte Carlo algorithms using completely uniformly distributed driving sequences
(2007) Stanford Univ.] reports substantial improvements when those random
numbers are replaced by carefully balanced inputs from completely uniformly
distributed sequences. The previous theoretical justification for using
anything other than i.i.d. U(0,1) points shows consistency for estimated means,
but only applies for discrete stationary distributions. We extend those results
to some MCMC algorithms for continuous stationary distributions. The main
motivation is the search for quasi-Monte Carlo versions of MCMC. As a side
benefit, the results also establish consistency for the usual method of using
pseudo-random numbers in place of random ones."@2011
Fabienne Comte@http://arxiv.org/abs/1105.2424v1@"Estimation for Lévy processes from high frequency data within a long
  time interval"@"In this paper, we study nonparametric estimation of the L\'{e}vy density for
L\'{e}vy processes, with and without Brownian component. For this, we consider
$n$ discrete time observations with step $\Delta$. The asymptotic framework is:
$n$ tends to infinity, $\Delta=\Delta_n$ tends to zero while $n\Delta_n$ tends
to infinity. We use a Fourier approach to construct an adaptive nonparametric
estimator of the L\'{e}vy density and to provide a bound for the global
${\mathbb{L}}^2$-risk. Estimators of the drift and of the variance of the
Gaussian component are also studied. We discuss rates of convergence and give
examples and simulation results for processes fitting in our framework."@2011
Valentine Genon-Catalot@http://arxiv.org/abs/1105.2424v1@"Estimation for Lévy processes from high frequency data within a long
  time interval"@"In this paper, we study nonparametric estimation of the L\'{e}vy density for
L\'{e}vy processes, with and without Brownian component. For this, we consider
$n$ discrete time observations with step $\Delta$. The asymptotic framework is:
$n$ tends to infinity, $\Delta=\Delta_n$ tends to zero while $n\Delta_n$ tends
to infinity. We use a Fourier approach to construct an adaptive nonparametric
estimator of the L\'{e}vy density and to provide a bound for the global
${\mathbb{L}}^2$-risk. Estimators of the drift and of the variance of the
Gaussian component are also studied. We discuss rates of convergence and give
examples and simulation results for processes fitting in our framework."@2011
Eric Gautier@http://arxiv.org/abs/1105.2454v5@High-dimensional instrumental variables regression and confidence sets@"This article considers inference in linear models with K regressors, some or
many could be endogenous, and L instruments. L can range from less than K to
any order smaller than an exponential in the sample size and K is arbitrary.
For moderate K, identification robust confidence sets are obtained by solving a
hierarchy of semidefinite programs. For larger K, we propose the STIV
estimator. The analysis of its error uses sensitivity characteristics which are
sharper than those in the literature on sparsity. Data-driven bounds on them
and robust confidence sets are obtained by solving K linear programs. Results
on rates of convergence, variable selection, and confidence sets which ""adapt""
to the sparsity are given. We generalize our approach to models with
approximation errors, systems, endogenous instruments, and two-stage for
confidence bands for vectors of linear functionals and functions. The
application is to a demand system with many endogenous regressors."@2011
Alexandre Tsybakov@http://arxiv.org/abs/1105.2454v5@High-dimensional instrumental variables regression and confidence sets@"This article considers inference in linear models with K regressors, some or
many could be endogenous, and L instruments. L can range from less than K to
any order smaller than an exponential in the sample size and K is arbitrary.
For moderate K, identification robust confidence sets are obtained by solving a
hierarchy of semidefinite programs. For larger K, we propose the STIV
estimator. The analysis of its error uses sensitivity characteristics which are
sharper than those in the literature on sparsity. Data-driven bounds on them
and robust confidence sets are obtained by solving K linear programs. Results
on rates of convergence, variable selection, and confidence sets which ""adapt""
to the sparsity are given. We generalize our approach to models with
approximation errors, systems, endogenous instruments, and two-stage for
confidence bands for vectors of linear functionals and functions. The
application is to a demand system with many endogenous regressors."@2011
Christiern Rose@http://arxiv.org/abs/1105.2454v5@High-dimensional instrumental variables regression and confidence sets@"This article considers inference in linear models with K regressors, some or
many could be endogenous, and L instruments. L can range from less than K to
any order smaller than an exponential in the sample size and K is arbitrary.
For moderate K, identification robust confidence sets are obtained by solving a
hierarchy of semidefinite programs. For larger K, we propose the STIV
estimator. The analysis of its error uses sensitivity characteristics which are
sharper than those in the literature on sparsity. Data-driven bounds on them
and robust confidence sets are obtained by solving K linear programs. Results
on rates of convergence, variable selection, and confidence sets which ""adapt""
to the sparsity are given. We generalize our approach to models with
approximation errors, systems, endogenous instruments, and two-stage for
confidence bands for vectors of linear functionals and functions. The
application is to a demand system with many endogenous regressors."@2011
Runchu Zhang@http://arxiv.org/abs/1105.2698v1@"A trigonometric approach to quaternary code designs with application to
  one-eighth and one-sixteenth fractions"@"The study of good nonregular fractional factorial designs has received
significant attention over the last two decades. Recent research indicates that
designs constructed from quaternary codes (QC) are very promising in this
regard. The present paper shows how a trigonometric approach can facilitate a
systematic understanding of such QC designs and lead to new theoretical results
covering hitherto unexplored situations. We focus attention on one-eighth and
one-sixteenth fractions of two-level factorials and show that optimal QC
designs often have larger generalized resolution and projectivity than
comparable regular designs. Moreover, some of these designs are found to have
maximum projectivity among all designs."@2011
Frederick K. H. Phoa@http://arxiv.org/abs/1105.2698v1@"A trigonometric approach to quaternary code designs with application to
  one-eighth and one-sixteenth fractions"@"The study of good nonregular fractional factorial designs has received
significant attention over the last two decades. Recent research indicates that
designs constructed from quaternary codes (QC) are very promising in this
regard. The present paper shows how a trigonometric approach can facilitate a
systematic understanding of such QC designs and lead to new theoretical results
covering hitherto unexplored situations. We focus attention on one-eighth and
one-sixteenth fractions of two-level factorials and show that optimal QC
designs often have larger generalized resolution and projectivity than
comparable regular designs. Moreover, some of these designs are found to have
maximum projectivity among all designs."@2011
Rahul Mukerjee@http://arxiv.org/abs/1105.2698v1@"A trigonometric approach to quaternary code designs with application to
  one-eighth and one-sixteenth fractions"@"The study of good nonregular fractional factorial designs has received
significant attention over the last two decades. Recent research indicates that
designs constructed from quaternary codes (QC) are very promising in this
regard. The present paper shows how a trigonometric approach can facilitate a
systematic understanding of such QC designs and lead to new theoretical results
covering hitherto unexplored situations. We focus attention on one-eighth and
one-sixteenth fractions of two-level factorials and show that optimal QC
designs often have larger generalized resolution and projectivity than
comparable regular designs. Moreover, some of these designs are found to have
maximum projectivity among all designs."@2011
Hongquan Xu@http://arxiv.org/abs/1105.2698v1@"A trigonometric approach to quaternary code designs with application to
  one-eighth and one-sixteenth fractions"@"The study of good nonregular fractional factorial designs has received
significant attention over the last two decades. Recent research indicates that
designs constructed from quaternary codes (QC) are very promising in this
regard. The present paper shows how a trigonometric approach can facilitate a
systematic understanding of such QC designs and lead to new theoretical results
covering hitherto unexplored situations. We focus attention on one-eighth and
one-sixteenth fractions of two-level factorials and show that optimal QC
designs often have larger generalized resolution and projectivity than
comparable regular designs. Moreover, some of these designs are found to have
maximum projectivity among all designs."@2011
Junya Honda@http://arxiv.org/abs/1105.2879v1@Stochastic Bandit Based on Empirical Moments@"In the multiarmed bandit problem a gambler chooses an arm of a slot machine
to pull considering a tradeoff between exploration and exploitation. We study
the stochastic bandit problem where each arm has a reward distribution
supported in a known bounded interval, e.g. [0,1]. For this model, policies
which take into account the empirical variances (i.e. second moments) of the
arms are known to perform effectively. In this paper, we generalize this idea
and we propose a policy which exploits the first d empirical moments for
arbitrary d fixed in advance. The asymptotic upper bound of the regret of the
policy approaches the theoretical bound by Burnetas and Katehakis as d
increases. By choosing appropriate d, the proposed policy realizes a tradeoff
between the computational complexity and the expected regret."@2011
Akimichi Takemura@http://arxiv.org/abs/1105.2879v1@Stochastic Bandit Based on Empirical Moments@"In the multiarmed bandit problem a gambler chooses an arm of a slot machine
to pull considering a tradeoff between exploration and exploitation. We study
the stochastic bandit problem where each arm has a reward distribution
supported in a known bounded interval, e.g. [0,1]. For this model, policies
which take into account the empirical variances (i.e. second moments) of the
arms are known to perform effectively. In this paper, we generalize this idea
and we propose a policy which exploits the first d empirical moments for
arbitrary d fixed in advance. The asymptotic upper bound of the regret of the
policy approaches the theoretical bound by Burnetas and Katehakis as d
increases. By choosing appropriate d, the proposed policy realizes a tradeoff
between the computational complexity and the expected regret."@2011
S. H. Alizadeh@http://arxiv.org/abs/1105.2891v1@Hidden Markov Mixture Autoregressive Models: Parameter Estimation@"This report introduces a parsimonious structure for mixture of autoregressive
models, where the weighting coefficients are determined through latent random
variables as functions of all past observations. These variables follow a
hidden Markov model. We modify EM and Baum-Welch algorithms to estimate the
parameters of the model."@2011
S. Rezakhah@http://arxiv.org/abs/1105.2891v1@Hidden Markov Mixture Autoregressive Models: Parameter Estimation@"This report introduces a parsimonious structure for mixture of autoregressive
models, where the weighting coefficients are determined through latent random
variables as functions of all past observations. These variables follow a
hidden Markov model. We modify EM and Baum-Welch algorithms to estimate the
parameters of the model."@2011
Jose A. Diaz-Garcia@http://arxiv.org/abs/1105.2911v1@"Multiple response optimisation: Multiobjective stochastic programming
  methods"@"The multiresponse surface problem is modelled as one of multiobjective
stochastic optimisation, and diverse solutions are proposed. Several crucial
differences are highlighted between this approach and others that have been
proposed. Finally, in a numerical example, some particular solutions are
applied and described in detail."@2011
Mahdi Bashiri@http://arxiv.org/abs/1105.2911v1@"Multiple response optimisation: Multiobjective stochastic programming
  methods"@"The multiresponse surface problem is modelled as one of multiobjective
stochastic optimisation, and diverse solutions are proposed. Several crucial
differences are highlighted between this approach and others that have been
proposed. Finally, in a numerical example, some particular solutions are
applied and described in detail."@2011
Runlong Tang@http://arxiv.org/abs/1105.3018v1@"A two-stage hybrid procedure for estimating an inverse regression
  function"@"We consider a two-stage procedure (TSP) for estimating an inverse regression
function at a given point, where isotonic regression is used at stage one to
obtain an initial estimate and a local linear approximation in the vicinity of
this estimate is used at stage two. We establish that the convergence rate of
the second-stage estimate can attain the parametric $n^{1/2}$ rate.
Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its
consistency properties studied. This variant manages to overcome the slow speed
of the convergence in distribution and the estimation of the derivative of the
regression function at the unknown target quantity. Finally, the finite sample
performance of BTSP is studied through simulations and the method is
illustrated on a data set."@2011
Moulinath Banerjee@http://arxiv.org/abs/1105.3018v1@"A two-stage hybrid procedure for estimating an inverse regression
  function"@"We consider a two-stage procedure (TSP) for estimating an inverse regression
function at a given point, where isotonic regression is used at stage one to
obtain an initial estimate and a local linear approximation in the vicinity of
this estimate is used at stage two. We establish that the convergence rate of
the second-stage estimate can attain the parametric $n^{1/2}$ rate.
Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its
consistency properties studied. This variant manages to overcome the slow speed
of the convergence in distribution and the estimation of the derivative of the
regression function at the unknown target quantity. Finally, the finite sample
performance of BTSP is studied through simulations and the method is
illustrated on a data set."@2011
George Michailidis@http://arxiv.org/abs/1105.3018v1@"A two-stage hybrid procedure for estimating an inverse regression
  function"@"We consider a two-stage procedure (TSP) for estimating an inverse regression
function at a given point, where isotonic regression is used at stage one to
obtain an initial estimate and a local linear approximation in the vicinity of
this estimate is used at stage two. We establish that the convergence rate of
the second-stage estimate can attain the parametric $n^{1/2}$ rate.
Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its
consistency properties studied. This variant manages to overcome the slow speed
of the convergence in distribution and the estimation of the derivative of the
regression function at the unknown target quantity. Finally, the finite sample
performance of BTSP is studied through simulations and the method is
illustrated on a data set."@2011
T. Tony Cai@http://arxiv.org/abs/1105.3039v1@"Testing composite hypotheses, Hermite polynomials and optimal estimation
  of a nonsmooth functional"@"A general lower bound is developed for the minimax risk when estimating an
arbitrary functional. The bound is based on testing two composite hypotheses
and is shown to be effective in estimating the nonsmooth functional
${\frac{1}{n}}\sum|\theta_i|$ from an observation $Y\sim N(\theta,I_n)$. This
problem exhibits some features that are significantly different from those that
occur in estimating conventional smooth functionals. This is a setting where
standard techniques fail to yield sharp results. A sharp minimax lower bound is
established by applying the general lower bound technique based on testing two
composite hypotheses. A key step is the construction of two special priors and
bounding the chi-square distance between two normal mixtures. An estimator is
constructed using approximation theory and Hermite polynomials and is shown to
be asymptotically sharp minimax when the means are bounded by a given value
$M$. It is shown that the minimax risk equals $\beta_*^2M^2({\frac{\log\log
n}{\log n}})^2$ asymptotically, where $\beta_*$ is the Bernstein constant. The
general techniques and results developed in the present paper can also be used
to solve other related problems."@2011
Mark G. Low@http://arxiv.org/abs/1105.3039v1@"Testing composite hypotheses, Hermite polynomials and optimal estimation
  of a nonsmooth functional"@"A general lower bound is developed for the minimax risk when estimating an
arbitrary functional. The bound is based on testing two composite hypotheses
and is shown to be effective in estimating the nonsmooth functional
${\frac{1}{n}}\sum|\theta_i|$ from an observation $Y\sim N(\theta,I_n)$. This
problem exhibits some features that are significantly different from those that
occur in estimating conventional smooth functionals. This is a setting where
standard techniques fail to yield sharp results. A sharp minimax lower bound is
established by applying the general lower bound technique based on testing two
composite hypotheses. A key step is the construction of two special priors and
bounding the chi-square distance between two normal mixtures. An estimator is
constructed using approximation theory and Hermite polynomials and is shown to
be asymptotically sharp minimax when the means are bounded by a given value
$M$. It is shown that the minimax risk equals $\beta_*^2M^2({\frac{\log\log
n}{\log n}})^2$ asymptotically, where $\beta_*$ is the Bernstein constant. The
general techniques and results developed in the present paper can also be used
to solve other related problems."@2011
Jose A. Diaz-Garcia@http://arxiv.org/abs/1105.3224v1@"Optimum allocation in multivariate stratified random sampling:
  Stochastic matrix optimisation"@"The allocation problem for multivariate stratified random sampling as a
problem of stochastic matrix integer mathematical programming is considered.
With these aims the asymptotic normality of sample covariance matrices for each
strata is established. Some alternative approaches are suggested for its
solution. An example is solved by applying the proposed techniques."@2011
Rogelio Ramos-Quiroga@http://arxiv.org/abs/1105.3224v1@"Optimum allocation in multivariate stratified random sampling:
  Stochastic matrix optimisation"@"The allocation problem for multivariate stratified random sampling as a
problem of stochastic matrix integer mathematical programming is considered.
With these aims the asymptotic normality of sample covariance matrices for each
strata is established. Some alternative approaches are suggested for its
solution. An example is solved by applying the proposed techniques."@2011
Alain Celisse@http://arxiv.org/abs/1105.3288v3@"Consistency of maximum-likelihood and variational estimators in the
  Stochastic Block Model"@"The stochastic block model (SBM) is a probabilistic model de- signed to
describe heterogeneous directed and undirected graphs. In this paper, we
address the asymptotic inference on SBM by use of maximum- likelihood and
variational approaches. The identi ability of SBM is proved, while asymptotic
properties of maximum-likelihood and variational esti- mators are provided. In
particular, the consistency of these estimators is settled, which is, to the
best of our knowledge, the rst result of this type for variational estimators
with random graphs."@2011
J. -J. Daudin@http://arxiv.org/abs/1105.3288v3@"Consistency of maximum-likelihood and variational estimators in the
  Stochastic Block Model"@"The stochastic block model (SBM) is a probabilistic model de- signed to
describe heterogeneous directed and undirected graphs. In this paper, we
address the asymptotic inference on SBM by use of maximum- likelihood and
variational approaches. The identi ability of SBM is proved, while asymptotic
properties of maximum-likelihood and variational esti- mators are provided. In
particular, the consistency of these estimators is settled, which is, to the
best of our knowledge, the rst result of this type for variational estimators
with random graphs."@2011
Laurent Pierre@http://arxiv.org/abs/1105.3288v3@"Consistency of maximum-likelihood and variational estimators in the
  Stochastic Block Model"@"The stochastic block model (SBM) is a probabilistic model de- signed to
describe heterogeneous directed and undirected graphs. In this paper, we
address the asymptotic inference on SBM by use of maximum- likelihood and
variational approaches. The identi ability of SBM is proved, while asymptotic
properties of maximum-likelihood and variational esti- mators are provided. In
particular, the consistency of these estimators is settled, which is, to the
best of our knowledge, the rst result of this type for variational estimators
with random graphs."@2011
Min Qian@http://arxiv.org/abs/1105.3369v1@Performance guarantees for individualized treatment rules@"Because many illnesses show heterogeneous response to treatment, there is
increasing interest in individualizing treatment to patients [Arch. Gen.
Psychiatry 66 (2009) 128--133]. An individualized treatment rule is a decision
rule that recommends treatment according to patient characteristics. We
consider the use of clinical trial data in the construction of an
individualized treatment rule leading to highest mean response. This is a
difficult computational problem because the objective function is the
expectation of a weighted indicator function that is nonconcave in the
parameters. Furthermore, there are frequently many pretreatment variables that
may or may not be useful in constructing an optimal individualized treatment
rule, yet cost and interpretability considerations imply that only a few
variables should be used by the individualized treatment rule. To address these
challenges, we consider estimation based on $l_1$-penalized least squares. This
approach is justified via a finite sample upper bound on the difference between
the mean response due to the estimated individualized treatment rule and the
mean response due to the optimal individualized treatment rule."@2011
Susan A. Murphy@http://arxiv.org/abs/1105.3369v1@Performance guarantees for individualized treatment rules@"Because many illnesses show heterogeneous response to treatment, there is
increasing interest in individualizing treatment to patients [Arch. Gen.
Psychiatry 66 (2009) 128--133]. An individualized treatment rule is a decision
rule that recommends treatment according to patient characteristics. We
consider the use of clinical trial data in the construction of an
individualized treatment rule leading to highest mean response. This is a
difficult computational problem because the objective function is the
expectation of a weighted indicator function that is nonconcave in the
parameters. Furthermore, there are frequently many pretreatment variables that
may or may not be useful in constructing an optimal individualized treatment
rule, yet cost and interpretability considerations imply that only a few
variables should be used by the individualized treatment rule. To address these
challenges, we consider estimation based on $l_1$-penalized least squares. This
approach is justified via a finite sample upper bound on the difference between
the mean response due to the estimated individualized treatment rule and the
mean response due to the optimal individualized treatment rule."@2011
Han Xiao@http://arxiv.org/abs/1105.3423v1@Asymptotic Inference of Autocovariances of Stationary Processes@"The paper presents a systematic theory for asymptotic inference of
autocovariances of stationary processes. We consider nonparametric tests for
serial correlations based on the maximum (or ${\cal L}^\infty$) and the
quadratic (or ${\cal L}^2$) deviations. For these two cases, with proper
centering and rescaling, the asymptotic distributions of the deviations are
Gumbel and Gaussian, respectively. To establish such an asymptotic theory, as
byproducts, we develop a normal comparison principle and propose a sufficient
condition for summability of joint cumulants of stationary processes. We adopt
a simulation-based block of blocks bootstrapping procedure that improves the
finite-sample performance."@2011
Wei Biao Wu@http://arxiv.org/abs/1105.3423v1@Asymptotic Inference of Autocovariances of Stationary Processes@"The paper presents a systematic theory for asymptotic inference of
autocovariances of stationary processes. We consider nonparametric tests for
serial correlations based on the maximum (or ${\cal L}^\infty$) and the
quadratic (or ${\cal L}^2$) deviations. For these two cases, with proper
centering and rescaling, the asymptotic distributions of the deviations are
Gumbel and Gaussian, respectively. To establish such an asymptotic theory, as
byproducts, we develop a normal comparison principle and propose a sufficient
condition for summability of joint cumulants of stationary processes. We adopt
a simulation-based block of blocks bootstrapping procedure that improves the
finite-sample performance."@2011
Fuqing Gao@http://arxiv.org/abs/1105.3552v1@Delta method in large deviations and moderate deviations for estimators@"The delta method is a popular and elementary tool for deriving limiting
distributions of transformed statistics, while applications of asymptotic
distributions do not allow one to obtain desirable accuracy of approximation
for tail probabilities. The large and moderate deviation theory can achieve
this goal. Motivated by the delta method in weak convergence, a general delta
method in large deviations is proposed. The new method can be widely applied to
driving the moderate deviations of estimators and is illustrated by examples
including the Wilcoxon statistic, the Kaplan--Meier estimator, the empirical
quantile processes and the empirical copula function. We also improve the
existing moderate deviations results for $M$-estimators and $L$-statistics by
the new method. Some applications of moderate deviations to statistical
hypothesis testing are provided."@2011
Xingqiu Zhao@http://arxiv.org/abs/1105.3552v1@Delta method in large deviations and moderate deviations for estimators@"The delta method is a popular and elementary tool for deriving limiting
distributions of transformed statistics, while applications of asymptotic
distributions do not allow one to obtain desirable accuracy of approximation
for tail probabilities. The large and moderate deviation theory can achieve
this goal. Motivated by the delta method in weak convergence, a general delta
method in large deviations is proposed. The new method can be widely applied to
driving the moderate deviations of estimators and is illustrated by examples
including the Wilcoxon statistic, the Kaplan--Meier estimator, the empirical
quantile processes and the empirical copula function. We also improve the
existing moderate deviations results for $M$-estimators and $L$-statistics by
the new method. Some applications of moderate deviations to statistical
hypothesis testing are provided."@2011
Jun Shao@http://arxiv.org/abs/1105.3561v1@"Sparse linear discriminant analysis by thresholding for high dimensional
  data"@"In many social, economical, biological and medical studies, one objective is
to classify a subject into one of several classes based on a set of variables
observed from the subject. Because the probability distribution of the
variables is usually unknown, the rule of classification is constructed using a
training sample. The well-known linear discriminant analysis (LDA) works well
for the situation where the number of variables used for classification is much
smaller than the training sample size. Because of the advance in technologies,
modern statistical studies often face classification problems with the number
of variables much larger than the sample size, and the LDA may perform poorly.
We explore when and why the LDA has poor performance and propose a sparse LDA
that is asymptotically optimal under some sparsity conditions on the unknown
parameters. For illustration of application, we discuss an example of
classifying human cancer into two classes of leukemia based on a set of 7,129
genes and a training sample of size 72. A simulation is also conducted to check
the performance of the proposed method."@2011
Yazhen Wang@http://arxiv.org/abs/1105.3561v1@"Sparse linear discriminant analysis by thresholding for high dimensional
  data"@"In many social, economical, biological and medical studies, one objective is
to classify a subject into one of several classes based on a set of variables
observed from the subject. Because the probability distribution of the
variables is usually unknown, the rule of classification is constructed using a
training sample. The well-known linear discriminant analysis (LDA) works well
for the situation where the number of variables used for classification is much
smaller than the training sample size. Because of the advance in technologies,
modern statistical studies often face classification problems with the number
of variables much larger than the sample size, and the LDA may perform poorly.
We explore when and why the LDA has poor performance and propose a sparse LDA
that is asymptotically optimal under some sparsity conditions on the unknown
parameters. For illustration of application, we discuss an example of
classifying human cancer into two classes of leukemia based on a set of 7,129
genes and a training sample of size 72. A simulation is also conducted to check
the performance of the proposed method."@2011
Xinwei Deng@http://arxiv.org/abs/1105.3561v1@"Sparse linear discriminant analysis by thresholding for high dimensional
  data"@"In many social, economical, biological and medical studies, one objective is
to classify a subject into one of several classes based on a set of variables
observed from the subject. Because the probability distribution of the
variables is usually unknown, the rule of classification is constructed using a
training sample. The well-known linear discriminant analysis (LDA) works well
for the situation where the number of variables used for classification is much
smaller than the training sample size. Because of the advance in technologies,
modern statistical studies often face classification problems with the number
of variables much larger than the sample size, and the LDA may perform poorly.
We explore when and why the LDA has poor performance and propose a sparse LDA
that is asymptotically optimal under some sparsity conditions on the unknown
parameters. For illustration of application, we discuss an example of
classifying human cancer into two classes of leukemia based on a set of 7,129
genes and a training sample of size 72. A simulation is also conducted to check
the performance of the proposed method."@2011
Sijian Wang@http://arxiv.org/abs/1105.3561v1@"Sparse linear discriminant analysis by thresholding for high dimensional
  data"@"In many social, economical, biological and medical studies, one objective is
to classify a subject into one of several classes based on a set of variables
observed from the subject. Because the probability distribution of the
variables is usually unknown, the rule of classification is constructed using a
training sample. The well-known linear discriminant analysis (LDA) works well
for the situation where the number of variables used for classification is much
smaller than the training sample size. Because of the advance in technologies,
modern statistical studies often face classification problems with the number
of variables much larger than the sample size, and the LDA may perform poorly.
We explore when and why the LDA has poor performance and propose a sparse LDA
that is asymptotically optimal under some sparsity conditions on the unknown
parameters. For illustration of application, we discuss an example of
classifying human cancer into two classes of leukemia based on a set of 7,129
genes and a training sample of size 72. A simulation is also conducted to check
the performance of the proposed method."@2011
Holger Dette@http://arxiv.org/abs/1105.3575v1@A note on the de la Garza phenomenon for locally optimal designs@"The celebrated de la Garza phenomenon states that for a polynomial regression
model of degree $p-1$ any optimal design can be based on at most $p$ design
points. In a remarkable paper, Yang [Ann. Statist. 38 (2010) 2499--2524] showed
that this phenomenon exists in many locally optimal design problems for
nonlinear models. In the present note, we present a different view point on
these findings using results about moment theory and Chebyshev systems. In
particular, we show that this phenomenon occurs in an even larger class of
models than considered so far."@2011
Viatcheslav B. Melas@http://arxiv.org/abs/1105.3575v1@A note on the de la Garza phenomenon for locally optimal designs@"The celebrated de la Garza phenomenon states that for a polynomial regression
model of degree $p-1$ any optimal design can be based on at most $p$ design
points. In a remarkable paper, Yang [Ann. Statist. 38 (2010) 2499--2524] showed
that this phenomenon exists in many locally optimal design problems for
nonlinear models. In the present note, we present a different view point on
these findings using results about moment theory and Chebyshev systems. In
particular, we show that this phenomenon occurs in an even larger class of
models than considered so far."@2011
Jérémie Bigot@http://arxiv.org/abs/1105.3625v1@"Intensity estimation of non-homogeneous Poisson processes from shifted
  trajectories"@"This paper considers the problem of adaptive estimation of a non-homogeneous
intensity function from the observation of n independent Poisson processes
having a common intensity that is randomly shifted for each observed
trajectory. We show that estimating this intensity is a deconvolution problem
for which the density of the random shifts plays the role of the convolution
operator. In an asymptotic setting where the number n of observed trajectories
tends to infinity, we derive upper and lower bounds for the minimax quadratic
risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used
to derive an adaptive estimator of the intensity. The proposed estimator is
shown to achieve a near-minimax rate of convergence. This rate depends both on
the smoothness of the intensity function and the density of the random shifts,
which makes a connection between the classical deconvolution problem in
nonparametric statistics and the estimation of a mean intensity from the
observations of independent Poisson processes."@2011
Sébastien Gadat@http://arxiv.org/abs/1105.3625v1@"Intensity estimation of non-homogeneous Poisson processes from shifted
  trajectories"@"This paper considers the problem of adaptive estimation of a non-homogeneous
intensity function from the observation of n independent Poisson processes
having a common intensity that is randomly shifted for each observed
trajectory. We show that estimating this intensity is a deconvolution problem
for which the density of the random shifts plays the role of the convolution
operator. In an asymptotic setting where the number n of observed trajectories
tends to infinity, we derive upper and lower bounds for the minimax quadratic
risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used
to derive an adaptive estimator of the intensity. The proposed estimator is
shown to achieve a near-minimax rate of convergence. This rate depends both on
the smoothness of the intensity function and the density of the random shifts,
which makes a connection between the classical deconvolution problem in
nonparametric statistics and the estimation of a mean intensity from the
observations of independent Poisson processes."@2011
Thierry Klein@http://arxiv.org/abs/1105.3625v1@"Intensity estimation of non-homogeneous Poisson processes from shifted
  trajectories"@"This paper considers the problem of adaptive estimation of a non-homogeneous
intensity function from the observation of n independent Poisson processes
having a common intensity that is randomly shifted for each observed
trajectory. We show that estimating this intensity is a deconvolution problem
for which the density of the random shifts plays the role of the convolution
operator. In an asymptotic setting where the number n of observed trajectories
tends to infinity, we derive upper and lower bounds for the minimax quadratic
risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used
to derive an adaptive estimator of the intensity. The proposed estimator is
shown to achieve a near-minimax rate of convergence. This rate depends both on
the smoothness of the intensity function and the density of the random shifts,
which makes a connection between the classical deconvolution problem in
nonparametric statistics and the estimation of a mean intensity from the
observations of independent Poisson processes."@2011
Clément Marteau@http://arxiv.org/abs/1105.3625v1@"Intensity estimation of non-homogeneous Poisson processes from shifted
  trajectories"@"This paper considers the problem of adaptive estimation of a non-homogeneous
intensity function from the observation of n independent Poisson processes
having a common intensity that is randomly shifted for each observed
trajectory. We show that estimating this intensity is a deconvolution problem
for which the density of the random shifts plays the role of the convolution
operator. In an asymptotic setting where the number n of observed trajectories
tends to infinity, we derive upper and lower bounds for the minimax quadratic
risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used
to derive an adaptive estimator of the intensity. The proposed estimator is
shown to achieve a near-minimax rate of convergence. This rate depends both on
the smoothness of the intensity function and the density of the random shifts,
which makes a connection between the classical deconvolution problem in
nonparametric statistics and the estimation of a mean intensity from the
observations of independent Poisson processes."@2011
Thomas A. Dean@http://arxiv.org/abs/1105.3655v1@Asymptotic Behaviour of Approximate Bayesian Estimators@"Although approximate Bayesian computation (ABC) has become a popular
technique for performing parameter estimation when the likelihood functions are
analytically intractable there has not as yet been a complete investigation of
the theoretical properties of the resulting estimators. In this paper we give a
theoretical analysis of the asymptotic properties of ABC based parameter
estimators for hidden Markov models and show that ABC based estimators satisfy
asymptotically biased versions of the standard results in the statistical
literature."@2011
Sumeetpal S. Singh@http://arxiv.org/abs/1105.3655v1@Asymptotic Behaviour of Approximate Bayesian Estimators@"Although approximate Bayesian computation (ABC) has become a popular
technique for performing parameter estimation when the likelihood functions are
analytically intractable there has not as yet been a complete investigation of
the theoretical properties of the resulting estimators. In this paper we give a
theoretical analysis of the asymptotic properties of ABC based parameter
estimators for hidden Markov models and show that ABC based estimators satisfy
asymptotically biased versions of the standard results in the statistical
literature."@2011
Fasheng Sun@http://arxiv.org/abs/1105.3816v1@On construction of optimal mixed-level supersaturated designs@"Supersaturated design (SSD) has received much recent interest because of its
potential in factor screening experiments. In this paper, we provide equivalent
conditions for two columns to be fully aliased and consequently propose methods
for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs
without fully aliased columns, via equidistant designs and difference matrices.
The methods can be easily performed and many new optimal mixed-level SSDs have
been obtained. Furthermore, it is proved that the nonorthogonality between
columns of the resulting design is well controlled by the source designs. A
rather complete list of newly generated optimal mixed-level SSDs are tabulated
for practical use."@2011
Dennis K. J. Lin@http://arxiv.org/abs/1105.3816v1@On construction of optimal mixed-level supersaturated designs@"Supersaturated design (SSD) has received much recent interest because of its
potential in factor screening experiments. In this paper, we provide equivalent
conditions for two columns to be fully aliased and consequently propose methods
for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs
without fully aliased columns, via equidistant designs and difference matrices.
The methods can be easily performed and many new optimal mixed-level SSDs have
been obtained. Furthermore, it is proved that the nonorthogonality between
columns of the resulting design is well controlled by the source designs. A
rather complete list of newly generated optimal mixed-level SSDs are tabulated
for practical use."@2011
Min-Qian Liu@http://arxiv.org/abs/1105.3816v1@On construction of optimal mixed-level supersaturated designs@"Supersaturated design (SSD) has received much recent interest because of its
potential in factor screening experiments. In this paper, we provide equivalent
conditions for two columns to be fully aliased and consequently propose methods
for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs
without fully aliased columns, via equidistant designs and difference matrices.
The methods can be easily performed and many new optimal mixed-level SSDs have
been obtained. Furthermore, it is proved that the nonorthogonality between
columns of the resulting design is well controlled by the source designs. A
rather complete list of newly generated optimal mixed-level SSDs are tabulated
for practical use."@2011
Han Xiao@http://arxiv.org/abs/1105.4563v2@Covariance matrix estimation for stationary time series@"We obtain a sharp convergence rate for banded covariance matrix estimates of
stationary processes. A precise order of magnitude is derived for spectral
radius of sample covariance matrices. We also consider a thresholded covariance
matrix estimator that can better characterize sparsity if the true covariance
matrix is sparse. As our main tool, we implement Toeplitz [Math. Ann. 70 (1911)
351-376] idea and relate eigenvalues of covariance matrices to the spectral
densities or Fourier transforms of the covariances. We develop a large
deviation result for quadratic forms of stationary processes using m-dependence
approximation, under the framework of causal representation and physical
dependence measures."@2011
Wei Biao Wu@http://arxiv.org/abs/1105.4563v2@Covariance matrix estimation for stationary time series@"We obtain a sharp convergence rate for banded covariance matrix estimates of
stationary processes. A precise order of magnitude is derived for spectral
radius of sample covariance matrices. We also consider a thresholded covariance
matrix estimator that can better characterize sparsity if the true covariance
matrix is sparse. As our main tool, we implement Toeplitz [Math. Ann. 70 (1911)
351-376] idea and relate eigenvalues of covariance matrices to the spectral
densities or Fourier transforms of the covariances. We develop a large
deviation result for quadratic forms of stationary processes using m-dependence
approximation, under the framework of causal representation and physical
dependence measures."@2011
Elchanan Mossel@http://arxiv.org/abs/1105.4765v5@From Agreement to Asymptotic Learning@"We consider a group of Bayesian agents who are each given an independent
signal about an unknown state of the world, and proceed to communicate with
each other. We study the question of asymptotic learning: do agents learn the
state of the world with probability that approaches one as the number of agents
tends to infinity?
  We show that under general conditions asymptotic learning follows from
agreement on posterior actions or posterior beliefs, regardless of the
communication dynamics. In particular, we prove that asymptotic learning holds
for the Gale-Kariv model on undirected networks and non-atomic private beliefs."@2011
Allan Sly@http://arxiv.org/abs/1105.4765v5@From Agreement to Asymptotic Learning@"We consider a group of Bayesian agents who are each given an independent
signal about an unknown state of the world, and proceed to communicate with
each other. We study the question of asymptotic learning: do agents learn the
state of the world with probability that approaches one as the number of agents
tends to infinity?
  We show that under general conditions asymptotic learning follows from
agreement on posterior actions or posterior beliefs, regardless of the
communication dynamics. In particular, we prove that asymptotic learning holds
for the Gale-Kariv model on undirected networks and non-atomic private beliefs."@2011
Omer Tamuz@http://arxiv.org/abs/1105.4765v5@From Agreement to Asymptotic Learning@"We consider a group of Bayesian agents who are each given an independent
signal about an unknown state of the world, and proceed to communicate with
each other. We study the question of asymptotic learning: do agents learn the
state of the world with probability that approaches one as the number of agents
tends to infinity?
  We show that under general conditions asymptotic learning follows from
agreement on posterior actions or posterior beliefs, regardless of the
communication dynamics. In particular, we prove that asymptotic learning holds
for the Gale-Kariv model on undirected networks and non-atomic private beliefs."@2011
Yuan Liao@http://arxiv.org/abs/1105.4847v2@"Posterior consistency of nonparametric conditional moment restricted
  models"@"This paper addresses the estimation of the nonparametric conditional moment
restricted model that involves an infinite-dimensional parameter $g_0$. We
estimate it in a quasi-Bayesian way, based on the limited information
likelihood, and investigate the impact of three types of priors on the
posterior consistency: (i) truncated prior (priors supported on a bounded set),
(ii) thin-tail prior (a prior that has very thin tail outside a growing bounded
set) and (iii) normal prior with nonshrinking variance. In addition, $g_0$ is
allowed to be only partially identified in the frequentist sense, and the
parameter space does not need to be compact. The posterior is regularized using
a slowly growing sieve dimension, and it is shown that the posterior converges
to any small neighborhood of the identified region. We then apply our results
to the nonparametric instrumental regression model. Finally, the posterior
consistency using a random sieve dimension parameter is studied."@2011
Wenxin Jiang@http://arxiv.org/abs/1105.4847v2@"Posterior consistency of nonparametric conditional moment restricted
  models"@"This paper addresses the estimation of the nonparametric conditional moment
restricted model that involves an infinite-dimensional parameter $g_0$. We
estimate it in a quasi-Bayesian way, based on the limited information
likelihood, and investigate the impact of three types of priors on the
posterior consistency: (i) truncated prior (priors supported on a bounded set),
(ii) thin-tail prior (a prior that has very thin tail outside a growing bounded
set) and (iii) normal prior with nonshrinking variance. In addition, $g_0$ is
allowed to be only partially identified in the frequentist sense, and the
parameter space does not need to be compact. The posterior is regularized using
a slowly growing sieve dimension, and it is shown that the posterior converges
to any small neighborhood of the identified region. We then apply our results
to the nonparametric instrumental regression model. Finally, the posterior
consistency using a random sieve dimension parameter is studied."@2011
Sébastien Bubeck@http://arxiv.org/abs/1105.5041v2@Lipschitz Bandits without the Lipschitz Constant@"We consider the setting of stochastic bandit problems with a continuum of
arms. We first point out that the strategies considered so far in the
literature only provided theoretical guarantees of the form: given some tuning
parameters, the regret is small with respect to a class of environments that
depends on these parameters. This is however not the right perspective, as it
is the strategy that should adapt to the specific bandit environment at hand,
and not the other way round. Put differently, an adaptation issue is raised. We
solve it for the special case of environments whose mean-payoff functions are
globally Lipschitz. More precisely, we show that the minimax optimal orders of
magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an
environment $f$ with Lipschitz constant $L$ over $T$ time instances can be
achieved without knowing $L$ or $T$ in advance. This is in contrast to all
previously known strategies, which require to some extent the knowledge of $L$
to achieve this performance guarantee."@2011
Gilles Stoltz@http://arxiv.org/abs/1105.5041v2@Lipschitz Bandits without the Lipschitz Constant@"We consider the setting of stochastic bandit problems with a continuum of
arms. We first point out that the strategies considered so far in the
literature only provided theoretical guarantees of the form: given some tuning
parameters, the regret is small with respect to a class of environments that
depends on these parameters. This is however not the right perspective, as it
is the strategy that should adapt to the specific bandit environment at hand,
and not the other way round. Put differently, an adaptation issue is raised. We
solve it for the special case of environments whose mean-payoff functions are
globally Lipschitz. More precisely, we show that the minimax optimal orders of
magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an
environment $f$ with Lipschitz constant $L$ over $T$ time instances can be
achieved without knowing $L$ or $T$ in advance. This is in contrast to all
previously known strategies, which require to some extent the knowledge of $L$
to achieve this performance guarantee."@2011
Jia Yuan Yu@http://arxiv.org/abs/1105.5041v2@Lipschitz Bandits without the Lipschitz Constant@"We consider the setting of stochastic bandit problems with a continuum of
arms. We first point out that the strategies considered so far in the
literature only provided theoretical guarantees of the form: given some tuning
parameters, the regret is small with respect to a class of environments that
depends on these parameters. This is however not the right perspective, as it
is the strategy that should adapt to the specific bandit environment at hand,
and not the other way round. Put differently, an adaptation issue is raised. We
solve it for the special case of environments whose mean-payoff functions are
globally Lipschitz. More precisely, we show that the minimax optimal orders of
magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an
environment $f$ with Lipschitz constant $L$ over $T$ time instances can be
achieved without knowing $L$ or $T$ in advance. This is in contrast to all
previously known strategies, which require to some extent the knowledge of $L$
to achieve this performance guarantee."@2011
Rajen D. Shah@http://arxiv.org/abs/1105.5578v2@"Variable selection with error control: Another look at Stability
  Selection"@"Stability Selection was recently introduced by Meinshausen and Buhlmann
(2010) as a very general technique designed to improve the performance of a
variable selection algorithm. It is based on aggregating the results of
applying a selection procedure to subsamples of the data. We introduce a
variant, called Complementary Pairs Stability Selection (CPSS), and derive
bounds both on the expected number of variables included by CPSS that have low
selection probability under the original procedure, and on the expected number
of high selection probability variables that are excluded. These results
require no (e.g. exchangeability) assumptions on the underlying model or on the
quality of the original selection procedure. Under reasonable shape
restrictions, the bounds can be further tightened, yielding improved error
control, and therefore increasing the applicability of the methodology."@2011
Richard J. Samworth@http://arxiv.org/abs/1105.5578v2@"Variable selection with error control: Another look at Stability
  Selection"@"Stability Selection was recently introduced by Meinshausen and Buhlmann
(2010) as a very general technique designed to improve the performance of a
variable selection algorithm. It is based on aggregating the results of
applying a selection procedure to subsamples of the data. We introduce a
variant, called Complementary Pairs Stability Selection (CPSS), and derive
bounds both on the expected number of variables included by CPSS that have low
selection probability under the original procedure, and on the expected number
of high selection probability variables that are excluded. These results
require no (e.g. exchangeability) assumptions on the underlying model or on the
quality of the original selection procedure. Under reasonable shape
restrictions, the bounds can be further tightened, yielding improved error
control, and therefore increasing the applicability of the methodology."@2011
Odalric-Ambrym Maillard@http://arxiv.org/abs/1105.5820v1@"A Finite-Time Analysis of Multi-armed Bandits Problems with
  Kullback-Leibler Divergences"@"We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed
bandit problem in the case of distributions with finite supports (not
necessarily known beforehand), whose asymptotic regret matches the lower bound
of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of
this algorithm; we get bounds whose main terms are smaller than the ones of
previously known algorithms with finite-time analyses (like UCB-type
algorithms)."@2011
Rémi Munos@http://arxiv.org/abs/1105.5820v1@"A Finite-Time Analysis of Multi-armed Bandits Problems with
  Kullback-Leibler Divergences"@"We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed
bandit problem in the case of distributions with finite supports (not
necessarily known beforehand), whose asymptotic regret matches the lower bound
of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of
this algorithm; we get bounds whose main terms are smaller than the ones of
previously known algorithms with finite-time analyses (like UCB-type
algorithms)."@2011
Gilles Stoltz@http://arxiv.org/abs/1105.5820v1@"A Finite-Time Analysis of Multi-armed Bandits Problems with
  Kullback-Leibler Divergences"@"We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed
bandit problem in the case of distributions with finite supports (not
necessarily known beforehand), whose asymptotic regret matches the lower bound
of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of
this algorithm; we get bounds whose main terms are smaller than the ones of
previously known algorithms with finite-time analyses (like UCB-type
algorithms)."@2011
Takuya Kashimura@http://arxiv.org/abs/1105.6027v1@Properties of semi-elementary imsets as sums of elementary imsets@"We study properties of semi-elementary imsets and elementary imsets
introduced by Studeny (2005). The rules of the semi-graphoid axiom
(decomposition, weak union and contraction) for conditional independence
statements can be translated into a simple identity among three semi-elementary
imsets. By recursively applying the identity, any semi-elementary imset can be
written as a sum of elementary imsets, which we call a representation of the
semi-elementary imset. A semi-elementary imset has many representations. We
study properties of the set of possible representations of a semi-elementary
imset and prove that all representations are connected by relations among four
elementary imsets."@2011
Tomonari Sei@http://arxiv.org/abs/1105.6027v1@Properties of semi-elementary imsets as sums of elementary imsets@"We study properties of semi-elementary imsets and elementary imsets
introduced by Studeny (2005). The rules of the semi-graphoid axiom
(decomposition, weak union and contraction) for conditional independence
statements can be translated into a simple identity among three semi-elementary
imsets. By recursively applying the identity, any semi-elementary imset can be
written as a sum of elementary imsets, which we call a representation of the
semi-elementary imset. A semi-elementary imset has many representations. We
study properties of the set of possible representations of a semi-elementary
imset and prove that all representations are connected by relations among four
elementary imsets."@2011
Akimichi Takemura@http://arxiv.org/abs/1105.6027v1@Properties of semi-elementary imsets as sums of elementary imsets@"We study properties of semi-elementary imsets and elementary imsets
introduced by Studeny (2005). The rules of the semi-graphoid axiom
(decomposition, weak union and contraction) for conditional independence
statements can be translated into a simple identity among three semi-elementary
imsets. By recursively applying the identity, any semi-elementary imset can be
written as a sum of elementary imsets, which we call a representation of the
semi-elementary imset. A semi-elementary imset has many representations. We
study properties of the set of possible representations of a semi-elementary
imset and prove that all representations are connected by relations among four
elementary imsets."@2011
Kentaro Tanaka@http://arxiv.org/abs/1105.6027v1@Properties of semi-elementary imsets as sums of elementary imsets@"We study properties of semi-elementary imsets and elementary imsets
introduced by Studeny (2005). The rules of the semi-graphoid axiom
(decomposition, weak union and contraction) for conditional independence
statements can be translated into a simple identity among three semi-elementary
imsets. By recursively applying the identity, any semi-elementary imset can be
written as a sum of elementary imsets, which we call a representation of the
semi-elementary imset. A semi-elementary imset has many representations. We
study properties of the set of possible representations of a semi-elementary
imset and prove that all representations are connected by relations among four
elementary imsets."@2011
Abdelhakim Necir@http://arxiv.org/abs/1105.6031v1@"Coupled risk measures and their empirical estimation when losses follow
  heavy-tailed distributions"@"Considerable literature has been devoted to developing statistical
inferential results for risk measures, especially for those that are of the
form of L-functionals. However, practical and theoretical considerations have
highlighted quite a number of risk measures that are of the form of ratios, or
even more complex combinations, of two L-functionals. In the present paper we
call such combinations `coupled risk measures' and develop a statistical
inferential theory for them when losses follow heavy-tailed distributions. Our
theory implies -at a stroke- statistical inferential results for absolute and
relative distortion risk measures, weighted premium calculation principles, as
well as for many indices of economic inequality that have appeared in the
econometric literature."@2011
Ričardas Zitikis@http://arxiv.org/abs/1105.6031v1@"Coupled risk measures and their empirical estimation when losses follow
  heavy-tailed distributions"@"Considerable literature has been devoted to developing statistical
inferential results for risk measures, especially for those that are of the
form of L-functionals. However, practical and theoretical considerations have
highlighted quite a number of risk measures that are of the form of ratios, or
even more complex combinations, of two L-functionals. In the present paper we
call such combinations `coupled risk measures' and develop a statistical
inferential theory for them when losses follow heavy-tailed distributions. Our
theory implies -at a stroke- statistical inferential results for absolute and
relative distortion risk measures, weighted premium calculation principles, as
well as for many indices of economic inequality that have appeared in the
econometric literature."@2011
Antonio Cuevas@http://arxiv.org/abs/1105.6239v2@On statistical properties of sets fulfilling rolling-type conditions@"Motivated by set estimation problems, we consider three closely related shape
conditions for compact sets: positive reach, r-convexity and rolling condition.
First, the relations between these shape conditions are analyzed. Second, we
obtain for the estimation of sets fulfilling a rolling condition a result of
""full consistency"" (i.e., consistency with respect to the Hausdorff metric for
the target set and for its boundary). Third, the class of uniformly bounded
compact sets whose reach is not smaller than a given constant r is shown to be
a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in
particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the
r-convex hull of the sample is proved to be a fully consistent estimator of an
r-convex support in the two-dimensional case. Moreover, its boundary length is
shown to converge (a.s.) to that of the underlying support. Fifth, the above
results are applied to get new consistency statements for level set estimators
based on the excess mass methodology (Polonik, 1995)."@2011
Ricardo Fraiman@http://arxiv.org/abs/1105.6239v2@On statistical properties of sets fulfilling rolling-type conditions@"Motivated by set estimation problems, we consider three closely related shape
conditions for compact sets: positive reach, r-convexity and rolling condition.
First, the relations between these shape conditions are analyzed. Second, we
obtain for the estimation of sets fulfilling a rolling condition a result of
""full consistency"" (i.e., consistency with respect to the Hausdorff metric for
the target set and for its boundary). Third, the class of uniformly bounded
compact sets whose reach is not smaller than a given constant r is shown to be
a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in
particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the
r-convex hull of the sample is proved to be a fully consistent estimator of an
r-convex support in the two-dimensional case. Moreover, its boundary length is
shown to converge (a.s.) to that of the underlying support. Fifth, the above
results are applied to get new consistency statements for level set estimators
based on the excess mass methodology (Polonik, 1995)."@2011
Beatriz Pateiro-López@http://arxiv.org/abs/1105.6239v2@On statistical properties of sets fulfilling rolling-type conditions@"Motivated by set estimation problems, we consider three closely related shape
conditions for compact sets: positive reach, r-convexity and rolling condition.
First, the relations between these shape conditions are analyzed. Second, we
obtain for the estimation of sets fulfilling a rolling condition a result of
""full consistency"" (i.e., consistency with respect to the Hausdorff metric for
the target set and for its boundary). Third, the class of uniformly bounded
compact sets whose reach is not smaller than a given constant r is shown to be
a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in
particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the
r-convex hull of the sample is proved to be a fully consistent estimator of an
r-convex support in the two-dimensional case. Moreover, its boundary length is
shown to converge (a.s.) to that of the underlying support. Fifth, the above
results are applied to get new consistency statements for level set estimators
based on the excess mass methodology (Polonik, 1995)."@2011
Céline Duval@http://arxiv.org/abs/1106.1031v1@Statistical inference across time scales@"We investigate statistical inference across time scales. We take as toy model
the estimation of the intensity of a discretely observed compound Poisson
process with symmetric Bernoulli jumps. We have data at different time scales:
microscopic, intermediate and macroscopic. We quantify the smooth statistical
transition from a microscopic Poissonian regime to a macroscopic Gaussian
regime. The classical quadratic variation estimator is efficient in both
microscopic and macroscopic scales but surprisingly shows a substantial loss of
information in the intermediate scale that can be explicitly related to the
sampling rate. We discuss the implications of these findings beyond this
idealised framework."@2011
Marc Hoffmann@http://arxiv.org/abs/1106.1031v1@Statistical inference across time scales@"We investigate statistical inference across time scales. We take as toy model
the estimation of the intensity of a discretely observed compound Poisson
process with symmetric Bernoulli jumps. We have data at different time scales:
microscopic, intermediate and macroscopic. We quantify the smooth statistical
transition from a microscopic Poissonian regime to a macroscopic Gaussian
regime. The classical quadratic variation estimator is efficient in both
microscopic and macroscopic scales but surprisingly shows a substantial loss of
information in the intermediate scale that can be explicitly related to the
sampling rate. We discuss the implications of these findings beyond this
idealised framework."@2011
François Portier@http://arxiv.org/abs/1106.1056v1@Test function: A new approach for covering the central subspace@"In this paper we offer a complete methodology for sufficient dimension
reduction called the test function (TF). TF provides a new family of methods
for the estimation of the central subspace (CS) based on the introduction of a
nonlinear transformation of the response. Theoretical background of TF is
developed under weaker conditions than the existing methods. By considering
order 1 and 2 conditional moments of the predictor given the response, we
divide TF in two classes. In each class we provide conditions that guarantee an
exhaustive estimation of the CS. Besides, the optimal members are calculated
via the minimization of the asymptotic mean squared error deriving from the
distance between the CS and its estimate. This leads us to two plug-in methods
which are evaluated with several simulations."@2011
Bernard Delyon@http://arxiv.org/abs/1106.1056v1@Test function: A new approach for covering the central subspace@"In this paper we offer a complete methodology for sufficient dimension
reduction called the test function (TF). TF provides a new family of methods
for the estimation of the central subspace (CS) based on the introduction of a
nonlinear transformation of the response. Theoretical background of TF is
developed under weaker conditions than the existing methods. By considering
order 1 and 2 conditional moments of the predictor given the response, we
divide TF in two classes. In each class we provide conditions that guarantee an
exhaustive estimation of the CS. Besides, the optimal members are calculated
via the minimization of the asymptotic mean squared error deriving from the
distance between the CS and its estimate. This leads us to two plug-in methods
which are evaluated with several simulations."@2011
Ery Arias-Castro@http://arxiv.org/abs/1106.1193v2@Detection of correlations@"We consider the hypothesis testing problem of deciding whether an observed
high-dimensional vector has independent normal components or, alternatively, if
it has a small subset of correlated components. The correlated components may
have a certain combinatorial structure known to the statistician. We establish
upper and lower bounds for the worst-case (minimax) risk in terms of the size
of the correlated subset, the level of correlation, and the structure of the
class of possibly correlated sets. We show that some simple tests have
near-optimal performance in many cases, while the generalized likelihood ratio
test is suboptimal in some important cases."@2011
Sébastien Bubeck@http://arxiv.org/abs/1106.1193v2@Detection of correlations@"We consider the hypothesis testing problem of deciding whether an observed
high-dimensional vector has independent normal components or, alternatively, if
it has a small subset of correlated components. The correlated components may
have a certain combinatorial structure known to the statistician. We establish
upper and lower bounds for the worst-case (minimax) risk in terms of the size
of the correlated subset, the level of correlation, and the structure of the
class of possibly correlated sets. We show that some simple tests have
near-optimal performance in many cases, while the generalized likelihood ratio
test is suboptimal in some important cases."@2011
Gábor Lugosi@http://arxiv.org/abs/1106.1193v2@Detection of correlations@"We consider the hypothesis testing problem of deciding whether an observed
high-dimensional vector has independent normal components or, alternatively, if
it has a small subset of correlated components. The correlated components may
have a certain combinatorial structure known to the statistician. We establish
upper and lower bounds for the worst-case (minimax) risk in terms of the size
of the correlated subset, the level of correlation, and the structure of the
class of possibly correlated sets. We show that some simple tests have
near-optimal performance in many cases, while the generalized likelihood ratio
test is suboptimal in some important cases."@2011
Tilmann Gneiting@http://arxiv.org/abs/1106.1638v1@Combining Predictive Distributions@"Predictive distributions need to be aggregated when probabilistic forecasts
are merged, or when expert opinions expressed in terms of probability
distributions are fused. We take a prediction space approach that applies to
discrete, mixed discrete-continuous and continuous predictive distributions
alike, and study combination formulas for cumulative distribution functions
from the perspectives of coherence, probabilistic and conditional calibration,
and dispersion. Both linear and non-linear aggregation methods are
investigated, including generalized, spread-adjusted and beta-transformed
linear pools. The effects and techniques are demonstrated theoretically, in
simulation examples, and in case studies on density forecasts for S&P 500
returns and daily maximum temperature at Seattle-Tacoma Airport."@2011
Roopesh Ranjan@http://arxiv.org/abs/1106.1638v1@Combining Predictive Distributions@"Predictive distributions need to be aggregated when probabilistic forecasts
are merged, or when expert opinions expressed in terms of probability
distributions are fused. We take a prediction space approach that applies to
discrete, mixed discrete-continuous and continuous predictive distributions
alike, and study combination formulas for cumulative distribution functions
from the perspectives of coherence, probabilistic and conditional calibration,
and dispersion. Both linear and non-linear aggregation methods are
investigated, including generalized, spread-adjusted and beta-transformed
linear pools. The effects and techniques are demonstrated theoretically, in
simulation examples, and in case studies on density forecasts for S&P 500
returns and daily maximum temperature at Seattle-Tacoma Airport."@2011
Nicolai Meinshausen@http://arxiv.org/abs/1106.2068v2@"Asymptotic optimality of the Westfall--Young permutation procedure for
  multiple testing under dependence"@"Test statistics are often strongly dependent in large-scale multiple testing
applications. Most corrections for multiplicity are unduly conservative for
correlated test statistics, resulting in a loss of power to detect true
positives. We show that the Westfall--Young permutation method has
asymptotically optimal power for a broad class of testing problems with a
block-dependence and sparsity structure among the tests, when the number of
tests tends to infinity."@2011
Marloes H. Maathuis@http://arxiv.org/abs/1106.2068v2@"Asymptotic optimality of the Westfall--Young permutation procedure for
  multiple testing under dependence"@"Test statistics are often strongly dependent in large-scale multiple testing
applications. Most corrections for multiplicity are unduly conservative for
correlated test statistics, resulting in a loss of power to detect true
positives. We show that the Westfall--Young permutation method has
asymptotically optimal power for a broad class of testing problems with a
block-dependence and sparsity structure among the tests, when the number of
tests tends to infinity."@2011
Peter Bühlmann@http://arxiv.org/abs/1106.2068v2@"Asymptotic optimality of the Westfall--Young permutation procedure for
  multiple testing under dependence"@"Test statistics are often strongly dependent in large-scale multiple testing
applications. Most corrections for multiplicity are unduly conservative for
correlated test statistics, resulting in a loss of power to detect true
positives. We show that the Westfall--Young permutation method has
asymptotically optimal power for a broad class of testing problems with a
block-dependence and sparsity structure among the tests, when the number of
tests tends to infinity."@2011
Matthieu Lerasle@http://arxiv.org/abs/1106.2467v2@"Sharp oracle inequalities and slope heuristic for specification
  probabilities estimation in discrete random fields"@"We study the problem of estimating the one-point specification probabilities
in non-necessary finite discrete random fields from partially observed
independent samples. Our procedures are based on model selection by
minimization of a penalized empirical criterion. The selected estimators
satisfy sharp oracle inequalities in $L_2$-risk. We also obtain theoretical
results on the slope heuristic for this problem, justifying the slope algorithm
to calibrate the leading constant in the penalty. The practical performances of
our methods are investigated in two simulation studies. We illustrate the
usefulness of our approach by applying the methods to a multi-unit neuronal
data from a rat hippocampus."@2011
Daniel Y. Takahashi@http://arxiv.org/abs/1106.2467v2@"Sharp oracle inequalities and slope heuristic for specification
  probabilities estimation in discrete random fields"@"We study the problem of estimating the one-point specification probabilities
in non-necessary finite discrete random fields from partially observed
independent samples. Our procedures are based on model selection by
minimization of a penalized empirical criterion. The selected estimators
satisfy sharp oracle inequalities in $L_2$-risk. We also obtain theoretical
results on the slope heuristic for this problem, justifying the slope algorithm
to calibrate the leading constant in the penalty. The practical performances of
our methods are investigated in two simulation studies. We illustrate the
usefulness of our approach by applying the methods to a multi-unit neuronal
data from a rat hippocampus."@2011
Xiaolong Luo@http://arxiv.org/abs/1106.2790v2@Sequential Analysis of Cox Model under Response Dependent Allocation@"Sellke and Siegmund (1983) developed the Brownian approximation to the Cox
partial likelihood score as a process of calendar time, laying the foundation
for group sequential analysis of survival studies. We extend their results to
cover situations in which treatment allocations may depend on observed
outcomes. The new development makes use of the entry time and calendar time
along with the corresponding $\sigma$-filtrations to handle the natural
information accumulation. Large sample properties are established under
suitable regularity conditions."@2011
Gongjun Xu@http://arxiv.org/abs/1106.2790v2@Sequential Analysis of Cox Model under Response Dependent Allocation@"Sellke and Siegmund (1983) developed the Brownian approximation to the Cox
partial likelihood score as a process of calendar time, laying the foundation
for group sequential analysis of survival studies. We extend their results to
cover situations in which treatment allocations may depend on observed
outcomes. The new development makes use of the entry time and calendar time
along with the corresponding $\sigma$-filtrations to handle the natural
information accumulation. Large sample properties are established under
suitable regularity conditions."@2011
Zhiliang Ying@http://arxiv.org/abs/1106.2790v2@Sequential Analysis of Cox Model under Response Dependent Allocation@"Sellke and Siegmund (1983) developed the Brownian approximation to the Cox
partial likelihood score as a process of calendar time, laying the foundation
for group sequential analysis of survival studies. We extend their results to
cover situations in which treatment allocations may depend on observed
outcomes. The new development makes use of the entry time and calendar time
along with the corresponding $\sigma$-filtrations to handle the natural
information accumulation. Large sample properties are established under
suitable regularity conditions."@2011
Philipp Arbenz@http://arxiv.org/abs/1106.2920v1@"The AEP algorithm for the fast computation of the distribution of the
  sum of dependent random variables"@"We propose a new algorithm to compute numerically the distribution function
of the sum of $d$ dependent, non-negative random variables with given joint
distribution."@2011
Paul Embrechts@http://arxiv.org/abs/1106.2920v1@"The AEP algorithm for the fast computation of the distribution of the
  sum of dependent random variables"@"We propose a new algorithm to compute numerically the distribution function
of the sum of $d$ dependent, non-negative random variables with given joint
distribution."@2011
Giovanni Puccetti@http://arxiv.org/abs/1106.2920v1@"The AEP algorithm for the fast computation of the distribution of the
  sum of dependent random variables"@"We propose a new algorithm to compute numerically the distribution function
of the sum of $d$ dependent, non-negative random variables with given joint
distribution."@2011
Eric Gautier@http://arxiv.org/abs/1106.3503v3@"Adaptive estimation in the nonparametric random coefficients binary
  choice model by needlet thresholding"@"In the random coefficients binary choice model, a binary variable equals 1
iff an index $X^\top\beta$ is positive.The vectors $X$ and $\beta$ are
independent and belong to the sphere $\mathbb{S}^{d-1}$ in $\mathbb{R}^{d}$.We
prove lower bounds on the minimax risk for estimation of the density
$f\_{\beta}$ over Besov bodies where the loss is a power of the
$L^p(\mathbb{S}^{d-1})$ norm for $1\le p\le \infty$. We show that a hard
thresholding estimator based on a needlet expansion with data-driven thresholds
achieves these lower bounds up to logarithmic factors."@2011
Erwan Le Pennec@http://arxiv.org/abs/1106.3503v3@"Adaptive estimation in the nonparametric random coefficients binary
  choice model by needlet thresholding"@"In the random coefficients binary choice model, a binary variable equals 1
iff an index $X^\top\beta$ is positive.The vectors $X$ and $\beta$ are
independent and belong to the sphere $\mathbb{S}^{d-1}$ in $\mathbb{R}^{d}$.We
prove lower bounds on the minimax risk for estimation of the density
$f\_{\beta}$ over Besov bodies where the loss is a power of the
$L^p(\mathbb{S}^{d-1})$ norm for $1\le p\le \infty$. We show that a hard
thresholding estimator based on a needlet expansion with data-driven thresholds
achieves these lower bounds up to logarithmic factors."@2011
Yoav Benjamini@http://arxiv.org/abs/1106.3670v1@Adjusting for selection bias in testing multiple families of hypotheses@"In many large multiple testing problems the hypotheses are divided into
families. Given the data, families with evidence for true discoveries are
selected, and hypotheses within them are tested. Neither controlling the
error-rate in each family separately nor controlling the error-rate over all
hypotheses together can assure that an error-rate is controlled in the selected
families. We formulate this concern about selective inference in its
generality, for a very wide class of error-rates and for any selection
criterion, and present an adjustment of the testing level inside the selected
families that retains the average error-rate over the selected families."@2011
Marina Bogomolov@http://arxiv.org/abs/1106.3670v1@Adjusting for selection bias in testing multiple families of hypotheses@"In many large multiple testing problems the hypotheses are divided into
families. Given the data, families with evidence for true discoveries are
selected, and hypotheses within them are tested. Neither controlling the
error-rate in each family separately nor controlling the error-rate over all
hypotheses together can assure that an error-rate is controlled in the selected
families. We formulate this concern about selective inference in its
generality, for a very wide class of error-rates and for any selection
criterion, and present an adjustment of the testing level inside the selected
families that retains the average error-rate over the selected families."@2011
Mireille Gettler Summa@http://arxiv.org/abs/1106.3830v3@Factor PD-Clustering@"Factorial clustering methods have been developed in recent years thanks to
the improving of computational power. These methods perform a linear
transformation of data and a clustering on transformed data optimizing a common
criterion. Factorial PD-clustering is based on Probabilistic Distance
clustering (PD-clustering). PD-clustering is an iterative, distribution free,
probabilistic, clustering method. Factor PD-clustering make a linear
transformation of original variables into a reduced number of orthogonal ones
using a common criterion with PD-Clustering. It is demonstrated that Tucker 3
decomposition allows to obtain this transformation. Factor PD-clustering makes
alternatively a Tucker 3 decomposition and a PD-clustering on transformed data
until convergence. This method could significantly improve the algorithm
performance and allows to work with large dataset, to improve the stability and
the robustness of the method."@2011
Francesco Palumbo@http://arxiv.org/abs/1106.3830v3@Factor PD-Clustering@"Factorial clustering methods have been developed in recent years thanks to
the improving of computational power. These methods perform a linear
transformation of data and a clustering on transformed data optimizing a common
criterion. Factorial PD-clustering is based on Probabilistic Distance
clustering (PD-clustering). PD-clustering is an iterative, distribution free,
probabilistic, clustering method. Factor PD-clustering make a linear
transformation of original variables into a reduced number of orthogonal ones
using a common criterion with PD-Clustering. It is demonstrated that Tucker 3
decomposition allows to obtain this transformation. Factor PD-clustering makes
alternatively a Tucker 3 decomposition and a PD-clustering on transformed data
until convergence. This method could significantly improve the algorithm
performance and allows to work with large dataset, to improve the stability and
the robustness of the method."@2011
Cristina Tortora@http://arxiv.org/abs/1106.3830v3@Factor PD-Clustering@"Factorial clustering methods have been developed in recent years thanks to
the improving of computational power. These methods perform a linear
transformation of data and a clustering on transformed data optimizing a common
criterion. Factorial PD-clustering is based on Probabilistic Distance
clustering (PD-clustering). PD-clustering is an iterative, distribution free,
probabilistic, clustering method. Factor PD-clustering make a linear
transformation of original variables into a reduced number of orthogonal ones
using a common criterion with PD-Clustering. It is demonstrated that Tucker 3
decomposition allows to obtain this transformation. Factor PD-clustering makes
alternatively a Tucker 3 decomposition and a PD-clustering on transformed data
until convergence. This method could significantly improve the algorithm
performance and allows to work with large dataset, to improve the stability and
the robustness of the method."@2011
Laurens de Haan@http://arxiv.org/abs/1106.4149v2@On tail trend detection: modeling relative risk@"The climate change dispute is about changes over time of environmental
characteristics (such as rainfall). Some people say that a possible change is
not so much in the mean but rather in the extreme phenomena (that is, the
average rainfall may not change much but heavy storms may become more or less
frequent). The paper studies changes over time in the probability that some
high threshold is exceeded. The model is such that the threshold does not need
to be specified, the results hold for any high threshold. For simplicity a
certain linear trend is studied depending on one real parameter. Estimation and
testing procedures (is there a trend?) are developed. Simulation results are
presented. The method is applied to trends in heavy rainfall at 18 gauging
stations across Germany and The Netherlands. A tentative conclusion is that the
trend seems to depend on whether or not a station is close to the sea."@2011
Albert Klein Tank@http://arxiv.org/abs/1106.4149v2@On tail trend detection: modeling relative risk@"The climate change dispute is about changes over time of environmental
characteristics (such as rainfall). Some people say that a possible change is
not so much in the mean but rather in the extreme phenomena (that is, the
average rainfall may not change much but heavy storms may become more or less
frequent). The paper studies changes over time in the probability that some
high threshold is exceeded. The model is such that the threshold does not need
to be specified, the results hold for any high threshold. For simplicity a
certain linear trend is studied depending on one real parameter. Estimation and
testing procedures (is there a trend?) are developed. Simulation results are
presented. The method is applied to trends in heavy rainfall at 18 gauging
stations across Germany and The Netherlands. A tentative conclusion is that the
trend seems to depend on whether or not a station is close to the sea."@2011
Cláudia Neves@http://arxiv.org/abs/1106.4149v2@On tail trend detection: modeling relative risk@"The climate change dispute is about changes over time of environmental
characteristics (such as rainfall). Some people say that a possible change is
not so much in the mean but rather in the extreme phenomena (that is, the
average rainfall may not change much but heavy storms may become more or less
frequent). The paper studies changes over time in the probability that some
high threshold is exceeded. The model is such that the threshold does not need
to be specified, the results hold for any high threshold. For simplicity a
certain linear trend is studied depending on one real parameter. Estimation and
testing procedures (is there a trend?) are developed. Simulation results are
presented. The method is applied to trends in heavy rainfall at 18 gauging
stations across Germany and The Netherlands. A tentative conclusion is that the
trend seems to depend on whether or not a station is close to the sea."@2011
Laëtitia Comminges@http://arxiv.org/abs/1106.4293v4@"Tight conditions for consistency of variable selection in the context of
  high dimensionality"@"We address the issue of variable selection in the regression model with very
high ambient dimension, that is, when the number of variables is very large.
The main focus is on the situation where the number of relevant variables,
called intrinsic dimension, is much smaller than the ambient dimension d.
Without assuming any parametric form of the underlying regression function, we
get tight conditions making it possible to consistently estimate the set of
relevant variables. These conditions relate the intrinsic dimension to the
ambient dimension and to the sample size. The procedure that is provably
consistent under these tight conditions is based on comparing quadratic
functionals of the empirical Fourier coefficients with appropriately chosen
threshold values. The asymptotic analysis reveals the presence of two quite
different re gimes. The first regime is when the intrinsic dimension is fixed.
In this case the situation in nonparametric regression is the same as in linear
regression, that is, consistent variable selection is possible if and only if
log d is small compared to the sample size n. The picture is different in the
second regime, that is, when the number of relevant variables denoted by s
tends to infinity as $n\to\infty$. Then we prove that consistent variable
selection in nonparametric set-up is possible only if s+loglog d is small
compared to log n. We apply these results to derive minimax separation rates
for the problem of variable"@2011
Arnak Dalalyan@http://arxiv.org/abs/1106.4293v4@"Tight conditions for consistency of variable selection in the context of
  high dimensionality"@"We address the issue of variable selection in the regression model with very
high ambient dimension, that is, when the number of variables is very large.
The main focus is on the situation where the number of relevant variables,
called intrinsic dimension, is much smaller than the ambient dimension d.
Without assuming any parametric form of the underlying regression function, we
get tight conditions making it possible to consistently estimate the set of
relevant variables. These conditions relate the intrinsic dimension to the
ambient dimension and to the sample size. The procedure that is provably
consistent under these tight conditions is based on comparing quadratic
functionals of the empirical Fourier coefficients with appropriately chosen
threshold values. The asymptotic analysis reveals the presence of two quite
different re gimes. The first regime is when the intrinsic dimension is fixed.
In this case the situation in nonparametric regression is the same as in linear
regression, that is, consistent variable selection is possible if and only if
log d is small compared to the sample size n. The picture is different in the
second regime, that is, when the number of relevant variables denoted by s
tends to infinity as $n\to\infty$. Then we prove that consistent variable
selection in nonparametric set-up is possible only if s+loglog d is small
compared to log n. We apply these results to derive minimax separation rates
for the problem of variable"@2011
Séphane Gaïffas@http://arxiv.org/abs/1106.4662v2@High-dimensional additive hazard models and the Lasso@"We consider a general high-dimensional additive hazard model in a
non-asymptotic setting, including regression for censored-data. In this
context, we consider a Lasso estimator with a fully data-driven $\ell_1$
penalization, which is tuned for the estimation problem at hand. We prove sharp
oracle inequalities for this estimator. Our analysis involves a new
""data-driven"" Bernstein's inequality, that is of independent interest, where
the predictable variation is replaced by the optional variation."@2011
Agathe Guilloux@http://arxiv.org/abs/1106.4662v2@High-dimensional additive hazard models and the Lasso@"We consider a general high-dimensional additive hazard model in a
non-asymptotic setting, including regression for censored-data. In this
context, we consider a Lasso estimator with a fully data-driven $\ell_1$
penalization, which is tuned for the estimation problem at hand. We prove sharp
oracle inequalities for this estimator. Our analysis involves a new
""data-driven"" Bernstein's inequality, that is of independent interest, where
the predictable variation is replaced by the optional variation."@2011
Guillermo Henry@http://arxiv.org/abs/1106.4763v1@k-Nearest neighbor density estimation on Riemannian Manifolds@"In this paper, we consider a k-nearest neighbor kernel type estimator when
the random variables belong in a Riemannian manifolds. We study asymptotic
properties such as the consistency and the asymptotic distribution. A
simulation study is also consider to evaluate the performance of the proposal.
Finally, to illustrate the potential applications of the proposed estimator, we
analyzed two real example where two different manifolds are considered."@2011
Andrés Muñoz@http://arxiv.org/abs/1106.4763v1@k-Nearest neighbor density estimation on Riemannian Manifolds@"In this paper, we consider a k-nearest neighbor kernel type estimator when
the random variables belong in a Riemannian manifolds. We study asymptotic
properties such as the consistency and the asymptotic distribution. A
simulation study is also consider to evaluate the performance of the proposal.
Finally, to illustrate the potential applications of the proposed estimator, we
analyzed two real example where two different manifolds are considered."@2011
Daniela Rodriguez@http://arxiv.org/abs/1106.4763v1@k-Nearest neighbor density estimation on Riemannian Manifolds@"In this paper, we consider a k-nearest neighbor kernel type estimator when
the random variables belong in a Riemannian manifolds. We study asymptotic
properties such as the consistency and the asymptotic distribution. A
simulation study is also consider to evaluate the performance of the proposal.
Finally, to illustrate the potential applications of the proposed estimator, we
analyzed two real example where two different manifolds are considered."@2011
Olivier Wintenberger@http://arxiv.org/abs/1106.4983v3@"Parametric inference and forecasting in continuously invertible
  volatility models"@"We introduce the notion of continuously invertible volatility models that
relies on some Lyapunov condition and some regularity condition. We show that
it is almost equivalent to the ability of the volatilities forecasting using
the parametric inference approach based on the SRE given in [16]. Under very
weak assumptions, we prove the strong consistency and the asymptotic normality
of the parametric inference. Based on this parametric estimation, a natural
strongly consistent forecast of the volatility is given. We apply successfully
this approach to recover known results on univariate and multivariate GARCH
type models and to the EGARCH(1,1) model. We prove the strong consistency of
the forecasting as soon as the model is invertible and the asymptotic normality
of the parametric inference as soon as the limiting variance exists. Finally,
we give some encouraging empirical results of our approach on simulations and
real data."@2011
Sixiang Cai@http://arxiv.org/abs/1106.4983v3@"Parametric inference and forecasting in continuously invertible
  volatility models"@"We introduce the notion of continuously invertible volatility models that
relies on some Lyapunov condition and some regularity condition. We show that
it is almost equivalent to the ability of the volatilities forecasting using
the parametric inference approach based on the SRE given in [16]. Under very
weak assumptions, we prove the strong consistency and the asymptotic normality
of the parametric inference. Based on this parametric estimation, a natural
strongly consistent forecast of the volatility is given. We apply successfully
this approach to recover known results on univariate and multivariate GARCH
type models and to the EGARCH(1,1) model. We prove the strong consistency of
the forecasting as soon as the model is invertible and the asymptotic normality
of the parametric inference as soon as the limiting variance exists. Finally,
we give some encouraging empirical results of our approach on simulations and
real data."@2011
Shalosh B. Ekhad@http://arxiv.org/abs/1106.5531v1@Balls in Boxes: Variations on a Theme of Warren Ewens and Herbert Wilf@"We comment on, elaborate, and extend the work of Warren Ewens and Herbert
Wilf, described in their http://www.pnas.org/content/104/27/11189.full.pdf
about the maximum in balls-and-boxes problem. In particular we meta-apply their
ingenious method to show that it is not really needed, and that one is better
off using the so-called Poisson Approximation, at least in applications to the
real world, because extremely unlikely events mever happen in real life. This
article is accompanied by the Maple package
http://www.math.rutgers.edu/~zeilberg/tokhniot/BallsInBoxes"">BallsInBoxes."@2011
Doron Zeilberger@http://arxiv.org/abs/1106.5531v1@Balls in Boxes: Variations on a Theme of Warren Ewens and Herbert Wilf@"We comment on, elaborate, and extend the work of Warren Ewens and Herbert
Wilf, described in their http://www.pnas.org/content/104/27/11189.full.pdf
about the maximum in balls-and-boxes problem. In particular we meta-apply their
ingenious method to show that it is not really needed, and that one is better
off using the so-called Poisson Approximation, at least in applications to the
real world, because extremely unlikely events mever happen in real life. This
article is accompanied by the Maple package
http://www.math.rutgers.edu/~zeilberg/tokhniot/BallsInBoxes"">BallsInBoxes."@2011
Zheng-Yan Lin@http://arxiv.org/abs/1106.5547v2@Nonparametric Estimation of Second-Order Jump-Diffusion Model@"We study the nonparametric estimators of the infinitesimal coefficients of
the second-order jump-diffusion models. Under the mild conditions, we obtain
the weak consistency and the asymptotic normalities of the estimators."@2011
Yu-Ping Song@http://arxiv.org/abs/1106.5547v2@Nonparametric Estimation of Second-Order Jump-Diffusion Model@"We study the nonparametric estimators of the infinitesimal coefficients of
the second-order jump-diffusion models. Under the mild conditions, we obtain
the weak consistency and the asymptotic normalities of the estimators."@2011
Han-Chao Wang@http://arxiv.org/abs/1106.5547v2@Nonparametric Estimation of Second-Order Jump-Diffusion Model@"We study the nonparametric estimators of the infinitesimal coefficients of
the second-order jump-diffusion models. Under the mild conditions, we obtain
the weak consistency and the asymptotic normalities of the estimators."@2011
Piotr Graczyk@http://arxiv.org/abs/1107.0147v1@Riesz measures and Wishart laws associated to quadratic maps@"We introduce a natural definition of Riesz measures and Wishart laws
associated to an $\Omega$-positive (virtual) quadratic map, where $\Omega
\subset \real^n$ is a regular open convex cone. We give a general formula for
moments of the Wishart laws. Moreover, if the quadratic map has an equivariance
property under the action of a linear group acting on the cone $\Omega$
transitively, then the associated Riesz measure and Wishart law are described
explicitly by making use of theory of relatively invariant distributions on
homogeneous cones."@2011
Ishi Hideyuki@http://arxiv.org/abs/1107.0147v1@Riesz measures and Wishart laws associated to quadratic maps@"We introduce a natural definition of Riesz measures and Wishart laws
associated to an $\Omega$-positive (virtual) quadratic map, where $\Omega
\subset \real^n$ is a regular open convex cone. We give a general formula for
moments of the Wishart laws. Moreover, if the quadratic map has an equivariance
property under the action of a linear group acting on the cone $\Omega$
transitively, then the associated Riesz measure and Wishart law are described
explicitly by making use of theory of relatively invariant distributions on
homogeneous cones."@2011
Lutz Duembgen@http://arxiv.org/abs/1107.0417v3@On Low-Dimensional Projections of High-Dimensional Distributions@"Let $P$ be a probability distribution on $q$-dimensional space. The so-called
Diaconis-Freedman effect means that for a fixed dimension $d << q$, most
$d$-dimensional projections of $P$ look like a scale mixture of spherically
symmetric Gaussian distributions. The present paper provides necessary and
sufficient conditions for this phenomenon in a suitable asymptotic framework
with increasing dimension $q$. It turns out, that the conditions formulated by
Diaconis and Freedman (1984) are not only sufficient but necessary as well.
Moreover, letting $\hat{P}$ be the empirical distribution of $n$ independent
random vectors with distribution $P$, we investigate the behavior of the
empirical process $\sqrt{n}(\hat{P} - P)$ under random projections, conditional
on $\hat{P}$."@2011
Perla Zerial@http://arxiv.org/abs/1107.0417v3@On Low-Dimensional Projections of High-Dimensional Distributions@"Let $P$ be a probability distribution on $q$-dimensional space. The so-called
Diaconis-Freedman effect means that for a fixed dimension $d << q$, most
$d$-dimensional projections of $P$ look like a scale mixture of spherically
symmetric Gaussian distributions. The present paper provides necessary and
sufficient conditions for this phenomenon in a suitable asymptotic framework
with increasing dimension $q$. It turns out, that the conditions formulated by
Diaconis and Freedman (1984) are not only sufficient but necessary as well.
Moreover, letting $\hat{P}$ be the empirical distribution of $n$ independent
random vectors with distribution $P$, we investigate the behavior of the
empirical process $\sqrt{n}(\hat{P} - P)$ under random projections, conditional
on $\hat{P}$."@2011
Jean-François Coeurjolly@http://arxiv.org/abs/1107.0540v1@Expectiles for subordinated Gaussian processes with applications@"In this paper, we introduce a new class of estimators of the Hurst exponent
of the fractional Brownian motion (fBm) process. These estimators are based on
sample expectiles of discrete variations of a sample path of the fBm process.
In order to derive the statistical properties of the proposed estimators, we
establish asymptotic results for sample expectiles of subordinated stationary
Gaussian processes with unit variance and correlation function satisfying
$\rho(i)\sim \kappa|i|^{-\alpha}$ ($\kappa\in \RR$) with $\alpha>0$. Via a
simulation study, we demonstrate the relevance of the expectile-based
estimation method and show that the suggested estimators are more robust to
data rounding than their sample quantile-based counterparts."@2011
Hedi Kortas@http://arxiv.org/abs/1107.0540v1@Expectiles for subordinated Gaussian processes with applications@"In this paper, we introduce a new class of estimators of the Hurst exponent
of the fractional Brownian motion (fBm) process. These estimators are based on
sample expectiles of discrete variations of a sample path of the fBm process.
In order to derive the statistical properties of the proposed estimators, we
establish asymptotic results for sample expectiles of subordinated stationary
Gaussian processes with unit variance and correlation function satisfying
$\rho(i)\sim \kappa|i|^{-\alpha}$ ($\kappa\in \RR$) with $\alpha>0$. Via a
simulation study, we demonstrate the relevance of the expectile-based
estimation method and show that the suggested estimators are more robust to
data rounding than their sample quantile-based counterparts."@2011
Apostolos Batsidis@http://arxiv.org/abs/1107.0677v3@"Change point analysis of an exponential model based on Phi-divergence
  test-statistics: simulated critical points case"@"Recently Batsidis \textit{et al.} (2011) have presented a new procedure based
on divergence measures for testing the hypothesis of the existence of a change
point in exponential populations. A simulation study was carried out, in this
paper, using the asymptotic critical points obtained from the asymptotic
distribution of the new test statistics introduced there. The main purpose of
this paper is to study the behavior of the test statistics introduced in the
cited paper of Batsidis \textit{et al.} (2011), using simulated critical
points."@2011
Nirian Martín@http://arxiv.org/abs/1107.0677v3@"Change point analysis of an exponential model based on Phi-divergence
  test-statistics: simulated critical points case"@"Recently Batsidis \textit{et al.} (2011) have presented a new procedure based
on divergence measures for testing the hypothesis of the existence of a change
point in exponential populations. A simulation study was carried out, in this
paper, using the asymptotic critical points obtained from the asymptotic
distribution of the new test statistics introduced there. The main purpose of
this paper is to study the behavior of the test statistics introduced in the
cited paper of Batsidis \textit{et al.} (2011), using simulated critical
points."@2011
Leandro Pardo@http://arxiv.org/abs/1107.0677v3@"Change point analysis of an exponential model based on Phi-divergence
  test-statistics: simulated critical points case"@"Recently Batsidis \textit{et al.} (2011) have presented a new procedure based
on divergence measures for testing the hypothesis of the existence of a change
point in exponential populations. A simulation study was carried out, in this
paper, using the asymptotic critical points obtained from the asymptotic
distribution of the new test statistics introduced there. The main purpose of
this paper is to study the behavior of the test statistics introduced in the
cited paper of Batsidis \textit{et al.} (2011), using simulated critical
points."@2011
Konstantinos Zografos@http://arxiv.org/abs/1107.0677v3@"Change point analysis of an exponential model based on Phi-divergence
  test-statistics: simulated critical points case"@"Recently Batsidis \textit{et al.} (2011) have presented a new procedure based
on divergence measures for testing the hypothesis of the existence of a change
point in exponential populations. A simulation study was carried out, in this
paper, using the asymptotic critical points obtained from the asymptotic
distribution of the new test statistics introduced there. The main purpose of
this paper is to study the behavior of the test statistics introduced in the
cited paper of Batsidis \textit{et al.} (2011), using simulated critical
points."@2011
Apostolos Batsidis@http://arxiv.org/abs/1107.0864v1@"A procedure for the change point problem in parametric models based on
  phi-divergence test-statistics"@"This paper studies the change point problem for a general parametric,
univariate or multivariate family of distributions. An information theoretic
procedure is developed which is based on general divergence measures for
testing the hypothesis of the existence of a change. For comparing the accuracy
of the new test-statistic a simulation study is performed for the special case
of a univariate discrete model. Finally, the procedure proposed in this paper
is illustrated through a classical change-point example."@2011
Nirian Martín@http://arxiv.org/abs/1107.0864v1@"A procedure for the change point problem in parametric models based on
  phi-divergence test-statistics"@"This paper studies the change point problem for a general parametric,
univariate or multivariate family of distributions. An information theoretic
procedure is developed which is based on general divergence measures for
testing the hypothesis of the existence of a change. For comparing the accuracy
of the new test-statistic a simulation study is performed for the special case
of a univariate discrete model. Finally, the procedure proposed in this paper
is illustrated through a classical change-point example."@2011
Leandro Pardo@http://arxiv.org/abs/1107.0864v1@"A procedure for the change point problem in parametric models based on
  phi-divergence test-statistics"@"This paper studies the change point problem for a general parametric,
univariate or multivariate family of distributions. An information theoretic
procedure is developed which is based on general divergence measures for
testing the hypothesis of the existence of a change. For comparing the accuracy
of the new test-statistic a simulation study is performed for the special case
of a univariate discrete model. Finally, the procedure proposed in this paper
is illustrated through a classical change-point example."@2011
Konstantinos Zografos@http://arxiv.org/abs/1107.0864v1@"A procedure for the change point problem in parametric models based on
  phi-divergence test-statistics"@"This paper studies the change point problem for a general parametric,
univariate or multivariate family of distributions. An information theoretic
procedure is developed which is based on general divergence measures for
testing the hypothesis of the existence of a change. For comparing the accuracy
of the new test-statistic a simulation study is performed for the special case
of a univariate discrete model. Finally, the procedure proposed in this paper
is illustrated through a classical change-point example."@2011
Alexandre Lung-Yut-Fong@http://arxiv.org/abs/1107.1971v3@"Homogeneity and change-point detection tests for multivariate data using
  rank statistics"@"Detecting and locating changes in highly multivariate data is a major concern
in several current statistical applications. In this context, the first
contribution of the paper is a novel non-parametric two-sample homogeneity test
for multivariate data based on the well-known Wilcoxon rank statistic. The
proposed two-sample homogeneity test statistic can be extended to deal with
ordinal or censored data as well as to test for the homogeneity of more than
two samples. The second contribution of the paper concerns the use of the
proposed test statistic to perform retrospective change-point analysis. It is
first shown that the approach is computationally feasible even when looking for
a large number of change-points thanks to the use of dynamic programming.
Computable asymptotic $p$-values for the test are then provided in the case
where a single potential change-point is to be detected. Compared to available
alternatives, the proposed approach appears to be very reliable and robust.
This is particularly true in situations where the data is contaminated by
outliers or corrupted by noise and where the potential changes only affect
subsets of the coordinates of the data."@2011
Céline Lévy-Leduc@http://arxiv.org/abs/1107.1971v3@"Homogeneity and change-point detection tests for multivariate data using
  rank statistics"@"Detecting and locating changes in highly multivariate data is a major concern
in several current statistical applications. In this context, the first
contribution of the paper is a novel non-parametric two-sample homogeneity test
for multivariate data based on the well-known Wilcoxon rank statistic. The
proposed two-sample homogeneity test statistic can be extended to deal with
ordinal or censored data as well as to test for the homogeneity of more than
two samples. The second contribution of the paper concerns the use of the
proposed test statistic to perform retrospective change-point analysis. It is
first shown that the approach is computationally feasible even when looking for
a large number of change-points thanks to the use of dynamic programming.
Computable asymptotic $p$-values for the test are then provided in the case
where a single potential change-point is to be detected. Compared to available
alternatives, the proposed approach appears to be very reliable and robust.
This is particularly true in situations where the data is contaminated by
outliers or corrupted by noise and where the potential changes only affect
subsets of the coordinates of the data."@2011
Olivier Cappé@http://arxiv.org/abs/1107.1971v3@"Homogeneity and change-point detection tests for multivariate data using
  rank statistics"@"Detecting and locating changes in highly multivariate data is a major concern
in several current statistical applications. In this context, the first
contribution of the paper is a novel non-parametric two-sample homogeneity test
for multivariate data based on the well-known Wilcoxon rank statistic. The
proposed two-sample homogeneity test statistic can be extended to deal with
ordinal or censored data as well as to test for the homogeneity of more than
two samples. The second contribution of the paper concerns the use of the
proposed test statistic to perform retrospective change-point analysis. It is
first shown that the approach is computationally feasible even when looking for
a large number of change-points thanks to the use of dynamic programming.
Computable asymptotic $p$-values for the test are then provided in the case
where a single potential change-point is to be detected. Compared to available
alternatives, the proposed approach appears to be very reliable and robust.
This is particularly true in situations where the data is contaminated by
outliers or corrupted by noise and where the potential changes only affect
subsets of the coordinates of the data."@2011
Laurent Gardes@http://arxiv.org/abs/1107.2261v4@Functional kernel estimators of large conditional quantiles@"We address the estimation of conditional quantiles when the covariate is
functional and when the order of the quantiles converges to one as the sample
size increases. In a first time, we investigate to what extent these large
conditional quantiles can still be estimated through a functional kernel
estimator of the conditional survival function. Sufficient conditions on the
rate of convergence of their order to one are provided to obtain asymptotically
Gaussian distributed estimators. In a second time, basing on these result, a
functional Weissman estimator is derived, permitting to estimate large
conditional quantiles of arbitrary large order. These results are illustrated
on finite sample situations."@2011
Stéphane Girard@http://arxiv.org/abs/1107.2261v4@Functional kernel estimators of large conditional quantiles@"We address the estimation of conditional quantiles when the covariate is
functional and when the order of the quantiles converges to one as the sample
size increases. In a first time, we investigate to what extent these large
conditional quantiles can still be estimated through a functional kernel
estimator of the conditional survival function. Sufficient conditions on the
rate of convergence of their order to one are provided to obtain asymptotically
Gaussian distributed estimators. In a second time, basing on these result, a
functional Weissman estimator is derived, permitting to estimate large
conditional quantiles of arbitrary large order. These results are illustrated
on finite sample situations."@2011
Shan Luo@http://arxiv.org/abs/1107.2502v1@"Extended BIC for linear regression models with diverging number of
  relevant features and high or ultra-high feature spaces"@"In many conventional scientific investigations with high or ultra-high
dimensional feature spaces, the relevant features, though sparse, are large in
number compared with classical statistical problems, and the magnitude of their
effects tapers off. It is reasonable to model the number of relevant features
as a diverging sequence when sample size increases. In this article, we
investigate the properties of the extended Bayes information criterion (EBIC)
(Chen and Chen, 2008) for feature selection in linear regression models with
diverging number of relevant features in high or ultra-high dimensional feature
spaces. The selection consistency of the EBIC in this situation is established.
The application of EBIC to feature selection is considered in a two-stage
feature selection procedure. Simulation studies are conducted to demonstrate
the performance of the EBIC together with the two-stage feature selection
procedure in finite sample cases."@2011
Zehua Chen@http://arxiv.org/abs/1107.2502v1@"Extended BIC for linear regression models with diverging number of
  relevant features and high or ultra-high feature spaces"@"In many conventional scientific investigations with high or ultra-high
dimensional feature spaces, the relevant features, though sparse, are large in
number compared with classical statistical problems, and the magnitude of their
effects tapers off. It is reasonable to model the number of relevant features
as a diverging sequence when sample size increases. In this article, we
investigate the properties of the extended Bayes information criterion (EBIC)
(Chen and Chen, 2008) for feature selection in linear regression models with
diverging number of relevant features in high or ultra-high dimensional feature
spaces. The selection consistency of the EBIC in this situation is established.
The application of EBIC to feature selection is considered in a two-stage
feature selection procedure. Simulation studies are conducted to demonstrate
the performance of the EBIC together with the two-stage feature selection
procedure in finite sample cases."@2011
Gersende Fort@http://arxiv.org/abs/1107.2574v1@A central limit theorem for adaptive and interacting Markov chains@"Adaptive and interacting Markov Chains Monte Carlo (MCMC) algorithms are a
novel class of non-Markovian algorithms aimed at improving the simulation
efficiency for complicated target distributions. In this paper, we study a
general (non-Markovian) simulation framework covering both the adaptive and
interacting MCMC algorithms. We establish a Central Limit Theorem for additive
functionals of unbounded functions under a set of verifiable conditions, and
identify the asymptotic variance. Our result extends all the results reported
so far. An application to the interacting tempering algorithm (a simplified
version of the equi-energy sampler) is presented to support our claims."@2011
Eric Moulines@http://arxiv.org/abs/1107.2574v1@A central limit theorem for adaptive and interacting Markov chains@"Adaptive and interacting Markov Chains Monte Carlo (MCMC) algorithms are a
novel class of non-Markovian algorithms aimed at improving the simulation
efficiency for complicated target distributions. In this paper, we study a
general (non-Markovian) simulation framework covering both the adaptive and
interacting MCMC algorithms. We establish a Central Limit Theorem for additive
functionals of unbounded functions under a set of verifiable conditions, and
identify the asymptotic variance. Our result extends all the results reported
so far. An application to the interacting tempering algorithm (a simplified
version of the equi-energy sampler) is presented to support our claims."@2011
Pierre Priouret@http://arxiv.org/abs/1107.2574v1@A central limit theorem for adaptive and interacting Markov chains@"Adaptive and interacting Markov Chains Monte Carlo (MCMC) algorithms are a
novel class of non-Markovian algorithms aimed at improving the simulation
efficiency for complicated target distributions. In this paper, we study a
general (non-Markovian) simulation framework covering both the adaptive and
interacting MCMC algorithms. We establish a Central Limit Theorem for additive
functionals of unbounded functions under a set of verifiable conditions, and
identify the asymptotic variance. Our result extends all the results reported
so far. An application to the interacting tempering algorithm (a simplified
version of the equi-energy sampler) is presented to support our claims."@2011
Pierre Vandekerkhove@http://arxiv.org/abs/1107.2574v1@A central limit theorem for adaptive and interacting Markov chains@"Adaptive and interacting Markov Chains Monte Carlo (MCMC) algorithms are a
novel class of non-Markovian algorithms aimed at improving the simulation
efficiency for complicated target distributions. In this paper, we study a
general (non-Markovian) simulation framework covering both the adaptive and
interacting MCMC algorithms. We establish a Central Limit Theorem for additive
functionals of unbounded functions under a set of verifiable conditions, and
identify the asymptotic variance. Our result extends all the results reported
so far. An application to the interacting tempering algorithm (a simplified
version of the equi-energy sampler) is presented to support our claims."@2011
Gersende Fort@http://arxiv.org/abs/1107.2576v1@"A simple variance inequality for U-statistics of a Markov chain with
  applications"@"We establish a simple variance inequality for U-statistics whose underlying
sequence of random variables is an ergodic Markov Chain. The constants in this
inequality are explicit and depend on computable bounds on the mixing rate of
the Markov Chain. We apply this result to derive the strong law of large number
for U-statistics of a Markov Chain under conditions which are close from being
optimal."@2011
Eric Moulines@http://arxiv.org/abs/1107.2576v1@"A simple variance inequality for U-statistics of a Markov chain with
  applications"@"We establish a simple variance inequality for U-statistics whose underlying
sequence of random variables is an ergodic Markov Chain. The constants in this
inequality are explicit and depend on computable bounds on the mixing rate of
the Markov Chain. We apply this result to derive the strong law of large number
for U-statistics of a Markov Chain under conditions which are close from being
optimal."@2011
Pierre Priouret@http://arxiv.org/abs/1107.2576v1@"A simple variance inequality for U-statistics of a Markov chain with
  applications"@"We establish a simple variance inequality for U-statistics whose underlying
sequence of random variables is an ergodic Markov Chain. The constants in this
inequality are explicit and depend on computable bounds on the mixing rate of
the Markov Chain. We apply this result to derive the strong law of large number
for U-statistics of a Markov Chain under conditions which are close from being
optimal."@2011
Pierre Vandekerkhove@http://arxiv.org/abs/1107.2576v1@"A simple variance inequality for U-statistics of a Markov chain with
  applications"@"We establish a simple variance inequality for U-statistics whose underlying
sequence of random variables is an ergodic Markov Chain. The constants in this
inequality are explicit and depend on computable bounds on the mixing rate of
the Markov Chain. We apply this result to derive the strong law of large number
for U-statistics of a Markov Chain under conditions which are close from being
optimal."@2011
Paweł Hitczenko@http://arxiv.org/abs/1107.2753v1@Renorming divergent perpetuities@"We consider a sequence of random variables $(R_n)$ defined by the recurrence
$R_n=Q_n+M_nR_{n-1}$, $n\ge1$, where $R_0$ is arbitrary and $(Q_n,M_n)$,
$n\ge1$, are i.i.d. copies of a two-dimensional random vector $(Q,M)$, and
$(Q_n,M_n)$ is independent of $R_{n-1}$. It is well known that if $E{\ln}|M|<0$
and $E{\ln^+}|Q|<\infty$, then the sequence $(R_n)$ converges in distribution
to a random variable $R$ given by
$R\stackrel{d}{=}\sum_{k=1}^{\infty}Q_k\prod_{j=1}^{k-1}M_j$, and usually
referred to as perpetuity. In this paper we consider a situation in which the
sequence $(R_n)$ itself does not converge. We assume that $E{\ln}|M|$ exists
but that it is non-negative and we ask if in this situation the sequence
$(R_n)$, after suitable normalization, converges in distribution to a
non-degenerate limit."@2011
Jacek Wesołowski@http://arxiv.org/abs/1107.2753v1@Renorming divergent perpetuities@"We consider a sequence of random variables $(R_n)$ defined by the recurrence
$R_n=Q_n+M_nR_{n-1}$, $n\ge1$, where $R_0$ is arbitrary and $(Q_n,M_n)$,
$n\ge1$, are i.i.d. copies of a two-dimensional random vector $(Q,M)$, and
$(Q_n,M_n)$ is independent of $R_{n-1}$. It is well known that if $E{\ln}|M|<0$
and $E{\ln^+}|Q|<\infty$, then the sequence $(R_n)$ converges in distribution
to a random variable $R$ given by
$R\stackrel{d}{=}\sum_{k=1}^{\infty}Q_k\prod_{j=1}^{k-1}M_j$, and usually
referred to as perpetuity. In this paper we consider a situation in which the
sequence $(R_n)$ itself does not converge. We assume that $E{\ln}|M|$ exists
but that it is non-negative and we ask if in this situation the sequence
$(R_n)$, after suitable normalization, converges in distribution to a
non-degenerate limit."@2011
Felix Abramovich@http://arxiv.org/abs/1107.2766v3@Laplace deconvolution with noisy observations@"In the present paper we consider Laplace deconvolution for discrete noisy
data observed on the interval whose length may increase with a sample size.
Although this problem arises in a variety of applications, to the best of our
knowledge, it has been given very little attention by the statistical
community. Our objective is to fill this gap and provide statistical treatment
of Laplace deconvolution problem with noisy discrete data. The main
contribution of the paper is explicit construction of an asymptotically
rate-optimal (in the minimax sense) Laplace deconvolution estimator which is
adaptive to the regularity of the unknown function. We show that the original
Laplace deconvolution problem can be reduced to nonparametric estimation of a
regression function and its derivatives on the interval of growing length T_n.
Whereas the forms of the estimators remain standard, the choices of the
parameters and the minimax convergence rates, which are expressed in terms of
T_n^2/n in this case, are affected by the asymptotic growth of the length of
the interval.
  We derive an adaptive kernel estimator of the function of interest, and
establish its asymptotic minimaxity over a range of Sobolev classes. We
illustrate the theory by examples of construction of explicit expressions of
Laplace deconvolution estimators. A simulation study shows that, in addition to
providing asymptotic optimality as the number of observations turns to
infinity, the proposed estimator demonstrates good performance in finite sample
examples."@2011
Marianna Pensky@http://arxiv.org/abs/1107.2766v3@Laplace deconvolution with noisy observations@"In the present paper we consider Laplace deconvolution for discrete noisy
data observed on the interval whose length may increase with a sample size.
Although this problem arises in a variety of applications, to the best of our
knowledge, it has been given very little attention by the statistical
community. Our objective is to fill this gap and provide statistical treatment
of Laplace deconvolution problem with noisy discrete data. The main
contribution of the paper is explicit construction of an asymptotically
rate-optimal (in the minimax sense) Laplace deconvolution estimator which is
adaptive to the regularity of the unknown function. We show that the original
Laplace deconvolution problem can be reduced to nonparametric estimation of a
regression function and its derivatives on the interval of growing length T_n.
Whereas the forms of the estimators remain standard, the choices of the
parameters and the minimax convergence rates, which are expressed in terms of
T_n^2/n in this case, are affected by the asymptotic growth of the length of
the interval.
  We derive an adaptive kernel estimator of the function of interest, and
establish its asymptotic minimaxity over a range of Sobolev classes. We
illustrate the theory by examples of construction of explicit expressions of
Laplace deconvolution estimators. A simulation study shows that, in addition to
providing asymptotic optimality as the number of observations turns to
infinity, the proposed estimator demonstrates good performance in finite sample
examples."@2011
Yves Rozenholc@http://arxiv.org/abs/1107.2766v3@Laplace deconvolution with noisy observations@"In the present paper we consider Laplace deconvolution for discrete noisy
data observed on the interval whose length may increase with a sample size.
Although this problem arises in a variety of applications, to the best of our
knowledge, it has been given very little attention by the statistical
community. Our objective is to fill this gap and provide statistical treatment
of Laplace deconvolution problem with noisy discrete data. The main
contribution of the paper is explicit construction of an asymptotically
rate-optimal (in the minimax sense) Laplace deconvolution estimator which is
adaptive to the regularity of the unknown function. We show that the original
Laplace deconvolution problem can be reduced to nonparametric estimation of a
regression function and its derivatives on the interval of growing length T_n.
Whereas the forms of the estimators remain standard, the choices of the
parameters and the minimax convergence rates, which are expressed in terms of
T_n^2/n in this case, are affected by the asymptotic growth of the length of
the interval.
  We derive an adaptive kernel estimator of the function of interest, and
establish its asymptotic minimaxity over a range of Sobolev classes. We
illustrate the theory by examples of construction of explicit expressions of
Laplace deconvolution estimators. A simulation study shows that, in addition to
providing asymptotic optimality as the number of observations turns to
infinity, the proposed estimator demonstrates good performance in finite sample
examples."@2011
Weidong Liu@http://arxiv.org/abs/1107.2802v1@On non-stationary threshold autoregressive models@"In this paper we study the limiting distributions of the least-squares
estimators for the non-stationary first-order threshold autoregressive (TAR(1))
model. It is proved that the limiting behaviors of the TAR(1) process are very
different from those of the classical unit root model and the explosive AR(1)."@2011
Shiqing Ling@http://arxiv.org/abs/1107.2802v1@On non-stationary threshold autoregressive models@"In this paper we study the limiting distributions of the least-squares
estimators for the non-stationary first-order threshold autoregressive (TAR(1))
model. It is proved that the limiting behaviors of the TAR(1) process are very
different from those of the classical unit root model and the explosive AR(1)."@2011
Qi-Man Shao@http://arxiv.org/abs/1107.2802v1@On non-stationary threshold autoregressive models@"In this paper we study the limiting distributions of the least-squares
estimators for the non-stationary first-order threshold autoregressive (TAR(1))
model. It is proved that the limiting behaviors of the TAR(1) process are very
different from those of the classical unit root model and the explosive AR(1)."@2011
Christophe Andrieu@http://arxiv.org/abs/1107.3046v1@On nonlinear Markov chain Monte Carlo@"Let $\mathscr{P}(E)$ be the space of probability measures on a measurable
space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov
chain Monte Carlo (MCMC) methods for simulating from a probability measure
$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:
Genealogical and Interacting Particle Systems with Applications (2004)
Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be
constructed to, in some sense, improve over MCMC methods. However, such
nonlinear kernels cannot be simulated exactly, so approximations of the
nonlinear kernels are constructed using auxiliary or potentially
self-interacting chains. Several nonlinear kernels are presented and it is
demonstrated that, under some conditions, the associated approximations exhibit
a strong law of large numbers; our proof technique is via the Poisson equation
and Foster--Lyapunov conditions. We investigate the performance of our
approximations with some simulations."@2011
Ajay Jasra@http://arxiv.org/abs/1107.3046v1@On nonlinear Markov chain Monte Carlo@"Let $\mathscr{P}(E)$ be the space of probability measures on a measurable
space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov
chain Monte Carlo (MCMC) methods for simulating from a probability measure
$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:
Genealogical and Interacting Particle Systems with Applications (2004)
Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be
constructed to, in some sense, improve over MCMC methods. However, such
nonlinear kernels cannot be simulated exactly, so approximations of the
nonlinear kernels are constructed using auxiliary or potentially
self-interacting chains. Several nonlinear kernels are presented and it is
demonstrated that, under some conditions, the associated approximations exhibit
a strong law of large numbers; our proof technique is via the Poisson equation
and Foster--Lyapunov conditions. We investigate the performance of our
approximations with some simulations."@2011
Arnaud Doucet@http://arxiv.org/abs/1107.3046v1@On nonlinear Markov chain Monte Carlo@"Let $\mathscr{P}(E)$ be the space of probability measures on a measurable
space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov
chain Monte Carlo (MCMC) methods for simulating from a probability measure
$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:
Genealogical and Interacting Particle Systems with Applications (2004)
Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be
constructed to, in some sense, improve over MCMC methods. However, such
nonlinear kernels cannot be simulated exactly, so approximations of the
nonlinear kernels are constructed using auxiliary or potentially
self-interacting chains. Several nonlinear kernels are presented and it is
demonstrated that, under some conditions, the associated approximations exhibit
a strong law of large numbers; our proof technique is via the Poisson equation
and Foster--Lyapunov conditions. We investigate the performance of our
approximations with some simulations."@2011
Pierre Del Moral@http://arxiv.org/abs/1107.3046v1@On nonlinear Markov chain Monte Carlo@"Let $\mathscr{P}(E)$ be the space of probability measures on a measurable
space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov
chain Monte Carlo (MCMC) methods for simulating from a probability measure
$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:
Genealogical and Interacting Particle Systems with Applications (2004)
Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be
constructed to, in some sense, improve over MCMC methods. However, such
nonlinear kernels cannot be simulated exactly, so approximations of the
nonlinear kernels are constructed using auxiliary or potentially
self-interacting chains. Several nonlinear kernels are presented and it is
demonstrated that, under some conditions, the associated approximations exhibit
a strong law of large numbers; our proof technique is via the Poisson equation
and Foster--Lyapunov conditions. We investigate the performance of our
approximations with some simulations."@2011
Péter Kevei@http://arxiv.org/abs/1107.3365v1@A note on a maximal Bernstein inequality@"We show somewhat unexpectedly that whenever a general Bernstein-type maximal
inequality holds for partial sums of a sequence of random variables, a maximal
form of the inequality is also valid."@2011
David M. Mason@http://arxiv.org/abs/1107.3365v1@A note on a maximal Bernstein inequality@"We show somewhat unexpectedly that whenever a general Bernstein-type maximal
inequality holds for partial sums of a sequence of random variables, a maximal
form of the inequality is also valid."@2011
Karim Benhenni@http://arxiv.org/abs/1107.4058v1@Local Polynomial Regression Based on Functional Data@"Suppose that $n$ statistical units are observed, each following the model
$Y(x_j)=m(x_j)+ \epsilon(x_j),\, j=1,...,N,$ where $m$ is a regression
function, $0 \leq x_1 <...<x_N \leq 1$ are observation times spaced according
to a sampling density $f$, and $\epsilon$ is a continuous-time error process
having mean zero and regular covariance function. Considering the local
polynomial estimation of $m$ and its derivatives, we derive asymptotic
expressions for the bias and variance as $n,N\to\infty$. Such results are
particularly relevant in the context of functional data where essential
information is contained in the derivatives. Based on these results, we deduce
optimal sampling densities, optimal bandwidths and asymptotic normality of the
estimator. Simulations are conducted in order to compare the performances of
local polynomial estimators based on exact optimal bandwidths, asymptotic
optimal bandwidths, and cross-validated bandwidths."@2011
David Degras@http://arxiv.org/abs/1107.4058v1@Local Polynomial Regression Based on Functional Data@"Suppose that $n$ statistical units are observed, each following the model
$Y(x_j)=m(x_j)+ \epsilon(x_j),\, j=1,...,N,$ where $m$ is a regression
function, $0 \leq x_1 <...<x_N \leq 1$ are observation times spaced according
to a sampling density $f$, and $\epsilon$ is a continuous-time error process
having mean zero and regular covariance function. Considering the local
polynomial estimation of $m$ and its derivatives, we derive asymptotic
expressions for the bias and variance as $n,N\to\infty$. Such results are
particularly relevant in the context of functional data where essential
information is contained in the derivatives. Based on these results, we deduce
optimal sampling densities, optimal bandwidths and asymptotic normality of the
estimator. Simulations are conducted in order to compare the performances of
local polynomial estimators based on exact optimal bandwidths, asymptotic
optimal bandwidths, and cross-validated bandwidths."@2011
Peter Brockwell@http://arxiv.org/abs/1107.4468v4@"High-frequency sampling and kernel estimation for continuous-time moving
  average processes"@"Interest in continuous-time processes has increased rapidly in recent years,
largely because of high-frequency data available in many applications. We
develop a method for estimating the kernel function $g$ of a second-order
stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based
on observations of the discrete-time process $Y^\Delta$ obtained by sampling
$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by
$g^\Delta$ based on the Wold representation and prove its pointwise convergence
to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric
estimators of $g^\Delta$, based on the innovations algorithm and the
Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA
process we give conditions on the sample size $n$ and the grid-spacing
$\Delta(n)$ under which the innovations estimator is consistent and
asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated
from sampled observations of {\it any} CMA process and simulations suggest that
they perform well even outside the class of CARMA processes. We illustrate
their performance for simulated data and apply them to the Brookhaven turbulent
wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled
CARMA processes to a much wider class of CMA processes."@2011
Vincenzo Ferrazzano@http://arxiv.org/abs/1107.4468v4@"High-frequency sampling and kernel estimation for continuous-time moving
  average processes"@"Interest in continuous-time processes has increased rapidly in recent years,
largely because of high-frequency data available in many applications. We
develop a method for estimating the kernel function $g$ of a second-order
stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based
on observations of the discrete-time process $Y^\Delta$ obtained by sampling
$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by
$g^\Delta$ based on the Wold representation and prove its pointwise convergence
to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric
estimators of $g^\Delta$, based on the innovations algorithm and the
Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA
process we give conditions on the sample size $n$ and the grid-spacing
$\Delta(n)$ under which the innovations estimator is consistent and
asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated
from sampled observations of {\it any} CMA process and simulations suggest that
they perform well even outside the class of CARMA processes. We illustrate
their performance for simulated data and apply them to the Brookhaven turbulent
wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled
CARMA processes to a much wider class of CMA processes."@2011
Claudia Klüppelberg@http://arxiv.org/abs/1107.4468v4@"High-frequency sampling and kernel estimation for continuous-time moving
  average processes"@"Interest in continuous-time processes has increased rapidly in recent years,
largely because of high-frequency data available in many applications. We
develop a method for estimating the kernel function $g$ of a second-order
stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based
on observations of the discrete-time process $Y^\Delta$ obtained by sampling
$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by
$g^\Delta$ based on the Wold representation and prove its pointwise convergence
to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric
estimators of $g^\Delta$, based on the innovations algorithm and the
Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA
process we give conditions on the sample size $n$ and the grid-spacing
$\Delta(n)$ under which the innovations estimator is consistent and
asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated
from sampled observations of {\it any} CMA process and simulations suggest that
they perform well even outside the class of CARMA processes. We illustrate
their performance for simulated data and apply them to the Brookhaven turbulent
wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled
CARMA processes to a much wider class of CMA processes."@2011
Matthieu Solnon@http://arxiv.org/abs/1107.4512v3@Multi-task Regression using Minimal Penalties@"In this paper we study the kernel multiple ridge regression framework, which
we refer to as multi-task regression, using penalization techniques. The
theoretical analysis of this problem shows that the key element appearing for
an optimal calibration is the covariance matrix of the noise between the
different tasks. We present a new algorithm to estimate this covariance matrix,
based on the concept of minimal penalty, which was previously used in the
single-task regression framework to estimate the variance of the noise. We
show, in a non-asymptotic setting and under mild assumptions on the target
function, that this estimator converges towards the covariance matrix. Then
plugging this estimator into the corresponding ideal penalty leads to an oracle
inequality. We illustrate the behavior of our algorithm on synthetic examples."@2011
Sylvain Arlot@http://arxiv.org/abs/1107.4512v3@Multi-task Regression using Minimal Penalties@"In this paper we study the kernel multiple ridge regression framework, which
we refer to as multi-task regression, using penalization techniques. The
theoretical analysis of this problem shows that the key element appearing for
an optimal calibration is the covariance matrix of the noise between the
different tasks. We present a new algorithm to estimate this covariance matrix,
based on the concept of minimal penalty, which was previously used in the
single-task regression framework to estimate the variance of the noise. We
show, in a non-asymptotic setting and under mild assumptions on the target
function, that this estimator converges towards the covariance matrix. Then
plugging this estimator into the corresponding ideal penalty leads to an oracle
inequality. We illustrate the behavior of our algorithm on synthetic examples."@2011
Francis Bach@http://arxiv.org/abs/1107.4512v3@Multi-task Regression using Minimal Penalties@"In this paper we study the kernel multiple ridge regression framework, which
we refer to as multi-task regression, using penalization techniques. The
theoretical analysis of this problem shows that the key element appearing for
an optimal calibration is the covariance matrix of the noise between the
different tasks. We present a new algorithm to estimate this covariance matrix,
based on the concept of minimal penalty, which was previously used in the
single-task regression framework to estimate the variance of the noise. We
show, in a non-asymptotic setting and under mild assumptions on the target
function, that this estimator converges towards the covariance matrix. Then
plugging this estimator into the corresponding ideal penalty leads to an oracle
inequality. We illustrate the behavior of our algorithm on synthetic examples."@2011
Milan Studeny@http://arxiv.org/abs/1107.4708v3@On polyhedral approximations of polytopes for learning Bayes nets@"We review three vector encodings of Bayesian network structures. The first
one has recently been applied by Jaakkola 2010, the other two use special
integral vectors formerly introduced, called imsets [Studeny 2005, Studeny
2010]. The central topic is the comparison of outer polyhedral approximations
of the corresponding polytopes. We show how to transform the inequalities
suggested by Jaakkola et al. to the framework of imsets. The result of our
comparison is the observation that the implicit polyhedral approximation of the
standard imset polytope suggested in [Studeny 2011] gives a closer
approximation than the (transformed) explicit polyhedral approximation from
[Jaakkola 2010]. Finally, we confirm a conjecture from [Studeny 2011] that the
above-mentioned implicit polyhedral approximation of the standard imset
polytope is an LP relaxation of the polytope."@2011
David Haws@http://arxiv.org/abs/1107.4708v3@On polyhedral approximations of polytopes for learning Bayes nets@"We review three vector encodings of Bayesian network structures. The first
one has recently been applied by Jaakkola 2010, the other two use special
integral vectors formerly introduced, called imsets [Studeny 2005, Studeny
2010]. The central topic is the comparison of outer polyhedral approximations
of the corresponding polytopes. We show how to transform the inequalities
suggested by Jaakkola et al. to the framework of imsets. The result of our
comparison is the observation that the implicit polyhedral approximation of the
standard imset polytope suggested in [Studeny 2011] gives a closer
approximation than the (transformed) explicit polyhedral approximation from
[Jaakkola 2010]. Finally, we confirm a conjecture from [Studeny 2011] that the
above-mentioned implicit polyhedral approximation of the standard imset
polytope is an LP relaxation of the polytope."@2011
Fan Wei@http://arxiv.org/abs/1107.5356v2@Dvoretzky--Kiefer--Wolfowitz Inequalities for the Two-sample Case@"The Dvoretzky--Kiefer--Wolfowitz (DKW) inequality says that if $F_n$ is an
empirical distribution function for variables i.i.d.\ with a distribution
function $F$, and $K_n$ is the Kolmogorov statistic
$\sqrt{n}\sup_x|(F_n-F)(x)|$, then there is a finite constant $C$ such that for
any $M>0$, $\Pr(K_n>M) \leq C\exp(-2M^2).$ Massart proved that one can take C=2
(DKWM inequality) which is sharp for $F$ continuous. We consider the analogous
Kolmogorov--Smirnov statistic $KS_{m,n}$ for the two-sample case and show that
for $m=n$, the DKW inequality holds with C=2 if and only if $n\geq 458$. For
$n_0\leq n<458$ it holds for some $C>2$ depending on $n_0$.
  For $m\neq n$, the DKWM inequality fails for the three pairs $(m,n)$ with
$1\leq m < n\leq 3$. We found by computer search that for $n\geq 4$, the DKWM
inequality always holds for $1\leq m< n\leq 200$, and further that it holds for
$n=2m$ with $101\leq m\leq 300$. We conjecture that the DKWM inequality holds
for pairs $m\leq n$ with the $457+3 =460$ exceptions mentioned."@2011
Richard M Dudley@http://arxiv.org/abs/1107.5356v2@Dvoretzky--Kiefer--Wolfowitz Inequalities for the Two-sample Case@"The Dvoretzky--Kiefer--Wolfowitz (DKW) inequality says that if $F_n$ is an
empirical distribution function for variables i.i.d.\ with a distribution
function $F$, and $K_n$ is the Kolmogorov statistic
$\sqrt{n}\sup_x|(F_n-F)(x)|$, then there is a finite constant $C$ such that for
any $M>0$, $\Pr(K_n>M) \leq C\exp(-2M^2).$ Massart proved that one can take C=2
(DKWM inequality) which is sharp for $F$ continuous. We consider the analogous
Kolmogorov--Smirnov statistic $KS_{m,n}$ for the two-sample case and show that
for $m=n$, the DKW inequality holds with C=2 if and only if $n\geq 458$. For
$n_0\leq n<458$ it holds for some $C>2$ depending on $n_0$.
  For $m\neq n$, the DKWM inequality fails for the three pairs $(m,n)$ with
$1\leq m < n\leq 3$. We found by computer search that for $n\geq 4$, the DKWM
inequality always holds for $1\leq m< n\leq 200$, and further that it holds for
$n=2m$ with $101\leq m\leq 300$. We conjecture that the DKWM inequality holds
for pairs $m\leq n$ with the $457+3 =460$ exceptions mentioned."@2011
Rina Foygel@http://arxiv.org/abs/1107.5552v2@"Half-trek criterion for generic identifiability of linear structural
  equation models"@"A linear structural equation model relates random variables of interest and
corresponding Gaussian noise terms via a linear equation system. Each such
model can be represented by a mixed graph in which directed edges encode the
linear equations and bidirected edges indicate possible correlations among
noise terms. We study parameter identifiability in these models, that is, we
ask for conditions that ensure that the edge coefficients and correlations
appearing in a linear structural equation model can be uniquely recovered from
the covariance matrix of the associated distribution. We treat the case of
generic identifiability, where unique recovery is possible for almost every
choice of parameters. We give a new graphical condition that is sufficient for
generic identifiability and can be verified in time that is polynomial in the
size of the graph. It improves criteria from prior work and does not require
the directed part of the graph to be acyclic. We also develop a related
necessary condition and examine the ""gap"" between sufficient and necessary
conditions through simulations on graphs with 25 or 50 nodes, as well as
exhaustive algebraic computations for graphs with up to five nodes."@2011
Jan Draisma@http://arxiv.org/abs/1107.5552v2@"Half-trek criterion for generic identifiability of linear structural
  equation models"@"A linear structural equation model relates random variables of interest and
corresponding Gaussian noise terms via a linear equation system. Each such
model can be represented by a mixed graph in which directed edges encode the
linear equations and bidirected edges indicate possible correlations among
noise terms. We study parameter identifiability in these models, that is, we
ask for conditions that ensure that the edge coefficients and correlations
appearing in a linear structural equation model can be uniquely recovered from
the covariance matrix of the associated distribution. We treat the case of
generic identifiability, where unique recovery is possible for almost every
choice of parameters. We give a new graphical condition that is sufficient for
generic identifiability and can be verified in time that is polynomial in the
size of the graph. It improves criteria from prior work and does not require
the directed part of the graph to be acyclic. We also develop a related
necessary condition and examine the ""gap"" between sufficient and necessary
conditions through simulations on graphs with 25 or 50 nodes, as well as
exhaustive algebraic computations for graphs with up to five nodes."@2011
Mathias Drton@http://arxiv.org/abs/1107.5552v2@"Half-trek criterion for generic identifiability of linear structural
  equation models"@"A linear structural equation model relates random variables of interest and
corresponding Gaussian noise terms via a linear equation system. Each such
model can be represented by a mixed graph in which directed edges encode the
linear equations and bidirected edges indicate possible correlations among
noise terms. We study parameter identifiability in these models, that is, we
ask for conditions that ensure that the edge coefficients and correlations
appearing in a linear structural equation model can be uniquely recovered from
the covariance matrix of the associated distribution. We treat the case of
generic identifiability, where unique recovery is possible for almost every
choice of parameters. We give a new graphical condition that is sufficient for
generic identifiability and can be verified in time that is polynomial in the
size of the graph. It improves criteria from prior work and does not require
the directed part of the graph to be acyclic. We also develop a related
necessary condition and examine the ""gap"" between sufficient and necessary
conditions through simulations on graphs with 25 or 50 nodes, as well as
exhaustive algebraic computations for graphs with up to five nodes."@2011
Rina Foygel@http://arxiv.org/abs/1108.0373v1@Fast-rate and optimistic-rate error bounds for L1-regularized regression@"We consider the prediction error of linear regression with L1 regularization
when the number of covariates p is large relative to the sample size n. When
the model is k-sparse and well-specified, and restricted isometry or similar
conditions hold, the excess squared-error in prediction can be bounded on the
order of sigma^2*(k*log(p)/n), where sigma^2 is the noise variance. Although
these conditions are close to necessary for accurate recovery of the true
coefficient vector, it is possible to guarantee good predictive accuracy under
much milder conditions, avoiding the restricted isometry condition, but only
ensuring an excess error bound of order (k*log(p)/n)+sigma*\surd(k*log(p)/n).
Here we show that this is indeed the best bound possible (up to logarithmic
factors) without introducing stronger assumptions similar to restricted
isometry."@2011
Nathan Srebro@http://arxiv.org/abs/1108.0373v1@Fast-rate and optimistic-rate error bounds for L1-regularized regression@"We consider the prediction error of linear regression with L1 regularization
when the number of covariates p is large relative to the sample size n. When
the model is k-sparse and well-specified, and restricted isometry or similar
conditions hold, the excess squared-error in prediction can be bounded on the
order of sigma^2*(k*log(p)/n), where sigma^2 is the noise variance. Although
these conditions are close to necessary for accurate recovery of the true
coefficient vector, it is possible to guarantee good predictive accuracy under
much milder conditions, avoiding the restricted isometry condition, but only
ensuring an excess error bound of order (k*log(p)/n)+sigma*\surd(k*log(p)/n).
Here we show that this is indeed the best bound possible (up to logarithmic
factors) without introducing stronger assumptions similar to restricted
isometry."@2011
Servane Gey@http://arxiv.org/abs/1108.0757v2@Risk Bounds for Embedded Variable Selection in Classification Trees@"The problems of model and variable selections for classification trees are
jointly considered. A penalized criterion is proposed which explicitly takes
into account the number of variables, and a risk bound inequality is provided
for the tree classifier minimizing this criterion. This penalized criterion is
compared to the one used during the pruning step of the CART algorithm. It is
shown that the two criteria are similar under some specific margin assumptions.
In practice, the tuning parameter of the CART penalty has to be calibrated by
hold-out. Simulation studies are performed which confirm that the hold-out
procedure mimics the form of the proposed penalized criterion."@2011
Tristan Mary-Huard@http://arxiv.org/abs/1108.0757v2@Risk Bounds for Embedded Variable Selection in Classification Trees@"The problems of model and variable selections for classification trees are
jointly considered. A penalized criterion is proposed which explicitly takes
into account the number of variables, and a risk bound inequality is provided
for the tree classifier minimizing this criterion. This penalized criterion is
compared to the one used during the pruning step of the CART algorithm. It is
shown that the two criteria are similar under some specific margin assumptions.
In practice, the tuning parameter of the CART penalty has to be calibrated by
hold-out. Simulation studies are performed which confirm that the hold-out
procedure mimics the form of the proposed penalized criterion."@2011
Michael Falk@http://arxiv.org/abs/1108.0853v1@"Asymptotic Conditional Distribution of Exceedance Counts: Fragility
  Index with Different Margins"@"Let $\bm X=(X_1,...,X_d)$ be a random vector, whose components are not
necessarily independent nor are they required to have identical distribution
functions $F_1,...,F_d$. Denote by $N_s$ the number of exceedances among
$X_1,...,X_d$ above a high threshold $s$. The fragility index, defined by
$FI=\lim_{s\nearrow}E(N_s\mid N_s>0)$ if this limit exists, measures the
asymptotic stability of the stochastic system $\bm X$ as the threshold
increases. The system is called stable if $FI=1$ and fragile otherwise. In this
paper we show that the asymptotic conditional distribution of exceedance counts
(ACDEC) $p_k=\lim_{s\nearrow}P(N_s=k\mid N_s>0)$, $1\le k\le d$, exists, if the
copula of $\bm X$ is in the domain of attraction of a multivariate extreme
value distribution, and if
$\lim_{s\nearrow}(1-F_i(s))/(1-F_\kappa(s))=\gamma_i\in[0,\infty)$ exists for
$1\le i\le d$ and some $\kappa\in{1,...,d}$. This enables the computation of
the FI corresponding to $\bm X$ and of the extended FI as well as of the
asymptotic distribution of the exceedance cluster length also in that case,
where the components of $\bm X$ are not identically distributed."@2011
Diana Tichy@http://arxiv.org/abs/1108.0853v1@"Asymptotic Conditional Distribution of Exceedance Counts: Fragility
  Index with Different Margins"@"Let $\bm X=(X_1,...,X_d)$ be a random vector, whose components are not
necessarily independent nor are they required to have identical distribution
functions $F_1,...,F_d$. Denote by $N_s$ the number of exceedances among
$X_1,...,X_d$ above a high threshold $s$. The fragility index, defined by
$FI=\lim_{s\nearrow}E(N_s\mid N_s>0)$ if this limit exists, measures the
asymptotic stability of the stochastic system $\bm X$ as the threshold
increases. The system is called stable if $FI=1$ and fragile otherwise. In this
paper we show that the asymptotic conditional distribution of exceedance counts
(ACDEC) $p_k=\lim_{s\nearrow}P(N_s=k\mid N_s>0)$, $1\le k\le d$, exists, if the
copula of $\bm X$ is in the domain of attraction of a multivariate extreme
value distribution, and if
$\lim_{s\nearrow}(1-F_i(s))/(1-F_\kappa(s))=\gamma_i\in[0,\infty)$ exists for
$1\le i\le d$ and some $\kappa\in{1,...,d}$. This enables the computation of
the FI corresponding to $\bm X$ and of the extended FI as well as of the
asymptotic distribution of the exceedance cluster length also in that case,
where the components of $\bm X$ are not identically distributed."@2011
Naohiro Kato@http://arxiv.org/abs/1108.1033v4@Likelihood ratio tests for positivity in polynomial regressions@"A polynomial that is nonnegative over a given interval is called a positive
polynomial. The set of such positive polynomials forms a closed convex cone
$K$. In this paper, we consider the likelihood ratio test for the hypothesis of
positivity that the estimand polynomial regression curve is a positive
polynomial. By considering hierarchical hypotheses including the hypothesis of
positivity, we define nested likelihood ratio tests, and derive their null
distributions as mixtures of chi-square distributions by using the
volume-of-tubes method. The mixing probabilities are obtained by utilizing the
parameterizations for the cone $K$ and its dual provided in the framework of
Tchebycheff systems for polynomials of degree at most 4. For polynomials of
degree greater than 4, the upper and lower bounds for the null distributions
are provided. Moreover, we propose associated simultaneous confidence bounds
for polynomial regression curves. Regarding computation, we demonstrate that
symmetric cone programming is useful to obtain the test statistics. As an
illustrative example, we conduct data analysis on growth curves of two groups.
We examine the hypothesis that the growth rate (the derivative of growth curve)
of one group is always higher than the other."@2011
Satoshi Kuriki@http://arxiv.org/abs/1108.1033v4@Likelihood ratio tests for positivity in polynomial regressions@"A polynomial that is nonnegative over a given interval is called a positive
polynomial. The set of such positive polynomials forms a closed convex cone
$K$. In this paper, we consider the likelihood ratio test for the hypothesis of
positivity that the estimand polynomial regression curve is a positive
polynomial. By considering hierarchical hypotheses including the hypothesis of
positivity, we define nested likelihood ratio tests, and derive their null
distributions as mixtures of chi-square distributions by using the
volume-of-tubes method. The mixing probabilities are obtained by utilizing the
parameterizations for the cone $K$ and its dual provided in the framework of
Tchebycheff systems for polynomials of degree at most 4. For polynomials of
degree greater than 4, the upper and lower bounds for the null distributions
are provided. Moreover, we propose associated simultaneous confidence bounds
for polynomial regression curves. Regarding computation, we demonstrate that
symmetric cone programming is useful to obtain the test statistics. As an
illustrative example, we conduct data analysis on growth curves of two groups.
We examine the hypothesis that the growth rate (the derivative of growth curve)
of one group is always higher than the other."@2011
Tatiane F. N. Melo@http://arxiv.org/abs/1108.1098v1@"Adjusted likelihood inference in an elliptical multivariate
  errors-in-variables model"@"In this paper we obtain an adjusted version of the likelihood ratio test for
errors-in-variables multivariate linear regression models. The error terms are
allowed to follow a multivariate distribution in the class of the elliptical
distributions, which has the multivariate normal distribution as a special
case. We derive a modified likelihood ratio statistic that follows a
chi-squared distribution with a high degree of accuracy. Our results generalize
those in Melo and Ferrari(Advances in Statistical Analysis, 2010, 94, 75-87) by
allowing the parameter of interest to be vector-valued in the multivariate
errors-in-variables model. We report a simulation study which shows that the
proposed test displays superior finite sample behavior relative to the standard
likelihood ratio test."@2011
Silvia L. P. Ferrari@http://arxiv.org/abs/1108.1098v1@"Adjusted likelihood inference in an elliptical multivariate
  errors-in-variables model"@"In this paper we obtain an adjusted version of the likelihood ratio test for
errors-in-variables multivariate linear regression models. The error terms are
allowed to follow a multivariate distribution in the class of the elliptical
distributions, which has the multivariate normal distribution as a special
case. We derive a modified likelihood ratio statistic that follows a
chi-squared distribution with a high degree of accuracy. Our results generalize
those in Melo and Ferrari(Advances in Statistical Analysis, 2010, 94, 75-87) by
allowing the parameter of interest to be vector-valued in the multivariate
errors-in-variables model. We report a simulation study which shows that the
proposed test displays superior finite sample behavior relative to the standard
likelihood ratio test."@2011
E. Ostrovsky@http://arxiv.org/abs/1108.1386v1@"Adaptive Optimal Signal (Cardiogram) Processing, with boundary values
  and energy precisely measurement"@"We construct an adaptive asymptotically optimal in order in the weight
Hilbert space norms signal denoising on the background noise and its energy
measurement, with hight precision near the boundary of the signal. An offered
method used the Fourier-Riesz expansion on the orthonormal polynomials, for
instance, Jacobi's polynomials, relative unbounded near the boundary weight
function. An applications: technical and medical, in particular, cardiac
diagnosis."@2011
L. Sirota@http://arxiv.org/abs/1108.1386v1@"Adaptive Optimal Signal (Cardiogram) Processing, with boundary values
  and energy precisely measurement"@"We construct an adaptive asymptotically optimal in order in the weight
Hilbert space norms signal denoising on the background noise and its energy
measurement, with hight precision near the boundary of the signal. An offered
method used the Fourier-Riesz expansion on the orthonormal polynomials, for
instance, Jacobi's polynomials, relative unbounded near the boundary weight
function. An applications: technical and medical, in particular, cardiac
diagnosis."@2011
Francesco Bartolucci@http://arxiv.org/abs/1108.1498v1@Mixture latent autoregressive models for longitudinal data@"Many relevant statistical and econometric models for the analysis of
longitudinal data include a latent process to account for the unobserved
heterogeneity between subjects in a dynamic fashion. Such a process may be
continuous (typically an AR(1)) or discrete (typically a Markov chain). In this
paper, we propose a model for longitudinal data which is based on a mixture of
AR(1) processes with different means and correlation coefficients, but with
equal variances. This model belongs to the class of models based on a
continuous latent process, and then it has a natural interpretation in many
contexts of application, but it is more flexible than other models in this
class, reaching a goodness-of-fit similar to that of a discrete latent process
model, with a reduced number of parameters. We show how to perform maximum
likelihood estimation of the proposed model by the joint use of an
Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented
by means of recursions developed in the hidden Markov literature. We also
introduce a simple method to obtain standard errors for the parameter estimates
and a criterion to choose the number of mixture components. The proposed
approach is illustrated by an application to a longitudinal dataset, coming
from the Health and Retirement Study, about self-evaluation of the health
status by a sample of subjects. In this application, the response variable is
ordinal and time-constant and time-varying individual covariates are available."@2011
Silvia Bacci@http://arxiv.org/abs/1108.1498v1@Mixture latent autoregressive models for longitudinal data@"Many relevant statistical and econometric models for the analysis of
longitudinal data include a latent process to account for the unobserved
heterogeneity between subjects in a dynamic fashion. Such a process may be
continuous (typically an AR(1)) or discrete (typically a Markov chain). In this
paper, we propose a model for longitudinal data which is based on a mixture of
AR(1) processes with different means and correlation coefficients, but with
equal variances. This model belongs to the class of models based on a
continuous latent process, and then it has a natural interpretation in many
contexts of application, but it is more flexible than other models in this
class, reaching a goodness-of-fit similar to that of a discrete latent process
model, with a reduced number of parameters. We show how to perform maximum
likelihood estimation of the proposed model by the joint use of an
Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented
by means of recursions developed in the hidden Markov literature. We also
introduce a simple method to obtain standard errors for the parameter estimates
and a criterion to choose the number of mixture components. The proposed
approach is illustrated by an application to a longitudinal dataset, coming
from the Health and Retirement Study, about self-evaluation of the health
status by a sample of subjects. In this application, the response variable is
ordinal and time-constant and time-varying individual covariates are available."@2011
Fulvia Pennoni@http://arxiv.org/abs/1108.1498v1@Mixture latent autoregressive models for longitudinal data@"Many relevant statistical and econometric models for the analysis of
longitudinal data include a latent process to account for the unobserved
heterogeneity between subjects in a dynamic fashion. Such a process may be
continuous (typically an AR(1)) or discrete (typically a Markov chain). In this
paper, we propose a model for longitudinal data which is based on a mixture of
AR(1) processes with different means and correlation coefficients, but with
equal variances. This model belongs to the class of models based on a
continuous latent process, and then it has a natural interpretation in many
contexts of application, but it is more flexible than other models in this
class, reaching a goodness-of-fit similar to that of a discrete latent process
model, with a reduced number of parameters. We show how to perform maximum
likelihood estimation of the proposed model by the joint use of an
Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented
by means of recursions developed in the hidden Markov literature. We also
introduce a simple method to obtain standard errors for the parameter estimates
and a criterion to choose the number of mixture components. The proposed
approach is illustrated by an application to a longitudinal dataset, coming
from the Health and Retirement Study, about self-evaluation of the health
status by a sample of subjects. In this application, the response variable is
ordinal and time-constant and time-varying individual covariates are available."@2011
Iryna Kyrychynska@http://arxiv.org/abs/1108.1513v1@"Asymptotic behaviour of the S-stopped branching processes with countable
  state space"@"he starting process with countable number of types \mu(t) generates a stopped
branching process \xi(t). The starting process stops, by falling into the
nonempty set S. It is assumed, that the starting process is subcritical,
indecomposable and noncyclic. It is proved, that the extinction probability
converges to the cyclic function with period 1."@2011
Ostap Okhrin@http://arxiv.org/abs/1108.1513v1@"Asymptotic behaviour of the S-stopped branching processes with countable
  state space"@"he starting process with countable number of types \mu(t) generates a stopped
branching process \xi(t). The starting process stops, by falling into the
nonempty set S. It is assumed, that the starting process is subcritical,
indecomposable and noncyclic. It is proved, that the extinction probability
converges to the cyclic function with period 1."@2011
Yaroslav Yeleyko@http://arxiv.org/abs/1108.1513v1@"Asymptotic behaviour of the S-stopped branching processes with countable
  state space"@"he starting process with countable number of types \mu(t) generates a stopped
branching process \xi(t). The starting process stops, by falling into the
nonempty set S. It is assumed, that the starting process is subcritical,
indecomposable and noncyclic. It is proved, that the extinction probability
converges to the cyclic function with period 1."@2011
B. G. Manjunath@http://arxiv.org/abs/1108.1647v3@A note on gaussian distributions in R^n@"Given any finite set F of (n - 1)-dimensional subspaces of R^n we give
examples of nongaussian probability measures in R^n whose marginal distribution
in each subspace from F is gaussian. However, if F is an infinite family of
such (n - 1)-dimensional subspaces then such a nongaussian probability measure
in R^n does not exist."@2011
K. R. Parthasarathy@http://arxiv.org/abs/1108.1647v3@A note on gaussian distributions in R^n@"Given any finite set F of (n - 1)-dimensional subspaces of R^n we give
examples of nongaussian probability measures in R^n whose marginal distribution
in each subspace from F is gaussian. However, if F is an infinite family of
such (n - 1)-dimensional subspaces then such a nongaussian probability measure
in R^n does not exist."@2011
Cari Kaufman@http://arxiv.org/abs/1108.1851v2@"The Role of the Range Parameter for Estimation and Prediction in
  Geostatistics"@"Two canonical problems in geostatistics are estimating the parameters in a
specified family of stochastic process models and predicting the process at new
locations. A number of asymptotic results addressing these problems over a
fixed spatial domain indicate that, for a Gaussian process with Mat\'ern
covariance function, one can fix the range parameter controlling the rate of
decay of the process and obtain results that are asymptotically equivalent to
the case that the range parameter is known. In this paper we show that the same
asymptotic results can be obtained by jointly estimating both the range and the
variance of the process using maximum likelihood or maximum tapered likelihood.
Moreover, we show that intuition and approximations derived from asymptotic
arguments using a fixed range parameter can be problematic when applied to
finite samples, even for moderate to large sample sizes. In contrast, we show
via simulation that performance on a variety of metrics is improved and
asymptotic approximations are applicable for smaller sample sizes when the
range and variance parameters are jointly estimated. These effects are
particularly apparent when the process is mean square differentiable or the
effective range of spatial correlation is small."@2011
Benjamin Shaby@http://arxiv.org/abs/1108.1851v2@"The Role of the Range Parameter for Estimation and Prediction in
  Geostatistics"@"Two canonical problems in geostatistics are estimating the parameters in a
specified family of stochastic process models and predicting the process at new
locations. A number of asymptotic results addressing these problems over a
fixed spatial domain indicate that, for a Gaussian process with Mat\'ern
covariance function, one can fix the range parameter controlling the rate of
decay of the process and obtain results that are asymptotically equivalent to
the case that the range parameter is known. In this paper we show that the same
asymptotic results can be obtained by jointly estimating both the range and the
variance of the process using maximum likelihood or maximum tapered likelihood.
Moreover, we show that intuition and approximations derived from asymptotic
arguments using a fixed range parameter can be problematic when applied to
finite samples, even for moderate to large sample sizes. In contrast, we show
via simulation that performance on a variety of metrics is improved and
asymptotic approximations are applicable for smaller sample sizes when the
range and variance parameters are jointly estimated. These effects are
particularly apparent when the process is mean square differentiable or the
effective range of spatial correlation is small."@2011
Zhan Wang@http://arxiv.org/abs/1108.1961v4@Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls@"Given a dictionary of $M_n$ initial estimates of the unknown true regression
function, we aim to construct linearly aggregated estimators that target the
best performance among all the linear combinations under a sparse $q$-norm ($0
\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the
optimal rates of aggregation for these $\ell_q$-aggregation problems, our
multi-directional (or universal) aggregation strategies by model mixing or
model selection achieve the optimal rates simultaneously over the full range of
$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both
random and fixed designs, with known or unknown error variance, are handled,
and the $\ell_q$-aggregations examined in this work cover major types of
aggregation problems previously studied in the literature. Consequences on
minimax-rate adaptive regression under $\ell_q$-constrained true coefficients
($0 \leq q \leq 1$) are also provided.
  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q
\leq 1$) is basically determined by an effective model size, which is a
sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an
easily interpretable way based on a classical model selection theory that deals
with a large number of models. In addition, in the fixed design case, the model
selection approach is seen to yield optimal rates of convergence not only in
expectation but also with exponential decay of deviation probability. In
contrast, the model mixing approach can have leading constant one in front of
the target risk in the oracle inequality while not offering optimality in
deviation probability."@2011
Sandra Paterlini@http://arxiv.org/abs/1108.1961v4@Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls@"Given a dictionary of $M_n$ initial estimates of the unknown true regression
function, we aim to construct linearly aggregated estimators that target the
best performance among all the linear combinations under a sparse $q$-norm ($0
\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the
optimal rates of aggregation for these $\ell_q$-aggregation problems, our
multi-directional (or universal) aggregation strategies by model mixing or
model selection achieve the optimal rates simultaneously over the full range of
$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both
random and fixed designs, with known or unknown error variance, are handled,
and the $\ell_q$-aggregations examined in this work cover major types of
aggregation problems previously studied in the literature. Consequences on
minimax-rate adaptive regression under $\ell_q$-constrained true coefficients
($0 \leq q \leq 1$) are also provided.
  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q
\leq 1$) is basically determined by an effective model size, which is a
sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an
easily interpretable way based on a classical model selection theory that deals
with a large number of models. In addition, in the fixed design case, the model
selection approach is seen to yield optimal rates of convergence not only in
expectation but also with exponential decay of deviation probability. In
contrast, the model mixing approach can have leading constant one in front of
the target risk in the oracle inequality while not offering optimality in
deviation probability."@2011
Frank Gao@http://arxiv.org/abs/1108.1961v4@Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls@"Given a dictionary of $M_n$ initial estimates of the unknown true regression
function, we aim to construct linearly aggregated estimators that target the
best performance among all the linear combinations under a sparse $q$-norm ($0
\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the
optimal rates of aggregation for these $\ell_q$-aggregation problems, our
multi-directional (or universal) aggregation strategies by model mixing or
model selection achieve the optimal rates simultaneously over the full range of
$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both
random and fixed designs, with known or unknown error variance, are handled,
and the $\ell_q$-aggregations examined in this work cover major types of
aggregation problems previously studied in the literature. Consequences on
minimax-rate adaptive regression under $\ell_q$-constrained true coefficients
($0 \leq q \leq 1$) are also provided.
  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q
\leq 1$) is basically determined by an effective model size, which is a
sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an
easily interpretable way based on a classical model selection theory that deals
with a large number of models. In addition, in the fixed design case, the model
selection approach is seen to yield optimal rates of convergence not only in
expectation but also with exponential decay of deviation probability. In
contrast, the model mixing approach can have leading constant one in front of
the target risk in the oracle inequality while not offering optimality in
deviation probability."@2011
Yuhong Yang@http://arxiv.org/abs/1108.1961v4@Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls@"Given a dictionary of $M_n$ initial estimates of the unknown true regression
function, we aim to construct linearly aggregated estimators that target the
best performance among all the linear combinations under a sparse $q$-norm ($0
\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the
optimal rates of aggregation for these $\ell_q$-aggregation problems, our
multi-directional (or universal) aggregation strategies by model mixing or
model selection achieve the optimal rates simultaneously over the full range of
$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both
random and fixed designs, with known or unknown error variance, are handled,
and the $\ell_q$-aggregations examined in this work cover major types of
aggregation problems previously studied in the literature. Consequences on
minimax-rate adaptive regression under $\ell_q$-constrained true coefficients
($0 \leq q \leq 1$) are also provided.
  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q
\leq 1$) is basically determined by an effective model size, which is a
sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an
easily interpretable way based on a classical model selection theory that deals
with a large number of models. In addition, in the fixed design case, the model
selection approach is seen to yield optimal rates of convergence not only in
expectation but also with exponential decay of deviation probability. In
contrast, the model mixing approach can have leading constant one in front of
the target risk in the oracle inequality while not offering optimality in
deviation probability."@2011
Delphine Cassart@http://arxiv.org/abs/1108.2171v1@"A class of optimal tests for symmetry based on local Edgeworth
  approximations"@"The objective of this paper is to provide, for the problem of univariate
symmetry (with respect to specified or unspecified location), a concept of
optimality, and to construct tests achieving such optimality. This requires
embedding symmetry into adequate families of asymmetric (local) alternatives.
We construct such families by considering non-Gaussian generalizations of
classical first-order Edgeworth expansions indexed by a measure of skewness
such that (i) location, scale and skewness play well-separated roles
(diagonality of the corresponding information matrices) and (ii) the classical
tests based on the Pearson--Fisher coefficient of skewness are optimal in the
vicinity of Gaussian densities."@2011
Marc Hallin@http://arxiv.org/abs/1108.2171v1@"A class of optimal tests for symmetry based on local Edgeworth
  approximations"@"The objective of this paper is to provide, for the problem of univariate
symmetry (with respect to specified or unspecified location), a concept of
optimality, and to construct tests achieving such optimality. This requires
embedding symmetry into adequate families of asymmetric (local) alternatives.
We construct such families by considering non-Gaussian generalizations of
classical first-order Edgeworth expansions indexed by a measure of skewness
such that (i) location, scale and skewness play well-separated roles
(diagonality of the corresponding information matrices) and (ii) the classical
tests based on the Pearson--Fisher coefficient of skewness are optimal in the
vicinity of Gaussian densities."@2011
Davy Paindaveine@http://arxiv.org/abs/1108.2171v1@"A class of optimal tests for symmetry based on local Edgeworth
  approximations"@"The objective of this paper is to provide, for the problem of univariate
symmetry (with respect to specified or unspecified location), a concept of
optimality, and to construct tests achieving such optimality. This requires
embedding symmetry into adequate families of asymmetric (local) alternatives.
We construct such families by considering non-Gaussian generalizations of
classical first-order Edgeworth expansions indexed by a measure of skewness
such that (i) location, scale and skewness play well-separated roles
(diagonality of the corresponding information matrices) and (ii) the classical
tests based on the Pearson--Fisher coefficient of skewness are optimal in the
vicinity of Gaussian densities."@2011
Suprateek Kundu@http://arxiv.org/abs/1108.2720v2@Latent Factor Models for Density Estimation@"Although discrete mixture modeling has formed the backbone of the literature
on Bayesian density estimation, there are some well known disadvantages. We
propose an alternative class of priors based on random nonlinear functions of a
uniform latent variable with an additive residual. The induced prior for the
density is shown to have desirable properties including ease of centering on an
initial guess for the density, large support, posterior consistency and
straightforward computation via Gibbs sampling. Some advantages over discrete
mixtures, such as Dirichlet process mixtures of Gaussian kernels, are discussed
and illustrated via simulations and an epidemiology application."@2011
David B. Dunson@http://arxiv.org/abs/1108.2720v2@Latent Factor Models for Density Estimation@"Although discrete mixture modeling has formed the backbone of the literature
on Bayesian density estimation, there are some well known disadvantages. We
propose an alternative class of priors based on random nonlinear functions of a
uniform latent variable with an additive residual. The induced prior for the
density is shown to have desirable properties including ease of centering on an
initial guess for the density, large support, posterior consistency and
straightforward computation via Gibbs sampling. Some advantages over discrete
mixtures, such as Dirichlet process mixtures of Gaussian kernels, are discussed
and illustrated via simulations and an epidemiology application."@2011
Rafał Kulik@http://arxiv.org/abs/1108.3136v1@"Estimation of limiting conditional distributions for the heavy tailed
  long memory stochastic volatility process"@"We consider Stochastic Volatility processes with heavy tails and possible
long memory in volatility. We study the limiting conditional distribution of
future events given that some present or past event was extreme (i.e. above a
level which tends to infinity). Even though extremes of stochastic volatility
processes are asymptotically independent (in the sense of extreme value
theory), these limiting conditional distributions differ from the i.i.d. case.
We introduce estimators of these limiting conditional distributions and study
their asymptotic properties. If volatility has long memory, then the rate of
convergence and the limiting distribution of the centered estimators can depend
on the long memory parameter (Hurst index)."@2011
Philippe Soulier@http://arxiv.org/abs/1108.3136v1@"Estimation of limiting conditional distributions for the heavy tailed
  long memory stochastic volatility process"@"We consider Stochastic Volatility processes with heavy tails and possible
long memory in volatility. We study the limiting conditional distribution of
future events given that some present or past event was extreme (i.e. above a
level which tends to infinity). Even though extremes of stochastic volatility
processes are asymptotically independent (in the sense of extreme value
theory), these limiting conditional distributions differ from the i.i.d. case.
We introduce estimators of these limiting conditional distributions and study
their asymptotic properties. If volatility has long memory, then the rate of
convergence and the limiting distribution of the centered estimators can depend
on the long memory parameter (Hurst index)."@2011
Dominique Guillot@http://arxiv.org/abs/1108.3325v1@Retaining positive definiteness in thresholded matrices@"Positive definite (p.d.) matrices arise naturally in many areas within
mathematics and also feature extensively in scientific applications. In modern
high-dimensional applications, a common approach to finding sparse positive
definite matrices is to threshold their small off-diagonal elements. This
thresholding, sometimes referred to as hard-thresholding, sets small elements
to zero. Thresholding has the attractive property that the resulting matrices
are sparse, and are thus easier to interpret and work with. In many
applications, it is often required, and thus implicitly assumed, that
thresholded matrices retain positive definiteness. In this paper we formally
investigate the algebraic properties of p.d. matrices which are thresholded. We
demonstrate that for positive definiteness to be preserved, the pattern of
elements to be set to zero has to necessarily correspond to a graph which is a
union of disconnected complete components. This result rigorously demonstrates
that, except in special cases, positive definiteness can be easily lost. We
then proceed to demonstrate that the class of diagonally dominant matrices is
not maximal in terms of retaining positive definiteness when thresholded.
Consequently, we derive characterizations of matrices which retain positive
definiteness when thresholded with respect to important classes of graphs. In
particular, we demonstrate that retaining positive definiteness upon
thresholding is governed by complex algebraic conditions."@2011
Bala Rajaratnam@http://arxiv.org/abs/1108.3325v1@Retaining positive definiteness in thresholded matrices@"Positive definite (p.d.) matrices arise naturally in many areas within
mathematics and also feature extensively in scientific applications. In modern
high-dimensional applications, a common approach to finding sparse positive
definite matrices is to threshold their small off-diagonal elements. This
thresholding, sometimes referred to as hard-thresholding, sets small elements
to zero. Thresholding has the attractive property that the resulting matrices
are sparse, and are thus easier to interpret and work with. In many
applications, it is often required, and thus implicitly assumed, that
thresholded matrices retain positive definiteness. In this paper we formally
investigate the algebraic properties of p.d. matrices which are thresholded. We
demonstrate that for positive definiteness to be preserved, the pattern of
elements to be set to zero has to necessarily correspond to a graph which is a
union of disconnected complete components. This result rigorously demonstrates
that, except in special cases, positive definiteness can be easily lost. We
then proceed to demonstrate that the class of diagonally dominant matrices is
not maximal in terms of retaining positive definiteness when thresholded.
Consequently, we derive characterizations of matrices which retain positive
definiteness when thresholded with respect to important classes of graphs. In
particular, we demonstrate that retaining positive definiteness upon
thresholding is governed by complex algebraic conditions."@2011
John T. Flam@http://arxiv.org/abs/1108.3410v1@Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics@"This paper investigates the minimum mean square error (MMSE) estimation of x,
given the observation y = Hx+n, when x and n are independent and Gaussian
Mixture (GM) distributed. The introduction of GM distributions, represents a
generalization of the more familiar and simpler Gaussian signal and Gaussian
noise instance. We present the necessary theoretical foundation and derive the
MMSE estimator for x in a closed form. Furthermore, we provide upper and lower
bounds for its mean square error (MSE). These bounds are validated through
Monte Carlo simulations."@2011
Saikat Chatterjee@http://arxiv.org/abs/1108.3410v1@Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics@"This paper investigates the minimum mean square error (MMSE) estimation of x,
given the observation y = Hx+n, when x and n are independent and Gaussian
Mixture (GM) distributed. The introduction of GM distributions, represents a
generalization of the more familiar and simpler Gaussian signal and Gaussian
noise instance. We present the necessary theoretical foundation and derive the
MMSE estimator for x in a closed form. Furthermore, we provide upper and lower
bounds for its mean square error (MSE). These bounds are validated through
Monte Carlo simulations."@2011
Kimmo Kansanen@http://arxiv.org/abs/1108.3410v1@Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics@"This paper investigates the minimum mean square error (MMSE) estimation of x,
given the observation y = Hx+n, when x and n are independent and Gaussian
Mixture (GM) distributed. The introduction of GM distributions, represents a
generalization of the more familiar and simpler Gaussian signal and Gaussian
noise instance. We present the necessary theoretical foundation and derive the
MMSE estimator for x in a closed form. Furthermore, we provide upper and lower
bounds for its mean square error (MSE). These bounds are validated through
Monte Carlo simulations."@2011
Torbjorn Ekman@http://arxiv.org/abs/1108.3410v1@Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics@"This paper investigates the minimum mean square error (MMSE) estimation of x,
given the observation y = Hx+n, when x and n are independent and Gaussian
Mixture (GM) distributed. The introduction of GM distributions, represents a
generalization of the more familiar and simpler Gaussian signal and Gaussian
noise instance. We present the necessary theoretical foundation and derive the
MMSE estimator for x in a closed form. Furthermore, we provide upper and lower
bounds for its mean square error (MSE). These bounds are validated through
Monte Carlo simulations."@2011
Junlong Zhao@http://arxiv.org/abs/1108.3755v2@New Error Analysis for Lasso@"The Lasso is one of the most important approaches for parameter estimation
and variable selection in high dimensional linear regression. At the heart of
its success is the attractive rate of convergence result even when $p$, the
dimension of the problem, is much larger than the sample size $n$. In
particular, Bickel et al. (2009) showed that this rate, in terms of the
$\ell_1$ norm, is of the order $s\sqrt{(\log p)/n}$ for a sparsity index $s$.
In this paper, we obtain a new bound on the convergence rate by taking
advantage of the distributional information of the model. Under the normality
or sub-Gaussian assumption, the rate can be improved to nearly $s/\sqrt{n}$ for
certain design matrices. We further outline a general partitioning technique
that helps to derive sharper convergence rate for the Lasso. The result is
applicable to many covariance matrices suitable for high-dimensional data
analysis."@2011
Chenlei Leng@http://arxiv.org/abs/1108.3755v2@New Error Analysis for Lasso@"The Lasso is one of the most important approaches for parameter estimation
and variable selection in high dimensional linear regression. At the heart of
its success is the attractive rate of convergence result even when $p$, the
dimension of the problem, is much larger than the sample size $n$. In
particular, Bickel et al. (2009) showed that this rate, in terms of the
$\ell_1$ norm, is of the order $s\sqrt{(\log p)/n}$ for a sparsity index $s$.
In this paper, we obtain a new bound on the convergence rate by taking
advantage of the distributional information of the model. Under the normality
or sub-Gaussian assumption, the rate can be improved to nearly $s/\sqrt{n}$ for
certain design matrices. We further outline a general partitioning technique
that helps to derive sharper convergence rate for the Lasso. The result is
applicable to many covariance matrices suitable for high-dimensional data
analysis."@2011
Sylvain Le Corff@http://arxiv.org/abs/1108.3968v3@"Online Expectation Maximization based algorithms for inference in hidden
  Markov models"@"The Expectation Maximization (EM) algorithm is a versatile tool for model
parameter estimation in latent data models. When processing large data sets or
data stream however, EM becomes intractable since it requires the whole data
set to be available at each iteration of the algorithm. In this contribution, a
new generic online EM algorithm for model parameter inference in general Hidden
Markov Model is proposed. This new algorithm updates the parameter estimate
after a block of observations is processed (online). The convergence of this
new algorithm is established, and the rate of convergence is studied showing
the impact of the block size. An averaging procedure is also proposed to
improve the rate of convergence. Finally, practical illustrations are presented
to highlight the performance of these algorithms in comparison to other online
maximum likelihood procedures."@2011
Gersende Fort@http://arxiv.org/abs/1108.3968v3@"Online Expectation Maximization based algorithms for inference in hidden
  Markov models"@"The Expectation Maximization (EM) algorithm is a versatile tool for model
parameter estimation in latent data models. When processing large data sets or
data stream however, EM becomes intractable since it requires the whole data
set to be available at each iteration of the algorithm. In this contribution, a
new generic online EM algorithm for model parameter inference in general Hidden
Markov Model is proposed. This new algorithm updates the parameter estimate
after a block of observations is processed (online). The convergence of this
new algorithm is established, and the rate of convergence is studied showing
the impact of the block size. An averaging procedure is also proposed to
improve the rate of convergence. Finally, practical illustrations are presented
to highlight the performance of these algorithms in comparison to other online
maximum likelihood procedures."@2011
Sylvain Le Corff@http://arxiv.org/abs/1108.4130v3@"Supplement paper to ""Online Expectation Maximization based algorithms
  for inference in hidden Markov models"""@"This is a supplementary material to the paper ""Online Expectation
Maximization based algorithms for inference in hidden Markov models"". It
contains further technical derivations and additional simulation results."@2011
Gersende Fort@http://arxiv.org/abs/1108.4130v3@"Supplement paper to ""Online Expectation Maximization based algorithms
  for inference in hidden Markov models"""@"This is a supplementary material to the paper ""Online Expectation
Maximization based algorithms for inference in hidden Markov models"". It
contains further technical derivations and additional simulation results."@2011
Höpfner Reinhard@http://arxiv.org/abs/1108.5314v1@On frequency estimation of periodic ergodic diffusion process@"We consider the problem of frequency estimation by observations of the
periodic diffusion process possesing ergodic properties in two different
situations. The first one corresponds to continuously differentiable with
respect to parameter trend coefficient and the second - to discontinuous trend
coefficient. It is shown that in the first case the maximum likelihood and
bayesian estimators are asymptotically normal with rate $T^{3/2}$ and in the
second case these estimators have different limit distributions with the rate
$T^2$."@2011
Yury A Kutoyants@http://arxiv.org/abs/1108.5314v1@On frequency estimation of periodic ergodic diffusion process@"We consider the problem of frequency estimation by observations of the
periodic diffusion process possesing ergodic properties in two different
situations. The first one corresponds to continuously differentiable with
respect to parameter trend coefficient and the second - to discontinuous trend
coefficient. It is shown that in the first case the maximum likelihood and
bayesian estimators are asymptotically normal with rate $T^{3/2}$ and in the
second case these estimators have different limit distributions with the rate
$T^2$."@2011
Marius Hofert@http://arxiv.org/abs/1108.6032v1@Likelihood inference for Archimedean copulas@"Explicit functional forms for the generator derivatives of well-known
one-parameter Archimedean copulas are derived. These derivatives are essential
for likelihood inference as they appear in the copula density, conditional
distribution functions, or the Kendall distribution function. They are also
required for several asymmetric extensions of Archimedean copulas such as
Khoudraji-transformed Archimedean copulas. Access to the generator derivatives
makes maximum-likelihood estimation for Archimedean copulas feasible in terms
of both precision and run time, even in large dimensions. It is shown by
simulation that the root mean squared error is decreasing in the dimension.
This decrease is of the same order as the decrease in sample size. Furthermore,
confidence intervals for the parameter vector are derived. Moreover, extensions
to multi-parameter Archimedean families are given. All presented methods are
implemented in the open-source R package nacopula and can thus easily be
accessed and studied."@2011
Martin Mächler@http://arxiv.org/abs/1108.6032v1@Likelihood inference for Archimedean copulas@"Explicit functional forms for the generator derivatives of well-known
one-parameter Archimedean copulas are derived. These derivatives are essential
for likelihood inference as they appear in the copula density, conditional
distribution functions, or the Kendall distribution function. They are also
required for several asymmetric extensions of Archimedean copulas such as
Khoudraji-transformed Archimedean copulas. Access to the generator derivatives
makes maximum-likelihood estimation for Archimedean copulas feasible in terms
of both precision and run time, even in large dimensions. It is shown by
simulation that the root mean squared error is decreasing in the dimension.
This decrease is of the same order as the decrease in sample size. Furthermore,
confidence intervals for the parameter vector are derived. Moreover, extensions
to multi-parameter Archimedean families are given. All presented methods are
implemented in the open-source R package nacopula and can thus easily be
accessed and studied."@2011
Alexander J. McNeil@http://arxiv.org/abs/1108.6032v1@Likelihood inference for Archimedean copulas@"Explicit functional forms for the generator derivatives of well-known
one-parameter Archimedean copulas are derived. These derivatives are essential
for likelihood inference as they appear in the copula density, conditional
distribution functions, or the Kendall distribution function. They are also
required for several asymmetric extensions of Archimedean copulas such as
Khoudraji-transformed Archimedean copulas. Access to the generator derivatives
makes maximum-likelihood estimation for Archimedean copulas feasible in terms
of both precision and run time, even in large dimensions. It is shown by
simulation that the root mean squared error is decreasing in the dimension.
This decrease is of the same order as the decrease in sample size. Furthermore,
confidence intervals for the parameter vector are derived. Moreover, extensions
to multi-parameter Archimedean families are given. All presented methods are
implemented in the open-source R package nacopula and can thus easily be
accessed and studied."@2011
Jose A. Diaz-Garcia@http://arxiv.org/abs/1108.6104v1@"A modified Prékopa's approach in optimum allocation in multivariate
  stratified random sampling"@"A modified Prekopa's approach is considered for the problem of optimum
allocation in multivariate stratified random sampling. An example is solved by
applying the proposed methodology."@2011
Rogelio Raos-Quiroga@http://arxiv.org/abs/1108.6104v1@"A modified Prékopa's approach in optimum allocation in multivariate
  stratified random sampling"@"A modified Prekopa's approach is considered for the problem of optimum
allocation in multivariate stratified random sampling. An example is solved by
applying the proposed methodology."@2011
Eric Gautier@http://arxiv.org/abs/1109.0362v4@"A triangular treatment effect model with random coefficients in the
  selection equation"@"This paper considers treatment effects under endogeneity with complex
heterogeneity in the selection equation. We model the outcome of an endogenous
treatment as a triangular system, where both the outcome and first-stage
equations consist of a random coefficients model. The first-stage specifically
allows for nonmonotone selection into treatment. We provide conditions under
which marginal distributions of potential outcomes, average and quantile
treatment effects, all conditional on first-stage random coefficients, are
identified. Under the same conditions, we derive bounds on the (conditional)
joint distributions of potential outcomes and gains from treatment, and provide
additional conditions for their point identification. All conditional
quantities yield unconditional effects (\emph{e.g.}, the average treatment
effect) by weighted integration."@2011
Stefan Hoderlein@http://arxiv.org/abs/1109.0362v4@"A triangular treatment effect model with random coefficients in the
  selection equation"@"This paper considers treatment effects under endogeneity with complex
heterogeneity in the selection equation. We model the outcome of an endogenous
treatment as a triangular system, where both the outcome and first-stage
equations consist of a random coefficients model. The first-stage specifically
allows for nonmonotone selection into treatment. We provide conditions under
which marginal distributions of potential outcomes, average and quantile
treatment effects, all conditional on first-stage random coefficients, are
identified. Under the same conditions, we derive bounds on the (conditional)
joint distributions of potential outcomes and gains from treatment, and provide
additional conditions for their point identification. All conditional
quantities yield unconditional effects (\emph{e.g.}, the average treatment
effect) by weighted integration."@2011
Han Xiao@http://arxiv.org/abs/1109.0524v1@Simultaneous Inference of Covariances@"We consider asymptotic distributions of maximum deviations of sample
covariance matrices, a fundamental problem in high-dimensional inference of
covariances. Under mild dependence conditions on the entries of the data
matrices, we establish the Gumbel convergence of the maximum deviations. Our
result substantially generalizes earlier ones where the entries are assumed to
be independent and identically distributed, and it provides a theoretical
foundation for high-dimensional simultaneous inference of covariances."@2011
Wei Biao Wu@http://arxiv.org/abs/1109.0524v1@Simultaneous Inference of Covariances@"We consider asymptotic distributions of maximum deviations of sample
covariance matrices, a fundamental problem in high-dimensional inference of
covariances. Under mild dependence conditions on the entries of the data
matrices, we establish the Gumbel convergence of the maximum deviations. Our
result substantially generalizes earlier ones where the entries are assumed to
be independent and identically distributed, and it provides a theoretical
foundation for high-dimensional simultaneous inference of covariances."@2011
Cristina Butucea@http://arxiv.org/abs/1109.0898v2@Detection of a sparse submatrix of a high-dimensional noisy matrix@"We observe a $N\times M$ matrix $Y_{ij}=s_{ij}+\xi_{ij}$ with $\xi_{ij}\sim
{\mathcal {N}}(0,1)$ i.i.d. in $i,j$, and $s_{ij}\in \mathbb {R}$. We test the
null hypothesis $s_{ij}=0$ for all $i,j$ against the alternative that there
exists some submatrix of size $n\times m$ with significant elements in the
sense that $s_{ij}\ge a>0$. We propose a test procedure and compute the
asymptotical detection boundary $a$ so that the maximal testing risk tends to 0
as $M\to\infty$, $N\to\infty$, $p=n/N\to0$, $q=m/M\to0$. We prove that this
boundary is asymptotically sharp minimax under some additional constraints.
Relations with other testing problems are discussed. We propose a testing
procedure which adapts to unknown $(n,m)$ within some given set and compute the
adaptive sharp rates. The implementation of our test procedure on synthetic
data shows excellent behavior for sparse, not necessarily squared matrices. We
extend our sharp minimax results in different directions: first, to Gaussian
matrices with unknown variance, next, to matrices of random variables having a
distribution from an exponential family (non-Gaussian) and, finally, to a
two-sided alternative for matrices with Gaussian elements."@2011
Yuri I. Ingster@http://arxiv.org/abs/1109.0898v2@Detection of a sparse submatrix of a high-dimensional noisy matrix@"We observe a $N\times M$ matrix $Y_{ij}=s_{ij}+\xi_{ij}$ with $\xi_{ij}\sim
{\mathcal {N}}(0,1)$ i.i.d. in $i,j$, and $s_{ij}\in \mathbb {R}$. We test the
null hypothesis $s_{ij}=0$ for all $i,j$ against the alternative that there
exists some submatrix of size $n\times m$ with significant elements in the
sense that $s_{ij}\ge a>0$. We propose a test procedure and compute the
asymptotical detection boundary $a$ so that the maximal testing risk tends to 0
as $M\to\infty$, $N\to\infty$, $p=n/N\to0$, $q=m/M\to0$. We prove that this
boundary is asymptotically sharp minimax under some additional constraints.
Relations with other testing problems are discussed. We propose a testing
procedure which adapts to unknown $(n,m)$ within some given set and compute the
adaptive sharp rates. The implementation of our test procedure on synthetic
data shows excellent behavior for sparse, not necessarily squared matrices. We
extend our sharp minimax results in different directions: first, to Gaussian
matrices with unknown variance, next, to matrices of random variables having a
distribution from an exponential family (non-Gaussian) and, finally, to a
two-sided alternative for matrices with Gaussian elements."@2011
Christoph Breunig@http://arxiv.org/abs/1109.0961v1@"Adaptive estimation of functionals in nonparametric instrumental
  regression"@"We consider the problem of estimating the value l({\phi}) of a linear
functional, where the structural function {\phi} models a nonparametric
relationship in presence of instrumental variables. We propose a plug-in
estimator which is based on a dimension reduction technique and additional
thresholding. It is shown that this estimator is consistent and can attain the
minimax optimal rate of convergence under additional regularity conditions.
This, however, requires an optimal choice of the dimension parameter m
depending on certain characteristics of the structural function {\phi} and the
joint distribution of the regressor and the instrument, which are unknown in
practice. We propose a fully data driven choice of m which combines model
selection and Lepski's method. We show that the adaptive estimator attains the
optimal rate of convergence up to a logarithmic factor. The theory in this
paper is illustrated by considering classical smoothness assumptions and we
discuss examples such as pointwise estimation or estimation of averages of the
structural function {\phi}."@2011
Jan Johannes@http://arxiv.org/abs/1109.0961v1@"Adaptive estimation of functionals in nonparametric instrumental
  regression"@"We consider the problem of estimating the value l({\phi}) of a linear
functional, where the structural function {\phi} models a nonparametric
relationship in presence of instrumental variables. We propose a plug-in
estimator which is based on a dimension reduction technique and additional
thresholding. It is shown that this estimator is consistent and can attain the
minimax optimal rate of convergence under additional regularity conditions.
This, however, requires an optimal choice of the dimension parameter m
depending on certain characteristics of the structural function {\phi} and the
joint distribution of the regressor and the instrument, which are unknown in
practice. We propose a fully data driven choice of m which combines model
selection and Lepski's method. We show that the adaptive estimator attains the
optimal rate of convergence up to a logarithmic factor. The theory in this
paper is illustrated by considering classical smoothness assumptions and we
discuss examples such as pointwise estimation or estimation of averages of the
structural function {\phi}."@2011
Takuya Kashimura@http://arxiv.org/abs/1109.2408v1@"Cones of elementary imsets and supermodular functions: a review and some
  new results"@"In this paper we give a review of the method of imsets introduced by Studeny
(2005) from a geometric point of view. Elementary imsets span a polyhedral cone
and its dual cone is the cone of supermodular functions. We review basic facts
on the structure of these cones. Then we derive some new results on the
following topics: i) extreme rays of the cone of standardized supermodular
functions, ii) faces of the cones, iii) small relations among elementary
imsets, and iv) some computational results on Markov basis for the toric ideal
defined by elementary imsets."@2011
Tomonari Sei@http://arxiv.org/abs/1109.2408v1@"Cones of elementary imsets and supermodular functions: a review and some
  new results"@"In this paper we give a review of the method of imsets introduced by Studeny
(2005) from a geometric point of view. Elementary imsets span a polyhedral cone
and its dual cone is the cone of supermodular functions. We review basic facts
on the structure of these cones. Then we derive some new results on the
following topics: i) extreme rays of the cone of standardized supermodular
functions, ii) faces of the cones, iii) small relations among elementary
imsets, and iv) some computational results on Markov basis for the toric ideal
defined by elementary imsets."@2011
Akimichi Takemura@http://arxiv.org/abs/1109.2408v1@"Cones of elementary imsets and supermodular functions: a review and some
  new results"@"In this paper we give a review of the method of imsets introduced by Studeny
(2005) from a geometric point of view. Elementary imsets span a polyhedral cone
and its dual cone is the cone of supermodular functions. We review basic facts
on the structure of these cones. Then we derive some new results on the
following topics: i) extreme rays of the cone of standardized supermodular
functions, ii) faces of the cones, iii) small relations among elementary
imsets, and iv) some computational results on Markov basis for the toric ideal
defined by elementary imsets."@2011
Kentaro Tanaka@http://arxiv.org/abs/1109.2408v1@"Cones of elementary imsets and supermodular functions: a review and some
  new results"@"In this paper we give a review of the method of imsets introduced by Studeny
(2005) from a geometric point of view. Elementary imsets span a polyhedral cone
and its dual cone is the cone of supermodular functions. We review basic facts
on the structure of these cones. Then we derive some new results on the
following topics: i) extreme rays of the cone of standardized supermodular
functions, ii) faces of the cones, iii) small relations among elementary
imsets, and iv) some computational results on Markov basis for the toric ideal
defined by elementary imsets."@2011
Sílvia R. C. Lopes@http://arxiv.org/abs/1109.2628v2@Copulas Related to Manneville-Pomeau Processes@"In this work we derive the copulas related to Manneville-Pomeau processes. We
examine both bidimensional and multidimensional cases and derive some
properties for the related copulas. Computational issues, approximations and
random variate generation problems are addressed and simple numerical
experiments to test the approximations developed are also performed. In
particular, we propose an approximation to the copulas derived which we show to
converge uniformly to the true copula. To illustrate the usefulness of the
theory, we derive a fast procedure to estimate the underlying parameter in
Manneville-Pomeau processes."@2011
Guilherme Pumi@http://arxiv.org/abs/1109.2628v2@Copulas Related to Manneville-Pomeau Processes@"In this work we derive the copulas related to Manneville-Pomeau processes. We
examine both bidimensional and multidimensional cases and derive some
properties for the related copulas. Computational issues, approximations and
random variate generation problems are addressed and simple numerical
experiments to test the approximations developed are also performed. In
particular, we propose an approximation to the copulas derived which we show to
converge uniformly to the true copula. To illustrate the usefulness of the
theory, we derive a fast procedure to estimate the underlying parameter in
Manneville-Pomeau processes."@2011
Aleksey S. Polunchenko@http://arxiv.org/abs/1109.2938v1@State-of-the-Art in Sequential Change-Point Detection@"We provide an overview of the state-of-the-art in the area of sequential
change-point detection assuming discrete time and known pre- and post-change
distributions. The overview spans over all major formulations of the underlying
optimization problem, namely, Bayesian, generalized Bayesian, and minimax. We
pay particular attention to the latest advances in each. Also, we link together
the generalized Bayesian problem with multi-cyclic disorder detection in a
stationary regime when the change occurs at a distant time horizon. We conclude
with two case studies to illustrate the cutting edge of the field at work."@2011
Alexander G. Tartakovsky@http://arxiv.org/abs/1109.2938v1@State-of-the-Art in Sequential Change-Point Detection@"We provide an overview of the state-of-the-art in the area of sequential
change-point detection assuming discrete time and known pre- and post-change
distributions. The overview spans over all major formulations of the underlying
optimization problem, namely, Bayesian, generalized Bayesian, and minimax. We
pay particular attention to the latest advances in each. Also, we link together
the generalized Bayesian problem with multi-cyclic disorder detection in a
stationary regime when the change occurs at a distant time horizon. We conclude
with two case studies to illustrate the cutting edge of the field at work."@2011
Nathan Huntley@http://arxiv.org/abs/1109.3607v1@"Subtree perfectness, backward induction, and normal-extensive form
  equivalence for single agent sequential decision making under arbitrary
  choice functions"@"We revisit and reinterpret Selten's concept of subgame perfectness in the
context of single agent normal form sequential decision making, which leads us
to the concept of subtree perfectness. Thereby, we extend Hammond's
characterization of extensive form consequentialist consistent behaviour norms
to the normal form and to arbitrary choice functions under very few
assumptions. In particular, we do not need to assume probabilities on any event
or utilities on any reward. We show that subtree perfectness is equivalent to
normal-extensive form equivalence, and is sufficient, but, perhaps
surprisingly, not necessary, for backward induction to work."@2011
Matthias C. M. Troffaes@http://arxiv.org/abs/1109.3607v1@"Subtree perfectness, backward induction, and normal-extensive form
  equivalence for single agent sequential decision making under arbitrary
  choice functions"@"We revisit and reinterpret Selten's concept of subgame perfectness in the
context of single agent normal form sequential decision making, which leads us
to the concept of subtree perfectness. Thereby, we extend Hammond's
characterization of extensive form consequentialist consistent behaviour norms
to the normal form and to arbitrary choice functions under very few
assumptions. In particular, we do not need to assume probabilities on any event
or utilities on any reward. We show that subtree perfectness is equivalent to
normal-extensive form equivalence, and is sufficient, but, perhaps
surprisingly, not necessary, for backward induction to work."@2011
Debdeep Pati@http://arxiv.org/abs/1109.5000v1@Posterior convergence rates in non-linear latent variable models@"Non-linear latent variable models have become increasingly popular in a
variety of applications. However, there has been little study on theoretical
properties of these models. In this article, we study rates of posterior
contraction in univariate density estimation for a class of non-linear latent
variable models where unobserved U(0,1) latent variables are related to the
response variables via a random non-linear regression with an additive error.
Our approach relies on characterizing the space of densities induced by the
above model as kernel convolutions with a general class of continuous mixing
measures. The literature on posterior rates of contraction in density
estimation almost entirely focuses on finite or countably infinite mixture
models. We develop approximation results for our class of continuous mixing
measures. Using an appropriate Gaussian process prior on the unknown regression
function, we obtain the optimal frequentist rate up to a logarithmic factor
under standard regularity conditions on the true density."@2011
Anirban Bhattacharya@http://arxiv.org/abs/1109.5000v1@Posterior convergence rates in non-linear latent variable models@"Non-linear latent variable models have become increasingly popular in a
variety of applications. However, there has been little study on theoretical
properties of these models. In this article, we study rates of posterior
contraction in univariate density estimation for a class of non-linear latent
variable models where unobserved U(0,1) latent variables are related to the
response variables via a random non-linear regression with an additive error.
Our approach relies on characterizing the space of densities induced by the
above model as kernel convolutions with a general class of continuous mixing
measures. The literature on posterior rates of contraction in density
estimation almost entirely focuses on finite or countably infinite mixture
models. We develop approximation results for our class of continuous mixing
measures. Using an appropriate Gaussian process prior on the unknown regression
function, we obtain the optimal frequentist rate up to a logarithmic factor
under standard regularity conditions on the true density."@2011
David B. Dunson@http://arxiv.org/abs/1109.5000v1@Posterior convergence rates in non-linear latent variable models@"Non-linear latent variable models have become increasingly popular in a
variety of applications. However, there has been little study on theoretical
properties of these models. In this article, we study rates of posterior
contraction in univariate density estimation for a class of non-linear latent
variable models where unobserved U(0,1) latent variables are related to the
response variables via a random non-linear regression with an additive error.
Our approach relies on characterizing the space of densities induced by the
above model as kernel convolutions with a general class of continuous mixing
measures. The literature on posterior rates of contraction in density
estimation almost entirely focuses on finite or countably infinite mixture
models. We develop approximation results for our class of continuous mixing
measures. Using an appropriate Gaussian process prior on the unknown regression
function, we obtain the optimal frequentist rate up to a logarithmic factor
under standard regularity conditions on the true density."@2011
Luai Al Labadi@http://arxiv.org/abs/1109.5261v3@The Dirichlet Process with Large Concentration Parameter@"Ferguson's Dirichlet process plays an important role in nonparametric
Bayesian inference. Let $P_a$ be the Dirichlet process in $\mathbb{R}$ with a
base probability measure $H$ and a concentration parameter $a>0.$ In this
paper, we show that $\sqrt {a} \big(P_a((-\infty,t]) -H((-\infty,t])\big)$
converges to a certain Brownian bridge as $a \to \infty.$ We also derive a
certain Glivenko-Cantelli theorem for the Dirichlet process. Using the
functional delta method, the weak convergence of the quantile process is also
obtained. A large concentration parameter occurs when a statistician puts too
much emphasize on his/her prior guess. This scenario also happens when the
sample size is large and the posterior is used to make inference."@2011
Mahmoud Zarepour@http://arxiv.org/abs/1109.5261v3@The Dirichlet Process with Large Concentration Parameter@"Ferguson's Dirichlet process plays an important role in nonparametric
Bayesian inference. Let $P_a$ be the Dirichlet process in $\mathbb{R}$ with a
base probability measure $H$ and a concentration parameter $a>0.$ In this
paper, we show that $\sqrt {a} \big(P_a((-\infty,t]) -H((-\infty,t])\big)$
converges to a certain Brownian bridge as $a \to \infty.$ We also derive a
certain Glivenko-Cantelli theorem for the Dirichlet process. Using the
functional delta method, the weak convergence of the quantile process is also
obtained. A large concentration parameter occurs when a statistician puts too
much emphasize on his/her prior guess. This scenario also happens when the
sample size is large and the posterior is used to make inference."@2011
Rafal Kulik@http://arxiv.org/abs/1109.5298v2@"Limit theorems for long memory stochastic volatility models with
  infinite variance: Partial Sums and Sample Covariances"@"Long Memory Stochastic volatility (LMSV) models capture two standardized
features of financial data: the log-returns are uncorrelated, but their
squares, or absolute values are (highly) dependent and they may have heavy
tails. EGARCH and related models were introduced to model leverage, i.e.
negative dependence between previous returns and future volatility. Limit
theorems for partial sums, sample variance and sample covariances are basic
tools to investigate the presence of long memory and heavy tails and their
consequences. In this paper we extend the existing literature on the asymptotic
behaviour of the partial sums and the sample covariances of long memory
stochastic volatility models in the case of infinite variance. We also consider
models with leverage, for which our results are entirely new in the infinite
variance case. Depending on the nterplay between the tail behaviour and the
intensity of dependence, wo types of convergence rates and limiting
distributions can arise. In articular, we show that the asymptotic behaviour of
partial sums is the same for both LMSV and models with leverage, whereas there
is a crucial difference when sample covariances are considered."@2011
Philippe Soulier@http://arxiv.org/abs/1109.5298v2@"Limit theorems for long memory stochastic volatility models with
  infinite variance: Partial Sums and Sample Covariances"@"Long Memory Stochastic volatility (LMSV) models capture two standardized
features of financial data: the log-returns are uncorrelated, but their
squares, or absolute values are (highly) dependent and they may have heavy
tails. EGARCH and related models were introduced to model leverage, i.e.
negative dependence between previous returns and future volatility. Limit
theorems for partial sums, sample variance and sample covariances are basic
tools to investigate the presence of long memory and heavy tails and their
consequences. In this paper we extend the existing literature on the asymptotic
behaviour of the partial sums and the sample covariances of long memory
stochastic volatility models in the case of infinite variance. We also consider
models with leverage, for which our results are entirely new in the infinite
variance case. Depending on the nterplay between the tail behaviour and the
intensity of dependence, wo types of convergence rates and limiting
distributions can arise. In articular, we show that the asymptotic behaviour of
partial sums is the same for both LMSV and models with leverage, whereas there
is a crucial difference when sample covariances are considered."@2011
Jie Yang@http://arxiv.org/abs/1109.5320v8@Optimal Designs for 2^k Factorial Experiments with Binary Response@"We consider the problem of obtaining D-optimal designs for factorial
experiments with a binary response and $k$ qualitative factors each at two
levels. We obtain a characterization for a design to be locally D-optimal.
Based on this characterization, we develop efficient numerical techniques to
search for locally D-optimal designs. Using prior distributions on the
parameters, we investigate EW D-optimal designs, which are designs that
maximize the determinant of the expected information matrix. It turns out that
these designs can be obtained very easily using our algorithm for locally
D-optimal designs and are very good surrogates for Bayes D-optimal designs. We
also investigate the properties of fractional factorial designs and study the
robustness with respect to the assumed parameter values of locally D-optimal
designs."@2011
Abhyuday Mandal@http://arxiv.org/abs/1109.5320v8@Optimal Designs for 2^k Factorial Experiments with Binary Response@"We consider the problem of obtaining D-optimal designs for factorial
experiments with a binary response and $k$ qualitative factors each at two
levels. We obtain a characterization for a design to be locally D-optimal.
Based on this characterization, we develop efficient numerical techniques to
search for locally D-optimal designs. Using prior distributions on the
parameters, we investigate EW D-optimal designs, which are designs that
maximize the determinant of the expected information matrix. It turns out that
these designs can be obtained very easily using our algorithm for locally
D-optimal designs and are very good surrogates for Bayes D-optimal designs. We
also investigate the properties of fractional factorial designs and study the
robustness with respect to the assumed parameter values of locally D-optimal
designs."@2011
Dibyen Majumdar@http://arxiv.org/abs/1109.5320v8@Optimal Designs for 2^k Factorial Experiments with Binary Response@"We consider the problem of obtaining D-optimal designs for factorial
experiments with a binary response and $k$ qualitative factors each at two
levels. We obtain a characterization for a design to be locally D-optimal.
Based on this characterization, we develop efficient numerical techniques to
search for locally D-optimal designs. Using prior distributions on the
parameters, we investigate EW D-optimal designs, which are designs that
maximize the determinant of the expected information matrix. It turns out that
these designs can be obtained very easily using our algorithm for locally
D-optimal designs and are very good surrogates for Bayes D-optimal designs. We
also investigate the properties of fractional factorial designs and study the
robustness with respect to the assumed parameter values of locally D-optimal
designs."@2011
Christophe Giraud@http://arxiv.org/abs/1109.5587v2@High-dimensional regression with unknown variance@"We review recent results for high-dimensional sparse linear regression in the
practical case of unknown variance. Different sparsity settings are covered,
including coordinate-sparsity, group-sparsity and variation-sparsity. The
emphasis is put on non-asymptotic analyses and feasible procedures. In
addition, a small numerical study compares the practical performance of three
schemes for tuning the Lasso estimator and some references are collected for
some more general models, including multivariate regression and nonparametric
regression."@2011
Sylvie Huet@http://arxiv.org/abs/1109.5587v2@High-dimensional regression with unknown variance@"We review recent results for high-dimensional sparse linear regression in the
practical case of unknown variance. Different sparsity settings are covered,
including coordinate-sparsity, group-sparsity and variation-sparsity. The
emphasis is put on non-asymptotic analyses and feasible procedures. In
addition, a small numerical study compares the practical performance of three
schemes for tuning the Lasso estimator and some references are collected for
some more general models, including multivariate regression and nonparametric
regression."@2011
Nicolas Verzelen@http://arxiv.org/abs/1109.5587v2@High-dimensional regression with unknown variance@"We review recent results for high-dimensional sparse linear regression in the
practical case of unknown variance. Different sparsity settings are covered,
including coordinate-sparsity, group-sparsity and variation-sparsity. The
emphasis is put on non-asymptotic analyses and feasible procedures. In
addition, a small numerical study compares the practical performance of three
schemes for tuning the Lasso estimator and some references are collected for
some more general models, including multivariate regression and nonparametric
regression."@2011
Daniel J. McDonald@http://arxiv.org/abs/1109.5998v3@Estimating beta-mixing coefficients via histograms@"The literature on statistical learning for time series often assumes
asymptotic independence or ""mixing"" of the data-generating process. These
mixing assumptions are never tested, nor are there methods for estimating
mixing coefficients from data. Additionally, for many common classes of
processes (Markov processes, ARMA processes, etc.) general functional forms for
various mixing rates are known, but not specific coefficients. We present the
first estimator for beta-mixing coefficients based on a single stationary
sample path and show that it is risk consistent. Since mixing rates depend on
infinite-dimensional dependence, we use a Markov approximation based on only a
finite memory length $d$. We present convergence rates for the Markov
approximation and show that as $d\rightarrow\infty$, the Markov approximation
converges to the true mixing coefficient. Our estimator is constructed using
$d$-dimensional histogram density estimates. Allowing asymptotics in the
bandwidth as well as the dimension, we prove $L^1$ concentration for the
histogram as an intermediate step. Simulations wherein the mixing rates are
calculable and a real-data example demonstrate our methodology."@2011
Cosma Rohilla Shalizi@http://arxiv.org/abs/1109.5998v3@Estimating beta-mixing coefficients via histograms@"The literature on statistical learning for time series often assumes
asymptotic independence or ""mixing"" of the data-generating process. These
mixing assumptions are never tested, nor are there methods for estimating
mixing coefficients from data. Additionally, for many common classes of
processes (Markov processes, ARMA processes, etc.) general functional forms for
various mixing rates are known, but not specific coefficients. We present the
first estimator for beta-mixing coefficients based on a single stationary
sample path and show that it is risk consistent. Since mixing rates depend on
infinite-dimensional dependence, we use a Markov approximation based on only a
finite memory length $d$. We present convergence rates for the Markov
approximation and show that as $d\rightarrow\infty$, the Markov approximation
converges to the true mixing coefficient. Our estimator is constructed using
$d$-dimensional histogram density estimates. Allowing asymptotics in the
bandwidth as well as the dimension, we prove $L^1$ concentration for the
histogram as an intermediate step. Simulations wherein the mixing rates are
calculable and a real-data example demonstrate our methodology."@2011
Mark Schervish@http://arxiv.org/abs/1109.5998v3@Estimating beta-mixing coefficients via histograms@"The literature on statistical learning for time series often assumes
asymptotic independence or ""mixing"" of the data-generating process. These
mixing assumptions are never tested, nor are there methods for estimating
mixing coefficients from data. Additionally, for many common classes of
processes (Markov processes, ARMA processes, etc.) general functional forms for
various mixing rates are known, but not specific coefficients. We present the
first estimator for beta-mixing coefficients based on a single stationary
sample path and show that it is risk consistent. Since mixing rates depend on
infinite-dimensional dependence, we use a Markov approximation based on only a
finite memory length $d$. We present convergence rates for the Markov
approximation and show that as $d\rightarrow\infty$, the Markov approximation
converges to the true mixing coefficient. Our estimator is constructed using
$d$-dimensional histogram density estimates. Allowing asymptotics in the
bandwidth as well as the dimension, we prove $L^1$ concentration for the
histogram as an intermediate step. Simulations wherein the mixing rates are
calculable and a real-data example demonstrate our methodology."@2011
Kanti V. Mardia@http://arxiv.org/abs/1109.6042v2@Some Fundamental Properties of a Multivariate von Mises Distribution@"In application areas like bioinformatics multivariate distributions on angles
are encountered which show significant clustering. One approach to statistical
modelling of such situations is to use mixtures of unimodal distributions. In
the literature (Mardia et al., 2011), the multivariate von Mises distribution,
also known as the multivariate sine distribution, has been suggested for
components of such models, but work in the area has been hampered by the fact
that no good criteria for the von Mises distribution to be unimodal were
available. In this article we study the question about when a multivariate von
Mises distribution is unimodal. We give sufficient criteria for this to be the
case and show examples of distributions with multiple modes when these criteria
are violated. In addition, we propose a method to generate samples from the von
Mises distribution in the case of high concentration."@2011
Jochen Voss@http://arxiv.org/abs/1109.6042v2@Some Fundamental Properties of a Multivariate von Mises Distribution@"In application areas like bioinformatics multivariate distributions on angles
are encountered which show significant clustering. One approach to statistical
modelling of such situations is to use mixtures of unimodal distributions. In
the literature (Mardia et al., 2011), the multivariate von Mises distribution,
also known as the multivariate sine distribution, has been suggested for
components of such models, but work in the area has been hampered by the fact
that no good criteria for the von Mises distribution to be unimodal were
available. In this article we study the question about when a multivariate von
Mises distribution is unimodal. We give sufficient criteria for this to be the
case and show examples of distributions with multiple modes when these criteria
are violated. In addition, we propose a method to generate samples from the von
Mises distribution in the case of high concentration."@2011
Piet Groeneboom@http://arxiv.org/abs/1109.6176v2@Estimators for the interval censoring problem@"We study three estimators for the interval censoring case 2 problem, a
histogram-type estimator, proposed in Birg\'e (1999), the maximum likelihood
estimator (MLE) and the smoothed MLE, using a smoothing kernel. Our focus is on
the asymptotic distribution of the estimators at a fixed point. The estimators
are compared in a simulation study."@2011
Tom Ketelaars@http://arxiv.org/abs/1109.6176v2@Estimators for the interval censoring problem@"We study three estimators for the interval censoring case 2 problem, a
histogram-type estimator, proposed in Birg\'e (1999), the maximum likelihood
estimator (MLE) and the smoothed MLE, using a smoothing kernel. Our focus is on
the asymptotic distribution of the estimators at a fixed point. The estimators
are compared in a simulation study."@2011
Weining Shen@http://arxiv.org/abs/1109.6406v3@"Adaptive Bayesian multivariate density estimation with Dirichlet
  mixtures"@"We show that rate-adaptive multivariate density estimation can be performed
using Bayesian methods based on Dirichlet mixtures of normal kernels with a
prior distribution on the kernel's covariance matrix parameter. We derive
sufficient conditions on the prior specification that guarantee convergence to
a true density at a rate that is optimal minimax for the smoothness class to
which the true density belongs. No prior knowledge of smoothness is assumed.
The sufficient conditions are shown to hold for the Dirichlet location mixture
of normals prior with a Gaussian base measure and an inverse-Wishart prior on
the covariance matrix parameter. Locally H\""older smoothness classes and their
anisotropic extensions are considered. Our study involves several technical
novelties, including sharp approximation of finitely differentiable
multivariate densities by normal mixtures and a new sieve on the space of such
densities."@2011
Surya T. Tokdar@http://arxiv.org/abs/1109.6406v3@"Adaptive Bayesian multivariate density estimation with Dirichlet
  mixtures"@"We show that rate-adaptive multivariate density estimation can be performed
using Bayesian methods based on Dirichlet mixtures of normal kernels with a
prior distribution on the kernel's covariance matrix parameter. We derive
sufficient conditions on the prior specification that guarantee convergence to
a true density at a rate that is optimal minimax for the smoothness class to
which the true density belongs. No prior knowledge of smoothness is assumed.
The sufficient conditions are shown to hold for the Dirichlet location mixture
of normals prior with a Gaussian base measure and an inverse-Wishart prior on
the covariance matrix parameter. Locally H\""older smoothness classes and their
anisotropic extensions are considered. Our study involves several technical
novelties, including sharp approximation of finitely differentiable
multivariate densities by normal mixtures and a new sieve on the space of such
densities."@2011
Subhashis Ghosal@http://arxiv.org/abs/1109.6406v3@"Adaptive Bayesian multivariate density estimation with Dirichlet
  mixtures"@"We show that rate-adaptive multivariate density estimation can be performed
using Bayesian methods based on Dirichlet mixtures of normal kernels with a
prior distribution on the kernel's covariance matrix parameter. We derive
sufficient conditions on the prior specification that guarantee convergence to
a true density at a rate that is optimal minimax for the smoothness class to
which the true density belongs. No prior knowledge of smoothness is assumed.
The sufficient conditions are shown to hold for the Dirichlet location mixture
of normals prior with a Gaussian base measure and an inverse-Wishart prior on
the covariance matrix parameter. Locally H\""older smoothness classes and their
anisotropic extensions are considered. Our study involves several technical
novelties, including sharp approximation of finitely differentiable
multivariate densities by normal mixtures and a new sieve on the space of such
densities."@2011
Axel Bücher@http://arxiv.org/abs/1109.6501v1@A test for Archimedeanity in bivariate copula models@"We propose a new test for the hypothesis that a bivariate copula is an
Archimedean copula. The test statistic is based on a combination of two
measures resulting from the characterization of Archimedean copulas by the
property of associativity and by a strict upper bound on the diagonal by the
Fr\'echet-upper bound. We prove weak convergence of this statistic and show
that the critical values of the corresponding test can be determined by the
multiplier bootstrap method. The test is shown to be consistent against all
departures from Archimedeanity if the copula satisfies weak smoothness
assumptions. A simulation study is presented which illustrates the finite
sample properties of the new test."@2011
Holger Dette@http://arxiv.org/abs/1109.6501v1@A test for Archimedeanity in bivariate copula models@"We propose a new test for the hypothesis that a bivariate copula is an
Archimedean copula. The test statistic is based on a combination of two
measures resulting from the characterization of Archimedean copulas by the
property of associativity and by a strict upper bound on the diagonal by the
Fr\'echet-upper bound. We prove weak convergence of this statistic and show
that the critical values of the corresponding test can be determined by the
multiplier bootstrap method. The test is shown to be consistent against all
departures from Archimedeanity if the copula satisfies weak smoothness
assumptions. A simulation study is presented which illustrates the finite
sample properties of the new test."@2011
Stanislav Volgushev@http://arxiv.org/abs/1109.6501v1@A test for Archimedeanity in bivariate copula models@"We propose a new test for the hypothesis that a bivariate copula is an
Archimedean copula. The test statistic is based on a combination of two
measures resulting from the characterization of Archimedean copulas by the
property of associativity and by a strict upper bound on the diagonal by the
Fr\'echet-upper bound. We prove weak convergence of this statistic and show
that the critical values of the corresponding test can be determined by the
multiplier bootstrap method. The test is shown to be consistent against all
departures from Archimedeanity if the copula satisfies weak smoothness
assumptions. A simulation study is presented which illustrates the finite
sample properties of the new test."@2011
Maxime Lenormand@http://arxiv.org/abs/1109.6759v3@Generating French virtual commuting network at municipality level@"We aim to generate virtual commuting networks in the French rural regions in
order to study the dynamics of their municipalities. Since we have to model
small commuting flows between municipalities with a few hundreds or thousands
inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.
It reproduces the various possible complete networks using an iterative
process, stochastically choosing a workplace in the region for each commuter
living in the municipality of a region. The choice is made considering the job
offers in each municipality of the region and the distance to all the possible
destinations. This paper presents how to adapt and implement this model to
generate French regions commuting networks between municipalities. We address
three different questions: How to generate a reliable virtual commuting network
for a region highly dependant of other regions for the satisfaction of its
resident's demand for employment? What about a convenient deterrence function?
How to calibrate the model when detailed data is not available? We answer
proposing an extended job search geographical base for commuters living in the
municipalities, we compare two different deterrence functions and we show that
the parameter is a constant for network linking French municipalities."@2011
Sylvie Huet@http://arxiv.org/abs/1109.6759v3@Generating French virtual commuting network at municipality level@"We aim to generate virtual commuting networks in the French rural regions in
order to study the dynamics of their municipalities. Since we have to model
small commuting flows between municipalities with a few hundreds or thousands
inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.
It reproduces the various possible complete networks using an iterative
process, stochastically choosing a workplace in the region for each commuter
living in the municipality of a region. The choice is made considering the job
offers in each municipality of the region and the distance to all the possible
destinations. This paper presents how to adapt and implement this model to
generate French regions commuting networks between municipalities. We address
three different questions: How to generate a reliable virtual commuting network
for a region highly dependant of other regions for the satisfaction of its
resident's demand for employment? What about a convenient deterrence function?
How to calibrate the model when detailed data is not available? We answer
proposing an extended job search geographical base for commuters living in the
municipalities, we compare two different deterrence functions and we show that
the parameter is a constant for network linking French municipalities."@2011
Floriana Gargiulo@http://arxiv.org/abs/1109.6759v3@Generating French virtual commuting network at municipality level@"We aim to generate virtual commuting networks in the French rural regions in
order to study the dynamics of their municipalities. Since we have to model
small commuting flows between municipalities with a few hundreds or thousands
inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.
It reproduces the various possible complete networks using an iterative
process, stochastically choosing a workplace in the region for each commuter
living in the municipality of a region. The choice is made considering the job
offers in each municipality of the region and the distance to all the possible
destinations. This paper presents how to adapt and implement this model to
generate French regions commuting networks between municipalities. We address
three different questions: How to generate a reliable virtual commuting network
for a region highly dependant of other regions for the satisfaction of its
resident's demand for employment? What about a convenient deterrence function?
How to calibrate the model when detailed data is not available? We answer
proposing an extended job search geographical base for commuters living in the
municipalities, we compare two different deterrence functions and we show that
the parameter is a constant for network linking French municipalities."@2011
Alfred Hero@http://arxiv.org/abs/1109.6846v2@Hub discovery in partial correlation graphical models@"This paper treats the problem of screening a p-variate sample for strongly
and multiply connected vertices in the partial correlation graph associated
with the the partial correlation matrix of the sample. This problem, called hub
screening, is important in many applications ranging from network security to
computational biology to finance to social networks. In the area of network
security, a node that becomes a hub of high correlation with neighboring nodes
might signal anomalous activity such as a coordinated flooding attack. In the
area of computational biology the set of hubs of a gene expression correlation
graph can serve as potential targets for drug treatment to block a pathway or
modulate host response. In the area of finance a hub might indicate a
vulnerable financial instrument or sector whose collapse might have major
repercussions on the market. In the area of social networks a hub of observed
interactions between criminal suspects could be an influential ringleader. The
techniques and theory presented in this paper permit scalable and reliable
screening for such hubs. This paper extends our previous work on correlation
screening [arXiv:1102.1204] to the more challenging problem of partial
correlation screening for variables with a high degree of connectivity. In
particular we consider 1) extension to the more difficult problem of screening
for partial correlations exceeding a specified magnitude; 2) extension to
screening variables whose vertex degree in the associated partial correlation
graph, often called the concentration graph, exceeds a specified degree."@2011
Bala Rajaratnam@http://arxiv.org/abs/1109.6846v2@Hub discovery in partial correlation graphical models@"This paper treats the problem of screening a p-variate sample for strongly
and multiply connected vertices in the partial correlation graph associated
with the the partial correlation matrix of the sample. This problem, called hub
screening, is important in many applications ranging from network security to
computational biology to finance to social networks. In the area of network
security, a node that becomes a hub of high correlation with neighboring nodes
might signal anomalous activity such as a coordinated flooding attack. In the
area of computational biology the set of hubs of a gene expression correlation
graph can serve as potential targets for drug treatment to block a pathway or
modulate host response. In the area of finance a hub might indicate a
vulnerable financial instrument or sector whose collapse might have major
repercussions on the market. In the area of social networks a hub of observed
interactions between criminal suspects could be an influential ringleader. The
techniques and theory presented in this paper permit scalable and reliable
screening for such hubs. This paper extends our previous work on correlation
screening [arXiv:1102.1204] to the more challenging problem of partial
correlation screening for variables with a high degree of connectivity. In
particular we consider 1) extension to the more difficult problem of screening
for partial correlations exceeding a specified magnitude; 2) extension to
screening variables whose vertex degree in the associated partial correlation
graph, often called the concentration graph, exceeds a specified degree."@2011
Magda Peligrad@http://arxiv.org/abs/1111.0537v3@Exact Moderate and Large Deviations for Linear Processes@"Large and moderate deviation probabilities play an important role in many
applied areas, such as insurance and risk analysis. This paper studies the
exact moderate and large deviation asymptotics in non-logarithmic form for
linear processes with independent innovations. The linear processes we analyze
are general and therefore they include the long memory case. We give an
asymptotic representation for probability of the tail of the normalized sums
and specify the zones in which it can be approximated either by a standard
normal distribution or by the marginal distribution of the innovation process.
The results are then applied to regression estimates, moving averages,
fractionally integrated processes, linear processes with regularly varying
exponents and functions of linear processes. We also consider the computation
of value at risk and expected shortfall, fundamental quantities in risk theory
and finance."@2011
Hailin Sang@http://arxiv.org/abs/1111.0537v3@Exact Moderate and Large Deviations for Linear Processes@"Large and moderate deviation probabilities play an important role in many
applied areas, such as insurance and risk analysis. This paper studies the
exact moderate and large deviation asymptotics in non-logarithmic form for
linear processes with independent innovations. The linear processes we analyze
are general and therefore they include the long memory case. We give an
asymptotic representation for probability of the tail of the normalized sums
and specify the zones in which it can be approximated either by a standard
normal distribution or by the marginal distribution of the innovation process.
The results are then applied to regression estimates, moving averages,
fractionally integrated processes, linear processes with regularly varying
exponents and functions of linear processes. We also consider the computation
of value at risk and expected shortfall, fundamental quantities in risk theory
and finance."@2011
Yunda Zhong@http://arxiv.org/abs/1111.0537v3@Exact Moderate and Large Deviations for Linear Processes@"Large and moderate deviation probabilities play an important role in many
applied areas, such as insurance and risk analysis. This paper studies the
exact moderate and large deviation asymptotics in non-logarithmic form for
linear processes with independent innovations. The linear processes we analyze
are general and therefore they include the long memory case. We give an
asymptotic representation for probability of the tail of the normalized sums
and specify the zones in which it can be approximated either by a standard
normal distribution or by the marginal distribution of the innovation process.
The results are then applied to regression estimates, moving averages,
fractionally integrated processes, linear processes with regularly varying
exponents and functions of linear processes. We also consider the computation
of value at risk and expected shortfall, fundamental quantities in risk theory
and finance."@2011
Wei Biao Wu@http://arxiv.org/abs/1111.0537v3@Exact Moderate and Large Deviations for Linear Processes@"Large and moderate deviation probabilities play an important role in many
applied areas, such as insurance and risk analysis. This paper studies the
exact moderate and large deviation asymptotics in non-logarithmic form for
linear processes with independent innovations. The linear processes we analyze
are general and therefore they include the long memory case. We give an
asymptotic representation for probability of the tail of the normalized sums
and specify the zones in which it can be approximated either by a standard
normal distribution or by the marginal distribution of the innovation process.
The results are then applied to regression estimates, moving averages,
fractionally integrated processes, linear processes with regularly varying
exponents and functions of linear processes. We also consider the computation
of value at risk and expected shortfall, fundamental quantities in risk theory
and finance."@2011
Ryan J. Tibshirani@http://arxiv.org/abs/1111.0653v4@Degrees of freedom in lasso problems@"We derive the degrees of freedom of the lasso fit, placing no assumptions on
the predictor matrix $X$. Like the well-known result of Zou, Hastie and
Tibshirani [Ann. Statist. 35 (2007) 2173-2192], which gives the degrees of
freedom of the lasso fit when $X$ has full column rank, we express our result
in terms of the active set of a lasso solution. We extend this result to cover
the degrees of freedom of the generalized lasso fit for an arbitrary predictor
matrix $X$ (and an arbitrary penalty matrix $D$). Though our focus is degrees
of freedom, we establish some intermediate results on the lasso and generalized
lasso that may be interesting on their own."@2011
Jonathan Taylor@http://arxiv.org/abs/1111.0653v4@Degrees of freedom in lasso problems@"We derive the degrees of freedom of the lasso fit, placing no assumptions on
the predictor matrix $X$. Like the well-known result of Zou, Hastie and
Tibshirani [Ann. Statist. 35 (2007) 2173-2192], which gives the degrees of
freedom of the lasso fit when $X$ has full column rank, we express our result
in terms of the active set of a lasso solution. We extend this result to cover
the degrees of freedom of the generalized lasso fit for an arbitrary predictor
matrix $X$ (and an arbitrary penalty matrix $D$). Though our focus is degrees
of freedom, we establish some intermediate results on the lasso and generalized
lasso that may be interesting on their own."@2011
Anirban Bhattacharya@http://arxiv.org/abs/1111.1044v4@Anisotropic function estimation using multi-bandwidth Gaussian processes@"In nonparametric regression problems involving multiple predictors, there is
typically interest in estimating an anisotropic multivariate regression surface
in the important predictors while discarding the unimportant ones. Our focus is
on defining a Bayesian procedure that leads to the minimax optimal rate of
posterior contraction (up to a log factor) adapting to the unknown dimension
and anisotropic smoothness of the true surface. We propose such an approach
based on a Gaussian process prior with dimension-specific scalings, which are
assigned carefully-chosen hyperpriors. We additionally show that using a
homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate
in anisotropic cases."@2011
Debdeep Pati@http://arxiv.org/abs/1111.1044v4@Anisotropic function estimation using multi-bandwidth Gaussian processes@"In nonparametric regression problems involving multiple predictors, there is
typically interest in estimating an anisotropic multivariate regression surface
in the important predictors while discarding the unimportant ones. Our focus is
on defining a Bayesian procedure that leads to the minimax optimal rate of
posterior contraction (up to a log factor) adapting to the unknown dimension
and anisotropic smoothness of the true surface. We propose such an approach
based on a Gaussian process prior with dimension-specific scalings, which are
assigned carefully-chosen hyperpriors. We additionally show that using a
homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate
in anisotropic cases."@2011
David Dunson@http://arxiv.org/abs/1111.1044v4@Anisotropic function estimation using multi-bandwidth Gaussian processes@"In nonparametric regression problems involving multiple predictors, there is
typically interest in estimating an anisotropic multivariate regression surface
in the important predictors while discarding the unimportant ones. Our focus is
on defining a Bayesian procedure that leads to the minimax optimal rate of
posterior contraction (up to a log factor) adapting to the unknown dimension
and anisotropic smoothness of the true surface. We propose such an approach
based on a Gaussian process prior with dimension-specific scalings, which are
assigned carefully-chosen hyperpriors. We additionally show that using a
homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate
in anisotropic cases."@2011
Serge Cohen@http://arxiv.org/abs/1111.1077v1@LAN property for some fractional type Brownian motion@"We study asymptotic expansion of the likelihood of a certain class of
Gaussian processes characterized by their spectral density $f_\theta$. We
consider the case where $f_\theta\PAR{x} \sim_{x\to 0}
\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function
and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models
which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha
\geq 1/2$ or ARFIMA processes."@2011
Fabrice Gamboa@http://arxiv.org/abs/1111.1077v1@LAN property for some fractional type Brownian motion@"We study asymptotic expansion of the likelihood of a certain class of
Gaussian processes characterized by their spectral density $f_\theta$. We
consider the case where $f_\theta\PAR{x} \sim_{x\to 0}
\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function
and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models
which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha
\geq 1/2$ or ARFIMA processes."@2011
Céline Lacaux@http://arxiv.org/abs/1111.1077v1@LAN property for some fractional type Brownian motion@"We study asymptotic expansion of the likelihood of a certain class of
Gaussian processes characterized by their spectral density $f_\theta$. We
consider the case where $f_\theta\PAR{x} \sim_{x\to 0}
\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function
and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models
which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha
\geq 1/2$ or ARFIMA processes."@2011
Jean-Michel Loubes@http://arxiv.org/abs/1111.1077v1@LAN property for some fractional type Brownian motion@"We study asymptotic expansion of the likelihood of a certain class of
Gaussian processes characterized by their spectral density $f_\theta$. We
consider the case where $f_\theta\PAR{x} \sim_{x\to 0}
\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function
and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models
which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha
\geq 1/2$ or ARFIMA processes."@2011
Shota Gugushvili@http://arxiv.org/abs/1111.1120v2@"Parametric inference for stochastic differential equations: a smooth and
  match approach"@"We study the problem of parameter estimation for a univariate discretely
observed ergodic diffusion process given as a solution to a stochastic
differential equation. The estimation procedure we propose consists of two
steps. In the first step, which is referred to as a smoothing step, we smooth
the data and construct a nonparametric estimator of the invariant density of
the process. In the second step, which is referred to as a matching step, we
exploit a characterisation of the invariant density as a solution of a certain
ordinary differential equation, replace the invariant density in this equation
by its nonparametric estimator from the smoothing step in order to arrive at an
intuitively appealing criterion function, and next define our estimator of the
parameter of interest as a minimiser of this criterion function. Our main
results show that under suitable conditions our estimator is
$\sqrt{n}$-consistent, and even asymptotically normal. We also discuss a way of
improving its asymptotic performance through a one-step Newton-Raphson type
procedure and present results of a small scale simulation study."@2011
Peter Spreij@http://arxiv.org/abs/1111.1120v2@"Parametric inference for stochastic differential equations: a smooth and
  match approach"@"We study the problem of parameter estimation for a univariate discretely
observed ergodic diffusion process given as a solution to a stochastic
differential equation. The estimation procedure we propose consists of two
steps. In the first step, which is referred to as a smoothing step, we smooth
the data and construct a nonparametric estimator of the invariant density of
the process. In the second step, which is referred to as a matching step, we
exploit a characterisation of the invariant density as a solution of a certain
ordinary differential equation, replace the invariant density in this equation
by its nonparametric estimator from the smoothing step in order to arrive at an
intuitively appealing criterion function, and next define our estimator of the
parameter of interest as a minimiser of this criterion function. Our main
results show that under suitable conditions our estimator is
$\sqrt{n}$-consistent, and even asymptotically normal. We also discuss a way of
improving its asymptotic performance through a one-step Newton-Raphson type
procedure and present results of a small scale simulation study."@2011
Sylvain Le Corff@http://arxiv.org/abs/1111.1307v3@"Convergence of a Particle-based Approximation of the Block Online
  Expectation Maximization Algorithm"@"Online variants of the Expectation Maximization (EM) algorithm have recently
been proposed to perform parameter inference with large data sets or data
streams, in independent latent models and in hidden Markov models.
Nevertheless, the convergence properties of these algorithms remain an open
problem at least in the hidden Markov case. This contribution deals with a new
online EM algorithm which updates the parameter at some deterministic times.
Some convergence results have been derived even in general latent models such
as hidden Markov models. These properties rely on the assumption that some
intermediate quantities are available in closed form or can be approximated by
Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this
paper, we propose an algorithm which approximates these quantities using
Sequential Monte Carlo methods. The convergence of this algorithm and of an
averaged version is established and their performance is illustrated through
Monte Carlo experiments."@2011
Gersende Fort@http://arxiv.org/abs/1111.1307v3@"Convergence of a Particle-based Approximation of the Block Online
  Expectation Maximization Algorithm"@"Online variants of the Expectation Maximization (EM) algorithm have recently
been proposed to perform parameter inference with large data sets or data
streams, in independent latent models and in hidden Markov models.
Nevertheless, the convergence properties of these algorithms remain an open
problem at least in the hidden Markov case. This contribution deals with a new
online EM algorithm which updates the parameter at some deterministic times.
Some convergence results have been derived even in general latent models such
as hidden Markov models. These properties rely on the assumption that some
intermediate quantities are available in closed form or can be approximated by
Monte Carlo methods when the Monte Carlo error vanishes rapidly enough. In this
paper, we propose an algorithm which approximates these quantities using
Sequential Monte Carlo methods. The convergence of this algorithm and of an
averaged version is established and their performance is illustrated through
Monte Carlo experiments."@2011
Koshi Yamada@http://arxiv.org/abs/1111.1832v2@Statistical Learning Theory of Quasi-Regular Cases@"Many learning machines such as normal mixtures and layered neural networks
are not regular but singular statistical models, because the map from a
parameter to a probability distribution is not one-to-one. The conventional
statistical asymptotic theory can not be applied to such learning machines
because the likelihood function can not be approximated by any normal
distribution. Recently, new statistical theory has been established based on
algebraic geometry and it was clarified that the generalization and training
errors are determined by two birational invariants, the real log canonical
threshold and the singular fluctuation. However, their concrete values are left
unknown. In the present paper, we propose a new concept, a quasi-regular case
in statistical learning theory. A quasi-regular case is not a regular case but
a singular case, however, it has the same property as a regular case. In fact,
we prove that, in a quasi-regular case, two birational invariants are equal to
each other, resulting that the symmetry of the generalization and training
errors holds. Moreover, the concrete values of two birational invariants are
explicitly obtained, the quasi-regular case is useful to study statistical
learning theory."@2011
Sumio Watanabe@http://arxiv.org/abs/1111.1832v2@Statistical Learning Theory of Quasi-Regular Cases@"Many learning machines such as normal mixtures and layered neural networks
are not regular but singular statistical models, because the map from a
parameter to a probability distribution is not one-to-one. The conventional
statistical asymptotic theory can not be applied to such learning machines
because the likelihood function can not be approximated by any normal
distribution. Recently, new statistical theory has been established based on
algebraic geometry and it was clarified that the generalization and training
errors are determined by two birational invariants, the real log canonical
threshold and the singular fluctuation. However, their concrete values are left
unknown. In the present paper, we propose a new concept, a quasi-regular case
in statistical learning theory. A quasi-regular case is not a regular case but
a singular case, however, it has the same property as a regular case. In fact,
we prove that, in a quasi-regular case, two birational invariants are equal to
each other, resulting that the symmetry of the generalization and training
errors holds. Moreover, the concrete values of two birational invariants are
explicitly obtained, the quasi-regular case is useful to study statistical
learning theory."@2011
Sebastian Krumscheid@http://arxiv.org/abs/1111.1916v3@Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions@"We consider the problem of statistical inference for the effective dynamics
of multiscale diffusion processes with (at least) two widely separated
characteristic time scales. More precisely, we seek to determine parameters in
the effective equation describing the dynamics on the longer diffusive time
scale, i.e. in a homogenization framework. We examine the case where both the
drift and the diffusion coefficients in the effective dynamics are
space-dependent and depend on multiple unknown parameters. It is known that
classical estimators, such as Maximum Likelihood and Quadratic Variation of the
Path Estimators, fail to obtain reasonable estimates for parameters in the
effective dynamics when based on observations of the underlying multiscale
diffusion. We propose a novel algorithm for estimating both the drift and
diffusion coefficients in the effective dynamics based on a semi-parametric
framework. We demonstrate by means of extensive numerical simulations of a
number of selected examples that the algorithm performs well when applied to
data from a multiscale diffusion. These examples also illustrate that the
algorithm can be used effectively to obtain accurate and unbiased estimates."@2011
Grigorios A. Pavliotis@http://arxiv.org/abs/1111.1916v3@Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions@"We consider the problem of statistical inference for the effective dynamics
of multiscale diffusion processes with (at least) two widely separated
characteristic time scales. More precisely, we seek to determine parameters in
the effective equation describing the dynamics on the longer diffusive time
scale, i.e. in a homogenization framework. We examine the case where both the
drift and the diffusion coefficients in the effective dynamics are
space-dependent and depend on multiple unknown parameters. It is known that
classical estimators, such as Maximum Likelihood and Quadratic Variation of the
Path Estimators, fail to obtain reasonable estimates for parameters in the
effective dynamics when based on observations of the underlying multiscale
diffusion. We propose a novel algorithm for estimating both the drift and
diffusion coefficients in the effective dynamics based on a semi-parametric
framework. We demonstrate by means of extensive numerical simulations of a
number of selected examples that the algorithm performs well when applied to
data from a multiscale diffusion. These examples also illustrate that the
algorithm can be used effectively to obtain accurate and unbiased estimates."@2011
Serafim Kalliadasis@http://arxiv.org/abs/1111.1916v3@Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions@"We consider the problem of statistical inference for the effective dynamics
of multiscale diffusion processes with (at least) two widely separated
characteristic time scales. More precisely, we seek to determine parameters in
the effective equation describing the dynamics on the longer diffusive time
scale, i.e. in a homogenization framework. We examine the case where both the
drift and the diffusion coefficients in the effective dynamics are
space-dependent and depend on multiple unknown parameters. It is known that
classical estimators, such as Maximum Likelihood and Quadratic Variation of the
Path Estimators, fail to obtain reasonable estimates for parameters in the
effective dynamics when based on observations of the underlying multiscale
diffusion. We propose a novel algorithm for estimating both the drift and
diffusion coefficients in the effective dynamics based on a semi-parametric
framework. We demonstrate by means of extensive numerical simulations of a
number of selected examples that the algorithm performs well when applied to
data from a multiscale diffusion. These examples also illustrate that the
algorithm can be used effectively to obtain accurate and unbiased estimates."@2011
A. Garivier@http://arxiv.org/abs/1111.2191v1@Oracle approach and slope heuristic in context tree estimation@"We introduce a general approach to prove oracle properties in context tree
selection. The results derive from a concentration condition that is verified,
for example, by mixing processes. Moreover, we show the superiority of the
oracle approach from a non-asymptotic point of view in simulations where the
classical BIC estimator has nice oracle properties even when it does not
recover the source.
  Our second objective is to extend the slope algorithm of \cite{AM08} to
context tree estimation. The algorithm gives a practical way to evaluate the
leading constant in front of the penalties. We study the slope heuristic
underlying this algorithm and obtain the first results on the slope phenomenon
in a discrete, non i.i.d framework. We illustrate in simulations the
improvement of the oracle properties of BIC estimators by the slope algorithm."@2011
M. Lerasle@http://arxiv.org/abs/1111.2191v1@Oracle approach and slope heuristic in context tree estimation@"We introduce a general approach to prove oracle properties in context tree
selection. The results derive from a concentration condition that is verified,
for example, by mixing processes. Moreover, we show the superiority of the
oracle approach from a non-asymptotic point of view in simulations where the
classical BIC estimator has nice oracle properties even when it does not
recover the source.
  Our second objective is to extend the slope algorithm of \cite{AM08} to
context tree estimation. The algorithm gives a practical way to evaluate the
leading constant in front of the penalties. We study the slope heuristic
underlying this algorithm and obtain the first results on the slope phenomenon
in a discrete, non i.i.d framework. We illustrate in simulations the
improvement of the oracle properties of BIC estimators by the slope algorithm."@2011
Sándor Baran@http://arxiv.org/abs/1111.2205v1@Parameter estimation in linear regression driven by a Gaussian sheet@"The problem of estimating the parameters of a linear regression model
$Z(s,t)=m_1g_1(s,t)+ \cdots + m_pg_p(s,t)+U(s,t)$ based on observations of $Z$
on a spatial domain $G$ of special shape is considered, where the driving
process $U$ is a Gaussian random field and $g_1, \ldots, g_p$ are known
functions. Explicit forms of the maximum likelihood estimators of the
parameters are derived in the cases when $U$ is either a Wiener or a stationary
or nonstationary Ornstein-Uhlenbeck sheet. Simulation results are also
presented, where the driving random sheets are simulated with the help of their
Karhunen-Lo\`eve expansions."@2011
Kinga Sikolya@http://arxiv.org/abs/1111.2205v1@Parameter estimation in linear regression driven by a Gaussian sheet@"The problem of estimating the parameters of a linear regression model
$Z(s,t)=m_1g_1(s,t)+ \cdots + m_pg_p(s,t)+U(s,t)$ based on observations of $Z$
on a spatial domain $G$ of special shape is considered, where the driving
process $U$ is a Gaussian random field and $g_1, \ldots, g_p$ are known
functions. Explicit forms of the maximum likelihood estimators of the
parameters are derived in the cases when $U$ is either a Wiener or a stationary
or nonstationary Ornstein-Uhlenbeck sheet. Simulation results are also
presented, where the driving random sheets are simulated with the help of their
Karhunen-Lo\`eve expansions."@2011
Cristina Butucea@http://arxiv.org/abs/1111.2247v1@Semiparametric mixtures of symmetric distributions@"We consider in this paper the semiparametric mixture of two distributions
equal up to a shift parameter. The model is said to be semiparametric in the
sense that the mixed distribution is not supposed to belong to a parametric
family. In order to insure the identifiability of the model it is assumed that
the mixed distribution is symmetric, the model being then defined by the mixing
proportion, two location parameters, and the probability density function of
the mixed distribution. We propose a new class of M-estimators of these
parameters based on a Fourier approach, and prove that they are square root
consistent under mild regularity conditions. Their finite-sample properties are
illustrated by a Monte Carlo study and a benchmark real dataset is also studied
with our method."@2011
Pierre Vandekerkhove@http://arxiv.org/abs/1111.2247v1@Semiparametric mixtures of symmetric distributions@"We consider in this paper the semiparametric mixture of two distributions
equal up to a shift parameter. The model is said to be semiparametric in the
sense that the mixed distribution is not supposed to belong to a parametric
family. In order to insure the identifiability of the model it is assumed that
the mixed distribution is symmetric, the model being then defined by the mixing
proportion, two location parameters, and the probability density function of
the mixed distribution. We propose a new class of M-estimators of these
parameters based on a Fourier approach, and prove that they are square root
consistent under mild regularity conditions. Their finite-sample properties are
illustrated by a Monte Carlo study and a benchmark real dataset is also studied
with our method."@2011
Daniel Berend@http://arxiv.org/abs/1111.2328v1@The Missing Mass Problem@"We give tight lower and upper bounds on the expected missing mass for
distributions over finite and countably infinite spaces. An essential
characterization of the extremal distributions is given. We also provide an
extension to totally bounded metric spaces that may be of independent interest."@2011
Aryeh Kontorovich@http://arxiv.org/abs/1111.2328v1@The Missing Mass Problem@"We give tight lower and upper bounds on the expected missing mass for
distributions over finite and countably infinite spaces. An essential
characterization of the extremal distributions is given. We also provide an
extension to totally bounded metric spaces that may be of independent interest."@2011
Axel Bücher@http://arxiv.org/abs/1111.2778v1@"Empirical and sequential empirical copula processes under serial
  dependence"@"The empirical copula process plays a central role for statistical inference
on copulas. Recently, Segers (2011) investigated the asymptotic behavior of
this process under non-restrictive smoothness assumptions for the case of
i.i.d. random variables. In the present paper we extend his main result to the
case of serial dependent random variables by means of the powerful and elegant
functional delta method. Moreover, we utilize the functional delta method in
order to obtain conditional consistency of certain bootstrap procedures.
Finally, we extend the results to the more general sequential empirical copula
process under serial dependence."@2011
Stanislav Volgushev@http://arxiv.org/abs/1111.2778v1@"Empirical and sequential empirical copula processes under serial
  dependence"@"The empirical copula process plays a central role for statistical inference
on copulas. Recently, Segers (2011) investigated the asymptotic behavior of
this process under non-restrictive smoothness assumptions for the case of
i.i.d. random variables. In the present paper we extend his main result to the
case of serial dependent random variables by means of the powerful and elegant
functional delta method. Moreover, we utilize the functional delta method in
order to obtain conditional consistency of certain bootstrap procedures.
Finally, we extend the results to the more general sequential empirical copula
process under serial dependence."@2011
Florian Gach@http://arxiv.org/abs/1111.2807v2@Spatially Adaptive Density Estimation by Localised Haar Projections@"Given a random sample from some unknown density $f_0: \mathbb R \to [0,
\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution
levels constructed from localised test procedures (as in Lepski, Mammen, and
Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to
spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$
in a fixed interval, in sup-norm loss. The thresholding constants involved in
the test procedures can be chosen in practice under the idealised assumption
that the true density is locally constant in a neighborhood of the point $x$ of
estimation, and an information theoretic justification of this practice is
given."@2011
Richard Nickl@http://arxiv.org/abs/1111.2807v2@Spatially Adaptive Density Estimation by Localised Haar Projections@"Given a random sample from some unknown density $f_0: \mathbb R \to [0,
\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution
levels constructed from localised test procedures (as in Lepski, Mammen, and
Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to
spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$
in a fixed interval, in sup-norm loss. The thresholding constants involved in
the test procedures can be chosen in practice under the idealised assumption
that the true density is locally constant in a neighborhood of the point $x$ of
estimation, and an information theoretic justification of this practice is
given."@2011
Vladimir Spokoiny@http://arxiv.org/abs/1111.2807v2@Spatially Adaptive Density Estimation by Localised Haar Projections@"Given a random sample from some unknown density $f_0: \mathbb R \to [0,
\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution
levels constructed from localised test procedures (as in Lepski, Mammen, and
Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to
spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$
in a fixed interval, in sup-norm loss. The thresholding constants involved in
the test procedures can be chosen in practice under the idealised assumption
that the true density is locally constant in a neighborhood of the point $x$ of
estimation, and an information theoretic justification of this practice is
given."@2011
Cosma Rohilla Shalizi@http://arxiv.org/abs/1111.3054v4@Consistency under sampling of exponential random graph models@"The growing availability of network data and of scientific interest in
distributed systems has led to the rapid development of statistical models of
network structure. Typically, however, these are models for the entire network,
while the data consists only of a sampled sub-network. Parameters for the whole
network, which is what is of interest, are estimated by applying the model to
the sub-network. This assumes that the model is consistent under sampling, or,
in terms of the theory of stochastic processes, that it defines a projective
family. Focusing on the popular class of exponential random graph models
(ERGMs), we show that this apparently trivial condition is in fact violated by
many popular and scientifically appealing models, and that satisfying it
drastically limits ERGM's expressive power. These results are actually special
cases of more general results about exponential families of dependent random
variables, which we also prove. Using such results, we offer easily checked
conditions for the consistency of maximum likelihood estimation in ERGMs, and
discuss some possible constructive responses."@2011
Alessandro Rinaldo@http://arxiv.org/abs/1111.3054v4@Consistency under sampling of exponential random graph models@"The growing availability of network data and of scientific interest in
distributed systems has led to the rapid development of statistical models of
network structure. Typically, however, these are models for the entire network,
while the data consists only of a sampled sub-network. Parameters for the whole
network, which is what is of interest, are estimated by applying the model to
the sub-network. This assumes that the model is consistent under sampling, or,
in terms of the theory of stochastic processes, that it defines a projective
family. Focusing on the popular class of exponential random graph models
(ERGMs), we show that this apparently trivial condition is in fact violated by
many popular and scientifically appealing models, and that satisfying it
drastically limits ERGM's expressive power. These results are actually special
cases of more general results about exponential families of dependent random
variables, which we also prove. Using such results, we offer easily checked
conditions for the consistency of maximum likelihood estimation in ERGMs, and
discuss some possible constructive responses."@2011
Jorge Carlos Román@http://arxiv.org/abs/1111.3210v2@"Convergence analysis of the Gibbs sampler for Bayesian general linear
  mixed models with improper priors"@"Bayesian analysis of data from the general linear mixed model is challenging
because any nontrivial prior leads to an intractable posterior density.
However, if a conditionally conjugate prior density is adopted, then there is a
simple Gibbs sampler that can be employed to explore the posterior density. A
popular default among the conditionally conjugate priors is an improper prior
that takes a product form with a flat prior on the regression parameter, and
so-called power priors on each of the variance components. In this paper, a
convergence rate analysis of the corresponding Gibbs sampler is undertaken. The
main result is a simple, easily-checked sufficient condition for geometric
ergodicity of the Gibbs-Markov chain. This result is close to the best possible
result in the sense that the sufficient condition is only slightly stronger
than what is required to ensure posterior propriety. The theory developed in
this paper is extremely important from a practical standpoint because it
guarantees the existence of central limit theorems that allow for the
computation of valid asymptotic standard errors for the estimates computed
using the Gibbs sampler."@2011
James P. Hobert@http://arxiv.org/abs/1111.3210v2@"Convergence analysis of the Gibbs sampler for Bayesian general linear
  mixed models with improper priors"@"Bayesian analysis of data from the general linear mixed model is challenging
because any nontrivial prior leads to an intractable posterior density.
However, if a conditionally conjugate prior density is adopted, then there is a
simple Gibbs sampler that can be employed to explore the posterior density. A
popular default among the conditionally conjugate priors is an improper prior
that takes a product form with a flat prior on the regression parameter, and
so-called power priors on each of the variance components. In this paper, a
convergence rate analysis of the corresponding Gibbs sampler is undertaken. The
main result is a simple, easily-checked sufficient condition for geometric
ergodicity of the Gibbs-Markov chain. This result is close to the best possible
result in the sense that the sufficient condition is only slightly stronger
than what is required to ensure posterior propriety. The theory developed in
this paper is extremely important from a practical standpoint because it
guarantees the existence of central limit theorems that allow for the
computation of valid asymptotic standard errors for the estimates computed
using the Gibbs sampler."@2011
Elizabeth Gross@http://arxiv.org/abs/1111.3308v1@Maximum likelihood degree of variance component models@"Most statistical software packages implement numerical strategies for
computation of maximum likelihood estimates in random effects models. Little is
known, however, about the algebraic complexity of this problem. For the one-way
layout with random effects and unbalanced group sizes, we give formulas for the
algebraic degree of the likelihood equations as well as the equations for
restricted maximum likelihood estimation. In particular, the latter approach is
shown to be algebraically less complex. The formulas are obtained by studying a
univariate rational equation whose solutions correspond to the solutions of the
likelihood equations. Applying techniques from computational algebra, we also
show that balanced two-way layouts with or without interaction have likelihood
equations of degree four. Our work suggests that algebraic methods allow one to
reliably find global optima of likelihood functions of linear mixed models with
a small number of variance components."@2011
Mathias Drton@http://arxiv.org/abs/1111.3308v1@Maximum likelihood degree of variance component models@"Most statistical software packages implement numerical strategies for
computation of maximum likelihood estimates in random effects models. Little is
known, however, about the algebraic complexity of this problem. For the one-way
layout with random effects and unbalanced group sizes, we give formulas for the
algebraic degree of the likelihood equations as well as the equations for
restricted maximum likelihood estimation. In particular, the latter approach is
shown to be algebraically less complex. The formulas are obtained by studying a
univariate rational equation whose solutions correspond to the solutions of the
likelihood equations. Applying techniques from computational algebra, we also
show that balanced two-way layouts with or without interaction have likelihood
equations of degree four. Our work suggests that algebraic methods allow one to
reliably find global optima of likelihood functions of linear mixed models with
a small number of variance components."@2011
Sonja Petrović@http://arxiv.org/abs/1111.3308v1@Maximum likelihood degree of variance component models@"Most statistical software packages implement numerical strategies for
computation of maximum likelihood estimates in random effects models. Little is
known, however, about the algebraic complexity of this problem. For the one-way
layout with random effects and unbalanced group sizes, we give formulas for the
algebraic degree of the likelihood equations as well as the equations for
restricted maximum likelihood estimation. In particular, the latter approach is
shown to be algebraically less complex. The formulas are obtained by studying a
univariate rational equation whose solutions correspond to the solutions of the
likelihood equations. Applying techniques from computational algebra, we also
show that balanced two-way layouts with or without interaction have likelihood
equations of degree four. Our work suggests that algebraic methods allow one to
reliably find global optima of likelihood functions of linear mixed models with
a small number of variance components."@2011
Oleg Lepski@http://arxiv.org/abs/1111.3563v4@Adaptive estimation in the single-index model via oracle approach@"In the framework of nonparametric multivariate function estimation we are
interested in structural adaptation. We assume that the function to be
estimated has the ""single-index"" structure where neither the link function nor
the index vector is known. We suggest a novel procedure that adapts
simultaneously to the unknown index and smoothness of the link function. For
the proposed procedure, we prove a ""local"" oracle inequality (described by the
pointwise seminorm), which is then used to obtain the upper bound on the
maximal risk of the adaptive estimator under assumption that the link function
belongs to a scale of H\""{o}lder classes. The lower bound on the minimax risk
shows that in the case of estimating at a given point the constructed estimator
is optimally rate adaptive over the considered range of classes. For the same
procedure we also establish a ""global"" oracle inequality (under the $ L_r $
norm, $r< \infty $) and examine its performance over the Nikol'skii classes.
This study shows that the proposed method can be applied to estimating
functions of inhomogeneous smoothness, that is whose smoothness may vary from
point to point."@2011
Nora Serdyukova@http://arxiv.org/abs/1111.3563v4@Adaptive estimation in the single-index model via oracle approach@"In the framework of nonparametric multivariate function estimation we are
interested in structural adaptation. We assume that the function to be
estimated has the ""single-index"" structure where neither the link function nor
the index vector is known. We suggest a novel procedure that adapts
simultaneously to the unknown index and smoothness of the link function. For
the proposed procedure, we prove a ""local"" oracle inequality (described by the
pointwise seminorm), which is then used to obtain the upper bound on the
maximal risk of the adaptive estimator under assumption that the link function
belongs to a scale of H\""{o}lder classes. The lower bound on the minimax risk
shows that in the case of estimating at a given point the constructed estimator
is optimally rate adaptive over the considered range of classes. For the same
procedure we also establish a ""global"" oracle inequality (under the $ L_r $
norm, $r< \infty $) and examine its performance over the Nikol'skii classes.
This study shows that the proposed method can be applied to estimating
functions of inhomogeneous smoothness, that is whose smoothness may vary from
point to point."@2011
Christophe Chesneau@http://arxiv.org/abs/1111.3994v2@"Adaptive estimation of an additive regression function from weakly
  dependent data"@"A $d$-dimensional nonparametric additive regression model with dependent
observations is considered. Using the marginal integration technique and
wavelets methodology, we develop a new adaptive estimator for a component of
the additive regression function. Its asymptotic properties are investigated
via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We
prove that it attains a sharp rate of convergence which turns to be the one
obtained in the $\iid$ case for the standard univariate regression estimation
problem."@2011
Jalal M. Fadili@http://arxiv.org/abs/1111.3994v2@"Adaptive estimation of an additive regression function from weakly
  dependent data"@"A $d$-dimensional nonparametric additive regression model with dependent
observations is considered. Using the marginal integration technique and
wavelets methodology, we develop a new adaptive estimator for a component of
the additive regression function. Its asymptotic properties are investigated
via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We
prove that it attains a sharp rate of convergence which turns to be the one
obtained in the $\iid$ case for the standard univariate regression estimation
problem."@2011
Bertrand Maillot@http://arxiv.org/abs/1111.3994v2@"Adaptive estimation of an additive regression function from weakly
  dependent data"@"A $d$-dimensional nonparametric additive regression model with dependent
observations is considered. Using the marginal integration technique and
wavelets methodology, we develop a new adaptive estimator for a component of
the additive regression function. Its asymptotic properties are investigated
via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We
prove that it attains a sharp rate of convergence which turns to be the one
obtained in the $\iid$ case for the standard univariate regression estimation
problem."@2011
Nadir Maaroufi@http://arxiv.org/abs/1111.4517v2@A remark on the ARE between Wilcoxon's and van~der~Waerden's scores@"This paper is concerned with a comparison of van der Waerden's and Wilcoxon's
scores."@2011
Camille Sabbah@http://arxiv.org/abs/1111.4517v2@A remark on the ARE between Wilcoxon's and van~der~Waerden's scores@"This paper is concerned with a comparison of van der Waerden's and Wilcoxon's
scores."@2011
Yvik Swan@http://arxiv.org/abs/1111.4517v2@A remark on the ARE between Wilcoxon's and van~der~Waerden's scores@"This paper is concerned with a comparison of van der Waerden's and Wilcoxon's
scores."@2011
Thomas Verdebout@http://arxiv.org/abs/1111.4517v2@A remark on the ARE between Wilcoxon's and van~der~Waerden's scores@"This paper is concerned with a comparison of van der Waerden's and Wilcoxon's
scores."@2011
J. E. Chacón@http://arxiv.org/abs/1111.4542v1@A note on kernel density estimation at a parametric rate@"In the context of kernel density estimation, we give a characterization of
the kernels for which the parametric mean integrated squared error rate
$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases
where this rate is attainable, we give an asymptotic bandwidth choice that
makes the kernel estimator consistent in mean integrated squared error at that
rate and a numerical example showing the superior performance of the
superkernel estimator when the bandwidth is properly chosen."@2011
J. Montanero@http://arxiv.org/abs/1111.4542v1@A note on kernel density estimation at a parametric rate@"In the context of kernel density estimation, we give a characterization of
the kernels for which the parametric mean integrated squared error rate
$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases
where this rate is attainable, we give an asymptotic bandwidth choice that
makes the kernel estimator consistent in mean integrated squared error at that
rate and a numerical example showing the superior performance of the
superkernel estimator when the bandwidth is properly chosen."@2011
A. G. Nogales@http://arxiv.org/abs/1111.4542v1@A note on kernel density estimation at a parametric rate@"In the context of kernel density estimation, we give a characterization of
the kernels for which the parametric mean integrated squared error rate
$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases
where this rate is attainable, we give an asymptotic bandwidth choice that
makes the kernel estimator consistent in mean integrated squared error at that
rate and a numerical example showing the superior performance of the
superkernel estimator when the bandwidth is properly chosen."@2011
Adam D. Bull@http://arxiv.org/abs/1111.5568v3@Adaptive confidence sets in L^2@"The problem of constructing confidence sets that are adaptive in L^2-loss
over a continuous scale of Sobolev classes of probability densities is
considered. Adaptation holds, where possible, with respect to both the radius
of the Sobolev ball and its smoothness degree, and over maximal parameter
spaces for which adaptation is possible. Two key regimes of parameter
constellations are identified: one where full adaptation is possible, and one
where adaptation requires critical regions be removed. Techniques used to
derive these results include a general nonparametric minimax test for
infinite-dimensional null- and alternative hypotheses, and new lower bounds for
L^2-adaptive confidence sets."@2011
Richard Nickl@http://arxiv.org/abs/1111.5568v3@Adaptive confidence sets in L^2@"The problem of constructing confidence sets that are adaptive in L^2-loss
over a continuous scale of Sobolev classes of probability densities is
considered. Adaptation holds, where possible, with respect to both the radius
of the Sobolev ball and its smoothness degree, and over maximal parameter
spaces for which adaptation is possible. Two key regimes of parameter
constellations are identified: one where full adaptation is possible, and one
where adaptation requires critical regions be removed. Techniques used to
derive these results include a general nonparametric minimax test for
infinite-dimensional null- and alternative hypotheses, and new lower bounds for
L^2-adaptive confidence sets."@2011
B. T. Knapik@http://arxiv.org/abs/1111.5876v3@Bayesian recovery of the initial condition for the heat equation@"We study a Bayesian approach to recovering the initial condition for the heat
equation from noisy observations of the solution at a later time. We consider a
class of prior distributions indexed by a parameter quantifying ""smoothness""
and show that the corresponding posterior distributions contract around the
true parameter at a rate that depends on the smoothness of the true initial
condition and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the optimal minimax rate. One type of priors
leads to a rate-adaptive Bayesian procedure. The frequentist coverage of
credible sets is shown to depend on the combination of the prior and true
parameter as well, with smoother priors leading to zero coverage and rougher
priors to (extremely) conservative results. In the latter case credible sets
are much larger than frequentist confidence sets, in that the ratio of
diameters diverges to infinity. The results are numerically illustrated by a
simulated data example."@2011
A. W. van der Vaart@http://arxiv.org/abs/1111.5876v3@Bayesian recovery of the initial condition for the heat equation@"We study a Bayesian approach to recovering the initial condition for the heat
equation from noisy observations of the solution at a later time. We consider a
class of prior distributions indexed by a parameter quantifying ""smoothness""
and show that the corresponding posterior distributions contract around the
true parameter at a rate that depends on the smoothness of the true initial
condition and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the optimal minimax rate. One type of priors
leads to a rate-adaptive Bayesian procedure. The frequentist coverage of
credible sets is shown to depend on the combination of the prior and true
parameter as well, with smoother priors leading to zero coverage and rougher
priors to (extremely) conservative results. In the latter case credible sets
are much larger than frequentist confidence sets, in that the ratio of
diameters diverges to infinity. The results are numerically illustrated by a
simulated data example."@2011
J. H. van Zanten@http://arxiv.org/abs/1111.5876v3@Bayesian recovery of the initial condition for the heat equation@"We study a Bayesian approach to recovering the initial condition for the heat
equation from noisy observations of the solution at a later time. We consider a
class of prior distributions indexed by a parameter quantifying ""smoothness""
and show that the corresponding posterior distributions contract around the
true parameter at a rate that depends on the smoothness of the true initial
condition and the smoothness and scale of the prior. Correct combinations of
these characteristics lead to the optimal minimax rate. One type of priors
leads to a rate-adaptive Bayesian procedure. The frequentist coverage of
credible sets is shown to depend on the combination of the prior and true
parameter as well, with smoother priors leading to zero coverage and rougher
priors to (extremely) conservative results. In the latter case credible sets
are much larger than frequentist confidence sets, in that the ratio of
diameters diverges to infinity. The results are numerically illustrated by a
simulated data example."@2011
Emmanuel Boissard@http://arxiv.org/abs/1111.5927v2@Distribution's template estimate with Wasserstein metrics@"In this paper we tackle the problem of comparing distributions of random
variables and defining a mean pattern between a sample of random events. Using
barycenters of measures in the Wasserstein space, we propose an iterative
version as an estimation of the mean distribution. Moreover, when the
distributions are a common measure warped by a centered random operator, then
the barycenter enables to recover this distribution template."@2011
Thibaut Le Gouic@http://arxiv.org/abs/1111.5927v2@Distribution's template estimate with Wasserstein metrics@"In this paper we tackle the problem of comparing distributions of random
variables and defining a mean pattern between a sample of random events. Using
barycenters of measures in the Wasserstein space, we propose an iterative
version as an estimation of the mean distribution. Moreover, when the
distributions are a common measure warped by a centered random operator, then
the barycenter enables to recover this distribution template."@2011
Jean-Michel Loubes@http://arxiv.org/abs/1111.5927v2@Distribution's template estimate with Wasserstein metrics@"In this paper we tackle the problem of comparing distributions of random
variables and defining a mean pattern between a sample of random events. Using
barycenters of measures in the Wasserstein space, we propose an iterative
version as an estimation of the mean distribution. Moreover, when the
distributions are a common measure warped by a centered random operator, then
the barycenter enables to recover this distribution template."@2011
Cécile Durot@http://arxiv.org/abs/1111.5934v3@"The limit distribution of the $L_{\infty}$-error of Grenander-type
  estimators"@"Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard
regularity conditions, we derive the asymptotic distribution of the supremum
norm of the difference between $f$ and its Grenander-type estimator on
sub-intervals of $[0,1]$. The rate of convergence is found to be of order
$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel."@2011
Vladimir N. Kulikov@http://arxiv.org/abs/1111.5934v3@"The limit distribution of the $L_{\infty}$-error of Grenander-type
  estimators"@"Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard
regularity conditions, we derive the asymptotic distribution of the supremum
norm of the difference between $f$ and its Grenander-type estimator on
sub-intervals of $[0,1]$. The rate of convergence is found to be of order
$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel."@2011
Hendrik P. Lopuhaä@http://arxiv.org/abs/1111.5934v3@"The limit distribution of the $L_{\infty}$-error of Grenander-type
  estimators"@"Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard
regularity conditions, we derive the asymptotic distribution of the supremum
norm of the difference between $f$ and its Grenander-type estimator on
sub-intervals of $[0,1]$. The rate of convergence is found to be of order
$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel."@2011
Olivier Lopez@http://arxiv.org/abs/1111.6232v2@"Single index regression models in the presence of censoring depending on
  the covariates"@"Consider a random vector (X',Y)', where X is d-dimensional and Y is
one-dimensional. We assume that Y is subject to random right censoring. The aim
of this paper is twofold. First, we propose a new estimator of the joint
distribution of (X',Y)'. This estimator overcomes the common
curse-of-dimensionality problem, by using a new dimension reduction technique.
Second, we assume that the relation between X and Y is given by a mean
regression single index model, and propose a new estimator of the parameters in
this model. The asymptotic properties of all proposed estimators are obtained."@2011
Valentin Patilea@http://arxiv.org/abs/1111.6232v2@"Single index regression models in the presence of censoring depending on
  the covariates"@"Consider a random vector (X',Y)', where X is d-dimensional and Y is
one-dimensional. We assume that Y is subject to random right censoring. The aim
of this paper is twofold. First, we propose a new estimator of the joint
distribution of (X',Y)'. This estimator overcomes the common
curse-of-dimensionality problem, by using a new dimension reduction technique.
Second, we assume that the relation between X and Y is given by a mean
regression single index model, and propose a new estimator of the parameters in
this model. The asymptotic properties of all proposed estimators are obtained."@2011
Ingrid Van Keilegom@http://arxiv.org/abs/1111.6232v2@"Single index regression models in the presence of censoring depending on
  the covariates"@"Consider a random vector (X',Y)', where X is d-dimensional and Y is
one-dimensional. We assume that Y is subject to random right censoring. The aim
of this paper is twofold. First, we propose a new estimator of the joint
distribution of (X',Y)'. This estimator overcomes the common
curse-of-dimensionality problem, by using a new dimension reduction technique.
Second, we assume that the relation between X and Y is given by a mean
regression single index model, and propose a new estimator of the parameters in
this model. The asymptotic properties of all proposed estimators are obtained."@2011
Henrik Ohlsson@http://arxiv.org/abs/1111.6323v3@"Compressive Phase Retrieval From Squared Output Measurements Via
  Semidefinite Programming"@"Given a linear system in a real or complex domain, linear regression aims to
recover the model parameters from a set of observations. Recent studies in
compressive sensing have successfully shown that under certain conditions, a
linear program, namely, l1-minimization, guarantees recovery of sparse
parameter signals even when the system is underdetermined. In this paper, we
consider a more challenging problem: when the phase of the output measurements
from a linear system is omitted. Using a lifting technique, we show that even
though the phase information is missing, the sparse signal can be recovered
exactly by solving a simple semidefinite program when the sampling rate is
sufficiently high, albeit the exact solutions to both sparse signal recovery
and phase retrieval are combinatorial. The results extend the type of
applications that compressive sensing can be applied to those where only output
magnitudes can be observed. We demonstrate the accuracy of the algorithms
through theoretical analysis, extensive simulations and a practical experiment."@2011
Allen Y. Yang@http://arxiv.org/abs/1111.6323v3@"Compressive Phase Retrieval From Squared Output Measurements Via
  Semidefinite Programming"@"Given a linear system in a real or complex domain, linear regression aims to
recover the model parameters from a set of observations. Recent studies in
compressive sensing have successfully shown that under certain conditions, a
linear program, namely, l1-minimization, guarantees recovery of sparse
parameter signals even when the system is underdetermined. In this paper, we
consider a more challenging problem: when the phase of the output measurements
from a linear system is omitted. Using a lifting technique, we show that even
though the phase information is missing, the sparse signal can be recovered
exactly by solving a simple semidefinite program when the sampling rate is
sufficiently high, albeit the exact solutions to both sparse signal recovery
and phase retrieval are combinatorial. The results extend the type of
applications that compressive sensing can be applied to those where only output
magnitudes can be observed. We demonstrate the accuracy of the algorithms
through theoretical analysis, extensive simulations and a practical experiment."@2011
Roy Dong@http://arxiv.org/abs/1111.6323v3@"Compressive Phase Retrieval From Squared Output Measurements Via
  Semidefinite Programming"@"Given a linear system in a real or complex domain, linear regression aims to
recover the model parameters from a set of observations. Recent studies in
compressive sensing have successfully shown that under certain conditions, a
linear program, namely, l1-minimization, guarantees recovery of sparse
parameter signals even when the system is underdetermined. In this paper, we
consider a more challenging problem: when the phase of the output measurements
from a linear system is omitted. Using a lifting technique, we show that even
though the phase information is missing, the sparse signal can be recovered
exactly by solving a simple semidefinite program when the sampling rate is
sufficiently high, albeit the exact solutions to both sparse signal recovery
and phase retrieval are combinatorial. The results extend the type of
applications that compressive sensing can be applied to those where only output
magnitudes can be observed. We demonstrate the accuracy of the algorithms
through theoretical analysis, extensive simulations and a practical experiment."@2011
S. Shankar Sastry@http://arxiv.org/abs/1111.6323v3@"Compressive Phase Retrieval From Squared Output Measurements Via
  Semidefinite Programming"@"Given a linear system in a real or complex domain, linear regression aims to
recover the model parameters from a set of observations. Recent studies in
compressive sensing have successfully shown that under certain conditions, a
linear program, namely, l1-minimization, guarantees recovery of sparse
parameter signals even when the system is underdetermined. In this paper, we
consider a more challenging problem: when the phase of the output measurements
from a linear system is omitted. Using a lifting technique, we show that even
though the phase information is missing, the sparse signal can be recovered
exactly by solving a simple semidefinite program when the sampling rate is
sufficiently high, albeit the exact solutions to both sparse signal recovery
and phase retrieval are combinatorial. The results extend the type of
applications that compressive sensing can be applied to those where only output
magnitudes can be observed. We demonstrate the accuracy of the algorithms
through theoretical analysis, extensive simulations and a practical experiment."@2011
Marian Hristache@http://arxiv.org/abs/1111.6428v1@"Semiparametric efficiency bounds for seemingly unrelated conditional
  moment restrictions"@"This paper addresses the problem of semiparametric efficiency bounds for
conditional moment restriction models with different conditioning variables. We
characterize such an efficiency bound, that in general is not explicit, as a
limit of explicit efficiency bounds for a decreasing sequence of unconditional
(marginal) moment restriction models. An iterative procedure for approximating
the efficient score when this is not explicit is provided. Our theoretical
results complete and extend existing results in the literature, provide new
insight for the theory of semiparametric efficiency bounds literature and open
the door to new applications. In particular, we investigate a class of
regression-like (mean regression, quantile regression,...) models with missing
data."@2011
Valentin Patilea@http://arxiv.org/abs/1111.6428v1@"Semiparametric efficiency bounds for seemingly unrelated conditional
  moment restrictions"@"This paper addresses the problem of semiparametric efficiency bounds for
conditional moment restriction models with different conditioning variables. We
characterize such an efficiency bound, that in general is not explicit, as a
limit of explicit efficiency bounds for a decreasing sequence of unconditional
(marginal) moment restriction models. An iterative procedure for approximating
the efficient score when this is not explicit is provided. Our theoretical
results complete and extend existing results in the literature, provide new
insight for the theory of semiparametric efficiency bounds literature and open
the door to new applications. In particular, we investigate a class of
regression-like (mean regression, quantile regression,...) models with missing
data."@2011
Parikshit Shah@http://arxiv.org/abs/1111.7061v1@Group Symmetry and Covariance Regularization@"Statistical models that possess symmetry arise in diverse settings such as
random fields associated to geophysical phenomena, exchangeable processes in
Bayesian statistics, and cyclostationary processes in engineering. We formalize
the notion of a symmetric model via group invariance. We propose projection
onto a group fixed point subspace as a fundamental way of regularizing
covariance matrices in the high-dimensional regime. In terms of parameters
associated to the group we derive precise rates of convergence of the
regularized covariance matrix and demonstrate that significant statistical
gains may be expected in terms of the sample complexity. We further explore the
consequences of symmetry on related model-selection problems such as the
learning of sparse covariance and inverse covariance matrices. We also verify
our results with simulations."@2011
Venkat Chandrasekaran@http://arxiv.org/abs/1111.7061v1@Group Symmetry and Covariance Regularization@"Statistical models that possess symmetry arise in diverse settings such as
random fields associated to geophysical phenomena, exchangeable processes in
Bayesian statistics, and cyclostationary processes in engineering. We formalize
the notion of a symmetric model via group invariance. We propose projection
onto a group fixed point subspace as a fundamental way of regularizing
covariance matrices in the high-dimensional regime. In terms of parameters
associated to the group we derive precise rates of convergence of the
regularized covariance matrix and demonstrate that significant statistical
gains may be expected in terms of the sample complexity. We further explore the
consequences of symmetry on related model-selection problems such as the
learning of sparse covariance and inverse covariance matrices. We also verify
our results with simulations."@2011
Holger Dette@http://arxiv.org/abs/1111.7205v3@"Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral
  analysis"@"In this paper, we present an alternative method for the spectral analysis of
a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We
define a ""new"" spectrum as the Fourier transform of the differences between
copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object
is called a copula spectral density kernel and allows to separate the marginal
and serial aspects of a time series. We show that this spectrum is closely
related to the concept of quantile regression. Like quantile regression, which
provides much more information about conditional distributions than classical
location-scale regression models, copula spectral density kernels are more
informative than traditional spectral densities obtained from classical
autocovariances. In particular, copula spectral density kernels, in their
population versions, provide (asymptotically provide, in their sample versions)
a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,
they inherit the robustness properties of classical quantile regression, and do
not require any distributional assumptions such as the existence of finite
moments. In order to estimate the copula spectral density kernel, we introduce
rank-based Laplace periodograms which are calculated as bilinear forms of
weighted $L_1$-projections of the ranks of the observed time series onto a
harmonic regression model. We establish the asymptotic distribution of those
periodograms, and the consistency of adequately smoothed versions. The
finite-sample properties of the new methodology, and its potential for
applications are briefly investigated by simulations and a short empirical
example."@2011
Marc Hallin@http://arxiv.org/abs/1111.7205v3@"Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral
  analysis"@"In this paper, we present an alternative method for the spectral analysis of
a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We
define a ""new"" spectrum as the Fourier transform of the differences between
copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object
is called a copula spectral density kernel and allows to separate the marginal
and serial aspects of a time series. We show that this spectrum is closely
related to the concept of quantile regression. Like quantile regression, which
provides much more information about conditional distributions than classical
location-scale regression models, copula spectral density kernels are more
informative than traditional spectral densities obtained from classical
autocovariances. In particular, copula spectral density kernels, in their
population versions, provide (asymptotically provide, in their sample versions)
a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,
they inherit the robustness properties of classical quantile regression, and do
not require any distributional assumptions such as the existence of finite
moments. In order to estimate the copula spectral density kernel, we introduce
rank-based Laplace periodograms which are calculated as bilinear forms of
weighted $L_1$-projections of the ranks of the observed time series onto a
harmonic regression model. We establish the asymptotic distribution of those
periodograms, and the consistency of adequately smoothed versions. The
finite-sample properties of the new methodology, and its potential for
applications are briefly investigated by simulations and a short empirical
example."@2011
Tobias Kley@http://arxiv.org/abs/1111.7205v3@"Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral
  analysis"@"In this paper, we present an alternative method for the spectral analysis of
a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We
define a ""new"" spectrum as the Fourier transform of the differences between
copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object
is called a copula spectral density kernel and allows to separate the marginal
and serial aspects of a time series. We show that this spectrum is closely
related to the concept of quantile regression. Like quantile regression, which
provides much more information about conditional distributions than classical
location-scale regression models, copula spectral density kernels are more
informative than traditional spectral densities obtained from classical
autocovariances. In particular, copula spectral density kernels, in their
population versions, provide (asymptotically provide, in their sample versions)
a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,
they inherit the robustness properties of classical quantile regression, and do
not require any distributional assumptions such as the existence of finite
moments. In order to estimate the copula spectral density kernel, we introduce
rank-based Laplace periodograms which are calculated as bilinear forms of
weighted $L_1$-projections of the ranks of the observed time series onto a
harmonic regression model. We establish the asymptotic distribution of those
periodograms, and the consistency of adequately smoothed versions. The
finite-sample properties of the new methodology, and its potential for
applications are briefly investigated by simulations and a short empirical
example."@2011
Stanislav Volgushev@http://arxiv.org/abs/1111.7205v3@"Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral
  analysis"@"In this paper, we present an alternative method for the spectral analysis of
a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We
define a ""new"" spectrum as the Fourier transform of the differences between
copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object
is called a copula spectral density kernel and allows to separate the marginal
and serial aspects of a time series. We show that this spectrum is closely
related to the concept of quantile regression. Like quantile regression, which
provides much more information about conditional distributions than classical
location-scale regression models, copula spectral density kernels are more
informative than traditional spectral densities obtained from classical
autocovariances. In particular, copula spectral density kernels, in their
population versions, provide (asymptotically provide, in their sample versions)
a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,
they inherit the robustness properties of classical quantile regression, and do
not require any distributional assumptions such as the existence of finite
moments. In order to estimate the copula spectral density kernel, we introduce
rank-based Laplace periodograms which are calculated as bilinear forms of
weighted $L_1$-projections of the ranks of the observed time series onto a
harmonic regression model. We establish the asymptotic distribution of those
periodograms, and the consistency of adequately smoothed versions. The
finite-sample properties of the new methodology, and its potential for
applications are briefly investigated by simulations and a short empirical
example."@2011
Rémi Gribonval@http://arxiv.org/abs/1111.7248v1@Blind calibration for compressed sensing by convex optimization@"We consider the problem of calibrating a compressed sensing measurement
system under the assumption that the decalibration consists in unknown gains on
each measure. We focus on {\em blind} calibration, using measures performed on
a few unknown (but sparse) signals. A naive formulation of this blind
calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind
source separation and dictionary learning, which are known to be highly
non-convex and riddled with local minima. In the considered context, we show
that in fact this formulation can be exactly expressed as a convex optimization
problem, and can be solved using off-the-shelf algorithms. Numerical
simulations demonstrate the effectiveness of the approach even for highly
uncalibrated measures, when a sufficient number of (unknown, but sparse)
calibrating signals is provided. We observe that the success/failure of the
approach seems to obey sharp phase transitions."@2011
Gilles Chardon@http://arxiv.org/abs/1111.7248v1@Blind calibration for compressed sensing by convex optimization@"We consider the problem of calibrating a compressed sensing measurement
system under the assumption that the decalibration consists in unknown gains on
each measure. We focus on {\em blind} calibration, using measures performed on
a few unknown (but sparse) signals. A naive formulation of this blind
calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind
source separation and dictionary learning, which are known to be highly
non-convex and riddled with local minima. In the considered context, we show
that in fact this formulation can be exactly expressed as a convex optimization
problem, and can be solved using off-the-shelf algorithms. Numerical
simulations demonstrate the effectiveness of the approach even for highly
uncalibrated measures, when a sufficient number of (unknown, but sparse)
calibrating signals is provided. We observe that the success/failure of the
approach seems to obey sharp phase transitions."@2011
Laurent Daudet@http://arxiv.org/abs/1111.7248v1@Blind calibration for compressed sensing by convex optimization@"We consider the problem of calibrating a compressed sensing measurement
system under the assumption that the decalibration consists in unknown gains on
each measure. We focus on {\em blind} calibration, using measures performed on
a few unknown (but sparse) signals. A naive formulation of this blind
calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind
source separation and dictionary learning, which are known to be highly
non-convex and riddled with local minima. In the considered context, we show
that in fact this formulation can be exactly expressed as a convex optimization
problem, and can be solved using off-the-shelf algorithms. Numerical
simulations demonstrate the effectiveness of the approach even for highly
uncalibrated measures, when a sufficient number of (unknown, but sparse)
calibrating signals is provided. We observe that the success/failure of the
approach seems to obey sharp phase transitions."@2011
John H. J. Einmahl@http://arxiv.org/abs/1112.0905v3@An M-estimator for tail dependence in arbitrary dimensions@"Consider a random sample in the max-domain of attraction of a multivariate
extreme value distribution such that the dependence structure of the attractor
belongs to a parametric model. A new estimator for the unknown parameter is
defined as the value that minimizes the distance between a vector of weighted
integrals of the tail dependence function and their empirical counterparts. The
minimization problem has, with probability tending to one, a unique, global
solution. The estimator is consistent and asymptotically normal. The spectral
measures of the tail dependence models to which the method applies can be
discrete or continuous. Examples demonstrate the applicability and the
performance of the method."@2011
Andrea Krajina@http://arxiv.org/abs/1112.0905v3@An M-estimator for tail dependence in arbitrary dimensions@"Consider a random sample in the max-domain of attraction of a multivariate
extreme value distribution such that the dependence structure of the attractor
belongs to a parametric model. A new estimator for the unknown parameter is
defined as the value that minimizes the distance between a vector of weighted
integrals of the tail dependence function and their empirical counterparts. The
minimization problem has, with probability tending to one, a unique, global
solution. The estimator is consistent and asymptotically normal. The spectral
measures of the tail dependence models to which the method applies can be
discrete or continuous. Examples demonstrate the applicability and the
performance of the method."@2011
Johan Segers@http://arxiv.org/abs/1112.0905v3@An M-estimator for tail dependence in arbitrary dimensions@"Consider a random sample in the max-domain of attraction of a multivariate
extreme value distribution such that the dependence structure of the attractor
belongs to a parametric model. A new estimator for the unknown parameter is
defined as the value that minimizes the distance between a vector of weighted
integrals of the tail dependence function and their empirical counterparts. The
minimization problem has, with probability tending to one, a unique, global
solution. The estimator is consistent and asymptotically normal. The spectral
measures of the tail dependence models to which the method applies can be
discrete or continuous. Examples demonstrate the applicability and the
performance of the method."@2011
Markus Bibinger@http://arxiv.org/abs/1112.0939v3@"Spectral covolatility estimation from noisy observations using local
  weights"@"We propose localized spectral estimators for the quadratic covariation and
the spot covolatility of diffusion processes which are observed discretely with
additive observation noise. The eligibility of this approach to lead to an
appropriate estimation for time-varying volatilities stems from an asymptotic
equivalence of the underlying statistical model to a white noise model with
correlation and volatility processes being constant over small intervals. The
asymptotic equivalence of the continuous-time and the discrete-time experiments
are proved by a construction with linear interpolation in one direction and
local means for the other. The new estimator outperforms earlier nonparametric
approaches in the considered model. We investigate its finite sample size
characteristics in simulations and draw a comparison between the various
proposed methods."@2011
Markus Reiß@http://arxiv.org/abs/1112.0939v3@"Spectral covolatility estimation from noisy observations using local
  weights"@"We propose localized spectral estimators for the quadratic covariation and
the spot covolatility of diffusion processes which are observed discretely with
additive observation noise. The eligibility of this approach to lead to an
appropriate estimation for time-varying volatilities stems from an asymptotic
equivalence of the underlying statistical model to a white noise model with
correlation and volatility processes being constant over small intervals. The
asymptotic equivalence of the continuous-time and the discrete-time experiments
are proved by a construction with linear interpolation in one direction and
local means for the other. The new estimator outperforms earlier nonparametric
approaches in the considered model. We investigate its finite sample size
characteristics in simulations and draw a comparison between the various
proposed methods."@2011
Helena Ferreira@http://arxiv.org/abs/1112.1490v1@Fragility Index of block tailed vectors@"Financial crises are a recurrent phenomenon with important effects on the
real economy. The financial system is inherently fragile and it is therefore of
great importance to be able to measure and characterize its systemic stability.
Multivariate extreme value theory provide us such a framework through the
\emph{fragility index} (Geluk \cite{gel+}, \emph{et al.}, 2007; Falk and Tichy,
\cite{falk+tichy1,falk+tichy2} 2010, 2011). Here we generalize this concept and
contribute to the modeling of the stability of a stochastic system divided into
blocks. We will find several relations with well-known tail dependence measures
in literature, which will provide us immediate estimators. We end with an
application to financial data."@2011
Marta Ferreira@http://arxiv.org/abs/1112.1490v1@Fragility Index of block tailed vectors@"Financial crises are a recurrent phenomenon with important effects on the
real economy. The financial system is inherently fragile and it is therefore of
great importance to be able to measure and characterize its systemic stability.
Multivariate extreme value theory provide us such a framework through the
\emph{fragility index} (Geluk \cite{gel+}, \emph{et al.}, 2007; Falk and Tichy,
\cite{falk+tichy1,falk+tichy2} 2010, 2011). Here we generalize this concept and
contribute to the modeling of the stability of a stochastic system divided into
blocks. We will find several relations with well-known tail dependence measures
in literature, which will provide us immediate estimators. We end with an
application to financial data."@2011
Gaëlle Chastaing@http://arxiv.org/abs/1112.1788v3@"Generalized Hoeffding-Sobol Decomposition for Dependent Variables
  -Application to Sensitivity Analysis"@"In this paper, we consider a regression model built on dependent variables.
This regression modelizes an input output relationship. Under boundedness
assumptions on the joint distribution function of the input variables, we show
that a generalized Hoeffding-Sobol decomposition is available. This leads to
new indices measuring the sensitivity of the output with respect to the input
variables. We also study and discuss the estimation of these new indices."@2011
Fabrice Gamboa@http://arxiv.org/abs/1112.1788v3@"Generalized Hoeffding-Sobol Decomposition for Dependent Variables
  -Application to Sensitivity Analysis"@"In this paper, we consider a regression model built on dependent variables.
This regression modelizes an input output relationship. Under boundedness
assumptions on the joint distribution function of the input variables, we show
that a generalized Hoeffding-Sobol decomposition is available. This leads to
new indices measuring the sensitivity of the output with respect to the input
variables. We also study and discuss the estimation of these new indices."@2011
Clémentine Prieur@http://arxiv.org/abs/1112.1788v3@"Generalized Hoeffding-Sobol Decomposition for Dependent Variables
  -Application to Sensitivity Analysis"@"In this paper, we consider a regression model built on dependent variables.
This regression modelizes an input output relationship. Under boundedness
assumptions on the joint distribution function of the input variables, we show
that a generalized Hoeffding-Sobol decomposition is available. This leads to
new indices measuring the sensitivity of the output with respect to the input
variables. We also study and discuss the estimation of these new indices."@2011
Li Wang@http://arxiv.org/abs/1112.2502v1@"Estimation and variable selection for generalized additive partial
  linear models"@"We study generalized additive partial linear models, proposing the use of
polynomial spline smoothing for estimation of nonparametric functions, and
deriving quasi-likelihood based estimators for the linear parameters. We
establish asymptotic normality for the estimators of the parametric components.
The procedure avoids solving large systems of equations as in kernel-based
procedures and thus results in gains in computational simplicity. We further
develop a class of variable selection procedures for the linear parameters by
employing a nonconcave penalized quasi-likelihood, which is shown to have an
asymptotic oracle property. Monte Carlo simulations and an empirical example
are presented for illustration."@2011
Xiang Liu@http://arxiv.org/abs/1112.2502v1@"Estimation and variable selection for generalized additive partial
  linear models"@"We study generalized additive partial linear models, proposing the use of
polynomial spline smoothing for estimation of nonparametric functions, and
deriving quasi-likelihood based estimators for the linear parameters. We
establish asymptotic normality for the estimators of the parametric components.
The procedure avoids solving large systems of equations as in kernel-based
procedures and thus results in gains in computational simplicity. We further
develop a class of variable selection procedures for the linear parameters by
employing a nonconcave penalized quasi-likelihood, which is shown to have an
asymptotic oracle property. Monte Carlo simulations and an empirical example
are presented for illustration."@2011
Hua Liang@http://arxiv.org/abs/1112.2502v1@"Estimation and variable selection for generalized additive partial
  linear models"@"We study generalized additive partial linear models, proposing the use of
polynomial spline smoothing for estimation of nonparametric functions, and
deriving quasi-likelihood based estimators for the linear parameters. We
establish asymptotic normality for the estimators of the parametric components.
The procedure avoids solving large systems of equations as in kernel-based
procedures and thus results in gains in computational simplicity. We further
develop a class of variable selection procedures for the linear parameters by
employing a nonconcave penalized quasi-likelihood, which is shown to have an
asymptotic oracle property. Monte Carlo simulations and an empirical example
are presented for illustration."@2011
Raymond J. Carroll@http://arxiv.org/abs/1112.2502v1@"Estimation and variable selection for generalized additive partial
  linear models"@"We study generalized additive partial linear models, proposing the use of
polynomial spline smoothing for estimation of nonparametric functions, and
deriving quasi-likelihood based estimators for the linear parameters. We
establish asymptotic normality for the estimators of the parametric components.
The procedure avoids solving large systems of equations as in kernel-based
procedures and thus results in gains in computational simplicity. We further
develop a class of variable selection procedures for the linear parameters by
employing a nonconcave penalized quasi-likelihood, which is shown to have an
asymptotic oracle property. Monte Carlo simulations and an empirical example
are presented for illustration."@2011
Fabienne Comte@http://arxiv.org/abs/1112.2509v2@Adaptive functional linear regression@"We consider the estimation of the slope function in functional linear
regression, where scalar responses are modeled in dependence of random
functions. Cardot and Johannes [J. Multivariate Anal. 101 (2010) 395-408] have
shown that a thresholded projection estimator can attain up to a constant
minimax-rates of convergence in a general framework which allows us to cover
the prediction problem with respect to the mean squared prediction error as
well as the estimation of the slope function and its derivatives. This
estimation procedure, however, requires an optimal choice of a tuning parameter
with regard to certain characteristics of the slope function and the covariance
operator associated with the functional regressor. As this information is
usually inaccessible in practice, we investigate a fully data-driven choice of
the tuning parameter which combines model selection and Lepski's method. It is
inspired by the recent work of Goldenshluger and Lepski [Ann. Statist. 39
(2011) 1608-1632]. The tuning parameter is selected as minimizer of a
stochastic penalized contrast function imitating Lepski's method among a random
collection of admissible values. This choice of the tuning parameter depends
only on the data and we show that within the general framework the resulting
data-driven thresholded projection estimator can attain minimax-rates up to a
constant over a variety of classes of slope functions and covariance operators.
The results are illustrated considering different configurations which cover in
particular the prediction problem as well as the estimation of the slope and
its derivatives. A simulation study shows the reasonable performance of the
fully data-driven estimation procedure."@2011
Jan Johannes@http://arxiv.org/abs/1112.2509v2@Adaptive functional linear regression@"We consider the estimation of the slope function in functional linear
regression, where scalar responses are modeled in dependence of random
functions. Cardot and Johannes [J. Multivariate Anal. 101 (2010) 395-408] have
shown that a thresholded projection estimator can attain up to a constant
minimax-rates of convergence in a general framework which allows us to cover
the prediction problem with respect to the mean squared prediction error as
well as the estimation of the slope function and its derivatives. This
estimation procedure, however, requires an optimal choice of a tuning parameter
with regard to certain characteristics of the slope function and the covariance
operator associated with the functional regressor. As this information is
usually inaccessible in practice, we investigate a fully data-driven choice of
the tuning parameter which combines model selection and Lepski's method. It is
inspired by the recent work of Goldenshluger and Lepski [Ann. Statist. 39
(2011) 1608-1632]. The tuning parameter is selected as minimizer of a
stochastic penalized contrast function imitating Lepski's method among a random
collection of admissible values. This choice of the tuning parameter depends
only on the data and we show that within the general framework the resulting
data-driven thresholded projection estimator can attain minimax-rates up to a
constant over a variety of classes of slope functions and covariance operators.
The results are illustrated considering different configurations which cover in
particular the prediction problem as well as the estimation of the slope and
its derivatives. A simulation study shows the reasonable performance of the
fully data-driven estimation procedure."@2011
Junbum Lee@http://arxiv.org/abs/1112.2759v2@"The quantile spectral density and comparison based tests for nonlinear
  time series"@"In this paper we consider tests for nonlinear time series, which are
motivated by the notion of serial dependence. The proposed tests are based on
comparisons with the quantile spectral density, which can be considered as a
quantile version of the usual spectral density function. The quantile spectral
density 'measures' sequential dependence structure of a time series, and is
well defined under relatively weak mixing conditions. We propose an estimator
for the quantile spectral density and derive its asympototic sampling
properties. We use the quantile spectral density to construct a goodness of fit
test for time series and explain how this test can also be used for comparing
the sequential dependence structure of two time series. The method is
illustrated with simulations and some real data examples."@2011
Suhasini Subba Rao@http://arxiv.org/abs/1112.2759v2@"The quantile spectral density and comparison based tests for nonlinear
  time series"@"In this paper we consider tests for nonlinear time series, which are
motivated by the notion of serial dependence. The proposed tests are based on
comparisons with the quantile spectral density, which can be considered as a
quantile version of the usual spectral density function. The quantile spectral
density 'measures' sequential dependence structure of a time series, and is
well defined under relatively weak mixing conditions. We propose an estimator
for the quantile spectral density and derive its asympototic sampling
properties. We use the quantile spectral density to construct a goodness of fit
test for time series and explain how this test can also be used for comparing
the sequential dependence structure of two time series. The method is
illustrated with simulations and some real data examples."@2011
Shan Luo@http://arxiv.org/abs/1112.2815v1@"Selection Consistency of EBIC for GLIM with Non-canonical Links and
  Diverging Number of Parameters"@"In this article, we investigate the properties of the EBIC in variable
selection for generalized linear models with non-canonical links and diverging
number of parameters in ultra-high dimensional feature space. The selection
consistency of the EBIC in this situation is established under moderate
conditions. The finite sample performance of the EBIC coupled with a forward
selection procedure is demonstrated through simulation studies and a real data
analysis."@2011
Zehua Chen@http://arxiv.org/abs/1112.2815v1@"Selection Consistency of EBIC for GLIM with Non-canonical Links and
  Diverging Number of Parameters"@"In this article, we investigate the properties of the EBIC in variable
selection for generalized linear models with non-canonical links and diverging
number of parameters in ultra-high dimensional feature space. The selection
consistency of the EBIC in this situation is established under moderate
conditions. The finite sample performance of the EBIC coupled with a forward
selection procedure is demonstrated through simulation studies and a real data
analysis."@2011
Jan Johannes@http://arxiv.org/abs/1112.2855v1@Adaptive estimation of linear functionals in functional linear models@"We consider the estimation of the value of a linear functional of the slope
parameter in functional linear regression, where scalar responses are modeled
in dependence of random functions. In Johannes and Schenk [2010] it has been
shown that a plug-in estimator based on dimension reduction and additional
thresholding can attain minimax optimal rates of convergence up to a constant.
However, this estimation procedure requires an optimal choice of a tuning
parameter with regard to certain characteristics of the slope function and the
covariance operator associated with the functional regressor. As these are
unknown in practice, we investigate a fully data-driven choice of the tuning
parameter based on a combination of model selection and Lepski's method, which
is inspired by the recent work of Goldenshluger and Lepski [2011]. The tuning
parameter is selected as the minimizer of a stochastic penalized contrast
function imitating Lepski's method among a random collection of admissible
values. We show that this adaptive procedure attains the lower bound for the
minimax risk up to a logarithmic factor over a wide range of classes of slope
functions and covariance operators. In particular, our theory covers point-wise
estimation as well as the estimation of local averages of the slope parameter."@2011
Rudolf Schenk@http://arxiv.org/abs/1112.2855v1@Adaptive estimation of linear functionals in functional linear models@"We consider the estimation of the value of a linear functional of the slope
parameter in functional linear regression, where scalar responses are modeled
in dependence of random functions. In Johannes and Schenk [2010] it has been
shown that a plug-in estimator based on dimension reduction and additional
thresholding can attain minimax optimal rates of convergence up to a constant.
However, this estimation procedure requires an optimal choice of a tuning
parameter with regard to certain characteristics of the slope function and the
covariance operator associated with the functional regressor. As these are
unknown in practice, we investigate a fully data-driven choice of the tuning
parameter based on a combination of model selection and Lepski's method, which
is inspired by the recent work of Goldenshluger and Lepski [2011]. The tuning
parameter is selected as the minimizer of a stochastic penalized contrast
function imitating Lepski's method among a random collection of admissible
values. We show that this adaptive procedure attains the lower bound for the
minimax risk up to a logarithmic factor over a wide range of classes of slope
functions and covariance operators. In particular, our theory covers point-wise
estimation as well as the estimation of local averages of the slope parameter."@2011
Olga Klopp@http://arxiv.org/abs/1112.3055v3@High dimensional matrix estimation with unknown variance of the noise@"We propose a new pivotal method for estimating high-dimensional matrices.
Assume that we observe a small set of entries or linear combinations of entries
of an unknown matrix $A\_0$ corrupted by noise. We propose a new method for
estimating $A\_0$ which does not rely on the knowledge or an estimation of the
standard deviation of the noise $\sigma$. Our estimator achieves, up to a
logarithmic factor, optimal rates of convergence under the Frobenius risk and,
thus, has the same prediction performance as previously proposed estimators
which rely on the knowledge of $\sigma$. Our method is based on the solution of
a convex optimization problem which makes it computationally attractive."@2011
Stéphane Gaiffas@http://arxiv.org/abs/1112.3055v3@High dimensional matrix estimation with unknown variance of the noise@"We propose a new pivotal method for estimating high-dimensional matrices.
Assume that we observe a small set of entries or linear combinations of entries
of an unknown matrix $A\_0$ corrupted by noise. We propose a new method for
estimating $A\_0$ which does not rely on the knowledge or an estimation of the
standard deviation of the noise $\sigma$. Our estimator achieves, up to a
logarithmic factor, optimal rates of convergence under the Frobenius risk and,
thus, has the same prediction performance as previously proposed estimators
which rely on the knowledge of $\sigma$. Our method is based on the solution of
a convex optimization problem which makes it computationally attractive."@2011
Robert L. Wolpert@http://arxiv.org/abs/1112.3149v1@"Stochastic expansions using continuous dictionaries: Lévy adaptive
  regression kernels"@"This article describes a new class of prior distributions for nonparametric
function estimation. The unknown function is modeled as a limit of weighted
sums of kernels or generator functions indexed by continuous parameters that
control local and global features such as their translation, dilation,
modulation and shape. L\'{e}vy random fields and their stochastic integrals are
employed to induce prior distributions for the unknown functions or,
equivalently, for the number of kernels and for the parameters governing their
features. Scaling, shape, and other features of the generating functions are
location-specific to allow quite different function properties in different
parts of the space, as with wavelet bases and other methods employing
overcomplete dictionaries. We provide conditions under which the stochastic
expansions converge in specified Besov or Sobolev norms. Under a Gaussian error
model, this may be viewed as a sparse regression problem, with regularization
induced via the L\'{e}vy random field prior distribution. Posterior inference
for the unknown functions is based on a reversible jump Markov chain Monte
Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)
method to wavelet-based methods using some of the standard test functions, and
illustrate its flexibility and adaptability in nonstationary applications."@2011
Merlise A. Clyde@http://arxiv.org/abs/1112.3149v1@"Stochastic expansions using continuous dictionaries: Lévy adaptive
  regression kernels"@"This article describes a new class of prior distributions for nonparametric
function estimation. The unknown function is modeled as a limit of weighted
sums of kernels or generator functions indexed by continuous parameters that
control local and global features such as their translation, dilation,
modulation and shape. L\'{e}vy random fields and their stochastic integrals are
employed to induce prior distributions for the unknown functions or,
equivalently, for the number of kernels and for the parameters governing their
features. Scaling, shape, and other features of the generating functions are
location-specific to allow quite different function properties in different
parts of the space, as with wavelet bases and other methods employing
overcomplete dictionaries. We provide conditions under which the stochastic
expansions converge in specified Besov or Sobolev norms. Under a Gaussian error
model, this may be viewed as a sparse regression problem, with regularization
induced via the L\'{e}vy random field prior distribution. Posterior inference
for the unknown functions is based on a reversible jump Markov chain Monte
Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)
method to wavelet-based methods using some of the standard test functions, and
illustrate its flexibility and adaptability in nonstationary applications."@2011
Chong Tu@http://arxiv.org/abs/1112.3149v1@"Stochastic expansions using continuous dictionaries: Lévy adaptive
  regression kernels"@"This article describes a new class of prior distributions for nonparametric
function estimation. The unknown function is modeled as a limit of weighted
sums of kernels or generator functions indexed by continuous parameters that
control local and global features such as their translation, dilation,
modulation and shape. L\'{e}vy random fields and their stochastic integrals are
employed to induce prior distributions for the unknown functions or,
equivalently, for the number of kernels and for the parameters governing their
features. Scaling, shape, and other features of the generating functions are
location-specific to allow quite different function properties in different
parts of the space, as with wavelet bases and other methods employing
overcomplete dictionaries. We provide conditions under which the stochastic
expansions converge in specified Besov or Sobolev norms. Under a Gaussian error
model, this may be viewed as a sparse regression problem, with regularization
induced via the L\'{e}vy random field prior distribution. Posterior inference
for the unknown functions is based on a reversible jump Markov chain Monte
Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)
method to wavelet-based methods using some of the standard test functions, and
illustrate its flexibility and adaptability in nonstationary applications."@2011
Peter McCullagh@http://arxiv.org/abs/1112.3228v1@On Bayes' theorem for improper mixtures@"Although Bayes's theorem demands a prior that is a probability distribution
on the parameter space, the calculus associated with Bayes's theorem sometimes
generates sensible procedures from improper priors, Pitman's estimator being a
good example. However, improper priors may also lead to Bayes procedures that
are paradoxical or otherwise unsatisfactory, prompting some authors to insist
that all priors be proper. This paper begins with the observation that an
improper measure on Theta satisfying Kingman's countability condition is in
fact a probability distribution on the power set. We show how to extend a model
in such a way that the extended parameter space is the power set. Under an
additional finiteness condition, which is needed for the existence of a
sampling region, the conditions for Bayes's theorem are satisfied by the
extension. Lack of interference ensures that the posterior distribution in the
extended space is compatible with the original parameter space. Provided that
the key finiteness condition is satisfied, this probabilistic analysis of the
extended model may be interpreted as a vindication of improper Bayes procedures
derived from the original model."@2011
Han Han@http://arxiv.org/abs/1112.3228v1@On Bayes' theorem for improper mixtures@"Although Bayes's theorem demands a prior that is a probability distribution
on the parameter space, the calculus associated with Bayes's theorem sometimes
generates sensible procedures from improper priors, Pitman's estimator being a
good example. However, improper priors may also lead to Bayes procedures that
are paradoxical or otherwise unsatisfactory, prompting some authors to insist
that all priors be proper. This paper begins with the observation that an
improper measure on Theta satisfying Kingman's countability condition is in
fact a probability distribution on the power set. We show how to extend a model
in such a way that the extended parameter space is the power set. Under an
additional finiteness condition, which is needed for the existence of a
sampling region, the conditions for Bayes's theorem are satisfied by the
extension. Lack of interference ensures that the posterior distribution in the
extended space is compatible with the original parameter space. Provided that
the key finiteness condition is satisfied, this probabilistic analysis of the
extended model may be interpreted as a vindication of improper Bayes procedures
derived from the original model."@2011
Karthik Bharath@http://arxiv.org/abs/1112.3427v4@Asymptotics of the Empirical Cross-over Function@"We consider a combination of heavily trimmed sums and sample quantiles which
arises when examining properties of clustering criteria and prove limit
theorems. The object of interest, which we call the Empirical Cross-over
Function, is an L-statistic whose weights do not comply with the requisite
regularity conditions for usage of ex- isting limit results. The law of large
numbers, CLT and a functional CLT are proven."@2011
Vladimir Pozdnyakov@http://arxiv.org/abs/1112.3427v4@Asymptotics of the Empirical Cross-over Function@"We consider a combination of heavily trimmed sums and sample quantiles which
arises when examining properties of clustering criteria and prove limit
theorems. The object of interest, which we call the Empirical Cross-over
Function, is an L-statistic whose weights do not comply with the requisite
regularity conditions for usage of ex- isting limit results. The law of large
numbers, CLT and a functional CLT are proven."@2011
Dipak Dey@http://arxiv.org/abs/1112.3427v4@Asymptotics of the Empirical Cross-over Function@"We consider a combination of heavily trimmed sums and sample quantiles which
arises when examining properties of clustering criteria and prove limit
theorems. The object of interest, which we call the Empirical Cross-over
Function, is an L-statistic whose weights do not comply with the requisite
regularity conditions for usage of ex- isting limit results. The law of large
numbers, CLT and a functional CLT are proven."@2011
Jian Huang@http://arxiv.org/abs/1112.3450v1@The sparse Laplacian shrinkage estimator for high-dimensional regression@"We propose a new penalized method for variable selection and estimation that
explicitly incorporates the correlation patterns among predictors. This method
is based on a combination of the minimax concave penalty and Laplacian
quadratic associated with a graph as the penalty function. We call it the
sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave
penalty for encouraging sparsity and Laplacian quadratic penalty for promoting
smoothness among coefficients associated with the correlated predictors. The
SLS has a generalized grouping property with respect to the graph represented
by the Laplacian quadratic. We show that the SLS possesses an oracle property
in the sense that it is selection consistent and equal to the oracle Laplacian
shrinkage estimator with high probability. This result holds in sparse,
high-dimensional settings with p >> n under reasonable conditions. We derive a
coordinate descent algorithm for computing the SLS estimates. Simulation
studies are conducted to evaluate the performance of the SLS method and a real
data example is used to illustrate its application."@2011
Shuangge Ma@http://arxiv.org/abs/1112.3450v1@The sparse Laplacian shrinkage estimator for high-dimensional regression@"We propose a new penalized method for variable selection and estimation that
explicitly incorporates the correlation patterns among predictors. This method
is based on a combination of the minimax concave penalty and Laplacian
quadratic associated with a graph as the penalty function. We call it the
sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave
penalty for encouraging sparsity and Laplacian quadratic penalty for promoting
smoothness among coefficients associated with the correlated predictors. The
SLS has a generalized grouping property with respect to the graph represented
by the Laplacian quadratic. We show that the SLS possesses an oracle property
in the sense that it is selection consistent and equal to the oracle Laplacian
shrinkage estimator with high probability. This result holds in sparse,
high-dimensional settings with p >> n under reasonable conditions. We derive a
coordinate descent algorithm for computing the SLS estimates. Simulation
studies are conducted to evaluate the performance of the SLS method and a real
data example is used to illustrate its application."@2011
Hongzhe Li@http://arxiv.org/abs/1112.3450v1@The sparse Laplacian shrinkage estimator for high-dimensional regression@"We propose a new penalized method for variable selection and estimation that
explicitly incorporates the correlation patterns among predictors. This method
is based on a combination of the minimax concave penalty and Laplacian
quadratic associated with a graph as the penalty function. We call it the
sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave
penalty for encouraging sparsity and Laplacian quadratic penalty for promoting
smoothness among coefficients associated with the correlated predictors. The
SLS has a generalized grouping property with respect to the graph represented
by the Laplacian quadratic. We show that the SLS possesses an oracle property
in the sense that it is selection consistent and equal to the oracle Laplacian
shrinkage estimator with high probability. This result holds in sparse,
high-dimensional settings with p >> n under reasonable conditions. We derive a
coordinate descent algorithm for computing the SLS estimates. Simulation
studies are conducted to evaluate the performance of the SLS method and a real
data example is used to illustrate its application."@2011
Cun-Hui Zhang@http://arxiv.org/abs/1112.3450v1@The sparse Laplacian shrinkage estimator for high-dimensional regression@"We propose a new penalized method for variable selection and estimation that
explicitly incorporates the correlation patterns among predictors. This method
is based on a combination of the minimax concave penalty and Laplacian
quadratic associated with a graph as the penalty function. We call it the
sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave
penalty for encouraging sparsity and Laplacian quadratic penalty for promoting
smoothness among coefficients associated with the correlated predictors. The
SLS has a generalized grouping property with respect to the graph represented
by the Laplacian quadratic. We show that the SLS possesses an oracle property
in the sense that it is selection consistent and equal to the oracle Laplacian
shrinkage estimator with high probability. This result holds in sparse,
high-dimensional settings with p >> n under reasonable conditions. We derive a
coordinate descent algorithm for computing the SLS estimates. Simulation
studies are conducted to evaluate the performance of the SLS method and a real
data example is used to illustrate its application."@2011
Benoîte de Saporta@http://arxiv.org/abs/1112.3745v1@"Asymmetry tests for Bifurcating Auto-Regressive Processes with missing
  data"@"We present symmetry tests for bifurcating autoregressive processes (BAR) when
some data are missing. BAR processes typically model cell division data. Each
cell can be of one of two types \emph{odd} or \emph{even}. The goal of this
paper is to study the possible asymmetry between odd and even cells in a single
observed lineage. We first derive asymmetry tests for the lineage itself,
modeled by a two-type Galton-Watson process, and then derive tests for the
observed BAR process. We present applications on both simulated and real data."@2011
Anne Gégout-Petit@http://arxiv.org/abs/1112.3745v1@"Asymmetry tests for Bifurcating Auto-Regressive Processes with missing
  data"@"We present symmetry tests for bifurcating autoregressive processes (BAR) when
some data are missing. BAR processes typically model cell division data. Each
cell can be of one of two types \emph{odd} or \emph{even}. The goal of this
paper is to study the possible asymmetry between odd and even cells in a single
observed lineage. We first derive asymmetry tests for the lineage itself,
modeled by a two-type Galton-Watson process, and then derive tests for the
observed BAR process. We present applications on both simulated and real data."@2011
Laurence Marsalle@http://arxiv.org/abs/1112.3745v1@"Asymmetry tests for Bifurcating Auto-Regressive Processes with missing
  data"@"We present symmetry tests for bifurcating autoregressive processes (BAR) when
some data are missing. BAR processes typically model cell division data. Each
cell can be of one of two types \emph{odd} or \emph{even}. The goal of this
paper is to study the possible asymmetry between odd and even cells in a single
observed lineage. We first derive asymmetry tests for the lineage itself,
modeled by a two-type Galton-Watson process, and then derive tests for the
observed BAR process. We present applications on both simulated and real data."@2011
M. Lerasle@http://arxiv.org/abs/1112.3914v1@Robust empirical mean Estimators@"We study robust estimators of the mean of a probability measure $P$, called
robust empirical mean estimators. This elementary construction is then used to
revisit a problem of aggregation and a problem of estimator selection,
extending these methods to not necessarily bounded collections of previous
estimators.
  We consider then the problem of robust $M$-estimation. We propose a slightly
more complicated construction to handle this problem and, as examples of
applications, we apply our general approach to least-squares density
estimation, to density estimation with K\""ullback loss and to a non-Gaussian,
unbounded, random design and heteroscedastic regression problem.
  Finally, we show that our strategy can be used when the data are only assumed
to be mixing."@2011
R. I. Oliveira@http://arxiv.org/abs/1112.3914v1@Robust empirical mean Estimators@"We study robust estimators of the mean of a probability measure $P$, called
robust empirical mean estimators. This elementary construction is then used to
revisit a problem of aggregation and a problem of estimator selection,
extending these methods to not necessarily bounded collections of previous
estimators.
  We consider then the problem of robust $M$-estimation. We propose a slightly
more complicated construction to handle this problem and, as examples of
applications, we apply our general approach to least-squares density
estimation, to density estimation with K\""ullback loss and to a non-Gaussian,
unbounded, random design and heteroscedastic regression problem.
  Finally, we show that our strategy can be used when the data are only assumed
to be mixing."@2011
Mathieu Rosenbaum@http://arxiv.org/abs/1112.4413v1@Improved Matrix Uncertainty Selector@"We consider the regression model with observation error in the design:
y=X\theta* + e, Z=X+N. Here the random vector y in R^n and the random n*p
matrix Z are observed, the n*p matrix X is unknown, N is an n*p random noise
matrix, e in R^n is a random noise vector, and \theta* is a vector of unknown
parameters to be estimated. We consider the setting where the dimension p can
be much larger than the sample size n and \theta* is sparse. Because of the
presence of the noise matrix N, the commonly used Lasso and Dantzig selector
are unstable. An alternative procedure called the Matrix Uncertainty (MU)
selector has been proposed in Rosenbaum and Tsybakov (2010) in order to account
for the noise. The properties of the MU selector have been studied in Rosenbaum
and Tsybakov (2010) for sparse \theta* under the assumption that the noise
matrix N is deterministic and its values are small. In this paper, we propose a
modification of the MU selector when N is a random matrix with zero-mean
entries having the variances that can be estimated. This is, for example, the
case in the model where the entries of X are missing at random. We show both
theoretically and numerically that, under these conditions, the new estimator
called the Compensated MU selector achieves better accuracy of estimation than
the original MU selector."@2011
Alexandre B. Tsybakov@http://arxiv.org/abs/1112.4413v1@Improved Matrix Uncertainty Selector@"We consider the regression model with observation error in the design:
y=X\theta* + e, Z=X+N. Here the random vector y in R^n and the random n*p
matrix Z are observed, the n*p matrix X is unknown, N is an n*p random noise
matrix, e in R^n is a random noise vector, and \theta* is a vector of unknown
parameters to be estimated. We consider the setting where the dimension p can
be much larger than the sample size n and \theta* is sparse. Because of the
presence of the noise matrix N, the commonly used Lasso and Dantzig selector
are unstable. An alternative procedure called the Matrix Uncertainty (MU)
selector has been proposed in Rosenbaum and Tsybakov (2010) in order to account
for the noise. The properties of the MU selector have been studied in Rosenbaum
and Tsybakov (2010) for sparse \theta* under the assumption that the noise
matrix N is deterministic and its values are small. In this paper, we propose a
modification of the MU selector when N is a random matrix with zero-mean
entries having the variances that can be estimated. This is, for example, the
case in the model where the entries of X are missing at random. We show both
theoretically and numerically that, under these conditions, the new estimator
called the Compensated MU selector achieves better accuracy of estimation than
the original MU selector."@2011
Djalel Eddine Meskaldji@http://arxiv.org/abs/1112.4519v5@A comprehensive error rate for multiple testing@"The higher criticism of a family of tests starts with the individual
uncorrected p-values of each test. It then requires a procedure for deciding
whether the collection of p-values indicates the presence of a real effect and
if possible selects the ones that deserve closer scrutiny. This paper
investigates procedures in which the ordered p-values are compared to an
arbitrary positive and non-decreasing threshold sequence."@2011
Dimitri Van De Ville@http://arxiv.org/abs/1112.4519v5@A comprehensive error rate for multiple testing@"The higher criticism of a family of tests starts with the individual
uncorrected p-values of each test. It then requires a procedure for deciding
whether the collection of p-values indicates the presence of a real effect and
if possible selects the ones that deserve closer scrutiny. This paper
investigates procedures in which the ordered p-values are compared to an
arbitrary positive and non-decreasing threshold sequence."@2011
Jean-Philippe Thiran@http://arxiv.org/abs/1112.4519v5@A comprehensive error rate for multiple testing@"The higher criticism of a family of tests starts with the individual
uncorrected p-values of each test. It then requires a procedure for deciding
whether the collection of p-values indicates the presence of a real effect and
if possible selects the ones that deserve closer scrutiny. This paper
investigates procedures in which the ordered p-values are compared to an
arbitrary positive and non-decreasing threshold sequence."@2011
Stephan Morgenthaler@http://arxiv.org/abs/1112.4519v5@A comprehensive error rate for multiple testing@"The higher criticism of a family of tests starts with the individual
uncorrected p-values of each test. It then requires a procedure for deciding
whether the collection of p-values indicates the presence of a real effect and
if possible selects the ones that deserve closer scrutiny. This paper
investigates procedures in which the ordered p-values are compared to an
arbitrary positive and non-decreasing threshold sequence."@2011
Hélène Lescornel@http://arxiv.org/abs/1112.4735v1@Unbiased risk estimation method for covariance estimation@"We consider a model selection estimator of the covariance of a random
process. Using the Unbiased Risk Estimation (URE) method, we build an estimator
of the risk which allows to select an estimator in a collection of model. Then,
we present an oracle inequality which ensures that the risk of the selected
estimator is close to the risk of the oracle. Simulations show the efficiency
of this methodology."@2011
Jean-Michel Loubes@http://arxiv.org/abs/1112.4735v1@Unbiased risk estimation method for covariance estimation@"We consider a model selection estimator of the covariance of a random
process. Using the Unbiased Risk Estimation (URE) method, we build an estimator
of the risk which allows to select an estimator in a collection of model. Then,
we present an oracle inequality which ensures that the risk of the selected
estimator is close to the risk of the oracle. Simulations show the efficiency
of this methodology."@2011
Claudie Chabriac@http://arxiv.org/abs/1112.4735v1@Unbiased risk estimation method for covariance estimation@"We consider a model selection estimator of the covariance of a random
process. Using the Unbiased Risk Estimation (URE) method, we build an estimator
of the risk which allows to select an estimator in a collection of model. Then,
we present an oracle inequality which ensures that the risk of the selected
estimator is close to the risk of the oracle. Simulations show the efficiency
of this methodology."@2011
Takumi Saegusa@http://arxiv.org/abs/1112.4951v3@Weighted likelihood estimation under two-phase sampling@"We develop asymptotic theory for weighted likelihood estimators (WLE) under
two-phase stratified sampling without replacement. We also consider several
variants of WLEs involving estimated weights and calibration. A set of
empirical process tools are developed including a Glivenko-Cantelli theorem, a
theorem for rates of convergence of M-estimators, and a Donsker theorem for the
inverse probability weighted empirical processes under two-phase sampling and
sampling without replacement at the second phase. Using these general results,
we derive asymptotic distributions of the WLE of a finite-dimensional parameter
in a general semiparametric model where an estimator of a nuisance parameter is
estimable either at regular or nonregular rates. We illustrate these results
and methods in the Cox model with right censoring and interval censoring. We
compare the methods via their asymptotic variances under both sampling without
replacement and the more usual (and easier to analyze) assumption of Bernoulli
sampling at the second phase."@2011
Jon A. Wellner@http://arxiv.org/abs/1112.4951v3@Weighted likelihood estimation under two-phase sampling@"We develop asymptotic theory for weighted likelihood estimators (WLE) under
two-phase stratified sampling without replacement. We also consider several
variants of WLEs involving estimated weights and calibration. A set of
empirical process tools are developed including a Glivenko-Cantelli theorem, a
theorem for rates of convergence of M-estimators, and a Donsker theorem for the
inverse probability weighted empirical processes under two-phase sampling and
sampling without replacement at the second phase. Using these general results,
we derive asymptotic distributions of the WLE of a finite-dimensional parameter
in a general semiparametric model where an estimator of a nuisance parameter is
estimable either at regular or nonregular rates. We illustrate these results
and methods in the Cox model with right censoring and interval censoring. We
compare the methods via their asymptotic variances under both sampling without
replacement and the more usual (and easier to analyze) assumption of Bernoulli
sampling at the second phase."@2011
Rina Foygel@http://arxiv.org/abs/1112.5635v1@"Bayesian model choice and information criteria in sparse generalized
  linear models"@"We consider Bayesian model selection in generalized linear models that are
high-dimensional, with the number of covariates p being large relative to the
sample size n, but sparse in that the number of active covariates is small
compared to p. Treating the covariates as random and adopting an asymptotic
scenario in which p increases with n, we show that Bayesian model selection
using certain priors on the set of models is asymptotically equivalent to
selecting a model using an extended Bayesian information criterion. Moreover,
we prove that the smallest true model is selected by either of these methods
with probability tending to one. Having addressed random covariates, we are
also able to give a consistency result for pseudo-likelihood approaches to
high-dimensional sparse graphical modeling. Experiments on real data
demonstrate good performance of the extended Bayesian information criterion for
regression and for graphical models."@2011
Mathias Drton@http://arxiv.org/abs/1112.5635v1@"Bayesian model choice and information criteria in sparse generalized
  linear models"@"We consider Bayesian model selection in generalized linear models that are
high-dimensional, with the number of covariates p being large relative to the
sample size n, but sparse in that the number of active covariates is small
compared to p. Treating the covariates as random and adopting an asymptotic
scenario in which p increases with n, we show that Bayesian model selection
using certain priors on the set of models is asymptotically equivalent to
selecting a model using an extended Bayesian information criterion. Moreover,
we prove that the smallest true model is selected by either of these methods
with probability tending to one. Having addressed random covariates, we are
also able to give a consistency result for pseudo-likelihood approaches to
high-dimensional sparse graphical modeling. Experiments on real data
demonstrate good performance of the extended Bayesian information criterion for
regression and for graphical models."@2011
Ting Yan@http://arxiv.org/abs/1201.0058v4@High-dimensional Wilks phenomena in the Bradley-Terry model@"In this paper, we show the Wilks type of results for the Bradley-Terry model.
Specifically, for some simple and composite null hypotheses of interest, we
show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square
approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda
-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the
corresponding degrees of freedom. Simulation studies and an application to NBA
data illustrate the theoretical results."@2011
Yuanzhang Li@http://arxiv.org/abs/1201.0058v4@High-dimensional Wilks phenomena in the Bradley-Terry model@"In this paper, we show the Wilks type of results for the Bradley-Terry model.
Specifically, for some simple and composite null hypotheses of interest, we
show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square
approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda
-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the
corresponding degrees of freedom. Simulation studies and an application to NBA
data illustrate the theoretical results."@2011
Jinfeng Xu@http://arxiv.org/abs/1201.0058v4@High-dimensional Wilks phenomena in the Bradley-Terry model@"In this paper, we show the Wilks type of results for the Bradley-Terry model.
Specifically, for some simple and composite null hypotheses of interest, we
show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square
approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda
-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the
corresponding degrees of freedom. Simulation studies and an application to NBA
data illustrate the theoretical results."@2011
Yaning Yang@http://arxiv.org/abs/1201.0058v4@High-dimensional Wilks phenomena in the Bradley-Terry model@"In this paper, we show the Wilks type of results for the Bradley-Terry model.
Specifically, for some simple and composite null hypotheses of interest, we
show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square
approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda
-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the
corresponding degrees of freedom. Simulation studies and an application to NBA
data illustrate the theoretical results."@2011
Ji Zhu@http://arxiv.org/abs/1201.0058v4@High-dimensional Wilks phenomena in the Bradley-Terry model@"In this paper, we show the Wilks type of results for the Bradley-Terry model.
Specifically, for some simple and composite null hypotheses of interest, we
show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square
approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda
-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the
corresponding degrees of freedom. Simulation studies and an application to NBA
data illustrate the theoretical results."@2011
Jianqing Fan@http://arxiv.org/abs/1201.0175v2@"Large Covariance Estimation by Thresholding Principal Orthogonal
  Complements"@"This paper deals with the estimation of a high-dimensional covariance with a
conditional sparsity structure and fast-diverging eigenvalues. By assuming
sparse error covariance matrix in an approximate factor model, we allow for the
presence of some cross-sectional correlation even after taking out common but
unobservable factors. We introduce the Principal Orthogonal complEment
Thresholding (POET) method to explore such an approximate factor structure with
sparsity. The POET estimator includes the sample covariance matrix, the
factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding
estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator
(Cai and Liu, 2011) as specific examples. We provide mathematical insights when
the factor analysis is approximately the same as the principal component
analysis for high-dimensional data. The rates of convergence of the sparse
residual covariance matrix and the conditional sparse covariance matrix are
studied under various norms. It is shown that the impact of estimating the
unknown factors vanishes as the dimensionality increases. The uniform rates of
convergence for the unobserved factors and their factor loadings are derived.
The asymptotic results are also verified by extensive simulation studies.
Finally, a real data application on portfolio allocation is presented."@2011
Yuan Liao@http://arxiv.org/abs/1201.0175v2@"Large Covariance Estimation by Thresholding Principal Orthogonal
  Complements"@"This paper deals with the estimation of a high-dimensional covariance with a
conditional sparsity structure and fast-diverging eigenvalues. By assuming
sparse error covariance matrix in an approximate factor model, we allow for the
presence of some cross-sectional correlation even after taking out common but
unobservable factors. We introduce the Principal Orthogonal complEment
Thresholding (POET) method to explore such an approximate factor structure with
sparsity. The POET estimator includes the sample covariance matrix, the
factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding
estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator
(Cai and Liu, 2011) as specific examples. We provide mathematical insights when
the factor analysis is approximately the same as the principal component
analysis for high-dimensional data. The rates of convergence of the sparse
residual covariance matrix and the conditional sparse covariance matrix are
studied under various norms. It is shown that the impact of estimating the
unknown factors vanishes as the dimensionality increases. The uniform rates of
convergence for the unobserved factors and their factor loadings are derived.
The asymptotic results are also verified by extensive simulation studies.
Finally, a real data application on portfolio allocation is presented."@2011
Martina Mincheva@http://arxiv.org/abs/1201.0175v2@"Large Covariance Estimation by Thresholding Principal Orthogonal
  Complements"@"This paper deals with the estimation of a high-dimensional covariance with a
conditional sparsity structure and fast-diverging eigenvalues. By assuming
sparse error covariance matrix in an approximate factor model, we allow for the
presence of some cross-sectional correlation even after taking out common but
unobservable factors. We introduce the Principal Orthogonal complEment
Thresholding (POET) method to explore such an approximate factor structure with
sparsity. The POET estimator includes the sample covariance matrix, the
factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding
estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator
(Cai and Liu, 2011) as specific examples. We provide mathematical insights when
the factor analysis is approximately the same as the principal component
analysis for high-dimensional data. The rates of convergence of the sparse
residual covariance matrix and the conditional sparse covariance matrix are
studied under various norms. It is shown that the impact of estimating the
unknown factors vanishes as the dimensionality increases. The uniform rates of
convergence for the unobserved factors and their factor loadings are derived.
The asymptotic results are also verified by extensive simulation studies.
Finally, a real data application on portfolio allocation is presented."@2011
Yizao Wang@http://arxiv.org/abs/1201.0238v1@"On the asymptotic normality of kernel density estimators for linear
  random fields"@"We establish sufficient conditions for the asymptotic normality of kernel
density estimators, applied to causal linear random fields. Our conditions on
the coefficients of linear random fields are weaker than known results,
although our assumption on the bandwidth is not minimal. The proof is based on
the $m$-approximation method. As a key step, we prove a central limit theorem
for triangular arrays of stationary $m$-dependent random fields with unbounded
$m$. We also apply a moment inequality recently established for stationary
random fields."@2011
Michael Woodroofe@http://arxiv.org/abs/1201.0238v1@"On the asymptotic normality of kernel density estimators for linear
  random fields"@"We establish sufficient conditions for the asymptotic normality of kernel
density estimators, applied to causal linear random fields. Our conditions on
the coefficients of linear random fields are weaker than known results,
although our assumption on the bandwidth is not minimal. The proof is based on
the $m$-approximation method. As a key step, we prove a central limit theorem
for triangular arrays of stationary $m$-dependent random fields with unbounded
$m$. We also apply a moment inequality recently established for stationary
random fields."@2011
