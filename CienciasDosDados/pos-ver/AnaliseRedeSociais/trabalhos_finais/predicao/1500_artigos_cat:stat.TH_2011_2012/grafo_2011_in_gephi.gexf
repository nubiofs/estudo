<?xml version='1.0' encoding='utf-8'?>
<gexf version="1.2" xmlns="http://www.gexf.net/1.2draft" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2001/XMLSchema-instance">
  <graph defaultedgetype="undirected" mode="static" name="">
    <attributes class="edge" mode="static">
      <attribute id="1" title="id_link" type="string" />
      <attribute id="2" title="title" type="string" />
      <attribute id="3" title="summary" type="string" />
      <attribute id="4" title="Weight" type="long" />
    </attributes>
    <attributes class="node" mode="static">
      <attribute id="0" title="summary_tokenize_stop_word" type="string" />
    </attributes>
    <meta>
      <creator>NetworkX 2.2</creator>
      <lastmodified>18/02/2019</lastmodified>
    </meta>
    <nodes>
      <node id="Fabienne Comte" label="Fabienne Comte">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]estimation[#]slope[#]function[#]functional[#]linear[#]regression[#]scalar[#]responses[#]modeled[#]dependence[#]random[#]functions[#]Cardot[#]Johannes[#]Multivariate[#]Anal[#]shown[#]thresholded[#]projection[#]estimator[#]attain[#]constant[#]convergence[#]general[#]framework[#]allows[#]us[#]cover[#]prediction[#]problem[#]respect[#]mean[#]squared[#]error[#]well[#]derivatives[#]This[#]procedure[#]however[#]requires[#]optimal[#]choice[#]tuning[#]parameter[#]regard[#]certain[#]characteristics[#]covariance[#]operator[#]associated[#]regressor[#]As[#]information[#]usually[#]inaccessible[#]practice[#]investigate[#]fully[#]combines[#]model[#]selection[#]Lepski[#]method[#]It[#]inspired[#]recent[#]work[#]Goldenshluger[#]Ann[#]Statist[#]The[#]selected[#]minimizer[#]stochastic[#]penalized[#]contrast[#]imitating[#]among[#]collection[#]admissible[#]values[#]depends[#]data[#]show[#]within[#]resulting[#]variety[#]classes[#]operators[#]results[#]illustrated[#]considering[#]different[#]configurations[#]particular[#]A[#]simulation[#]study[#]shows[#]reasonable[#]performance[#]In[#]paper[#]nonparametric[#]e[#]vy[#]density[#]processes[#]without[#]Brownian[#]component[#]For[#]n[#]discrete[#]time[#]observations[#]step[#]asymptotic[#]tends[#]infinity[#]zero[#]use[#]Fourier[#]approach[#]construct[#]adaptive[#]provide[#]bound[#]global[#]L[#]Estimators[#]drift[#]variance[#]Gaussian[#]also[#]studied[#]discuss[#]rates[#]give[#]examples[#]fitting" />
        </attvalues>
      </node>
      <node id="Jan Johannes" label="Jan Johannes">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]estimation[#]slope[#]function[#]functional[#]linear[#]regression[#]scalar[#]responses[#]modeled[#]dependence[#]random[#]functions[#]Cardot[#]Johannes[#]Multivariate[#]Anal[#]shown[#]thresholded[#]projection[#]estimator[#]attain[#]constant[#]convergence[#]general[#]framework[#]allows[#]us[#]cover[#]prediction[#]problem[#]respect[#]mean[#]squared[#]error[#]well[#]derivatives[#]This[#]procedure[#]however[#]requires[#]optimal[#]choice[#]tuning[#]parameter[#]regard[#]certain[#]characteristics[#]covariance[#]operator[#]associated[#]regressor[#]As[#]information[#]usually[#]inaccessible[#]practice[#]investigate[#]fully[#]combines[#]model[#]selection[#]Lepski[#]method[#]It[#]inspired[#]recent[#]work[#]Goldenshluger[#]Ann[#]Statist[#]The[#]selected[#]minimizer[#]stochastic[#]penalized[#]contrast[#]imitating[#]among[#]collection[#]admissible[#]values[#]depends[#]data[#]show[#]within[#]resulting[#]variety[#]classes[#]operators[#]results[#]illustrated[#]considering[#]different[#]configurations[#]particular[#]A[#]simulation[#]study[#]shows[#]reasonable[#]performance[#]value[#]In[#]Schenk[#]based[#]dimension[#]reduction[#]additional[#]thresholding[#]minimax[#]rates[#]However[#]unknown[#]combination[#]adaptive[#]attains[#]lower[#]bound[#]risk[#]logarithmic[#]factor[#]wide[#]range[#]theory[#]covers[#]local[#]averages[#]estimating[#]l[#]structural[#]models[#]nonparametric[#]relationship[#]presence[#]instrumental[#]variables[#]propose[#]technique[#]consistent[#]rate[#]regularity[#]conditions[#]depending[#]joint[#]distribution[#]instrument[#]driven[#]paper[#]classical[#]smoothness[#]assumptions[#]discuss[#]examples[#]pointwise" />
        </attvalues>
      </node>
      <node id="Brice Franke" label="Brice Franke">
        <attvalues>
          <attvalue for="0" value="In[#]article[#]merge[#]celebrated[#]results[#]Kesten[#]Spitzer[#]Wahrsch[#]Verw[#]Gebiete[#]Kawazu[#]Stat[#]Phys[#]A[#]random[#]walk[#]performs[#]motion[#]environment[#]observes[#]scenery[#]along[#]path[#]We[#]assume[#]domain[#]attraction[#]stable[#]distribution[#]prove[#]resulting[#]observations[#]satisfy[#]limit[#]theorem[#]The[#]process[#]stochastic[#]dependencies" />
        </attvalues>
      </node>
      <node id="Tatsuhiko Saigo" label="Tatsuhiko Saigo">
        <attvalues>
          <attvalue for="0" value="In[#]article[#]merge[#]celebrated[#]results[#]Kesten[#]Spitzer[#]Wahrsch[#]Verw[#]Gebiete[#]Kawazu[#]Stat[#]Phys[#]A[#]random[#]walk[#]performs[#]motion[#]environment[#]observes[#]scenery[#]along[#]path[#]We[#]assume[#]domain[#]attraction[#]stable[#]distribution[#]prove[#]resulting[#]observations[#]satisfy[#]limit[#]theorem[#]The[#]process[#]stochastic[#]dependencies" />
        </attvalues>
      </node>
      <node id="Michel Broniatowski" label="Michel Broniatowski">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Aida Toma" label="Aida Toma">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Igor Vajda" label="Igor Vajda">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="John T. Flam" label="John T. Flam">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]minimum[#]mean[#]square[#]error[#]MMSE[#]estimation[#]x[#]given[#]observation[#]n[#]independent[#]Gaussian[#]Mixture[#]GM[#]distributed[#]The[#]introduction[#]distributions[#]represents[#]generalization[#]familiar[#]simpler[#]signal[#]noise[#]instance[#]We[#]present[#]necessary[#]theoretical[#]foundation[#]derive[#]estimator[#]closed[#]form[#]Furthermore[#]provide[#]upper[#]lower[#]bounds[#]MSE[#]These[#]validated[#]Monte[#]Carlo[#]simulations" />
        </attvalues>
      </node>
      <node id="Saikat Chatterjee" label="Saikat Chatterjee">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]minimum[#]mean[#]square[#]error[#]MMSE[#]estimation[#]x[#]given[#]observation[#]n[#]independent[#]Gaussian[#]Mixture[#]GM[#]distributed[#]The[#]introduction[#]distributions[#]represents[#]generalization[#]familiar[#]simpler[#]signal[#]noise[#]instance[#]We[#]present[#]necessary[#]theoretical[#]foundation[#]derive[#]estimator[#]closed[#]form[#]Furthermore[#]provide[#]upper[#]lower[#]bounds[#]MSE[#]These[#]validated[#]Monte[#]Carlo[#]simulations" />
        </attvalues>
      </node>
      <node id="Kimmo Kansanen" label="Kimmo Kansanen">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]minimum[#]mean[#]square[#]error[#]MMSE[#]estimation[#]x[#]given[#]observation[#]n[#]independent[#]Gaussian[#]Mixture[#]GM[#]distributed[#]The[#]introduction[#]distributions[#]represents[#]generalization[#]familiar[#]simpler[#]signal[#]noise[#]instance[#]We[#]present[#]necessary[#]theoretical[#]foundation[#]derive[#]estimator[#]closed[#]form[#]Furthermore[#]provide[#]upper[#]lower[#]bounds[#]MSE[#]These[#]validated[#]Monte[#]Carlo[#]simulations" />
        </attvalues>
      </node>
      <node id="Torbjorn Ekman" label="Torbjorn Ekman">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]minimum[#]mean[#]square[#]error[#]MMSE[#]estimation[#]x[#]given[#]observation[#]n[#]independent[#]Gaussian[#]Mixture[#]GM[#]distributed[#]The[#]introduction[#]distributions[#]represents[#]generalization[#]familiar[#]simpler[#]signal[#]noise[#]instance[#]We[#]present[#]necessary[#]theoretical[#]foundation[#]derive[#]estimator[#]closed[#]form[#]Furthermore[#]provide[#]upper[#]lower[#]bounds[#]MSE[#]These[#]validated[#]Monte[#]Carlo[#]simulations" />
        </attvalues>
      </node>
      <node id="S. Chen" label="S. Chen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="J. Dick" label="J. Dick">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="A. B. Owen" label="A. B. Owen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Heping He" label="Heping He">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Thomas A. Severini" label="Thomas A. Severini">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Laëtitia Comminges" label="Laëtitia Comminges">
        <attvalues>
          <attvalue for="0" value="We[#]address[#]issue[#]variable[#]selection[#]regression[#]model[#]high[#]ambient[#]dimension[#]number[#]covariates[#]large[#]The[#]main[#]focus[#]situation[#]relevant[#]called[#]intrinsic[#]much[#]smaller[#]Without[#]assuming[#]parametric[#]form[#]underlying[#]function[#]get[#]tight[#]conditions[#]making[#]possible[#]consistently[#]estimate[#]set[#]variables[#]These[#]relate[#]sample[#]size[#]procedure[#]provably[#]consistent[#]simple[#]based[#]comparing[#]empirical[#]Fourier[#]coefficients[#]appropriately[#]chosen[#]threshold[#]value" />
        </attvalues>
      </node>
      <node id="Arnak Dalalyan" label="Arnak Dalalyan">
        <attvalues>
          <attvalue for="0" value="We[#]address[#]issue[#]variable[#]selection[#]regression[#]model[#]high[#]ambient[#]dimension[#]number[#]covariates[#]large[#]The[#]main[#]focus[#]situation[#]relevant[#]called[#]intrinsic[#]much[#]smaller[#]Without[#]assuming[#]parametric[#]form[#]underlying[#]function[#]get[#]tight[#]conditions[#]making[#]possible[#]consistently[#]estimate[#]set[#]variables[#]These[#]relate[#]sample[#]size[#]procedure[#]provably[#]consistent[#]simple[#]based[#]comparing[#]empirical[#]Fourier[#]coefficients[#]appropriately[#]chosen[#]threshold[#]value[#]consider[#]problem[#]combining[#]possibly[#]uncountably[#]infinite[#]affine[#]estimators[#]heteroscedastic[#]Gaussian[#]noise[#]Focusing[#]exponentially[#]weighted[#]aggregate[#]prove[#]type[#]inequality[#]leads[#]sharp[#]oracle[#]inequalities[#]discrete[#]also[#]continuous[#]settings[#]framework[#]general[#]enough[#]cover[#]combinations[#]various[#]procedures[#]least[#]square[#]kernel[#]ridge[#]shrinking[#]many[#]used[#]literature[#]statistical[#]inverse[#]problems[#]As[#]consequence[#]show[#]proposed[#]provides[#]adaptive[#]estimator[#]exact[#]minimax[#]sense[#]without[#]neither[#]discretizing[#]range[#]tuning[#]parameters[#]splitting[#]observations[#]illustrate[#]numerically[#]good[#]performance[#]achieved" />
        </attvalues>
      </node>
      <node id="Eric Gautier" label="Eric Gautier">
        <attvalues>
          <attvalue for="0" value="This[#]article[#]considers[#]inference[#]linear[#]models[#]K[#]regressors[#]many[#]could[#]endogenous[#]L[#]instruments[#]range[#]less[#]order[#]smaller[#]exponential[#]sample[#]size[#]arbitrary[#]For[#]moderate[#]identification[#]robust[#]confidence[#]sets[#]obtained[#]solving[#]hierarchy[#]semidefinite[#]programs[#]larger[#]propose[#]STIV[#]estimator[#]The[#]analysis[#]error[#]uses[#]sensitivity[#]characteristics[#]sharper[#]literature[#]sparsity[#]bounds[#]Results[#]rates[#]convergence[#]variable[#]selection[#]adapt[#]given[#]We[#]generalize[#]approach[#]approximation[#]errors[#]systems[#]bands[#]vectors[#]functionals[#]functions[#]application[#]demand[#]system" />
        </attvalues>
      </node>
      <node id="Stefan Hoderlein" label="Stefan Hoderlein">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="E. Ostrovsky" label="E. Ostrovsky">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="L. Sirota" label="L. Sirota">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Debdeep Pati" label="Debdeep Pati">
        <attvalues>
          <attvalue for="0" value="In[#]nonparametric[#]regression[#]problems[#]involving[#]multiple[#]predictors[#]typically[#]interest[#]estimating[#]anisotropic[#]multivariate[#]surface[#]important[#]discarding[#]unimportant[#]ones[#]Our[#]focus[#]defining[#]Bayesian[#]procedure[#]leads[#]minimax[#]optimal[#]rate[#]posterior[#]contraction[#]log[#]factor[#]adapting[#]unknown[#]dimension[#]smoothness[#]true[#]We[#]propose[#]approach[#]based[#]Gaussian[#]process[#]prior[#]scalings[#]assigned[#]hyperpriors[#]additionally[#]show[#]using[#]homogenous[#]single[#]bandwidth[#]cases[#]latent[#]variable[#]models[#]become[#]increasingly[#]popular[#]variety[#]applications[#]However[#]little[#]study[#]theoretical[#]properties[#]article[#]rates[#]univariate[#]density[#]estimation[#]class[#]unobserved[#]U[#]variables[#]related[#]response[#]via[#]random[#]additive[#]error[#]relies[#]characterizing[#]space[#]densities[#]induced[#]model[#]kernel[#]convolutions[#]general[#]continuous[#]mixing[#]measures[#]The[#]literature[#]almost[#]entirely[#]focuses[#]finite[#]countably[#]infinite[#]mixture[#]develop[#]approximation[#]results[#]Using[#]appropriate[#]function[#]obtain[#]frequentist[#]logarithmic[#]standard[#]regularity[#]conditions" />
        </attvalues>
      </node>
      <node id="Anirban Bhattacharya" label="Anirban Bhattacharya">
        <attvalues>
          <attvalue for="0" value="In[#]nonparametric[#]regression[#]problems[#]involving[#]multiple[#]predictors[#]typically[#]interest[#]estimating[#]anisotropic[#]multivariate[#]surface[#]important[#]discarding[#]unimportant[#]ones[#]Our[#]focus[#]defining[#]Bayesian[#]procedure[#]leads[#]minimax[#]optimal[#]rate[#]posterior[#]contraction[#]log[#]factor[#]adapting[#]unknown[#]dimension[#]smoothness[#]true[#]We[#]propose[#]approach[#]based[#]Gaussian[#]process[#]prior[#]scalings[#]assigned[#]hyperpriors[#]additionally[#]show[#]using[#]homogenous[#]single[#]bandwidth[#]cases[#]latent[#]variable[#]models[#]become[#]increasingly[#]popular[#]variety[#]applications[#]However[#]little[#]study[#]theoretical[#]properties[#]article[#]rates[#]univariate[#]density[#]estimation[#]class[#]unobserved[#]U[#]variables[#]related[#]response[#]via[#]random[#]additive[#]error[#]relies[#]characterizing[#]space[#]densities[#]induced[#]model[#]kernel[#]convolutions[#]general[#]continuous[#]mixing[#]measures[#]The[#]literature[#]almost[#]entirely[#]focuses[#]finite[#]countably[#]infinite[#]mixture[#]develop[#]approximation[#]results[#]Using[#]appropriate[#]function[#]obtain[#]frequentist[#]logarithmic[#]standard[#]regularity[#]conditions" />
        </attvalues>
      </node>
      <node id="David B. Dunson" label="David B. Dunson">
        <attvalues>
          <attvalue for="0" value="latent[#]variable[#]models[#]become[#]increasingly[#]popular[#]variety[#]applications[#]However[#]little[#]study[#]theoretical[#]properties[#]In[#]article[#]rates[#]posterior[#]contraction[#]univariate[#]density[#]estimation[#]class[#]unobserved[#]U[#]variables[#]related[#]response[#]via[#]random[#]regression[#]additive[#]error[#]Our[#]approach[#]relies[#]characterizing[#]space[#]densities[#]induced[#]model[#]kernel[#]convolutions[#]general[#]continuous[#]mixing[#]measures[#]The[#]literature[#]almost[#]entirely[#]focuses[#]finite[#]countably[#]infinite[#]mixture[#]We[#]develop[#]approximation[#]results[#]Using[#]appropriate[#]Gaussian[#]process[#]prior[#]unknown[#]function[#]obtain[#]optimal[#]frequentist[#]rate[#]logarithmic[#]factor[#]standard[#]regularity[#]conditions[#]true[#]Although[#]discrete[#]modeling[#]formed[#]backbone[#]Bayesian[#]well[#]known[#]disadvantages[#]propose[#]alternative[#]priors[#]based[#]nonlinear[#]functions[#]uniform[#]residual[#]shown[#]desirable[#]including[#]ease[#]centering[#]initial[#]guess[#]large[#]support[#]consistency[#]straightforward[#]computation[#]Gibbs[#]sampling[#]Some[#]advantages[#]mixtures[#]Dirichlet[#]kernels[#]discussed[#]illustrated[#]simulations[#]epidemiology[#]application" />
        </attvalues>
      </node>
      <node id="Rina Foygel" label="Rina Foygel">
        <attvalues>
          <attvalue for="0" value="A[#]linear[#]structural[#]equation[#]model[#]relates[#]random[#]variables[#]interest[#]corresponding[#]Gaussian[#]noise[#]terms[#]via[#]system[#]Each[#]represented[#]mixed[#]graph[#]directed[#]edges[#]encode[#]equations[#]bidirected[#]indicate[#]possible[#]correlations[#]among[#]We[#]study[#]parameter[#]identifiability[#]models[#]ask[#]conditions[#]ensure[#]edge[#]coefficients[#]appearing[#]uniquely[#]recovered[#]covariance[#]matrix[#]associated[#]distribution[#]treat[#]case[#]generic[#]unique[#]recovery[#]almost[#]every[#]choice[#]parameters[#]give[#]new[#]graphical[#]condition[#]sufficient[#]verified[#]time[#]polynomial[#]size[#]It[#]improves[#]criteria[#]prior[#]work[#]require[#]part[#]acyclic[#]also[#]develop[#]related[#]necessary[#]examine[#]gap[#]simulations[#]graphs[#]nodes[#]well[#]exhaustive[#]algebraic[#]computations[#]five" />
        </attvalues>
      </node>
      <node id="Nathan Srebro" label="Nathan Srebro">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Erwan Le Pennec" label="Erwan Le Pennec">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yizao Wang" label="Yizao Wang">
        <attvalues>
          <attvalue for="0" value="We[#]establish[#]sufficient[#]conditions[#]asymptotic[#]normality[#]kernel[#]density[#]estimators[#]applied[#]causal[#]linear[#]random[#]fields[#]Our[#]coefficients[#]weaker[#]known[#]results[#]although[#]assumption[#]bandwidth[#]minimal[#]The[#]proof[#]based[#]method[#]As[#]key[#]step[#]prove[#]central[#]limit[#]theorem[#]triangular[#]arrays[#]stationary[#]unbounded[#]also[#]apply[#]moment[#]inequality[#]recently[#]established" />
        </attvalues>
      </node>
      <node id="Michael Woodroofe" label="Michael Woodroofe">
        <attvalues>
          <attvalue for="0" value="We[#]establish[#]sufficient[#]conditions[#]asymptotic[#]normality[#]kernel[#]density[#]estimators[#]applied[#]causal[#]linear[#]random[#]fields[#]Our[#]coefficients[#]weaker[#]known[#]results[#]although[#]assumption[#]bandwidth[#]minimal[#]The[#]proof[#]based[#]method[#]As[#]key[#]step[#]prove[#]central[#]limit[#]theorem[#]triangular[#]arrays[#]stationary[#]unbounded[#]also[#]apply[#]moment[#]inequality[#]recently[#]established" />
        </attvalues>
      </node>
      <node id="Ery Arias-Castro" label="Ery Arias-Castro">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]task[#]detecting[#]salient[#]cluster[#]sensor[#]network[#]undirected[#]graph[#]random[#]variable[#]attached[#]node[#]Motivated[#]recent[#]research[#]environmental[#]statistics[#]drive[#]compete[#]reigning[#]scan[#]statistic[#]explore[#]alternatives[#]based[#]percolative[#]properties[#]The[#]first[#]method[#]size[#]largest[#]connected[#]component[#]removing[#]nodes[#]value[#]given[#]threshold[#]second[#]upper[#]level[#]set[#]test[#]introduced[#]Patil[#]Taillie[#]Statist[#]Sci[#]establish[#]performance[#]methods[#]asymptotic[#]theoretic[#]framework[#]increases[#]These[#]tests[#]two[#]advantages[#]conventional[#]require[#]previous[#]information[#]shape[#]computationally[#]feasible[#]make[#]abundant[#]use[#]percolation[#]theory[#]derive[#]theoretical[#]results[#]complement[#]numerical[#]experiments[#]hypothesis[#]testing[#]problem[#]deciding[#]whether[#]observed[#]vector[#]independent[#]normal[#]components[#]alternatively[#]small[#]subset[#]correlated[#]may[#]certain[#]combinatorial[#]structure[#]known[#]statistician[#]lower[#]bounds[#]minimax[#]risk[#]terms[#]correlation[#]class[#]possibly[#]sets[#]show[#]simple[#]many[#]cases[#]generalized[#]likelihood[#]ratio[#]suboptimal[#]important" />
        </attvalues>
      </node>
      <node id="Geoffrey R. Grimmett" label="Geoffrey R. Grimmett">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]task[#]detecting[#]salient[#]cluster[#]sensor[#]network[#]undirected[#]graph[#]random[#]variable[#]attached[#]node[#]Motivated[#]recent[#]research[#]environmental[#]statistics[#]drive[#]compete[#]reigning[#]scan[#]statistic[#]explore[#]alternatives[#]based[#]percolative[#]properties[#]The[#]first[#]method[#]size[#]largest[#]connected[#]component[#]removing[#]nodes[#]value[#]given[#]threshold[#]second[#]upper[#]level[#]set[#]test[#]introduced[#]Patil[#]Taillie[#]Statist[#]Sci[#]establish[#]performance[#]methods[#]asymptotic[#]theoretic[#]framework[#]increases[#]These[#]tests[#]two[#]advantages[#]conventional[#]require[#]previous[#]information[#]shape[#]computationally[#]feasible[#]make[#]abundant[#]use[#]percolation[#]theory[#]derive[#]theoretical[#]results[#]complement[#]numerical[#]experiments" />
        </attvalues>
      </node>
      <node id="Axel Bücher" label="Axel Bücher">
        <attvalues>
          <attvalue for="0" value="We[#]propose[#]new[#]class[#]estimators[#]Pickands[#]dependence[#]function[#]based[#]concept[#]minimum[#]distance[#]estimation[#]An[#]explicit[#]integral[#]representation[#]minimizes[#]weighted[#]logarithm[#]copula[#]C[#]functions[#]form[#]A[#]derived[#]If[#]unknown[#]coincides[#]Moreover[#]even[#]case[#]always[#]satisfies[#]boundary[#]conditions[#]The[#]obtained[#]replacing[#]empirical[#]counterpart[#]weak[#]convergence[#]corresponding[#]process[#]shown[#]comparison[#]commonly[#]used[#]performed[#]theoretical[#]point[#]view[#]means[#]simulation[#]study[#]Our[#]asymptotic[#]numerical[#]results[#]indicate[#]outperform[#]recently[#]proposed[#]Genest[#]Segers[#]Ann[#]Statist[#]As[#]obtain[#]simple[#]test[#]hypothesis[#]consistent[#]positive[#]quadrant[#]dependent[#]alternatives[#]satisfying[#]differentiability[#]assumptions[#]first[#]order" />
        </attvalues>
      </node>
      <node id="Holger Dette" label="Holger Dette">
        <attvalues>
          <attvalue for="0" value="We[#]propose[#]new[#]class[#]estimators[#]Pickands[#]dependence[#]function[#]based[#]concept[#]minimum[#]distance[#]estimation[#]An[#]explicit[#]integral[#]representation[#]minimizes[#]weighted[#]logarithm[#]copula[#]C[#]functions[#]form[#]A[#]derived[#]If[#]unknown[#]coincides[#]Moreover[#]even[#]case[#]always[#]satisfies[#]boundary[#]conditions[#]The[#]obtained[#]replacing[#]empirical[#]counterpart[#]weak[#]convergence[#]corresponding[#]process[#]shown[#]comparison[#]commonly[#]used[#]performed[#]theoretical[#]point[#]view[#]means[#]simulation[#]study[#]Our[#]asymptotic[#]numerical[#]results[#]indicate[#]outperform[#]recently[#]proposed[#]Genest[#]Segers[#]Ann[#]Statist[#]As[#]obtain[#]simple[#]test[#]hypothesis[#]consistent[#]positive[#]quadrant[#]dependent[#]alternatives[#]satisfying[#]differentiability[#]assumptions[#]first[#]order[#]celebrated[#]de[#]la[#]Garza[#]phenomenon[#]states[#]polynomial[#]regression[#]model[#]degree[#]optimal[#]design[#]p[#]points[#]In[#]remarkable[#]paper[#]Yang[#]showed[#]exists[#]many[#]locally[#]problems[#]nonlinear[#]models[#]present[#]note[#]different[#]findings[#]using[#]moment[#]theory[#]Chebyshev[#]systems[#]particular[#]show[#]occurs[#]larger[#]considered[#]far[#]alternative[#]method[#]spectral[#]analysis[#]univariate[#]strictly[#]stationary[#]time[#]series[#]Z[#]define[#]spectrum[#]Fourier[#]transform[#]differences[#]copulas[#]pairs[#]independence[#]This[#]object[#]called[#]density[#]kernel[#]allows[#]separate[#]marginal[#]serial[#]aspects[#]closely[#]related[#]quantile[#]Like[#]provides[#]much[#]information[#]conditional[#]distributions[#]classical[#]kernels[#]informative[#]traditional[#]densities[#]autocovariances[#]population[#]versions[#]provide[#]asymptotically[#]sample[#]complete[#]description[#]inherit[#]robustness[#]properties[#]require[#]distributional[#]existence[#]finite[#]moments[#]estimate[#]introduce[#]Laplace[#]periodograms[#]calculated[#]bilinear[#]forms[#]ranks[#]observed[#]onto[#]harmonic[#]establish[#]distribution[#]consistency[#]adequately[#]smoothed[#]methodology[#]potential[#]applications[#]briefly[#]investigated[#]simulations[#]short[#]example" />
        </attvalues>
      </node>
      <node id="Cristina Butucea" label="Cristina Butucea">
        <attvalues>
          <attvalue for="0" value="We[#]observe[#]M[#]matrix[#]ij[#]N[#]j[#]R[#]test[#]null[#]hypothesis[#]alternative[#]exists[#]submatrix[#]size[#]significant[#]elements[#]sense[#]propose[#]procedure[#]compute[#]asymptotical[#]detection[#]boundary[#]maximal[#]testing[#]risk[#]tends[#]prove[#]asymptotically[#]sharp[#]minimax[#]additional[#]constraints[#]Relations[#]problems[#]discussed[#]adapts[#]unknown[#]n[#]within[#]given[#]set[#]adaptive[#]rates[#]The[#]implementation[#]synthetic[#]data[#]shows[#]excellent[#]behavior[#]sparse[#]necessarily[#]squared[#]matrices[#]extend[#]results[#]different[#]directions[#]first[#]Gaussian[#]variance[#]next[#]random[#]variables[#]distribution[#]exponential[#]family[#]finally[#]consider[#]paper[#]semiparametric[#]mixture[#]two[#]distributions[#]equal[#]shift[#]parameter[#]model[#]said[#]mixed[#]supposed[#]belong[#]parametric[#]In[#]order[#]insure[#]identifiability[#]assumed[#]symmetric[#]defined[#]mixing[#]proportion[#]location[#]parameters[#]probability[#]density[#]function[#]new[#]class[#]based[#]Fourier[#]approach[#]square[#]root[#]consistent[#]mild[#]regularity[#]conditions[#]Their[#]properties[#]illustrated[#]Monte[#]Carlo[#]study[#]benchmark[#]real[#]dataset[#]also[#]studied[#]method" />
        </attvalues>
      </node>
      <node id="Yuri I. Ingster" label="Yuri I. Ingster">
        <attvalues>
          <attvalue for="0" value="We[#]observe[#]M[#]matrix[#]ij[#]N[#]j[#]R[#]test[#]null[#]hypothesis[#]alternative[#]exists[#]submatrix[#]size[#]significant[#]elements[#]sense[#]propose[#]procedure[#]compute[#]asymptotical[#]detection[#]boundary[#]maximal[#]testing[#]risk[#]tends[#]prove[#]asymptotically[#]sharp[#]minimax[#]additional[#]constraints[#]Relations[#]problems[#]discussed[#]adapts[#]unknown[#]n[#]within[#]given[#]set[#]adaptive[#]rates[#]The[#]implementation[#]synthetic[#]data[#]shows[#]excellent[#]behavior[#]sparse[#]necessarily[#]squared[#]matrices[#]extend[#]results[#]different[#]directions[#]first[#]Gaussian[#]variance[#]next[#]random[#]variables[#]distribution[#]exponential[#]family[#]finally[#]consider[#]problem[#]function[#]noisy[#]observations[#]integrals[#]lines[#]study[#]rate[#]asymptotics[#]error[#]probabilities[#]setup[#]By[#]construction[#]derived[#]tests[#]also[#]construct[#]rather[#]simple[#]structure" />
        </attvalues>
      </node>
      <node id="Masayuki Kumon" label="Masayuki Kumon">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]geometrical[#]method[#]analyzing[#]sequential[#]estimating[#]procedures[#]It[#]based[#]design[#]principle[#]efficient[#]estimation[#]provided[#]Okamoto[#]Amari[#]Takeuchi[#]By[#]introducing[#]dual[#]conformal[#]curvature[#]quantity[#]clarify[#]conditions[#]covariance[#]minimization[#]estimators[#]These[#]elabolated[#]multidimensional[#]curved[#]exponential[#]family[#]The[#]theoretical[#]results[#]numerically[#]examined[#]using[#]typical[#]statistical[#]models[#]von[#]hyperboloid" />
        </attvalues>
      </node>
      <node id="Akimichi Takemura" label="Akimichi Takemura">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]geometrical[#]method[#]analyzing[#]sequential[#]estimating[#]procedures[#]It[#]based[#]design[#]principle[#]efficient[#]estimation[#]provided[#]Okamoto[#]Amari[#]Takeuchi[#]By[#]introducing[#]dual[#]conformal[#]curvature[#]quantity[#]clarify[#]conditions[#]covariance[#]minimization[#]estimators[#]These[#]elabolated[#]multidimensional[#]curved[#]exponential[#]family[#]The[#]theoretical[#]results[#]numerically[#]examined[#]using[#]typical[#]statistical[#]models[#]von[#]hyperboloid[#]In[#]paper[#]give[#]review[#]imsets[#]introduced[#]Studeny[#]geometric[#]point[#]view[#]Elementary[#]span[#]polyhedral[#]cone[#]supermodular[#]functions[#]basic[#]facts[#]structure[#]cones[#]Then[#]derive[#]new[#]following[#]topics[#]extreme[#]rays[#]standardized[#]ii[#]faces[#]iii[#]small[#]relations[#]among[#]elementary[#]iv[#]computational[#]Markov[#]basis[#]toric[#]ideal[#]defined[#]multiarmed[#]bandit[#]problem[#]gambler[#]chooses[#]arm[#]slot[#]machine[#]pull[#]considering[#]tradeoff[#]exploration[#]exploitation[#]study[#]stochastic[#]reward[#]distribution[#]supported[#]known[#]bounded[#]interval[#]For[#]model[#]policies[#]take[#]account[#]empirical[#]variances[#]second[#]moments[#]arms[#]perform[#]effectively[#]generalize[#]idea[#]propose[#]policy[#]exploits[#]first[#]arbitrary[#]fixed[#]advance[#]asymptotic[#]upper[#]bound[#]regret[#]approaches[#]Burnetas[#]Katehakis[#]increases[#]choosing[#]appropriate[#]proposed[#]realizes[#]complexity[#]expected[#]lower[#]Graver[#]incidence[#]matrix[#]complete[#]bipartite[#]graph[#]size[#]Our[#]result[#]generalization[#]Berstein[#]Onn[#]graphs[#]r" />
        </attvalues>
      </node>
      <node id="Kei Takeuchi" label="Kei Takeuchi">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]geometrical[#]method[#]analyzing[#]sequential[#]estimating[#]procedures[#]It[#]based[#]design[#]principle[#]efficient[#]estimation[#]provided[#]Okamoto[#]Amari[#]Takeuchi[#]By[#]introducing[#]dual[#]conformal[#]curvature[#]quantity[#]clarify[#]conditions[#]covariance[#]minimization[#]estimators[#]These[#]elabolated[#]multidimensional[#]curved[#]exponential[#]family[#]The[#]theoretical[#]results[#]numerically[#]examined[#]using[#]typical[#]statistical[#]models[#]von[#]hyperboloid" />
        </attvalues>
      </node>
      <node id="Alain Celisse" label="Alain Celisse">
        <attvalues>
          <attvalue for="0" value="The[#]stochastic[#]block[#]model[#]SBM[#]probabilistic[#]signed[#]describe[#]heterogeneous[#]directed[#]undirected[#]graphs[#]In[#]paper[#]address[#]asymptotic[#]inference[#]use[#]likelihood[#]variational[#]approaches[#]identi[#]ability[#]proved[#]properties[#]mators[#]provided[#]particular[#]consistency[#]estimators[#]settled[#]best[#]knowledge[#]rst[#]result[#]type[#]random" />
        </attvalues>
      </node>
      <node id="J. -J. Daudin" label="J. -J. Daudin">
        <attvalues>
          <attvalue for="0" value="The[#]stochastic[#]block[#]model[#]SBM[#]probabilistic[#]signed[#]describe[#]heterogeneous[#]directed[#]undirected[#]graphs[#]In[#]paper[#]address[#]asymptotic[#]inference[#]use[#]likelihood[#]variational[#]approaches[#]identi[#]ability[#]proved[#]properties[#]mators[#]provided[#]particular[#]consistency[#]estimators[#]settled[#]best[#]knowledge[#]rst[#]result[#]type[#]random" />
        </attvalues>
      </node>
      <node id="Laurent Pierre" label="Laurent Pierre">
        <attvalues>
          <attvalue for="0" value="The[#]stochastic[#]block[#]model[#]SBM[#]probabilistic[#]signed[#]describe[#]heterogeneous[#]directed[#]undirected[#]graphs[#]In[#]paper[#]address[#]asymptotic[#]inference[#]use[#]likelihood[#]variational[#]approaches[#]identi[#]ability[#]proved[#]properties[#]mators[#]provided[#]particular[#]consistency[#]estimators[#]settled[#]best[#]knowledge[#]rst[#]result[#]type[#]random" />
        </attvalues>
      </node>
      <node id="Lajos Horvath" label="Lajos Horvath">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Piotr Kokoszka" label="Piotr Kokoszka">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]develops[#]framework[#]estimation[#]functional[#]mean[#]principal[#]components[#]functions[#]form[#]random[#]field[#]More[#]specifically[#]data[#]study[#]consist[#]curves[#]X[#]T[#]observed[#]spatial[#]points[#]We[#]establish[#]conditions[#]sample[#]average[#]space[#]consistent[#]estimator[#]population[#]function[#]usual[#]empirical[#]covariance[#]operator[#]These[#]involve[#]interplay[#]assumptions[#]appropriately[#]defined[#]dependence[#]distribution[#]The[#]rates[#]convergence[#]may[#]samples[#]generally[#]depend[#]strength[#]quantified[#]distances[#]also[#]formulate[#]lack[#]consistency" />
        </attvalues>
      </node>
      <node id="Ron Reeder" label="Ron Reeder">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]quadratic[#]functional[#]regression[#]model[#]scalar[#]response[#]depends[#]predictor[#]common[#]linear[#]special[#]case[#]wish[#]test[#]significance[#]nonlinear[#]term[#]develop[#]testing[#]method[#]based[#]projecting[#]observations[#]onto[#]suitably[#]chosen[#]finite[#]dimensional[#]space[#]using[#]principal[#]component[#]analysis[#]The[#]asymptotic[#]behavior[#]procedure[#]established[#]A[#]simulation[#]study[#]shows[#]good[#]size[#]power[#]sample[#]sizes[#]apply[#]data[#]set[#]provided[#]Tecator[#]consists[#]absorbance[#]spectra[#]fat[#]content[#]meat" />
        </attvalues>
      </node>
      <node id="Höpfner Reinhard" label="Höpfner Reinhard">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yury A Kutoyants" label="Yury A Kutoyants">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="T. Yoshida" label="T. Yoshida">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="K. Naito" label="K. Naito">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Daniel J. McDonald" label="Daniel J. McDonald">
        <attvalues>
          <attvalue for="0" value="The[#]literature[#]statistical[#]learning[#]time[#]series[#]often[#]assumes[#]asymptotic[#]independence[#]mixing[#]process[#]These[#]assumptions[#]never[#]tested[#]methods[#]estimating[#]coefficients[#]data[#]Additionally[#]many[#]common[#]classes[#]processes[#]Markov[#]ARMA[#]etc[#]general[#]functional[#]forms[#]various[#]rates[#]known[#]specific[#]We[#]present[#]first[#]estimator[#]based[#]single[#]stationary[#]sample[#]path[#]show[#]risk[#]consistent[#]Since[#]depend[#]dependence[#]use[#]approximation[#]finite[#]memory[#]length[#]convergence[#]converges[#]true[#]coefficient[#]Our[#]constructed[#]using[#]histogram[#]density[#]estimates[#]Allowing[#]asymptotics[#]bandwidth[#]well[#]dimension[#]prove[#]concentration[#]intermediate[#]step[#]Simulations[#]wherein[#]calculable[#]example[#]demonstrate[#]methodology" />
        </attvalues>
      </node>
      <node id="Cosma Rohilla Shalizi" label="Cosma Rohilla Shalizi">
        <attvalues>
          <attvalue for="0" value="The[#]literature[#]statistical[#]learning[#]time[#]series[#]often[#]assumes[#]asymptotic[#]independence[#]mixing[#]process[#]These[#]assumptions[#]never[#]tested[#]methods[#]estimating[#]coefficients[#]data[#]Additionally[#]many[#]common[#]classes[#]processes[#]Markov[#]ARMA[#]etc[#]general[#]functional[#]forms[#]various[#]rates[#]known[#]specific[#]We[#]present[#]first[#]estimator[#]based[#]single[#]stationary[#]sample[#]path[#]show[#]risk[#]consistent[#]Since[#]depend[#]dependence[#]use[#]approximation[#]finite[#]memory[#]length[#]convergence[#]converges[#]true[#]coefficient[#]Our[#]constructed[#]using[#]histogram[#]density[#]estimates[#]Allowing[#]asymptotics[#]bandwidth[#]well[#]dimension[#]prove[#]concentration[#]intermediate[#]step[#]Simulations[#]wherein[#]calculable[#]example[#]demonstrate[#]methodology" />
        </attvalues>
      </node>
      <node id="Mark Schervish" label="Mark Schervish">
        <attvalues>
          <attvalue for="0" value="The[#]literature[#]statistical[#]learning[#]time[#]series[#]often[#]assumes[#]asymptotic[#]independence[#]mixing[#]process[#]These[#]assumptions[#]never[#]tested[#]methods[#]estimating[#]coefficients[#]data[#]Additionally[#]many[#]common[#]classes[#]processes[#]Markov[#]ARMA[#]etc[#]general[#]functional[#]forms[#]various[#]rates[#]known[#]specific[#]We[#]present[#]first[#]estimator[#]based[#]single[#]stationary[#]sample[#]path[#]show[#]risk[#]consistent[#]Since[#]depend[#]dependence[#]use[#]approximation[#]finite[#]memory[#]length[#]convergence[#]converges[#]true[#]coefficient[#]Our[#]constructed[#]using[#]histogram[#]density[#]estimates[#]Allowing[#]asymptotics[#]bandwidth[#]well[#]dimension[#]prove[#]concentration[#]intermediate[#]step[#]Simulations[#]wherein[#]calculable[#]example[#]demonstrate[#]methodology" />
        </attvalues>
      </node>
      <node id="Min Qian" label="Min Qian">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Susan A. Murphy" label="Susan A. Murphy">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yuan Liao" label="Yuan Liao">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]addresses[#]estimation[#]nonparametric[#]conditional[#]moment[#]restricted[#]model[#]involves[#]parameter[#]We[#]estimate[#]way[#]based[#]limited[#]information[#]likelihood[#]investigate[#]impact[#]three[#]types[#]priors[#]posterior[#]consistency[#]truncated[#]prior[#]supported[#]bounded[#]set[#]ii[#]thin[#]tail[#]outside[#]growing[#]iii[#]normal[#]nonshrinking[#]variance[#]In[#]addition[#]allowed[#]partially[#]identified[#]frequentist[#]sense[#]space[#]need[#]compact[#]The[#]regularized[#]using[#]slowly[#]sieve[#]dimension[#]shown[#]converges[#]small[#]neighborhood[#]region[#]apply[#]results[#]instrumental[#]regression[#]Finally[#]random[#]studied[#]deals[#]covariance[#]sparsity[#]structure[#]eigenvalues[#]By[#]assuming[#]sparse[#]error[#]matrix[#]approximate[#]factor[#]allow[#]presence[#]correlation[#]even[#]taking[#]common[#]unobservable[#]factors[#]introduce[#]Principal[#]Orthogonal[#]complEment[#]Thresholding[#]POET[#]method[#]explore[#]estimator[#]includes[#]sample[#]Fan[#]Lv[#]thresholding[#]Bickel[#]Levina[#]adaptive[#]Cai[#]Liu[#]specific[#]examples[#]provide[#]mathematical[#]insights[#]analysis[#]approximately[#]principal[#]component[#]data[#]rates[#]convergence[#]residual[#]various[#]norms[#]It[#]estimating[#]unknown[#]vanishes[#]dimensionality[#]increases[#]uniform[#]unobserved[#]loadings[#]derived[#]asymptotic[#]also[#]verified[#]extensive[#]simulation[#]studies[#]real[#]application[#]portfolio[#]allocation[#]presented" />
        </attvalues>
      </node>
      <node id="Wenxin Jiang" label="Wenxin Jiang">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]addresses[#]estimation[#]nonparametric[#]conditional[#]moment[#]restricted[#]model[#]involves[#]parameter[#]We[#]estimate[#]way[#]based[#]limited[#]information[#]likelihood[#]investigate[#]impact[#]three[#]types[#]priors[#]posterior[#]consistency[#]truncated[#]prior[#]supported[#]bounded[#]set[#]ii[#]thin[#]tail[#]outside[#]growing[#]iii[#]normal[#]nonshrinking[#]variance[#]In[#]addition[#]allowed[#]partially[#]identified[#]frequentist[#]sense[#]space[#]need[#]compact[#]The[#]regularized[#]using[#]slowly[#]sieve[#]dimension[#]shown[#]converges[#]small[#]neighborhood[#]region[#]apply[#]results[#]instrumental[#]regression[#]Finally[#]random[#]studied" />
        </attvalues>
      </node>
      <node id="Carenne Ludeña" label="Carenne Ludeña">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]prove[#]central[#]limit[#]theorems[#]bias[#]reduced[#]estimators[#]structure[#]function[#]several[#]multifractal[#]processes[#]namely[#]mutiplicative[#]cascades[#]random[#]measures[#]walk[#]fractional[#]defined[#]n[#]Ann[#]Appl[#]Probab[#]Previous[#]functions[#]considered[#]literature[#]severely[#]biased[#]logarithmic[#]rate[#]convergence[#]whereas[#]polynomial" />
        </attvalues>
      </node>
      <node id="Philippe Soulier" label="Philippe Soulier">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]prove[#]central[#]limit[#]theorems[#]bias[#]reduced[#]estimators[#]structure[#]function[#]several[#]multifractal[#]processes[#]namely[#]mutiplicative[#]cascades[#]random[#]measures[#]walk[#]fractional[#]defined[#]n[#]Ann[#]Appl[#]Probab[#]Previous[#]functions[#]considered[#]literature[#]severely[#]biased[#]logarithmic[#]rate[#]convergence[#]whereas[#]polynomial[#]We[#]consider[#]models[#]asset[#]prices[#]continuous[#]time[#]driven[#]point[#]bivariate[#]model[#]admits[#]cointegration[#]allow[#]deformations[#]account[#]effects[#]intraday[#]seasonal[#]patterns[#]volatility[#]periods[#]may[#]different[#]two[#]assets[#]also[#]asymmetries[#]leverage[#]obtain[#]asymptotic[#]distribution[#]process[#]ordinary[#]estimator[#]cointegrating[#]parameter[#]based[#]data[#]sampled[#]discretization[#]calendar[#]case[#]weak[#]For[#]tapered[#]Long[#]Memory[#]Stochastic[#]LMSV[#]capture[#]standardized[#]features[#]financial[#]uncorrelated[#]squares[#]absolute[#]values[#]highly[#]dependent[#]heavy[#]tails[#]EGARCH[#]related[#]introduced[#]negative[#]dependence[#]previous[#]returns[#]future[#]Limit[#]partial[#]sums[#]sample[#]variance[#]covariances[#]basic[#]tools[#]investigate[#]presence[#]long[#]memory[#]consequences[#]extend[#]existing[#]behaviour[#]stochastic[#]infinite[#]results[#]entirely[#]new[#]Depending[#]nterplay[#]tail[#]intensity[#]wo[#]types[#]rates[#]limiting[#]distributions[#]arise[#]articular[#]show[#]crucial[#]difference[#]Volatility[#]possible[#]study[#]conditional[#]events[#]given[#]present[#]past[#]event[#]extreme[#]level[#]tends[#]infinity[#]Even[#]though[#]extremes[#]asymptotically[#]independent[#]sense[#]value[#]theory[#]differ[#]introduce[#]properties[#]If[#]centered[#]depend[#]Hurst[#]index" />
        </attvalues>
      </node>
      <node id="Delphine Cassart" label="Delphine Cassart">
        <attvalues>
          <attvalue for="0" value="The[#]objective[#]paper[#]provide[#]problem[#]univariate[#]symmetry[#]respect[#]specified[#]unspecified[#]location[#]concept[#]optimality[#]construct[#]tests[#]achieving[#]This[#]requires[#]embedding[#]adequate[#]families[#]asymmetric[#]local[#]alternatives[#]We[#]considering[#]generalizations[#]classical[#]Edgeworth[#]expansions[#]indexed[#]measure[#]skewness[#]scale[#]play[#]roles[#]diagonality[#]corresponding[#]information[#]matrices[#]ii[#]based[#]Pearson[#]Fisher[#]coefficient[#]optimal[#]vicinity[#]Gaussian[#]densities" />
        </attvalues>
      </node>
      <node id="Marc Hallin" label="Marc Hallin">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]present[#]alternative[#]method[#]spectral[#]analysis[#]univariate[#]strictly[#]stationary[#]time[#]series[#]Z[#]We[#]define[#]new[#]spectrum[#]Fourier[#]transform[#]differences[#]copulas[#]pairs[#]independence[#]copula[#]This[#]object[#]called[#]density[#]kernel[#]allows[#]separate[#]marginal[#]serial[#]aspects[#]show[#]closely[#]related[#]concept[#]quantile[#]regression[#]Like[#]provides[#]much[#]information[#]conditional[#]distributions[#]classical[#]models[#]kernels[#]informative[#]traditional[#]densities[#]obtained[#]autocovariances[#]particular[#]population[#]versions[#]provide[#]asymptotically[#]sample[#]complete[#]description[#]Moreover[#]inherit[#]robustness[#]properties[#]require[#]distributional[#]assumptions[#]existence[#]finite[#]moments[#]order[#]estimate[#]introduce[#]Laplace[#]periodograms[#]calculated[#]bilinear[#]forms[#]weighted[#]ranks[#]observed[#]onto[#]harmonic[#]model[#]establish[#]asymptotic[#]distribution[#]consistency[#]adequately[#]smoothed[#]The[#]methodology[#]potential[#]applications[#]briefly[#]investigated[#]simulations[#]short[#]empirical[#]example[#]objective[#]problem[#]symmetry[#]respect[#]specified[#]unspecified[#]location[#]optimality[#]construct[#]tests[#]achieving[#]requires[#]embedding[#]adequate[#]families[#]asymmetric[#]local[#]alternatives[#]considering[#]generalizations[#]Edgeworth[#]expansions[#]indexed[#]measure[#]skewness[#]scale[#]play[#]roles[#]diagonality[#]corresponding[#]matrices[#]ii[#]based[#]Pearson[#]Fisher[#]coefficient[#]optimal[#]vicinity[#]Gaussian" />
        </attvalues>
      </node>
      <node id="Davy Paindaveine" label="Davy Paindaveine">
        <attvalues>
          <attvalue for="0" value="The[#]objective[#]paper[#]provide[#]problem[#]univariate[#]symmetry[#]respect[#]specified[#]unspecified[#]location[#]concept[#]optimality[#]construct[#]tests[#]achieving[#]This[#]requires[#]embedding[#]adequate[#]families[#]asymmetric[#]local[#]alternatives[#]We[#]considering[#]generalizations[#]classical[#]Edgeworth[#]expansions[#]indexed[#]measure[#]skewness[#]scale[#]play[#]roles[#]diagonality[#]corresponding[#]information[#]matrices[#]ii[#]based[#]Pearson[#]Fisher[#]coefficient[#]optimal[#]vicinity[#]Gaussian[#]densities" />
        </attvalues>
      </node>
      <node id="Valentine Genon-Catalot" label="Valentine Genon-Catalot">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]nonparametric[#]estimation[#]e[#]vy[#]density[#]processes[#]without[#]Brownian[#]component[#]For[#]consider[#]n[#]discrete[#]time[#]observations[#]step[#]The[#]asymptotic[#]framework[#]tends[#]infinity[#]zero[#]We[#]use[#]Fourier[#]approach[#]construct[#]adaptive[#]estimator[#]provide[#]bound[#]global[#]L[#]Estimators[#]drift[#]variance[#]Gaussian[#]also[#]studied[#]discuss[#]rates[#]convergence[#]give[#]examples[#]simulation[#]results[#]fitting" />
        </attvalues>
      </node>
      <node id="Marius Hofert" label="Marius Hofert">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Martin Mächler" label="Martin Mächler">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexander J. McNeil" label="Alexander J. McNeil">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Sílvia R. C. Lopes" label="Sílvia R. C. Lopes">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Guilherme Pumi" label="Guilherme Pumi">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jian Huang" label="Jian Huang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Shuangge Ma" label="Shuangge Ma">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Hongzhe Li" label="Hongzhe Li">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Cun-Hui Zhang" label="Cun-Hui Zhang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Damien Passemier" label="Damien Passemier">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jian-Feng Yao" label="Jian-Feng Yao">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Guang Cheng" label="Guang Cheng">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]efficient[#]estimation[#]semiparametric[#]additive[#]transformation[#]model[#]current[#]status[#]data[#]A[#]wide[#]range[#]survival[#]models[#]econometric[#]incorporated[#]general[#]framework[#]apply[#]approach[#]simultaneously[#]estimate[#]linear[#]regression[#]vector[#]nondecreasing[#]function[#]set[#]nonparametric[#]functions[#]show[#]parametric[#]presence[#]multiple[#]nuisance[#]An[#]explicit[#]consistent[#]asymptotic[#]variance[#]also[#]provided[#]All[#]estimates[#]smooth[#]shown[#]uniformly[#]faster[#]cubic[#]rate[#]convergence[#]Interestingly[#]observe[#]interfere[#]phenomenon[#]rates[#]estimators[#]slowed[#]equal[#]slowest[#]one[#]The[#]constrained[#]optimization[#]required[#]implementation[#]Numerical[#]results[#]used[#]illustrate[#]finite[#]sample[#]performance[#]proposed" />
        </attvalues>
      </node>
      <node id="Xiao Wang" label="Xiao Wang">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]efficient[#]estimation[#]semiparametric[#]additive[#]transformation[#]model[#]current[#]status[#]data[#]A[#]wide[#]range[#]survival[#]models[#]econometric[#]incorporated[#]general[#]framework[#]apply[#]approach[#]simultaneously[#]estimate[#]linear[#]regression[#]vector[#]nondecreasing[#]function[#]set[#]nonparametric[#]functions[#]show[#]parametric[#]presence[#]multiple[#]nuisance[#]An[#]explicit[#]consistent[#]asymptotic[#]variance[#]also[#]provided[#]All[#]estimates[#]smooth[#]shown[#]uniformly[#]faster[#]cubic[#]rate[#]convergence[#]Interestingly[#]observe[#]interfere[#]phenomenon[#]rates[#]estimators[#]slowed[#]equal[#]slowest[#]one[#]The[#]constrained[#]optimization[#]required[#]implementation[#]Numerical[#]results[#]used[#]illustrate[#]finite[#]sample[#]performance[#]proposed" />
        </attvalues>
      </node>
      <node id="Runlong Tang" label="Runlong Tang">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]procedure[#]TSP[#]estimating[#]inverse[#]regression[#]function[#]given[#]point[#]isotonic[#]used[#]stage[#]one[#]obtain[#]initial[#]estimate[#]local[#]linear[#]approximation[#]vicinity[#]two[#]establish[#]convergence[#]rate[#]attain[#]parametric[#]Furthermore[#]bootstrapped[#]variant[#]BTSP[#]introduced[#]consistency[#]properties[#]studied[#]This[#]manages[#]overcome[#]slow[#]speed[#]distribution[#]estimation[#]derivative[#]unknown[#]target[#]quantity[#]Finally[#]finite[#]sample[#]performance[#]simulations[#]method[#]illustrated[#]data[#]set" />
        </attvalues>
      </node>
      <node id="Moulinath Banerjee" label="Moulinath Banerjee">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]procedure[#]TSP[#]estimating[#]inverse[#]regression[#]function[#]given[#]point[#]isotonic[#]used[#]stage[#]one[#]obtain[#]initial[#]estimate[#]local[#]linear[#]approximation[#]vicinity[#]two[#]establish[#]convergence[#]rate[#]attain[#]parametric[#]Furthermore[#]bootstrapped[#]variant[#]BTSP[#]introduced[#]consistency[#]properties[#]studied[#]This[#]manages[#]overcome[#]slow[#]speed[#]distribution[#]estimation[#]derivative[#]unknown[#]target[#]quantity[#]Finally[#]finite[#]sample[#]performance[#]simulations[#]method[#]illustrated[#]data[#]set" />
        </attvalues>
      </node>
      <node id="George Michailidis" label="George Michailidis">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]procedure[#]TSP[#]estimating[#]inverse[#]regression[#]function[#]given[#]point[#]isotonic[#]used[#]stage[#]one[#]obtain[#]initial[#]estimate[#]local[#]linear[#]approximation[#]vicinity[#]two[#]establish[#]convergence[#]rate[#]attain[#]parametric[#]Furthermore[#]bootstrapped[#]variant[#]BTSP[#]introduced[#]consistency[#]properties[#]studied[#]This[#]manages[#]overcome[#]slow[#]speed[#]distribution[#]estimation[#]derivative[#]unknown[#]target[#]quantity[#]Finally[#]finite[#]sample[#]performance[#]simulations[#]method[#]illustrated[#]data[#]set" />
        </attvalues>
      </node>
      <node id="Jérémie Bigot" label="Jérémie Bigot">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]considers[#]problem[#]adaptive[#]estimation[#]intensity[#]function[#]observation[#]n[#]independent[#]Poisson[#]processes[#]common[#]randomly[#]shifted[#]observed[#]trajectory[#]We[#]show[#]estimating[#]deconvolution[#]density[#]random[#]shifts[#]plays[#]role[#]convolution[#]operator[#]In[#]asymptotic[#]setting[#]number[#]trajectories[#]tends[#]infinity[#]derive[#]upper[#]lower[#]bounds[#]minimax[#]quadratic[#]risk[#]Besov[#]balls[#]thresholding[#]Meyer[#]wavelet[#]basis[#]used[#]estimator[#]The[#]proposed[#]shown[#]achieve[#]rate[#]convergence[#]depends[#]smoothness[#]makes[#]connection[#]classical[#]nonparametric[#]statistics[#]mean[#]observations" />
        </attvalues>
      </node>
      <node id="Sébastien Gadat" label="Sébastien Gadat">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]considers[#]problem[#]adaptive[#]estimation[#]intensity[#]function[#]observation[#]n[#]independent[#]Poisson[#]processes[#]common[#]randomly[#]shifted[#]observed[#]trajectory[#]We[#]show[#]estimating[#]deconvolution[#]density[#]random[#]shifts[#]plays[#]role[#]convolution[#]operator[#]In[#]asymptotic[#]setting[#]number[#]trajectories[#]tends[#]infinity[#]derive[#]upper[#]lower[#]bounds[#]minimax[#]quadratic[#]risk[#]Besov[#]balls[#]thresholding[#]Meyer[#]wavelet[#]basis[#]used[#]estimator[#]The[#]proposed[#]shown[#]achieve[#]rate[#]convergence[#]depends[#]smoothness[#]makes[#]connection[#]classical[#]nonparametric[#]statistics[#]mean[#]observations" />
        </attvalues>
      </node>
      <node id="Thierry Klein" label="Thierry Klein">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]considers[#]problem[#]adaptive[#]estimation[#]intensity[#]function[#]observation[#]n[#]independent[#]Poisson[#]processes[#]common[#]randomly[#]shifted[#]observed[#]trajectory[#]We[#]show[#]estimating[#]deconvolution[#]density[#]random[#]shifts[#]plays[#]role[#]convolution[#]operator[#]In[#]asymptotic[#]setting[#]number[#]trajectories[#]tends[#]infinity[#]derive[#]upper[#]lower[#]bounds[#]minimax[#]quadratic[#]risk[#]Besov[#]balls[#]thresholding[#]Meyer[#]wavelet[#]basis[#]used[#]estimator[#]The[#]proposed[#]shown[#]achieve[#]rate[#]convergence[#]depends[#]smoothness[#]makes[#]connection[#]classical[#]nonparametric[#]statistics[#]mean[#]observations" />
        </attvalues>
      </node>
      <node id="Clément Marteau" label="Clément Marteau">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]considers[#]problem[#]adaptive[#]estimation[#]intensity[#]function[#]observation[#]n[#]independent[#]Poisson[#]processes[#]common[#]randomly[#]shifted[#]observed[#]trajectory[#]We[#]show[#]estimating[#]deconvolution[#]density[#]random[#]shifts[#]plays[#]role[#]convolution[#]operator[#]In[#]asymptotic[#]setting[#]number[#]trajectories[#]tends[#]infinity[#]derive[#]upper[#]lower[#]bounds[#]minimax[#]quadratic[#]risk[#]Besov[#]balls[#]thresholding[#]Meyer[#]wavelet[#]basis[#]used[#]estimator[#]The[#]proposed[#]shown[#]achieve[#]rate[#]convergence[#]depends[#]smoothness[#]makes[#]connection[#]classical[#]nonparametric[#]statistics[#]mean[#]observations" />
        </attvalues>
      </node>
      <node id="Denys Pommeret" label="Denys Pommeret">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mohamed Boutahar" label="Mohamed Boutahar">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Badih Ghattas" label="Badih Ghattas">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Rudolf Schenk" label="Rudolf Schenk">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]estimation[#]value[#]linear[#]functional[#]slope[#]parameter[#]regression[#]scalar[#]responses[#]modeled[#]dependence[#]random[#]functions[#]In[#]Johannes[#]Schenk[#]shown[#]estimator[#]based[#]dimension[#]reduction[#]additional[#]thresholding[#]attain[#]minimax[#]optimal[#]rates[#]convergence[#]constant[#]However[#]procedure[#]requires[#]choice[#]tuning[#]regard[#]certain[#]characteristics[#]function[#]covariance[#]operator[#]associated[#]regressor[#]As[#]unknown[#]practice[#]investigate[#]fully[#]combination[#]model[#]selection[#]Lepski[#]method[#]inspired[#]recent[#]work[#]Goldenshluger[#]The[#]selected[#]minimizer[#]stochastic[#]penalized[#]contrast[#]imitating[#]among[#]collection[#]admissible[#]values[#]show[#]adaptive[#]attains[#]lower[#]bound[#]risk[#]logarithmic[#]factor[#]wide[#]range[#]classes[#]operators[#]particular[#]theory[#]covers[#]well[#]local[#]averages" />
        </attvalues>
      </node>
      <node id="Kyusang Yu" label="Kyusang Yu">
        <attvalues>
          <attvalue for="0" value="It[#]widely[#]admitted[#]structured[#]nonparametric[#]modeling[#]circumvents[#]curse[#]dimensionality[#]important[#]estimation[#]In[#]paper[#]show[#]holds[#]We[#]argue[#]parametric[#]component[#]model[#]improved[#]essentially[#]structure[#]put[#]part[#]illustrate[#]partially[#]linear[#]investigate[#]efficiency[#]gains[#]additive[#]present[#]Fisher[#]information[#]bound[#]estimating[#]provide[#]efficient[#]estimators[#]use[#]smooth[#]backfitting[#]technique[#]deal[#]also[#]finite[#]sample[#]performances[#]proposed[#]analyze[#]Boston[#]housing[#]data[#]illustration" />
        </attvalues>
      </node>
      <node id="Enno Mammen" label="Enno Mammen">
        <attvalues>
          <attvalue for="0" value="It[#]widely[#]admitted[#]structured[#]nonparametric[#]modeling[#]circumvents[#]curse[#]dimensionality[#]important[#]estimation[#]In[#]paper[#]show[#]holds[#]We[#]argue[#]parametric[#]component[#]model[#]improved[#]essentially[#]structure[#]put[#]part[#]illustrate[#]partially[#]linear[#]investigate[#]efficiency[#]gains[#]additive[#]present[#]Fisher[#]information[#]bound[#]estimating[#]provide[#]efficient[#]estimators[#]use[#]smooth[#]backfitting[#]technique[#]deal[#]also[#]finite[#]sample[#]performances[#]proposed[#]analyze[#]Boston[#]housing[#]data[#]illustration[#]general[#]principle[#]regression[#]function[#]nonparametrically[#]allowing[#]wide[#]variety[#]filtering[#]example[#]repeated[#]left[#]truncation[#]right[#]censoring[#]Both[#]mean[#]median[#]cases[#]considered[#]The[#]method[#]works[#]first[#]conditional[#]hazard[#]survivor[#]integrating[#]methods[#]take[#]account[#]independent[#]errors[#]improve[#]performance[#]true[#]establish[#]pointwise[#]asymptotic[#]normality" />
        </attvalues>
      </node>
      <node id="Byeong U. Park" label="Byeong U. Park">
        <attvalues>
          <attvalue for="0" value="It[#]widely[#]admitted[#]structured[#]nonparametric[#]modeling[#]circumvents[#]curse[#]dimensionality[#]important[#]estimation[#]In[#]paper[#]show[#]holds[#]We[#]argue[#]parametric[#]component[#]model[#]improved[#]essentially[#]structure[#]put[#]part[#]illustrate[#]partially[#]linear[#]investigate[#]efficiency[#]gains[#]additive[#]present[#]Fisher[#]information[#]bound[#]estimating[#]provide[#]efficient[#]estimators[#]use[#]smooth[#]backfitting[#]technique[#]deal[#]also[#]finite[#]sample[#]performances[#]proposed[#]analyze[#]Boston[#]housing[#]data[#]illustration" />
        </attvalues>
      </node>
      <node id="Naohiro Kato" label="Naohiro Kato">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Satoshi Kuriki" label="Satoshi Kuriki">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Stanislav Volgushev" label="Stanislav Volgushev">
        <attvalues>
          <attvalue for="0" value="We[#]propose[#]new[#]class[#]estimators[#]Pickands[#]dependence[#]function[#]based[#]concept[#]minimum[#]distance[#]estimation[#]An[#]explicit[#]integral[#]representation[#]minimizes[#]weighted[#]logarithm[#]copula[#]C[#]functions[#]form[#]A[#]derived[#]If[#]unknown[#]coincides[#]Moreover[#]even[#]case[#]always[#]satisfies[#]boundary[#]conditions[#]The[#]obtained[#]replacing[#]empirical[#]counterpart[#]weak[#]convergence[#]corresponding[#]process[#]shown[#]comparison[#]commonly[#]used[#]performed[#]theoretical[#]point[#]view[#]means[#]simulation[#]study[#]Our[#]asymptotic[#]numerical[#]results[#]indicate[#]outperform[#]recently[#]proposed[#]Genest[#]Segers[#]Ann[#]Statist[#]As[#]obtain[#]simple[#]test[#]hypothesis[#]consistent[#]positive[#]quadrant[#]dependent[#]alternatives[#]satisfying[#]differentiability[#]assumptions[#]first[#]order[#]In[#]paper[#]present[#]alternative[#]method[#]spectral[#]analysis[#]univariate[#]strictly[#]stationary[#]time[#]series[#]Z[#]define[#]spectrum[#]Fourier[#]transform[#]differences[#]copulas[#]pairs[#]independence[#]This[#]object[#]called[#]density[#]kernel[#]allows[#]separate[#]marginal[#]serial[#]aspects[#]show[#]closely[#]related[#]quantile[#]regression[#]Like[#]provides[#]much[#]information[#]conditional[#]distributions[#]classical[#]models[#]kernels[#]informative[#]traditional[#]densities[#]autocovariances[#]particular[#]population[#]versions[#]provide[#]asymptotically[#]sample[#]complete[#]description[#]inherit[#]robustness[#]properties[#]require[#]distributional[#]existence[#]finite[#]moments[#]estimate[#]introduce[#]Laplace[#]periodograms[#]calculated[#]bilinear[#]forms[#]ranks[#]observed[#]onto[#]harmonic[#]model[#]establish[#]distribution[#]consistency[#]adequately[#]smoothed[#]methodology[#]potential[#]applications[#]briefly[#]investigated[#]simulations[#]short[#]example" />
        </attvalues>
      </node>
      <node id="Giorgio Dall'Aglio" label="Giorgio Dall'Aglio">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Elisabetta Bona" label="Elisabetta Bona">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Chunming Zhang" label="Chunming Zhang">
        <attvalues>
          <attvalue for="0" value="The[#]multiple[#]testing[#]procedure[#]plays[#]important[#]role[#]detecting[#]presence[#]spatial[#]signals[#]imaging[#]data[#]Typically[#]sparse[#]clustered[#]This[#]paper[#]provides[#]empirical[#]evidence[#]range[#]commonly[#]used[#]control[#]levels[#]conventional[#]FDR[#]lack[#]ability[#]detect[#]statistical[#]significance[#]even[#]p[#]true[#]null[#]hypotheses[#]independent[#]uniformly[#]distributed[#]generally[#]ignoring[#]neighboring[#]information[#]spatially[#]structured[#]tend[#]diminish[#]detection[#]effectiveness[#]first[#]introduces[#]scalar[#]quantity[#]characterize[#]extent[#]identification[#]phenomenon[#]LIP[#]occurs[#]Second[#]propose[#]new[#]comparison[#]called[#]accommodate[#]via[#]local[#]aggregation[#]Theoretical[#]properties[#]investigated[#]weak[#]dependence[#]It[#]shown[#]alleviates[#]thus[#]substantially[#]facilitating[#]selection[#]stringent[#]Simulation[#]evaluations[#]indicate[#]improves[#]sensitivity[#]little[#]loss[#]specificity[#]computational[#]simplicity[#]illustrated[#]real[#]brain[#]fMRI[#]dataset" />
        </attvalues>
      </node>
      <node id="Jianqing Fan" label="Jianqing Fan">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]deals[#]estimation[#]covariance[#]conditional[#]sparsity[#]structure[#]eigenvalues[#]By[#]assuming[#]sparse[#]error[#]matrix[#]approximate[#]factor[#]model[#]allow[#]presence[#]correlation[#]even[#]taking[#]common[#]unobservable[#]factors[#]We[#]introduce[#]Principal[#]Orthogonal[#]complEment[#]Thresholding[#]POET[#]method[#]explore[#]The[#]estimator[#]includes[#]sample[#]Fan[#]Lv[#]thresholding[#]Bickel[#]Levina[#]adaptive[#]Cai[#]Liu[#]specific[#]examples[#]provide[#]mathematical[#]insights[#]analysis[#]approximately[#]principal[#]component[#]data[#]rates[#]convergence[#]residual[#]studied[#]various[#]norms[#]It[#]shown[#]impact[#]estimating[#]unknown[#]vanishes[#]dimensionality[#]increases[#]uniform[#]unobserved[#]loadings[#]derived[#]asymptotic[#]results[#]also[#]verified[#]extensive[#]simulation[#]studies[#]Finally[#]real[#]application[#]portfolio[#]allocation[#]presented[#]Functional[#]linear[#]regression[#]aims[#]relations[#]include[#]functional[#]predictor[#]analog[#]parameter[#]vector[#]conventional[#]multivariate[#]models[#]function[#]one[#]two[#]arguments[#]If[#]addition[#]scalar[#]predictors[#]often[#]case[#]applications[#]longitudinal[#]question[#]arises[#]incorporate[#]study[#]approach[#]covariates[#]modeled[#]additional[#]extension[#]analogous[#]shares[#]advantages[#]increased[#]flexibility[#]however[#]details[#]challenging[#]Our[#]methodology[#]combines[#]smoothing[#]methods[#]regularization[#]truncation[#]finite[#]number[#]components[#]A[#]practical[#]version[#]developed[#]perform[#]better[#]investigate[#]properties[#]establish[#]consistency[#]multiple[#]testing[#]procedure[#]plays[#]important[#]role[#]detecting[#]spatial[#]signals[#]imaging[#]Typically[#]clustered[#]provides[#]empirical[#]evidence[#]range[#]commonly[#]used[#]control[#]levels[#]FDR[#]lack[#]ability[#]detect[#]statistical[#]significance[#]p[#]true[#]null[#]hypotheses[#]independent[#]uniformly[#]distributed[#]generally[#]ignoring[#]neighboring[#]information[#]spatially[#]structured[#]tend[#]diminish[#]detection[#]effectiveness[#]first[#]introduces[#]quantity[#]characterize[#]extent[#]identification[#]phenomenon[#]LIP[#]occurs[#]Second[#]propose[#]new[#]comparison[#]called[#]accommodate[#]via[#]local[#]aggregation[#]Theoretical[#]investigated[#]weak[#]dependence[#]alleviates[#]thus[#]substantially[#]facilitating[#]selection[#]stringent[#]Simulation[#]evaluations[#]indicate[#]improves[#]sensitivity[#]little[#]loss[#]specificity[#]computational[#]simplicity[#]illustrated[#]brain[#]fMRI[#]dataset" />
        </attvalues>
      </node>
      <node id="Tao Yu" label="Tao Yu">
        <attvalues>
          <attvalue for="0" value="The[#]multiple[#]testing[#]procedure[#]plays[#]important[#]role[#]detecting[#]presence[#]spatial[#]signals[#]imaging[#]data[#]Typically[#]sparse[#]clustered[#]This[#]paper[#]provides[#]empirical[#]evidence[#]range[#]commonly[#]used[#]control[#]levels[#]conventional[#]FDR[#]lack[#]ability[#]detect[#]statistical[#]significance[#]even[#]p[#]true[#]null[#]hypotheses[#]independent[#]uniformly[#]distributed[#]generally[#]ignoring[#]neighboring[#]information[#]spatially[#]structured[#]tend[#]diminish[#]detection[#]effectiveness[#]first[#]introduces[#]scalar[#]quantity[#]characterize[#]extent[#]identification[#]phenomenon[#]LIP[#]occurs[#]Second[#]propose[#]new[#]comparison[#]called[#]accommodate[#]via[#]local[#]aggregation[#]Theoretical[#]properties[#]investigated[#]weak[#]dependence[#]It[#]shown[#]alleviates[#]thus[#]substantially[#]facilitating[#]selection[#]stringent[#]Simulation[#]evaluations[#]indicate[#]improves[#]sensitivity[#]little[#]loss[#]specificity[#]computational[#]simplicity[#]illustrated[#]real[#]brain[#]fMRI[#]dataset" />
        </attvalues>
      </node>
      <node id="Rafal Kulik" label="Rafal Kulik">
        <attvalues>
          <attvalue for="0" value="Long[#]Memory[#]Stochastic[#]volatility[#]LMSV[#]models[#]capture[#]two[#]standardized[#]features[#]financial[#]data[#]uncorrelated[#]squares[#]absolute[#]values[#]highly[#]dependent[#]may[#]heavy[#]tails[#]EGARCH[#]related[#]introduced[#]model[#]leverage[#]negative[#]dependence[#]previous[#]returns[#]future[#]Limit[#]theorems[#]partial[#]sums[#]sample[#]variance[#]covariances[#]basic[#]tools[#]investigate[#]presence[#]long[#]memory[#]consequences[#]In[#]paper[#]extend[#]existing[#]literature[#]asymptotic[#]behaviour[#]stochastic[#]case[#]infinite[#]We[#]also[#]consider[#]results[#]entirely[#]new[#]Depending[#]nterplay[#]tail[#]intensity[#]wo[#]types[#]convergence[#]rates[#]limiting[#]distributions[#]arise[#]articular[#]show[#]whereas[#]crucial[#]difference[#]considered" />
        </attvalues>
      </node>
      <node id="Felix Abramovich" label="Felix Abramovich">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]problem[#]estimating[#]sparse[#]group[#]normal[#]mean[#]vectors[#]The[#]proposed[#]approach[#]based[#]penalized[#]likelihood[#]estimation[#]complexity[#]penalties[#]number[#]nonzero[#]numbers[#]significant[#]components[#]performed[#]computationally[#]fast[#]algorithm[#]resulting[#]estimators[#]developed[#]within[#]Bayesian[#]framework[#]viewed[#]MAP[#]establish[#]adaptive[#]minimaxity[#]wide[#]range[#]dense[#]settings[#]presented[#]short[#]simulation[#]study[#]demonstrates[#]efficiency[#]successfully[#]competes[#]recently[#]lasso[#]estimator[#]In[#]present[#]paper[#]Laplace[#]deconvolution[#]discrete[#]noisy[#]data[#]observed[#]interval[#]whose[#]length[#]may[#]increase[#]sample[#]size[#]Although[#]arises[#]variety[#]applications[#]best[#]knowledge[#]given[#]little[#]attention[#]statistical[#]community[#]Our[#]objective[#]fill[#]gap[#]provide[#]treatment[#]main[#]contribution[#]explicit[#]construction[#]asymptotically[#]minimax[#]sense[#]regularity[#]unknown[#]function[#]show[#]original[#]reduced[#]nonparametric[#]regression[#]derivatives[#]growing[#]Whereas[#]forms[#]remain[#]standard[#]choices[#]parameters[#]convergence[#]rates[#]expressed[#]terms[#]case[#]affected[#]asymptotic[#]growth[#]derive[#]kernel[#]interest[#]Sobolev[#]classes[#]illustrate[#]theory[#]examples[#]expressions[#]A[#]shows[#]addition[#]providing[#]optimality[#]observations[#]turns[#]infinity[#]good[#]performance[#]finite" />
        </attvalues>
      </node>
      <node id="Vadim Grinshtein" label="Vadim Grinshtein">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]problem[#]estimating[#]sparse[#]group[#]normal[#]mean[#]vectors[#]The[#]proposed[#]approach[#]based[#]penalized[#]likelihood[#]estimation[#]complexity[#]penalties[#]number[#]nonzero[#]numbers[#]significant[#]components[#]performed[#]computationally[#]fast[#]algorithm[#]resulting[#]estimators[#]developed[#]within[#]Bayesian[#]framework[#]viewed[#]MAP[#]establish[#]adaptive[#]minimaxity[#]wide[#]range[#]dense[#]settings[#]presented[#]short[#]simulation[#]study[#]demonstrates[#]efficiency[#]successfully[#]competes[#]recently[#]lasso[#]estimator" />
        </attvalues>
      </node>
      <node id="Jean-Marc Bardet" label="Jean-Marc Bardet">
        <attvalues>
          <attvalue for="0" value="A[#]general[#]moment[#]bound[#]sums[#]products[#]Gaussian[#]vector[#]functions[#]extending[#]Taqqu[#]Lemma[#]established[#]central[#]limit[#]theorem[#]triangular[#]arrays[#]nonlinear[#]functionals[#]multidimensional[#]sequences[#]proved[#]This[#]extends[#]previous[#]results[#]Breuer[#]Major[#]Arcones[#]others[#]derived[#]following[#]Nourdin[#]Peccati[#]Podolskij[#]Two[#]applications[#]discussed[#]The[#]first[#]one[#]refers[#]asymptotic[#]behavior[#]roughness[#]statistic[#]processes[#]second[#]satisfied[#]long[#]memory[#]locally[#]stationary[#]process" />
        </attvalues>
      </node>
      <node id="Donatas Surgailis" label="Donatas Surgailis">
        <attvalues>
          <attvalue for="0" value="A[#]general[#]moment[#]bound[#]sums[#]products[#]Gaussian[#]vector[#]functions[#]extending[#]Taqqu[#]Lemma[#]established[#]central[#]limit[#]theorem[#]triangular[#]arrays[#]nonlinear[#]functionals[#]multidimensional[#]sequences[#]proved[#]This[#]extends[#]previous[#]results[#]Breuer[#]Major[#]Arcones[#]others[#]derived[#]following[#]Nourdin[#]Peccati[#]Podolskij[#]Two[#]applications[#]discussed[#]The[#]first[#]one[#]refers[#]asymptotic[#]behavior[#]roughness[#]statistic[#]processes[#]second[#]satisfied[#]long[#]memory[#]locally[#]stationary[#]process" />
        </attvalues>
      </node>
      <node id="Piet Groeneboom" label="Piet Groeneboom">
        <attvalues>
          <attvalue for="0" value="Two[#]new[#]test[#]statistics[#]introduced[#]null[#]hypotheses[#]sampling[#]distribution[#]increasing[#]hazard[#]rate[#]specified[#]interval[#]These[#]empirical[#]distances[#]isotonic[#]estimates[#]use[#]monotonicity[#]constraint[#]either[#]function[#]cumulative[#]They[#]measure[#]excursions[#]respect[#]due[#]local[#]Asymptotic[#]normality[#]strictly[#]established[#]mild[#]conditions[#]This[#]done[#]first[#]approximating[#]global[#]distance[#]underlying[#]The[#]resulting[#]integral[#]treated[#]sum[#]increasingly[#]many[#]integrals[#]CLT[#]applied[#]behavior[#]determined[#]canonical[#]process[#]difference[#]stochastic[#]x[#]W[#]standard[#]Brownian[#]Motion[#]greatest[#]convex[#]minorant[#]We[#]consider[#]problem[#]estimating[#]joint[#]event[#]time[#]continuous[#]mark[#]variable[#]subject[#]censoring[#]case[#]observed[#]occurred[#]inspection[#]nonparametric[#]maximum[#]likelihood[#]estimator[#]model[#]known[#]inconsistent[#]study[#]two[#]alternative[#]smooth[#]estimators[#]based[#]explicit[#]inverse[#]expression[#]interest[#]terms[#]density[#]observable[#]vector[#]derive[#]pointwise[#]asymptotic[#]three[#]proposed[#]e[#]MLE[#]smoothed[#]using[#]smoothing[#]kernel[#]Our[#]focus[#]fixed[#]point[#]compared[#]simulation" />
        </attvalues>
      </node>
      <node id="Geurt Jongbloed" label="Geurt Jongbloed">
        <attvalues>
          <attvalue for="0" value="Two[#]new[#]test[#]statistics[#]introduced[#]null[#]hypotheses[#]sampling[#]distribution[#]increasing[#]hazard[#]rate[#]specified[#]interval[#]These[#]empirical[#]distances[#]isotonic[#]estimates[#]use[#]monotonicity[#]constraint[#]either[#]function[#]cumulative[#]They[#]measure[#]excursions[#]respect[#]due[#]local[#]Asymptotic[#]normality[#]strictly[#]established[#]mild[#]conditions[#]This[#]done[#]first[#]approximating[#]global[#]distance[#]underlying[#]The[#]resulting[#]integral[#]treated[#]sum[#]increasingly[#]many[#]integrals[#]CLT[#]applied[#]behavior[#]determined[#]canonical[#]process[#]difference[#]stochastic[#]x[#]W[#]standard[#]Brownian[#]Motion[#]greatest[#]convex[#]minorant" />
        </attvalues>
      </node>
      <node id="Birgit Witte" label="Birgit Witte">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]problem[#]estimating[#]joint[#]distribution[#]function[#]event[#]time[#]continuous[#]mark[#]variable[#]subject[#]interval[#]censoring[#]case[#]observed[#]occurred[#]inspection[#]The[#]nonparametric[#]maximum[#]likelihood[#]estimator[#]model[#]known[#]inconsistent[#]study[#]two[#]alternative[#]smooth[#]estimators[#]based[#]explicit[#]inverse[#]expression[#]interest[#]terms[#]density[#]observable[#]vector[#]derive[#]pointwise[#]asymptotic" />
        </attvalues>
      </node>
      <node id="Pawel Lorek" label="Pawel Lorek">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Thomas A. Dean" label="Thomas A. Dean">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Sumeetpal S. Singh" label="Sumeetpal S. Singh">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Takuya Kashimura" label="Takuya Kashimura">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]give[#]review[#]method[#]imsets[#]introduced[#]Studeny[#]geometric[#]point[#]view[#]Elementary[#]span[#]polyhedral[#]cone[#]dual[#]supermodular[#]functions[#]We[#]basic[#]facts[#]structure[#]cones[#]Then[#]derive[#]new[#]results[#]following[#]topics[#]extreme[#]rays[#]standardized[#]ii[#]faces[#]iii[#]small[#]relations[#]among[#]elementary[#]iv[#]computational[#]Markov[#]basis[#]toric[#]ideal[#]defined" />
        </attvalues>
      </node>
      <node id="Jose A. Diaz-Garcia" label="Jose A. Diaz-Garcia">
        <attvalues>
          <attvalue for="0" value="A[#]modified[#]Prekopa[#]approach[#]considered[#]problem[#]optimum[#]allocation[#]multivariate[#]stratified[#]random[#]sampling[#]An[#]example[#]solved[#]applying[#]proposed[#]methodology[#]The[#]stochastic[#]matrix[#]integer[#]mathematical[#]programming[#]With[#]aims[#]asymptotic[#]normality[#]sample[#]covariance[#]matrices[#]strata[#]established[#]Some[#]alternative[#]approaches[#]suggested[#]solution[#]techniques[#]multiresponse[#]surface[#]modelled[#]one[#]multiobjective[#]optimisation[#]diverse[#]solutions[#]Several[#]crucial[#]differences[#]highlighted[#]others[#]Finally[#]numerical[#]particular[#]applied[#]described[#]detail" />
        </attvalues>
      </node>
      <node id="Rogelio Raos-Quiroga" label="Rogelio Raos-Quiroga">
        <attvalues>
          <attvalue for="0" value="A[#]modified[#]Prekopa[#]approach[#]considered[#]problem[#]optimum[#]allocation[#]multivariate[#]stratified[#]random[#]sampling[#]An[#]example[#]solved[#]applying[#]proposed[#]methodology" />
        </attvalues>
      </node>
      <node id="Michael Evans" label="Michael Evans">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gun Ho Jang" label="Gun Ho Jang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Elizabeth Gross" label="Elizabeth Gross">
        <attvalues>
          <attvalue for="0" value="Most[#]statistical[#]software[#]packages[#]implement[#]numerical[#]strategies[#]computation[#]maximum[#]likelihood[#]estimates[#]random[#]effects[#]models[#]Little[#]known[#]however[#]algebraic[#]complexity[#]problem[#]For[#]layout[#]unbalanced[#]group[#]sizes[#]give[#]formulas[#]degree[#]equations[#]well[#]restricted[#]estimation[#]In[#]particular[#]latter[#]approach[#]shown[#]algebraically[#]less[#]complex[#]The[#]obtained[#]studying[#]univariate[#]rational[#]equation[#]whose[#]solutions[#]correspond[#]Applying[#]techniques[#]computational[#]algebra[#]also[#]show[#]balanced[#]layouts[#]without[#]interaction[#]four[#]Our[#]work[#]suggests[#]methods[#]allow[#]one[#]reliably[#]find[#]global[#]optima[#]functions[#]linear[#]mixed[#]small[#]number[#]variance[#]components" />
        </attvalues>
      </node>
      <node id="Mathias Drton" label="Mathias Drton">
        <attvalues>
          <attvalue for="0" value="Most[#]statistical[#]software[#]packages[#]implement[#]numerical[#]strategies[#]computation[#]maximum[#]likelihood[#]estimates[#]random[#]effects[#]models[#]Little[#]known[#]however[#]algebraic[#]complexity[#]problem[#]For[#]layout[#]unbalanced[#]group[#]sizes[#]give[#]formulas[#]degree[#]equations[#]well[#]restricted[#]estimation[#]In[#]particular[#]latter[#]approach[#]shown[#]algebraically[#]less[#]complex[#]The[#]obtained[#]studying[#]univariate[#]rational[#]equation[#]whose[#]solutions[#]correspond[#]Applying[#]techniques[#]computational[#]algebra[#]also[#]show[#]balanced[#]layouts[#]without[#]interaction[#]four[#]Our[#]work[#]suggests[#]methods[#]allow[#]one[#]reliably[#]find[#]global[#]optima[#]functions[#]linear[#]mixed[#]small[#]number[#]variance[#]components[#]A[#]structural[#]model[#]relates[#]variables[#]interest[#]corresponding[#]Gaussian[#]noise[#]terms[#]via[#]system[#]Each[#]represented[#]graph[#]directed[#]edges[#]encode[#]bidirected[#]indicate[#]possible[#]correlations[#]among[#]We[#]study[#]parameter[#]identifiability[#]ask[#]conditions[#]ensure[#]edge[#]coefficients[#]appearing[#]uniquely[#]recovered[#]covariance[#]matrix[#]associated[#]distribution[#]treat[#]case[#]generic[#]unique[#]recovery[#]almost[#]every[#]choice[#]parameters[#]new[#]graphical[#]condition[#]sufficient[#]verified[#]time[#]polynomial[#]size[#]It[#]improves[#]criteria[#]prior[#]require[#]part[#]acyclic[#]develop[#]related[#]necessary[#]examine[#]gap[#]simulations[#]graphs[#]nodes[#]exhaustive[#]computations[#]five" />
        </attvalues>
      </node>
      <node id="Sonja Petrović" label="Sonja Petrović">
        <attvalues>
          <attvalue for="0" value="Most[#]statistical[#]software[#]packages[#]implement[#]numerical[#]strategies[#]computation[#]maximum[#]likelihood[#]estimates[#]random[#]effects[#]models[#]Little[#]known[#]however[#]algebraic[#]complexity[#]problem[#]For[#]layout[#]unbalanced[#]group[#]sizes[#]give[#]formulas[#]degree[#]equations[#]well[#]restricted[#]estimation[#]In[#]particular[#]latter[#]approach[#]shown[#]algebraically[#]less[#]complex[#]The[#]obtained[#]studying[#]univariate[#]rational[#]equation[#]whose[#]solutions[#]correspond[#]Applying[#]techniques[#]computational[#]algebra[#]also[#]show[#]balanced[#]layouts[#]without[#]interaction[#]four[#]Our[#]work[#]suggests[#]methods[#]allow[#]one[#]reliably[#]find[#]global[#]optima[#]functions[#]linear[#]mixed[#]small[#]number[#]variance[#]components" />
        </attvalues>
      </node>
      <node id="Raúl Jiménez" label="Raúl Jiménez">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="J. E. Yukich" label="J. E. Yukich">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Pierre Vandekerkhove" label="Pierre Vandekerkhove">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]paper[#]semiparametric[#]mixture[#]two[#]distributions[#]equal[#]shift[#]parameter[#]The[#]model[#]said[#]sense[#]mixed[#]distribution[#]supposed[#]belong[#]parametric[#]family[#]In[#]order[#]insure[#]identifiability[#]assumed[#]symmetric[#]defined[#]mixing[#]proportion[#]location[#]parameters[#]probability[#]density[#]function[#]propose[#]new[#]class[#]based[#]Fourier[#]approach[#]prove[#]square[#]root[#]consistent[#]mild[#]regularity[#]conditions[#]Their[#]properties[#]illustrated[#]Monte[#]Carlo[#]study[#]benchmark[#]real[#]dataset[#]also[#]studied[#]method[#]establish[#]simple[#]variance[#]inequality[#]whose[#]underlying[#]sequence[#]random[#]variables[#]ergodic[#]Markov[#]Chain[#]constants[#]explicit[#]depend[#]computable[#]bounds[#]rate[#]apply[#]result[#]derive[#]strong[#]law[#]large[#]number[#]close[#]optimal" />
        </attvalues>
      </node>
      <node id="Jie Yang" label="Jie Yang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Abhyuday Mandal" label="Abhyuday Mandal">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Dibyen Majumdar" label="Dibyen Majumdar">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Peter McCullagh" label="Peter McCullagh">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Han Han" label="Han Han">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Rogelio Ramos-Quiroga" label="Rogelio Ramos-Quiroga">
        <attvalues>
          <attvalue for="0" value="The[#]allocation[#]problem[#]multivariate[#]stratified[#]random[#]sampling[#]stochastic[#]matrix[#]integer[#]mathematical[#]programming[#]considered[#]With[#]aims[#]asymptotic[#]normality[#]sample[#]covariance[#]matrices[#]strata[#]established[#]Some[#]alternative[#]approaches[#]suggested[#]solution[#]An[#]example[#]solved[#]applying[#]proposed[#]techniques" />
        </attvalues>
      </node>
      <node id="Dominique Guillot" label="Dominique Guillot">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Bala Rajaratnam" label="Bala Rajaratnam">
        <attvalues>
          <attvalue for="0" value="Gaussian[#]covariance[#]graph[#]models[#]encode[#]marginal[#]independence[#]among[#]components[#]multivariate[#]random[#]vector[#]means[#]G[#]These[#]distinctly[#]different[#]traditional[#]concentration[#]often[#]also[#]referred[#]graphical[#]selection[#]since[#]zeros[#]parameter[#]reflected[#]matrix[#]compared[#]The[#]space[#]interest[#]cone[#]positive[#]definite[#]matrices[#]fixed[#]corresponding[#]missing[#]edges[#]As[#]Letac[#]Massam[#]Ann[#]Statist[#]consider[#]case[#]decomposable[#]In[#]paper[#]construct[#]family[#]Wishart[#]distributions[#]serve[#]similar[#]purpose[#]setting[#]constructed[#]Dawid[#]Lauritzen[#]We[#]proceed[#]undertake[#]rigorous[#]study[#]derive[#]several[#]deep[#]useful[#]properties[#]class" />
        </attvalues>
      </node>
      <node id="Helene Gehrmann" label="Helene Gehrmann">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Steffen L. Lauritzen" label="Steffen L. Lauritzen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ryan J. Tibshirani" label="Ryan J. Tibshirani">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jonathan Taylor" label="Jonathan Taylor">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Pierre-Olivier Amblard" label="Pierre-Olivier Amblard">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]deals[#]identification[#]multivariate[#]fractional[#]Brownian[#]motion[#]recently[#]developed[#]extension[#]case[#]process[#]p[#]Gaussian[#]parameterized[#]different[#]Hurst[#]exponents[#]scaling[#]coefficients[#]component[#]also[#]ij[#]j[#]allowing[#]two[#]components[#]less[#]strongly[#]correlated[#]time[#]reversible[#]We[#]investigate[#]use[#]discrete[#]filtering[#]techniques[#]estimate[#]jointly[#]separately[#]parameters[#]prove[#]efficiency[#]methodology[#]simulation[#]study[#]derivation[#]asymptotic[#]results" />
        </attvalues>
      </node>
      <node id="Jean-François Coeurjolly" label="Jean-François Coeurjolly">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]deals[#]identification[#]multivariate[#]fractional[#]Brownian[#]motion[#]recently[#]developed[#]extension[#]case[#]process[#]p[#]Gaussian[#]parameterized[#]different[#]Hurst[#]exponents[#]scaling[#]coefficients[#]component[#]also[#]ij[#]j[#]allowing[#]two[#]components[#]less[#]strongly[#]correlated[#]time[#]reversible[#]We[#]investigate[#]use[#]discrete[#]filtering[#]techniques[#]estimate[#]jointly[#]separately[#]parameters[#]prove[#]efficiency[#]methodology[#]simulation[#]study[#]derivation[#]asymptotic[#]results[#]In[#]introduce[#]new[#]class[#]estimators[#]exponent[#]fBm[#]These[#]based[#]sample[#]expectiles[#]variations[#]path[#]order[#]derive[#]statistical[#]properties[#]proposed[#]establish[#]subordinated[#]stationary[#]processes[#]unit[#]variance[#]correlation[#]function[#]satisfying[#]Via[#]demonstrate[#]relevance[#]estimation[#]method[#]show[#]suggested[#]robust[#]data[#]rounding[#]counterparts" />
        </attvalues>
      </node>
      <node id="Fabrice Rossi" label="Fabrice Rossi">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Nathalie Villa-Vialaneix" label="Nathalie Villa-Vialaneix">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Séphane Gaïffas" label="Séphane Gaïffas">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Agathe Guilloux" label="Agathe Guilloux">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Helena Ferreira" label="Helena Ferreira">
        <attvalues>
          <attvalue for="0" value="Financial[#]crises[#]recurrent[#]phenomenon[#]important[#]effects[#]real[#]economy[#]The[#]financial[#]system[#]inherently[#]fragile[#]therefore[#]great[#]importance[#]able[#]measure[#]characterize[#]systemic[#]stability[#]Multivariate[#]extreme[#]value[#]theory[#]provide[#]us[#]framework[#]fragility[#]index[#]Geluk[#]et[#]al[#]Falk[#]Tichy[#]Here[#]generalize[#]concept[#]contribute[#]modeling[#]stochastic[#]divided[#]blocks[#]We[#]find[#]several[#]relations[#]tail[#]dependence[#]measures[#]literature[#]immediate[#]estimators[#]end[#]application[#]data[#]Spatial[#]environmental[#]processes[#]often[#]exhibit[#]large[#]values[#]In[#]order[#]model[#]properties[#]must[#]characterized[#]quantified[#]paper[#]introduce[#]evaluates[#]among[#]observations[#]located[#]two[#]separated[#]regions[#]locations[#]compute[#]range[#]new[#]extends[#]existing[#]compare[#]extremal[#]coefficients[#]finding[#]generalizations[#]known[#]pairwise[#]approach[#]Estimators[#]introduced[#]asymptotic[#]normality[#]strong[#]consistency[#]shown[#]An[#]annual[#]maxima[#]precipitation[#]Portuguese[#]presented" />
        </attvalues>
      </node>
      <node id="Marta Ferreira" label="Marta Ferreira">
        <attvalues>
          <attvalue for="0" value="Financial[#]crises[#]recurrent[#]phenomenon[#]important[#]effects[#]real[#]economy[#]The[#]financial[#]system[#]inherently[#]fragile[#]therefore[#]great[#]importance[#]able[#]measure[#]characterize[#]systemic[#]stability[#]Multivariate[#]extreme[#]value[#]theory[#]provide[#]us[#]framework[#]fragility[#]index[#]Geluk[#]et[#]al[#]Falk[#]Tichy[#]Here[#]generalize[#]concept[#]contribute[#]modeling[#]stochastic[#]divided[#]blocks[#]We[#]find[#]several[#]relations[#]tail[#]dependence[#]measures[#]literature[#]immediate[#]estimators[#]end[#]application[#]data" />
        </attvalues>
      </node>
      <node id="Laurent Gardes" label="Laurent Gardes">
        <attvalues>
          <attvalue for="0" value="In[#]Li[#]Yin[#]X[#]Sliced[#]Inverse[#]Regression[#]Regularizations[#]Biometrics[#]ridge[#]SIR[#]estimator[#]introduced[#]solution[#]minimization[#]problem[#]computed[#]thanks[#]alternating[#]algorithm[#]This[#]methodology[#]reveals[#]good[#]performance[#]practice[#]note[#]focus[#]theoretical[#]properties[#]Is[#]shown[#]degenerated[#]sense[#]two[#]situations[#]occur[#]Either[#]exist[#]zero" />
        </attvalues>
      </node>
      <node id="Stéphane Girard" label="Stéphane Girard">
        <attvalues>
          <attvalue for="0" value="In[#]Li[#]Yin[#]X[#]Sliced[#]Inverse[#]Regression[#]Regularizations[#]Biometrics[#]ridge[#]SIR[#]estimator[#]introduced[#]solution[#]minimization[#]problem[#]computed[#]thanks[#]alternating[#]algorithm[#]This[#]methodology[#]reveals[#]good[#]performance[#]practice[#]note[#]focus[#]theoretical[#]properties[#]Is[#]shown[#]degenerated[#]sense[#]two[#]situations[#]occur[#]Either[#]exist[#]zero[#]paper[#]study[#]semiparametric[#]family[#]bivariate[#]copulas[#]The[#]generated[#]univariate[#]function[#]determining[#]symmetry[#]radial[#]joint[#]dependence[#]property[#]quadrant[#]total[#]positivity[#]We[#]provide[#]bounds[#]different[#]measures[#]association[#]Kendall[#]Tau[#]Spearman[#]Rho[#]several[#]choices[#]generating[#]functions[#]allowing[#]reach[#]give[#]sufficient[#]conditions[#]establish[#]central[#]limit[#]theorems[#]boundary[#]estimates[#]Poisson[#]point[#]processes[#]considered[#]obtained[#]smoothing[#]bias[#]corrected[#]extreme[#]values[#]process[#]show[#]leads[#]Gaussian[#]asymptotic[#]distributions[#]therefore[#]pointwise[#]confidence[#]intervals[#]Some[#]new[#]unidimensional[#]multidimensional[#]examples[#]provided" />
        </attvalues>
      </node>
      <node id="Fan Wei" label="Fan Wei">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Richard M Dudley" label="Richard M Dudley">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Holger Fink" label="Holger Fink">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Claudia Klüppelberg" label="Claudia Klüppelberg">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Milan Studeny" label="Milan Studeny">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="David Haws" label="David Haws">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Adam D. Bull" label="Adam D. Bull">
        <attvalues>
          <attvalue for="0" value="The[#]problem[#]constructing[#]confidence[#]sets[#]adaptive[#]continuous[#]scale[#]Sobolev[#]classes[#]probability[#]densities[#]considered[#]Adaptation[#]holds[#]possible[#]respect[#]radius[#]ball[#]smoothness[#]degree[#]maximal[#]parameter[#]spaces[#]adaptation[#]Two[#]key[#]regimes[#]constellations[#]identified[#]one[#]full[#]requires[#]critical[#]regions[#]removed[#]Techniques[#]used[#]derive[#]results[#]include[#]general[#]nonparametric[#]minimax[#]test[#]alternative[#]hypotheses[#]new[#]lower[#]bounds" />
        </attvalues>
      </node>
      <node id="Richard Nickl" label="Richard Nickl">
        <attvalues>
          <attvalue for="0" value="The[#]problem[#]constructing[#]confidence[#]sets[#]adaptive[#]continuous[#]scale[#]Sobolev[#]classes[#]probability[#]densities[#]considered[#]Adaptation[#]holds[#]possible[#]respect[#]radius[#]ball[#]smoothness[#]degree[#]maximal[#]parameter[#]spaces[#]adaptation[#]Two[#]key[#]regimes[#]constellations[#]identified[#]one[#]full[#]requires[#]critical[#]regions[#]removed[#]Techniques[#]used[#]derive[#]results[#]include[#]general[#]nonparametric[#]minimax[#]test[#]alternative[#]hypotheses[#]new[#]lower[#]bounds[#]We[#]consider[#]statistical[#]deconvolution[#]observes[#]n[#]replications[#]model[#]X[#]unobserved[#]random[#]signal[#]interest[#]independent[#]error[#]distribution[#]Under[#]weak[#]assumptions[#]decay[#]Fourier[#]transform[#]upper[#]risk[#]wavelet[#]density[#]estimators[#]f[#]R[#]assumed[#]bounded[#]Besov[#]balls[#]estimation[#]show[#]attain[#]linear[#]adapt[#]unknown[#]decays[#]exponentially[#]corresponding[#]result[#]true[#]hard[#]thresholding[#]estimator[#]polynomially[#]also[#]analyze[#]case[#]supersmooth[#]finally[#]recent[#]techniques[#]Rademacher[#]processes[#]applied[#]construct[#]global[#]bands[#]Given[#]sample[#]devise[#]Haar[#]variable[#]resolution[#]levels[#]constructed[#]localised[#]procedures[#]Lepski[#]Mammen[#]Spokoiny[#]Ann[#]spatially[#]heterogeneous[#]simultaneously[#]every[#]point[#]x[#]fixed[#]interval[#]loss[#]constants[#]involved[#]chosen[#]practice[#]idealised[#]assumption[#]locally[#]constant[#]neighborhood[#]information[#]theoretic[#]justification[#]given[#]Let[#]defined[#]compact[#]homogeneous[#]manifold[#]M[#]dimension[#]Consider[#]frame[#]j[#]describing[#]projection[#]onto[#]space[#]eigenfunctions[#]Laplace[#]operator[#]eigenvalues[#]less[#]prove[#]concentration[#]inequalities[#]uniform[#]deviations[#]needlet[#]obtained[#]empirical[#]estimate[#]apply[#]nonasymptotic[#]differentiable[#]older[#]functions[#]lder[#]exponents" />
        </attvalues>
      </node>
      <node id="Elchanan Mossel" label="Elchanan Mossel">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Allan Sly" label="Allan Sly">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Omer Tamuz" label="Omer Tamuz">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Oleg Lepski" label="Oleg Lepski">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Nora Serdyukova" label="Nora Serdyukova">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Robert L. Wolpert" label="Robert L. Wolpert">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Merlise A. Clyde" label="Merlise A. Clyde">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Chong Tu" label="Chong Tu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Sebastian Krumscheid" label="Sebastian Krumscheid">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Grigorios A. Pavliotis" label="Grigorios A. Pavliotis">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Serafim Kalliadasis" label="Serafim Kalliadasis">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Siegfried Hormann" label="Siegfried Hormann">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Sándor Baran" label="Sándor Baran">
        <attvalues>
          <attvalue for="0" value="Spatial[#]unilateral[#]autoregressive[#]model[#]k[#]investigated[#]unit[#]root[#]case[#]parameters[#]boundary[#]domain[#]stability[#]forms[#]tetrahedron[#]vertices[#]It[#]shown[#]limiting[#]distribution[#]least[#]squares[#]estimator[#]normal[#]rate[#]convergence[#]n[#]faces[#]edges[#]The[#]problem[#]estimating[#]linear[#]regression[#]Z[#]based[#]observations[#]spatial[#]G[#]special[#]shape[#]considered[#]driving[#]process[#]U[#]Gaussian[#]random[#]field[#]known[#]functions[#]Explicit[#]maximum[#]likelihood[#]estimators[#]derived[#]cases[#]either[#]Wiener[#]stationary[#]nonstationary[#]sheet[#]Simulation[#]results[#]also[#]presented[#]sheets[#]simulated[#]help[#]eve[#]expansions" />
        </attvalues>
      </node>
      <node id="Gyula Pap" label="Gyula Pap">
        <attvalues>
          <attvalue for="0" value="Spatial[#]unilateral[#]autoregressive[#]model[#]k[#]investigated[#]unit[#]root[#]case[#]parameters[#]boundary[#]domain[#]stability[#]forms[#]tetrahedron[#]vertices[#]It[#]shown[#]limiting[#]distribution[#]least[#]squares[#]estimator[#]normal[#]rate[#]convergence[#]n[#]faces[#]edges" />
        </attvalues>
      </node>
      <node id="Tony Cai" label="Tony Cai">
        <attvalues>
          <attvalue for="0" value="Testing[#]covariance[#]structure[#]significant[#]interest[#]many[#]areas[#]statistical[#]analysis[#]construction[#]compressed[#]sensing[#]matrices[#]important[#]problem[#]signal[#]processing[#]Motivated[#]applications[#]study[#]paper[#]limiting[#]laws[#]coherence[#]p[#]random[#]matrix[#]setting[#]much[#]larger[#]n[#]Both[#]law[#]large[#]numbers[#]distribution[#]derived[#]We[#]consider[#]testing[#]bandedness[#]high[#]dimensional[#]Gaussian[#]includes[#]independence[#]special[#]case[#]The[#]data[#]play[#]critical[#]role[#]test[#]also[#]apply[#]asymptotic[#]results" />
        </attvalues>
      </node>
      <node id="Tiefeng Jiang" label="Tiefeng Jiang">
        <attvalues>
          <attvalue for="0" value="Testing[#]covariance[#]structure[#]significant[#]interest[#]many[#]areas[#]statistical[#]analysis[#]construction[#]compressed[#]sensing[#]matrices[#]important[#]problem[#]signal[#]processing[#]Motivated[#]applications[#]study[#]paper[#]limiting[#]laws[#]coherence[#]p[#]random[#]matrix[#]setting[#]much[#]larger[#]n[#]Both[#]law[#]large[#]numbers[#]distribution[#]derived[#]We[#]consider[#]testing[#]bandedness[#]high[#]dimensional[#]Gaussian[#]includes[#]independence[#]special[#]case[#]The[#]data[#]play[#]critical[#]role[#]test[#]also[#]apply[#]asymptotic[#]results" />
        </attvalues>
      </node>
      <node id="Weidong Liu" label="Weidong Liu">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]limiting[#]distributions[#]estimators[#]threshold[#]autoregressive[#]TAR[#]model[#]It[#]proved[#]behaviors[#]process[#]different[#]classical[#]unit[#]root[#]explosive[#]AR" />
        </attvalues>
      </node>
      <node id="Shiqing Ling" label="Shiqing Ling">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]limiting[#]distributions[#]estimators[#]threshold[#]autoregressive[#]TAR[#]model[#]It[#]proved[#]behaviors[#]process[#]different[#]classical[#]unit[#]root[#]explosive[#]AR" />
        </attvalues>
      </node>
      <node id="Qi-Man Shao" label="Qi-Man Shao">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]limiting[#]distributions[#]estimators[#]threshold[#]autoregressive[#]TAR[#]model[#]It[#]proved[#]behaviors[#]process[#]different[#]classical[#]unit[#]root[#]explosive[#]AR" />
        </attvalues>
      </node>
      <node id="Zhengyan Lin" label="Zhengyan Lin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Hanchao Wang" label="Hanchao Wang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexander Aue" label="Alexander Aue">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]models[#]asset[#]prices[#]continuous[#]time[#]driven[#]point[#]processes[#]In[#]bivariate[#]model[#]admits[#]cointegration[#]allow[#]deformations[#]account[#]effects[#]intraday[#]seasonal[#]patterns[#]volatility[#]periods[#]may[#]different[#]two[#]assets[#]also[#]asymmetries[#]leverage[#]obtain[#]asymptotic[#]distribution[#]process[#]ordinary[#]estimator[#]cointegrating[#]parameter[#]based[#]data[#]sampled[#]discretization[#]calendar[#]case[#]weak[#]fractional[#]For[#]tapered" />
        </attvalues>
      </node>
      <node id="Lajos Horváth" label="Lajos Horváth">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]models[#]asset[#]prices[#]continuous[#]time[#]driven[#]point[#]processes[#]In[#]bivariate[#]model[#]admits[#]cointegration[#]allow[#]deformations[#]account[#]effects[#]intraday[#]seasonal[#]patterns[#]volatility[#]periods[#]may[#]different[#]two[#]assets[#]also[#]asymmetries[#]leverage[#]obtain[#]asymptotic[#]distribution[#]process[#]ordinary[#]estimator[#]cointegrating[#]parameter[#]based[#]data[#]sampled[#]discretization[#]calendar[#]case[#]weak[#]fractional[#]For[#]tapered[#]quadratic[#]functional[#]regression[#]scalar[#]response[#]depends[#]predictor[#]common[#]linear[#]special[#]wish[#]test[#]significance[#]nonlinear[#]term[#]develop[#]testing[#]method[#]projecting[#]observations[#]onto[#]suitably[#]chosen[#]finite[#]dimensional[#]space[#]using[#]principal[#]component[#]analysis[#]The[#]behavior[#]procedure[#]established[#]A[#]simulation[#]study[#]shows[#]good[#]size[#]power[#]sample[#]sizes[#]apply[#]set[#]provided[#]Tecator[#]consists[#]absorbance[#]spectra[#]fat[#]content[#]meat" />
        </attvalues>
      </node>
      <node id="Clifford M. Hurvich" label="Clifford M. Hurvich">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]models[#]asset[#]prices[#]continuous[#]time[#]driven[#]point[#]processes[#]In[#]bivariate[#]model[#]admits[#]cointegration[#]allow[#]deformations[#]account[#]effects[#]intraday[#]seasonal[#]patterns[#]volatility[#]periods[#]may[#]different[#]two[#]assets[#]also[#]asymmetries[#]leverage[#]obtain[#]asymptotic[#]distribution[#]process[#]ordinary[#]estimator[#]cointegrating[#]parameter[#]based[#]data[#]sampled[#]discretization[#]calendar[#]case[#]weak[#]fractional[#]For[#]tapered" />
        </attvalues>
      </node>
      <node id="Céline Duval" label="Céline Duval">
        <attvalues>
          <attvalue for="0" value="We[#]investigate[#]statistical[#]inference[#]across[#]time[#]scales[#]take[#]toy[#]model[#]estimation[#]intensity[#]discretely[#]observed[#]compound[#]Poisson[#]process[#]symmetric[#]Bernoulli[#]jumps[#]data[#]different[#]microscopic[#]intermediate[#]macroscopic[#]quantify[#]smooth[#]transition[#]Poissonian[#]regime[#]Gaussian[#]The[#]classical[#]quadratic[#]variation[#]estimator[#]efficient[#]surprisingly[#]shows[#]substantial[#]loss[#]information[#]scale[#]explicitly[#]related[#]sampling[#]rate[#]discuss[#]implications[#]findings[#]beyond[#]idealised[#]framework" />
        </attvalues>
      </node>
      <node id="Marc Hoffmann" label="Marc Hoffmann">
        <attvalues>
          <attvalue for="0" value="We[#]investigate[#]statistical[#]inference[#]across[#]time[#]scales[#]take[#]toy[#]model[#]estimation[#]intensity[#]discretely[#]observed[#]compound[#]Poisson[#]process[#]symmetric[#]Bernoulli[#]jumps[#]data[#]different[#]microscopic[#]intermediate[#]macroscopic[#]quantify[#]smooth[#]transition[#]Poissonian[#]regime[#]Gaussian[#]The[#]classical[#]quadratic[#]variation[#]estimator[#]efficient[#]surprisingly[#]shows[#]substantial[#]loss[#]information[#]scale[#]explicitly[#]related[#]sampling[#]rate[#]discuss[#]implications[#]findings[#]beyond[#]idealised[#]framework" />
        </attvalues>
      </node>
      <node id="Shan Luo" label="Shan Luo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Zehua Chen" label="Zehua Chen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="J. E. Chacón" label="J. E. Chacón">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="J. Montanero" label="J. Montanero">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="A. G. Nogales" label="A. G. Nogales">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Fasheng Sun" label="Fasheng Sun">
        <attvalues>
          <attvalue for="0" value="Supersaturated[#]design[#]SSD[#]received[#]much[#]recent[#]interest[#]potential[#]factor[#]screening[#]experiments[#]In[#]paper[#]provide[#]equivalent[#]conditions[#]two[#]columns[#]fully[#]aliased[#]consequently[#]propose[#]methods[#]constructing[#]E[#]NOD[#]SSDs[#]without[#]via[#]equidistant[#]designs[#]difference[#]matrices[#]The[#]easily[#]performed[#]many[#]new[#]optimal[#]obtained[#]Furthermore[#]proved[#]nonorthogonality[#]resulting[#]well[#]controlled[#]source[#]A[#]rather[#]complete[#]list[#]newly[#]generated[#]tabulated[#]practical[#]use" />
        </attvalues>
      </node>
      <node id="Dennis K. J. Lin" label="Dennis K. J. Lin">
        <attvalues>
          <attvalue for="0" value="Supersaturated[#]design[#]SSD[#]received[#]much[#]recent[#]interest[#]potential[#]factor[#]screening[#]experiments[#]In[#]paper[#]provide[#]equivalent[#]conditions[#]two[#]columns[#]fully[#]aliased[#]consequently[#]propose[#]methods[#]constructing[#]E[#]NOD[#]SSDs[#]without[#]via[#]equidistant[#]designs[#]difference[#]matrices[#]The[#]easily[#]performed[#]many[#]new[#]optimal[#]obtained[#]Furthermore[#]proved[#]nonorthogonality[#]resulting[#]well[#]controlled[#]source[#]A[#]rather[#]complete[#]list[#]newly[#]generated[#]tabulated[#]practical[#]use" />
        </attvalues>
      </node>
      <node id="Min-Qian Liu" label="Min-Qian Liu">
        <attvalues>
          <attvalue for="0" value="Supersaturated[#]design[#]SSD[#]received[#]much[#]recent[#]interest[#]potential[#]factor[#]screening[#]experiments[#]In[#]paper[#]provide[#]equivalent[#]conditions[#]two[#]columns[#]fully[#]aliased[#]consequently[#]propose[#]methods[#]constructing[#]E[#]NOD[#]SSDs[#]without[#]via[#]equidistant[#]designs[#]difference[#]matrices[#]The[#]easily[#]performed[#]many[#]new[#]optimal[#]obtained[#]Furthermore[#]proved[#]nonorthogonality[#]resulting[#]well[#]controlled[#]source[#]A[#]rather[#]complete[#]list[#]newly[#]generated[#]tabulated[#]practical[#]use" />
        </attvalues>
      </node>
      <node id="Piotr Graczyk" label="Piotr Graczyk">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ishi Hideyuki" label="Ishi Hideyuki">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Neal Madras" label="Neal Madras">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Deniz Sezer" label="Deniz Sezer">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Joseph Salmon" label="Joseph Salmon">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]problem[#]combining[#]possibly[#]uncountably[#]infinite[#]set[#]affine[#]estimators[#]regression[#]model[#]heteroscedastic[#]Gaussian[#]noise[#]Focusing[#]exponentially[#]weighted[#]aggregate[#]prove[#]type[#]inequality[#]leads[#]sharp[#]oracle[#]inequalities[#]discrete[#]also[#]continuous[#]settings[#]The[#]framework[#]general[#]enough[#]cover[#]combinations[#]various[#]procedures[#]least[#]square[#]kernel[#]ridge[#]shrinking[#]many[#]used[#]literature[#]statistical[#]inverse[#]problems[#]As[#]consequence[#]show[#]proposed[#]provides[#]adaptive[#]estimator[#]exact[#]minimax[#]sense[#]without[#]neither[#]discretizing[#]range[#]tuning[#]parameters[#]splitting[#]observations[#]illustrate[#]numerically[#]good[#]performance[#]achieved" />
        </attvalues>
      </node>
      <node id="Viatcheslav B. Melas" label="Viatcheslav B. Melas">
        <attvalues>
          <attvalue for="0" value="The[#]celebrated[#]de[#]la[#]Garza[#]phenomenon[#]states[#]polynomial[#]regression[#]model[#]degree[#]optimal[#]design[#]based[#]p[#]points[#]In[#]remarkable[#]paper[#]Yang[#]Ann[#]Statist[#]showed[#]exists[#]many[#]locally[#]problems[#]nonlinear[#]models[#]present[#]note[#]different[#]view[#]point[#]findings[#]using[#]results[#]moment[#]theory[#]Chebyshev[#]systems[#]particular[#]show[#]occurs[#]even[#]larger[#]class[#]considered[#]far" />
        </attvalues>
      </node>
      <node id="Markus Bibinger" label="Markus Bibinger">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Markus Reiß" label="Markus Reiß">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Tatiane F. N. Melo" label="Tatiane F. N. Melo">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]obtain[#]adjusted[#]version[#]likelihood[#]ratio[#]test[#]multivariate[#]linear[#]regression[#]models[#]The[#]error[#]terms[#]allowed[#]follow[#]distribution[#]class[#]elliptical[#]distributions[#]normal[#]special[#]case[#]We[#]derive[#]modified[#]statistic[#]follows[#]high[#]degree[#]accuracy[#]Our[#]results[#]generalize[#]Melo[#]Ferrari[#]Advances[#]Statistical[#]Analysis[#]allowing[#]parameter[#]interest[#]model[#]report[#]simulation[#]study[#]shows[#]proposed[#]displays[#]superior[#]finite[#]sample[#]behavior[#]relative[#]standard" />
        </attvalues>
      </node>
      <node id="Silvia L. P. Ferrari" label="Silvia L. P. Ferrari">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]obtain[#]adjusted[#]version[#]likelihood[#]ratio[#]test[#]multivariate[#]linear[#]regression[#]models[#]The[#]error[#]terms[#]allowed[#]follow[#]distribution[#]class[#]elliptical[#]distributions[#]normal[#]special[#]case[#]We[#]derive[#]modified[#]statistic[#]follows[#]high[#]degree[#]accuracy[#]Our[#]results[#]generalize[#]Melo[#]Ferrari[#]Advances[#]Statistical[#]Analysis[#]allowing[#]parameter[#]interest[#]model[#]report[#]simulation[#]study[#]shows[#]proposed[#]displays[#]superior[#]finite[#]sample[#]behavior[#]relative[#]standard[#]asymptotic[#]expansions[#]order[#]nonnull[#]functions[#]Wald[#]score[#]gradient[#]statistics[#]dispersion[#]sequence[#]Pitman[#]alternatives[#]obtained[#]testing[#]subset[#]parameters[#]precision[#]Based[#]shown[#]uniform[#]superiority[#]one[#]respect[#]others[#]Furthermore[#]compare[#]performance[#]tests[#]Monte[#]Carlo[#]simulations[#]presented[#]An[#]empirical[#]application[#]real[#]data[#]set[#]considered[#]illustrative[#]purposes" />
        </attvalues>
      </node>
      <node id="Cari Kaufman" label="Cari Kaufman">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Benjamin Shaby" label="Benjamin Shaby">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Parikshit Shah" label="Parikshit Shah">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Venkat Chandrasekaran" label="Venkat Chandrasekaran">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Karim Lounici" label="Karim Lounici">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]statistical[#]deconvolution[#]problem[#]one[#]observes[#]n[#]replications[#]model[#]X[#]unobserved[#]random[#]signal[#]interest[#]independent[#]error[#]distribution[#]Under[#]weak[#]assumptions[#]decay[#]Fourier[#]transform[#]derive[#]upper[#]bounds[#]risk[#]wavelet[#]density[#]estimators[#]f[#]R[#]assumed[#]bounded[#]lower[#]minimax[#]Besov[#]balls[#]estimation[#]show[#]attain[#]linear[#]adapt[#]unknown[#]smoothness[#]decays[#]exponentially[#]corresponding[#]result[#]holds[#]true[#]hard[#]thresholding[#]estimator[#]polynomially[#]also[#]analyze[#]case[#]supersmooth[#]finally[#]results[#]recent[#]techniques[#]Rademacher[#]processes[#]applied[#]construct[#]global[#]confidence[#]bands" />
        </attvalues>
      </node>
      <node id="Junlong Zhao" label="Junlong Zhao">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Chenlei Leng" label="Chenlei Leng">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marianne Clausel" label="Marianne Clausel">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="François Roueff" label="François Roueff">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Murad S. Taqqu" label="Murad S. Taqqu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ciprian A. Tudor" label="Ciprian A. Tudor">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Masoumeh Dashti" label="Masoumeh Dashti">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]inverse[#]problem[#]determining[#]permeability[#]pressure[#]Darcy[#]model[#]flow[#]porous[#]medium[#]Mathematically[#]find[#]diffusion[#]coefficient[#]linear[#]uniformly[#]elliptic[#]partial[#]differential[#]equation[#]divergence[#]form[#]bounded[#]domain[#]dimension[#]measurements[#]solution[#]interior[#]adopt[#]Bayesian[#]approach[#]place[#]prior[#]random[#]field[#]measure[#]log[#]specified[#]eve[#]expansion[#]draws[#]Gaussian[#]measures[#]constructed[#]way[#]study[#]regularity[#]functions[#]drawn[#]also[#]Lipschitz[#]properties[#]observation[#]operator[#]mapping[#]observations[#]Combining[#]continuity[#]estimates[#]show[#]posterior[#]suitable[#]Banach[#]space[#]Furthermore[#]shown[#]respect[#]data[#]Hellinger[#]metric[#]giving[#]rise[#]Determining[#]given[#]solves[#]uncertainty[#]quantification[#]In[#]practice[#]must[#]approximated[#]finite[#]dimensional[#]quantify[#]errors[#]incurred[#]employing[#]truncated[#]represent[#]meausure[#]particular[#]weak[#]convergence[#]general[#]class[#]locally[#]apply[#]theory[#]estimate[#]mean[#]covariance[#]refinement[#]truncation" />
        </attvalues>
      </node>
      <node id="Stephen Harris" label="Stephen Harris">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Andrew Stuart" label="Andrew Stuart">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gersende Fort" label="Gersende Fort">
        <attvalues>
          <attvalue for="0" value="We[#]establish[#]simple[#]variance[#]inequality[#]whose[#]underlying[#]sequence[#]random[#]variables[#]ergodic[#]Markov[#]Chain[#]The[#]constants[#]explicit[#]depend[#]computable[#]bounds[#]mixing[#]rate[#]apply[#]result[#]derive[#]strong[#]law[#]large[#]number[#]conditions[#]close[#]optimal[#]Expectation[#]Maximization[#]EM[#]algorithm[#]versatile[#]tool[#]model[#]parameter[#]estimation[#]latent[#]data[#]models[#]When[#]processing[#]sets[#]stream[#]however[#]becomes[#]intractable[#]since[#]requires[#]whole[#]set[#]available[#]iteration[#]In[#]contribution[#]new[#]generic[#]online[#]inference[#]general[#]Hidden[#]Model[#]proposed[#]This[#]updates[#]estimate[#]block[#]observations[#]processed[#]convergence[#]established[#]studied[#]showing[#]impact[#]size[#]An[#]averaging[#]procedure[#]also[#]improve[#]Finally[#]practical[#]illustrations[#]presented[#]highlight[#]performance[#]algorithms[#]comparison[#]maximum[#]likelihood[#]procedures" />
        </attvalues>
      </node>
      <node id="Eric Moulines" label="Eric Moulines">
        <attvalues>
          <attvalue for="0" value="We[#]establish[#]simple[#]variance[#]inequality[#]whose[#]underlying[#]sequence[#]random[#]variables[#]ergodic[#]Markov[#]Chain[#]The[#]constants[#]explicit[#]depend[#]computable[#]bounds[#]mixing[#]rate[#]apply[#]result[#]derive[#]strong[#]law[#]large[#]number[#]conditions[#]close[#]optimal" />
        </attvalues>
      </node>
      <node id="Pierre Priouret" label="Pierre Priouret">
        <attvalues>
          <attvalue for="0" value="We[#]establish[#]simple[#]variance[#]inequality[#]whose[#]underlying[#]sequence[#]random[#]variables[#]ergodic[#]Markov[#]Chain[#]The[#]constants[#]explicit[#]depend[#]computable[#]bounds[#]mixing[#]rate[#]apply[#]result[#]derive[#]strong[#]law[#]large[#]number[#]conditions[#]close[#]optimal" />
        </attvalues>
      </node>
      <node id="Noureddine El Karoui" label="Noureddine El Karoui">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Holger Koesters" label="Holger Koesters">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="François Portier" label="François Portier">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Bernard Delyon" label="Bernard Delyon">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Stephen E. Fienberg" label="Stephen E. Fienberg">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alessandro Rinaldo" label="Alessandro Rinaldo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gerard Letac" label="Gerard Letac">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Helene Massam" label="Helene Massam">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marian Hristache" label="Marian Hristache">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]addresses[#]problem[#]semiparametric[#]efficiency[#]bounds[#]conditional[#]moment[#]restriction[#]models[#]different[#]conditioning[#]variables[#]We[#]characterize[#]bound[#]general[#]explicit[#]limit[#]decreasing[#]sequence[#]unconditional[#]marginal[#]An[#]iterative[#]procedure[#]approximating[#]efficient[#]score[#]provided[#]Our[#]theoretical[#]results[#]complete[#]extend[#]existing[#]literature[#]provide[#]new[#]insight[#]theory[#]open[#]door[#]applications[#]In[#]particular[#]investigate[#]class[#]mean[#]regression[#]quantile[#]missing[#]data" />
        </attvalues>
      </node>
      <node id="Valentin Patilea" label="Valentin Patilea">
        <attvalues>
          <attvalue for="0" value="Consider[#]random[#]vector[#]X[#]Y[#]We[#]assume[#]subject[#]right[#]censoring[#]The[#]aim[#]paper[#]twofold[#]First[#]propose[#]new[#]estimator[#]joint[#]distribution[#]This[#]overcomes[#]common[#]problem[#]using[#]dimension[#]reduction[#]technique[#]Second[#]relation[#]given[#]mean[#]regression[#]single[#]index[#]model[#]parameters[#]asymptotic[#]properties[#]proposed[#]estimators[#]obtained[#]addresses[#]semiparametric[#]efficiency[#]bounds[#]conditional[#]moment[#]restriction[#]models[#]different[#]conditioning[#]variables[#]characterize[#]bound[#]general[#]explicit[#]limit[#]decreasing[#]sequence[#]unconditional[#]marginal[#]An[#]iterative[#]procedure[#]approximating[#]efficient[#]score[#]provided[#]Our[#]theoretical[#]results[#]complete[#]extend[#]existing[#]literature[#]provide[#]insight[#]theory[#]open[#]door[#]applications[#]In[#]particular[#]investigate[#]class[#]quantile[#]missing[#]data" />
        </attvalues>
      </node>
      <node id="Alfred Hero" label="Alfred Hero">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Olga Klopp" label="Olga Klopp">
        <attvalues>
          <attvalue for="0" value="We[#]propose[#]new[#]pivotal[#]method[#]estimating[#]matrices[#]Assume[#]observe[#]small[#]set[#]entries[#]linear[#]combinations[#]unknown[#]matrix[#]corrupted[#]noise[#]rely[#]knowledge[#]estimation[#]standard[#]deviation[#]Our[#]estimator[#]achieves[#]logarithmic[#]factor[#]optimal[#]rates[#]convergence[#]Frobenius[#]risk[#]thus[#]prediction[#]performance[#]previously[#]proposed[#]estimators[#]based[#]solution[#]convex[#]optimization[#]problem[#]makes[#]computationally[#]attractive" />
        </attvalues>
      </node>
      <node id="Stéphane Gaiffas" label="Stéphane Gaiffas">
        <attvalues>
          <attvalue for="0" value="We[#]propose[#]new[#]pivotal[#]method[#]estimating[#]matrices[#]Assume[#]observe[#]small[#]set[#]entries[#]linear[#]combinations[#]unknown[#]matrix[#]corrupted[#]noise[#]rely[#]knowledge[#]estimation[#]standard[#]deviation[#]Our[#]estimator[#]achieves[#]logarithmic[#]factor[#]optimal[#]rates[#]convergence[#]Frobenius[#]risk[#]thus[#]prediction[#]performance[#]previously[#]proposed[#]estimators[#]based[#]solution[#]convex[#]optimization[#]problem[#]makes[#]computationally[#]attractive" />
        </attvalues>
      </node>
      <node id="Tomonari Sei" label="Tomonari Sei">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]give[#]review[#]method[#]imsets[#]introduced[#]Studeny[#]geometric[#]point[#]view[#]Elementary[#]span[#]polyhedral[#]cone[#]dual[#]supermodular[#]functions[#]We[#]basic[#]facts[#]structure[#]cones[#]Then[#]derive[#]new[#]results[#]following[#]topics[#]extreme[#]rays[#]standardized[#]ii[#]faces[#]iii[#]small[#]relations[#]among[#]elementary[#]iv[#]computational[#]Markov[#]basis[#]toric[#]ideal[#]defined" />
        </attvalues>
      </node>
      <node id="Kentaro Tanaka" label="Kentaro Tanaka">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]give[#]review[#]method[#]imsets[#]introduced[#]Studeny[#]geometric[#]point[#]view[#]Elementary[#]span[#]polyhedral[#]cone[#]dual[#]supermodular[#]functions[#]We[#]basic[#]facts[#]structure[#]cones[#]Then[#]derive[#]new[#]results[#]following[#]topics[#]extreme[#]rays[#]standardized[#]ii[#]faces[#]iii[#]small[#]relations[#]among[#]elementary[#]iv[#]computational[#]Markov[#]basis[#]toric[#]ideal[#]defined" />
        </attvalues>
      </node>
      <node id="Ingo Steinwart" label="Ingo Steinwart">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Andreas Christmann" label="Andreas Christmann">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Christian Genest" label="Christian Genest">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ivan Kojadinovic" label="Ivan Kojadinovic">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Johanna Nešlehová" label="Johanna Nešlehová">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jun Yan" label="Jun Yan">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Cécile Amblard" label="Cécile Amblard">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]semiparametric[#]family[#]bivariate[#]copulas[#]The[#]generated[#]univariate[#]function[#]determining[#]symmetry[#]radial[#]joint[#]dependence[#]property[#]quadrant[#]total[#]positivity[#]We[#]provide[#]bounds[#]different[#]measures[#]association[#]Kendall[#]Tau[#]Spearman[#]Rho[#]several[#]choices[#]generating[#]functions[#]allowing[#]reach" />
        </attvalues>
      </node>
      <node id="Cecília Fonseca" label="Cecília Fonseca">
        <attvalues>
          <attvalue for="0" value="Spatial[#]environmental[#]processes[#]often[#]exhibit[#]dependence[#]large[#]values[#]In[#]order[#]model[#]properties[#]must[#]characterized[#]quantified[#]paper[#]introduce[#]measure[#]evaluates[#]among[#]extreme[#]observations[#]located[#]two[#]separated[#]regions[#]locations[#]We[#]compute[#]range[#]new[#]extends[#]existing[#]concept[#]compare[#]extremal[#]coefficients[#]finding[#]generalizations[#]known[#]relations[#]pairwise[#]approach[#]Estimators[#]introduced[#]asymptotic[#]normality[#]strong[#]consistency[#]shown[#]An[#]application[#]annual[#]maxima[#]precipitation[#]Portuguese[#]presented" />
        </attvalues>
      </node>
      <node id="Luísa Pereira" label="Luísa Pereira">
        <attvalues>
          <attvalue for="0" value="Spatial[#]environmental[#]processes[#]often[#]exhibit[#]dependence[#]large[#]values[#]In[#]order[#]model[#]properties[#]must[#]characterized[#]quantified[#]paper[#]introduce[#]measure[#]evaluates[#]among[#]extreme[#]observations[#]located[#]two[#]separated[#]regions[#]locations[#]We[#]compute[#]range[#]new[#]extends[#]existing[#]concept[#]compare[#]extremal[#]coefficients[#]finding[#]generalizations[#]known[#]relations[#]pairwise[#]approach[#]Estimators[#]introduced[#]asymptotic[#]normality[#]strong[#]consistency[#]shown[#]An[#]application[#]annual[#]maxima[#]precipitation[#]Portuguese[#]presented" />
        </attvalues>
      </node>
      <node id="Ana Paula Martins" label="Ana Paula Martins">
        <attvalues>
          <attvalue for="0" value="Spatial[#]environmental[#]processes[#]often[#]exhibit[#]dependence[#]large[#]values[#]In[#]order[#]model[#]properties[#]must[#]characterized[#]quantified[#]paper[#]introduce[#]measure[#]evaluates[#]among[#]extreme[#]observations[#]located[#]two[#]separated[#]regions[#]locations[#]We[#]compute[#]range[#]new[#]extends[#]existing[#]concept[#]compare[#]extremal[#]coefficients[#]finding[#]generalizations[#]known[#]relations[#]pairwise[#]approach[#]Estimators[#]introduced[#]asymptotic[#]normality[#]strong[#]consistency[#]shown[#]An[#]application[#]annual[#]maxima[#]precipitation[#]Portuguese[#]presented" />
        </attvalues>
      </node>
      <node id="Pierre Alquier" label="Pierre Alquier">
        <attvalues>
          <attvalue for="0" value="The[#]aim[#]paper[#]provide[#]comprehensive[#]introduction[#]study[#]estimators[#]context[#]dependent[#]observations[#]We[#]define[#]general[#]estimator[#]solving[#]problems[#]stochastic[#]optimization[#]This[#]turns[#]LASSO[#]regression[#]estimation[#]setting[#]Powerful[#]theoretical[#]guarantees[#]statistical[#]performances[#]provided[#]recent[#]papers[#]however[#]usually[#]deal[#]iid[#]case[#]Here[#]various[#]dependence[#]assumptions[#]Let[#]Y[#]random[#]pair[#]taking[#]values[#]R[#]In[#]model[#]one[#]T[#]unknown[#]univariate[#]measurable[#]function[#]vector[#]W[#]denotes[#]noise[#]satisfying[#]E[#]known[#]offer[#]flexible[#]way[#]variety[#]phenomena[#]However[#]despite[#]relative[#]simplicity[#]dimension[#]reduction[#]scheme[#]faced[#]severe[#]complications[#]soon[#]underlying[#]becomes[#]larger[#]number[#]p[#]n[#]paradigm[#]To[#]circumvent[#]difficulty[#]consider[#]problem[#]sparsity[#]perspective[#]using[#]approach[#]On[#]side[#]sharp[#]oracle[#]inequality[#]powerful[#]best[#]inequalities[#]common[#]procedures[#]recovery[#]proposed[#]method[#]implemented[#]means[#]reversible[#]jump[#]Markov[#]chain[#]Monte[#]Carlo[#]technique[#]performance[#]compared[#]standard" />
        </attvalues>
      </node>
      <node id="Paul Doukhan" label="Paul Doukhan">
        <attvalues>
          <attvalue for="0" value="The[#]aim[#]paper[#]provide[#]comprehensive[#]introduction[#]study[#]estimators[#]context[#]dependent[#]observations[#]We[#]define[#]general[#]estimator[#]solving[#]problems[#]stochastic[#]optimization[#]This[#]turns[#]LASSO[#]regression[#]estimation[#]setting[#]Powerful[#]theoretical[#]guarantees[#]statistical[#]performances[#]provided[#]recent[#]papers[#]however[#]usually[#]deal[#]iid[#]case[#]Here[#]various[#]dependence[#]assumptions" />
        </attvalues>
      </node>
      <node id="Jérôme Dedecker" label="Jérôme Dedecker">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Adeline Samson" label="Adeline Samson">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marie-Luce Taupin" label="Marie-Luce Taupin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Takumi Saegusa" label="Takumi Saegusa">
        <attvalues>
          <attvalue for="0" value="We[#]develop[#]asymptotic[#]theory[#]weighted[#]likelihood[#]estimators[#]WLE[#]stratified[#]sampling[#]without[#]replacement[#]also[#]consider[#]several[#]variants[#]WLEs[#]involving[#]estimated[#]weights[#]calibration[#]A[#]set[#]empirical[#]process[#]tools[#]developed[#]including[#]theorem[#]rates[#]convergence[#]Donsker[#]inverse[#]probability[#]processes[#]second[#]phase[#]Using[#]general[#]results[#]derive[#]distributions[#]parameter[#]semiparametric[#]model[#]estimator[#]nuisance[#]estimable[#]either[#]regular[#]nonregular[#]illustrate[#]methods[#]Cox[#]right[#]censoring[#]interval[#]compare[#]via[#]variances[#]usual[#]easier[#]analyze[#]assumption[#]Bernoulli" />
        </attvalues>
      </node>
      <node id="Jon A. Wellner" label="Jon A. Wellner">
        <attvalues>
          <attvalue for="0" value="We[#]develop[#]asymptotic[#]theory[#]weighted[#]likelihood[#]estimators[#]WLE[#]stratified[#]sampling[#]without[#]replacement[#]also[#]consider[#]several[#]variants[#]WLEs[#]involving[#]estimated[#]weights[#]calibration[#]A[#]set[#]empirical[#]process[#]tools[#]developed[#]including[#]theorem[#]rates[#]convergence[#]Donsker[#]inverse[#]probability[#]processes[#]second[#]phase[#]Using[#]general[#]results[#]derive[#]distributions[#]parameter[#]semiparametric[#]model[#]estimator[#]nuisance[#]estimable[#]either[#]regular[#]nonregular[#]illustrate[#]methods[#]Cox[#]right[#]censoring[#]interval[#]compare[#]via[#]variances[#]usual[#]easier[#]analyze[#]assumption[#]Bernoulli" />
        </attvalues>
      </node>
      <node id="Olivier Wintenberger" label="Olivier Wintenberger">
        <attvalues>
          <attvalue for="0" value="We[#]introduce[#]notion[#]continuously[#]invertible[#]volatility[#]models[#]relies[#]Lyapunov[#]condition[#]regularity[#]show[#]almost[#]equivalent[#]ability[#]volatilities[#]forecasting[#]using[#]parametric[#]inference[#]approach[#]based[#]SRE[#]given[#]Under[#]weak[#]assumptions[#]prove[#]strong[#]consistency[#]asymptotic[#]normality[#]Based[#]estimation[#]natural[#]strongly[#]consistent[#]forecast[#]apply[#]successfully[#]recover[#]known[#]results[#]univariate[#]multivariate[#]GARCH[#]type[#]EGARCH[#]model[#]soon[#]limiting[#]variance[#]exists[#]Finally[#]give[#]encouraging[#]empirical[#]simulations[#]real[#]data" />
        </attvalues>
      </node>
      <node id="Sixiang Cai" label="Sixiang Cai">
        <attvalues>
          <attvalue for="0" value="We[#]introduce[#]notion[#]continuously[#]invertible[#]volatility[#]models[#]relies[#]Lyapunov[#]condition[#]regularity[#]show[#]almost[#]equivalent[#]ability[#]volatilities[#]forecasting[#]using[#]parametric[#]inference[#]approach[#]based[#]SRE[#]given[#]Under[#]weak[#]assumptions[#]prove[#]strong[#]consistency[#]asymptotic[#]normality[#]Based[#]estimation[#]natural[#]strongly[#]consistent[#]forecast[#]apply[#]successfully[#]recover[#]known[#]results[#]univariate[#]multivariate[#]GARCH[#]type[#]EGARCH[#]model[#]soon[#]limiting[#]variance[#]exists[#]Finally[#]give[#]encouraging[#]empirical[#]simulations[#]real[#]data" />
        </attvalues>
      </node>
      <node id="Apostolos Batsidis" label="Apostolos Batsidis">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Nirian Martín" label="Nirian Martín">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Leandro Pardo" label="Leandro Pardo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Konstantinos Zografos" label="Konstantinos Zografos">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jan Draisma" label="Jan Draisma">
        <attvalues>
          <attvalue for="0" value="A[#]linear[#]structural[#]equation[#]model[#]relates[#]random[#]variables[#]interest[#]corresponding[#]Gaussian[#]noise[#]terms[#]via[#]system[#]Each[#]represented[#]mixed[#]graph[#]directed[#]edges[#]encode[#]equations[#]bidirected[#]indicate[#]possible[#]correlations[#]among[#]We[#]study[#]parameter[#]identifiability[#]models[#]ask[#]conditions[#]ensure[#]edge[#]coefficients[#]appearing[#]uniquely[#]recovered[#]covariance[#]matrix[#]associated[#]distribution[#]treat[#]case[#]generic[#]unique[#]recovery[#]almost[#]every[#]choice[#]parameters[#]give[#]new[#]graphical[#]condition[#]sufficient[#]verified[#]time[#]polynomial[#]size[#]It[#]improves[#]criteria[#]prior[#]work[#]require[#]part[#]acyclic[#]also[#]develop[#]related[#]necessary[#]examine[#]gap[#]simulations[#]graphs[#]nodes[#]well[#]exhaustive[#]algebraic[#]computations[#]five" />
        </attvalues>
      </node>
      <node id="Bo Kai" label="Bo Kai">
        <attvalues>
          <attvalue for="0" value="The[#]complexity[#]semiparametric[#]models[#]poses[#]new[#]challenges[#]statistical[#]inference[#]model[#]selection[#]frequently[#]arise[#]real[#]applications[#]In[#]work[#]propose[#]estimation[#]variable[#]procedures[#]partially[#]linear[#]We[#]first[#]study[#]quantile[#]regression[#]estimates[#]nonparametric[#]functions[#]parametric[#]coefficients[#]To[#]achieve[#]nice[#]efficiency[#]properties[#]develop[#]composite[#]procedure[#]establish[#]asymptotic[#]normality[#]proposed[#]estimators[#]parts[#]show[#]best[#]convergence[#]rate[#]Moreover[#]method[#]much[#]efficient[#]many[#]errors[#]loses[#]small[#]amount[#]normal[#]addition[#]shown[#]loss[#]estimating[#]varying[#]coefficient[#]greater[#]components[#]sparsity[#]covariates[#]adaptive[#]penalization[#]methods[#]prove[#]possess[#]oracle[#]property[#]Extensive[#]Monte[#]Carlo[#]simulation[#]studies[#]conducted[#]examine[#]performance[#]Finally[#]apply[#]analyze[#]plasma[#]level[#]data" />
        </attvalues>
      </node>
      <node id="Runze Li" label="Runze Li">
        <attvalues>
          <attvalue for="0" value="The[#]complexity[#]semiparametric[#]models[#]poses[#]new[#]challenges[#]statistical[#]inference[#]model[#]selection[#]frequently[#]arise[#]real[#]applications[#]In[#]work[#]propose[#]estimation[#]variable[#]procedures[#]partially[#]linear[#]We[#]first[#]study[#]quantile[#]regression[#]estimates[#]nonparametric[#]functions[#]parametric[#]coefficients[#]To[#]achieve[#]nice[#]efficiency[#]properties[#]develop[#]composite[#]procedure[#]establish[#]asymptotic[#]normality[#]proposed[#]estimators[#]parts[#]show[#]best[#]convergence[#]rate[#]Moreover[#]method[#]much[#]efficient[#]many[#]errors[#]loses[#]small[#]amount[#]normal[#]addition[#]shown[#]loss[#]estimating[#]varying[#]coefficient[#]greater[#]components[#]sparsity[#]covariates[#]adaptive[#]penalization[#]methods[#]prove[#]possess[#]oracle[#]property[#]Extensive[#]Monte[#]Carlo[#]simulation[#]studies[#]conducted[#]examine[#]performance[#]Finally[#]apply[#]analyze[#]plasma[#]level[#]data" />
        </attvalues>
      </node>
      <node id="Hui Zou" label="Hui Zou">
        <attvalues>
          <attvalue for="0" value="The[#]complexity[#]semiparametric[#]models[#]poses[#]new[#]challenges[#]statistical[#]inference[#]model[#]selection[#]frequently[#]arise[#]real[#]applications[#]In[#]work[#]propose[#]estimation[#]variable[#]procedures[#]partially[#]linear[#]We[#]first[#]study[#]quantile[#]regression[#]estimates[#]nonparametric[#]functions[#]parametric[#]coefficients[#]To[#]achieve[#]nice[#]efficiency[#]properties[#]develop[#]composite[#]procedure[#]establish[#]asymptotic[#]normality[#]proposed[#]estimators[#]parts[#]show[#]best[#]convergence[#]rate[#]Moreover[#]method[#]much[#]efficient[#]many[#]errors[#]loses[#]small[#]amount[#]normal[#]addition[#]shown[#]loss[#]estimating[#]varying[#]coefficient[#]greater[#]components[#]sparsity[#]covariates[#]adaptive[#]penalization[#]methods[#]prove[#]possess[#]oracle[#]property[#]Extensive[#]Monte[#]Carlo[#]simulation[#]studies[#]conducted[#]examine[#]performance[#]Finally[#]apply[#]analyze[#]plasma[#]level[#]data" />
        </attvalues>
      </node>
      <node id="Shota Gugushvili" label="Shota Gugushvili">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]problem[#]parameter[#]estimation[#]univariate[#]discretely[#]observed[#]ergodic[#]diffusion[#]process[#]given[#]solution[#]stochastic[#]differential[#]equation[#]The[#]procedure[#]propose[#]consists[#]two[#]steps[#]In[#]first[#]step[#]referred[#]smoothing[#]smooth[#]data[#]construct[#]nonparametric[#]estimator[#]invariant[#]density[#]second[#]matching[#]exploit[#]characterisation[#]certain[#]ordinary[#]replace[#]order[#]arrive[#]intuitively[#]appealing[#]criterion[#]function[#]next[#]define[#]interest[#]minimiser[#]Our[#]main[#]results[#]show[#]suitable[#]conditions[#]n[#]even[#]asymptotically[#]normal[#]also[#]discuss[#]way[#]improving[#]asymptotic[#]performance[#]type[#]present[#]small[#]scale[#]simulation" />
        </attvalues>
      </node>
      <node id="Peter Spreij" label="Peter Spreij">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]problem[#]parameter[#]estimation[#]univariate[#]discretely[#]observed[#]ergodic[#]diffusion[#]process[#]given[#]solution[#]stochastic[#]differential[#]equation[#]The[#]procedure[#]propose[#]consists[#]two[#]steps[#]In[#]first[#]step[#]referred[#]smoothing[#]smooth[#]data[#]construct[#]nonparametric[#]estimator[#]invariant[#]density[#]second[#]matching[#]exploit[#]characterisation[#]certain[#]ordinary[#]replace[#]order[#]arrive[#]intuitively[#]appealing[#]criterion[#]function[#]next[#]define[#]interest[#]minimiser[#]Our[#]main[#]results[#]show[#]suitable[#]conditions[#]n[#]even[#]asymptotically[#]normal[#]also[#]discuss[#]way[#]improving[#]asymptotic[#]performance[#]type[#]present[#]small[#]scale[#]simulation" />
        </attvalues>
      </node>
      <node id="Olivier Lopez" label="Olivier Lopez">
        <attvalues>
          <attvalue for="0" value="Consider[#]random[#]vector[#]X[#]Y[#]We[#]assume[#]subject[#]right[#]censoring[#]The[#]aim[#]paper[#]twofold[#]First[#]propose[#]new[#]estimator[#]joint[#]distribution[#]This[#]overcomes[#]common[#]problem[#]using[#]dimension[#]reduction[#]technique[#]Second[#]relation[#]given[#]mean[#]regression[#]single[#]index[#]model[#]parameters[#]asymptotic[#]properties[#]proposed[#]estimators[#]obtained" />
        </attvalues>
      </node>
      <node id="Ingrid Van Keilegom" label="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]general[#]principle[#]estimating[#]regression[#]function[#]nonparametrically[#]allowing[#]wide[#]variety[#]data[#]filtering[#]example[#]repeated[#]left[#]truncation[#]right[#]censoring[#]Both[#]mean[#]median[#]cases[#]considered[#]The[#]method[#]works[#]first[#]conditional[#]hazard[#]survivor[#]integrating[#]also[#]investigate[#]improved[#]methods[#]take[#]account[#]model[#]structure[#]independent[#]errors[#]show[#]improve[#]performance[#]true[#]establish[#]pointwise[#]asymptotic[#]normality[#]estimators[#]Consider[#]random[#]vector[#]X[#]Y[#]assume[#]subject[#]aim[#]paper[#]twofold[#]First[#]propose[#]new[#]estimator[#]joint[#]distribution[#]This[#]overcomes[#]common[#]problem[#]using[#]dimension[#]reduction[#]technique[#]Second[#]relation[#]given[#]single[#]index[#]parameters[#]properties[#]proposed[#]obtained" />
        </attvalues>
      </node>
      <node id="Weining Shen" label="Weining Shen">
        <attvalues>
          <attvalue for="0" value="We[#]show[#]multivariate[#]density[#]estimation[#]performed[#]using[#]Bayesian[#]methods[#]based[#]Dirichlet[#]mixtures[#]normal[#]kernels[#]prior[#]distribution[#]kernel[#]covariance[#]matrix[#]parameter[#]derive[#]sufficient[#]conditions[#]specification[#]guarantee[#]convergence[#]true[#]rate[#]optimal[#]minimax[#]smoothness[#]class[#]belongs[#]No[#]knowledge[#]assumed[#]The[#]shown[#]hold[#]location[#]mixture[#]normals[#]Gaussian[#]base[#]measure[#]Locally[#]older[#]classes[#]anisotropic[#]extensions[#]considered[#]Our[#]study[#]involves[#]several[#]technical[#]novelties[#]including[#]sharp[#]approximation[#]finitely[#]differentiable[#]densities[#]new[#]sieve[#]space" />
        </attvalues>
      </node>
      <node id="Surya T. Tokdar" label="Surya T. Tokdar">
        <attvalues>
          <attvalue for="0" value="We[#]show[#]multivariate[#]density[#]estimation[#]performed[#]using[#]Bayesian[#]methods[#]based[#]Dirichlet[#]mixtures[#]normal[#]kernels[#]prior[#]distribution[#]kernel[#]covariance[#]matrix[#]parameter[#]derive[#]sufficient[#]conditions[#]specification[#]guarantee[#]convergence[#]true[#]rate[#]optimal[#]minimax[#]smoothness[#]class[#]belongs[#]No[#]knowledge[#]assumed[#]The[#]shown[#]hold[#]location[#]mixture[#]normals[#]Gaussian[#]base[#]measure[#]Locally[#]older[#]classes[#]anisotropic[#]extensions[#]considered[#]Our[#]study[#]involves[#]several[#]technical[#]novelties[#]including[#]sharp[#]approximation[#]finitely[#]differentiable[#]densities[#]new[#]sieve[#]space" />
        </attvalues>
      </node>
      <node id="Subhashis Ghosal" label="Subhashis Ghosal">
        <attvalues>
          <attvalue for="0" value="We[#]show[#]multivariate[#]density[#]estimation[#]performed[#]using[#]Bayesian[#]methods[#]based[#]Dirichlet[#]mixtures[#]normal[#]kernels[#]prior[#]distribution[#]kernel[#]covariance[#]matrix[#]parameter[#]derive[#]sufficient[#]conditions[#]specification[#]guarantee[#]convergence[#]true[#]rate[#]optimal[#]minimax[#]smoothness[#]class[#]belongs[#]No[#]knowledge[#]assumed[#]The[#]shown[#]hold[#]location[#]mixture[#]normals[#]Gaussian[#]base[#]measure[#]Locally[#]older[#]classes[#]anisotropic[#]extensions[#]considered[#]Our[#]study[#]involves[#]several[#]technical[#]novelties[#]including[#]sharp[#]approximation[#]finitely[#]differentiable[#]densities[#]new[#]sieve[#]space" />
        </attvalues>
      </node>
      <node id="Tabea Rebafka" label="Tabea Rebafka">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Céline Lévy-Leduc" label="Céline Lévy-Leduc">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Maurice Charbit" label="Maurice Charbit">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Runchu Zhang" label="Runchu Zhang">
        <attvalues>
          <attvalue for="0" value="The[#]study[#]good[#]nonregular[#]fractional[#]factorial[#]designs[#]received[#]significant[#]attention[#]last[#]two[#]decades[#]Recent[#]research[#]indicates[#]constructed[#]quaternary[#]codes[#]QC[#]promising[#]regard[#]present[#]paper[#]shows[#]trigonometric[#]approach[#]facilitate[#]systematic[#]understanding[#]lead[#]new[#]theoretical[#]results[#]covering[#]hitherto[#]unexplored[#]situations[#]We[#]focus[#]fractions[#]factorials[#]show[#]optimal[#]often[#]larger[#]generalized[#]resolution[#]projectivity[#]comparable[#]regular[#]Moreover[#]found[#]maximum[#]among" />
        </attvalues>
      </node>
      <node id="Frederick K. H. Phoa" label="Frederick K. H. Phoa">
        <attvalues>
          <attvalue for="0" value="The[#]study[#]good[#]nonregular[#]fractional[#]factorial[#]designs[#]received[#]significant[#]attention[#]last[#]two[#]decades[#]Recent[#]research[#]indicates[#]constructed[#]quaternary[#]codes[#]QC[#]promising[#]regard[#]present[#]paper[#]shows[#]trigonometric[#]approach[#]facilitate[#]systematic[#]understanding[#]lead[#]new[#]theoretical[#]results[#]covering[#]hitherto[#]unexplored[#]situations[#]We[#]focus[#]fractions[#]factorials[#]show[#]optimal[#]often[#]larger[#]generalized[#]resolution[#]projectivity[#]comparable[#]regular[#]Moreover[#]found[#]maximum[#]among" />
        </attvalues>
      </node>
      <node id="Rahul Mukerjee" label="Rahul Mukerjee">
        <attvalues>
          <attvalue for="0" value="The[#]study[#]good[#]nonregular[#]fractional[#]factorial[#]designs[#]received[#]significant[#]attention[#]last[#]two[#]decades[#]Recent[#]research[#]indicates[#]constructed[#]quaternary[#]codes[#]QC[#]promising[#]regard[#]present[#]paper[#]shows[#]trigonometric[#]approach[#]facilitate[#]systematic[#]understanding[#]lead[#]new[#]theoretical[#]results[#]covering[#]hitherto[#]unexplored[#]situations[#]We[#]focus[#]fractions[#]factorials[#]show[#]optimal[#]often[#]larger[#]generalized[#]resolution[#]projectivity[#]comparable[#]regular[#]Moreover[#]found[#]maximum[#]among" />
        </attvalues>
      </node>
      <node id="Hongquan Xu" label="Hongquan Xu">
        <attvalues>
          <attvalue for="0" value="The[#]study[#]good[#]nonregular[#]fractional[#]factorial[#]designs[#]received[#]significant[#]attention[#]last[#]two[#]decades[#]Recent[#]research[#]indicates[#]constructed[#]quaternary[#]codes[#]QC[#]promising[#]regard[#]present[#]paper[#]shows[#]trigonometric[#]approach[#]facilitate[#]systematic[#]understanding[#]lead[#]new[#]theoretical[#]results[#]covering[#]hitherto[#]unexplored[#]situations[#]We[#]focus[#]fractions[#]factorials[#]show[#]optimal[#]often[#]larger[#]generalized[#]resolution[#]projectivity[#]comparable[#]regular[#]Moreover[#]found[#]maximum[#]among" />
        </attvalues>
      </node>
      <node id="Sébastien Bubeck" label="Sébastien Bubeck">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]hypothesis[#]testing[#]problem[#]deciding[#]whether[#]observed[#]vector[#]independent[#]normal[#]components[#]alternatively[#]small[#]subset[#]correlated[#]The[#]may[#]certain[#]combinatorial[#]structure[#]known[#]statistician[#]establish[#]upper[#]lower[#]bounds[#]minimax[#]risk[#]terms[#]size[#]level[#]correlation[#]class[#]possibly[#]sets[#]show[#]simple[#]tests[#]performance[#]many[#]cases[#]generalized[#]likelihood[#]ratio[#]test[#]suboptimal[#]important[#]setting[#]stochastic[#]bandit[#]problems[#]continuum[#]arms[#]first[#]point[#]strategies[#]considered[#]far[#]literature[#]provided[#]theoretical[#]guarantees[#]form[#]given[#]tuning[#]parameters[#]regret[#]respect[#]environments[#]depends[#]This[#]however[#]right[#]perspective[#]strategy[#]adapt[#]specific[#]environment[#]hand[#]way[#]round[#]Put[#]differently[#]adaptation[#]issue[#]raised[#]solve[#]special[#]case[#]whose[#]functions[#]globally[#]Lipschitz[#]More[#]precisely[#]optimal[#]orders[#]magnitude[#]bound[#]f[#]constant[#]L[#]T[#]time[#]instances[#]achieved[#]without[#]knowing[#]advance[#]contrast[#]previously[#]require[#]extent[#]knowledge[#]achieve[#]guarantee" />
        </attvalues>
      </node>
      <node id="Gilles Stoltz" label="Gilles Stoltz">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]setting[#]stochastic[#]bandit[#]problems[#]continuum[#]arms[#]first[#]point[#]strategies[#]considered[#]far[#]literature[#]provided[#]theoretical[#]guarantees[#]form[#]given[#]tuning[#]parameters[#]regret[#]small[#]respect[#]class[#]environments[#]depends[#]This[#]however[#]right[#]perspective[#]strategy[#]adapt[#]specific[#]environment[#]hand[#]way[#]round[#]Put[#]differently[#]adaptation[#]issue[#]raised[#]solve[#]special[#]case[#]whose[#]functions[#]globally[#]Lipschitz[#]More[#]precisely[#]show[#]minimax[#]optimal[#]orders[#]magnitude[#]bound[#]f[#]constant[#]L[#]T[#]time[#]instances[#]achieved[#]without[#]knowing[#]advance[#]contrast[#]previously[#]known[#]require[#]extent[#]knowledge[#]achieve[#]performance[#]guarantee[#]algorithm[#]problem[#]distributions[#]finite[#]supports[#]necessarily[#]beforehand[#]asymptotic[#]matches[#]lower[#]Our[#]contribution[#]provide[#]analysis[#]get[#]bounds[#]main[#]terms[#]smaller[#]ones[#]algorithms[#]analyses[#]like" />
        </attvalues>
      </node>
      <node id="Jia Yuan Yu" label="Jia Yuan Yu">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]setting[#]stochastic[#]bandit[#]problems[#]continuum[#]arms[#]first[#]point[#]strategies[#]considered[#]far[#]literature[#]provided[#]theoretical[#]guarantees[#]form[#]given[#]tuning[#]parameters[#]regret[#]small[#]respect[#]class[#]environments[#]depends[#]This[#]however[#]right[#]perspective[#]strategy[#]adapt[#]specific[#]environment[#]hand[#]way[#]round[#]Put[#]differently[#]adaptation[#]issue[#]raised[#]solve[#]special[#]case[#]whose[#]functions[#]globally[#]Lipschitz[#]More[#]precisely[#]show[#]minimax[#]optimal[#]orders[#]magnitude[#]bound[#]f[#]constant[#]L[#]T[#]time[#]instances[#]achieved[#]without[#]knowing[#]advance[#]contrast[#]previously[#]known[#]require[#]extent[#]knowledge[#]achieve[#]performance[#]guarantee" />
        </attvalues>
      </node>
      <node id="Samantha Leorato" label="Samantha Leorato">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Pierpaolo De Blasi" label="Pierpaolo De Blasi">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Lancelot F. James" label="Lancelot F. James">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="John W. Lau" label="John W. Lau">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="J. Diebolt" label="J. Diebolt">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]problem[#]estimation[#]Weibull[#]particular[#]propose[#]regression[#]model[#]derive[#]estimator[#]This[#]based[#]approach[#]The[#]asymptotic[#]normality[#]also[#]established[#]A[#]small[#]simulation[#]study[#]provided[#]order[#]prove[#]efficiency" />
        </attvalues>
      </node>
      <node id="L. Gardes" label="L. Gardes">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]problem[#]estimation[#]Weibull[#]particular[#]propose[#]regression[#]model[#]derive[#]estimator[#]This[#]based[#]approach[#]The[#]asymptotic[#]normality[#]also[#]established[#]A[#]small[#]simulation[#]study[#]provided[#]order[#]prove[#]efficiency[#]Sliced[#]Inverse[#]Regression[#]SIR[#]effective[#]method[#]dimension[#]reduction[#]problems[#]original[#]however[#]requires[#]inversion[#]predictors[#]covariance[#]matrix[#]case[#]collinearity[#]sample[#]sizes[#]compared[#]possible[#]regularization[#]technique[#]used[#]Our[#]Fisher[#]Lecture[#]given[#]Cook[#]shown[#]axes[#]interpreted[#]solutions[#]inverse[#]Gaussian[#]prior[#]distribution[#]introduced[#]unknown[#]parameters[#]regularize[#]We[#]show[#]existing[#]regularizations[#]enter[#]framework[#]permits[#]global[#]understanding[#]methods[#]Three[#]new[#]priors[#]proposed[#]leading[#]comparison[#]simulated[#]data" />
        </attvalues>
      </node>
      <node id="S. Girard" label="S. Girard">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]problem[#]estimation[#]Weibull[#]particular[#]propose[#]regression[#]model[#]derive[#]estimator[#]This[#]based[#]approach[#]The[#]asymptotic[#]normality[#]also[#]established[#]A[#]small[#]simulation[#]study[#]provided[#]order[#]prove[#]efficiency[#]Sliced[#]Inverse[#]Regression[#]SIR[#]effective[#]method[#]dimension[#]reduction[#]problems[#]original[#]however[#]requires[#]inversion[#]predictors[#]covariance[#]matrix[#]case[#]collinearity[#]sample[#]sizes[#]compared[#]possible[#]regularization[#]technique[#]used[#]Our[#]Fisher[#]Lecture[#]given[#]Cook[#]shown[#]axes[#]interpreted[#]solutions[#]inverse[#]Gaussian[#]prior[#]distribution[#]introduced[#]unknown[#]parameters[#]regularize[#]We[#]show[#]existing[#]regularizations[#]enter[#]framework[#]permits[#]global[#]understanding[#]methods[#]Three[#]new[#]priors[#]proposed[#]leading[#]comparison[#]simulated[#]data" />
        </attvalues>
      </node>
      <node id="A. Guillou" label="A. Guillou">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]problem[#]estimation[#]Weibull[#]particular[#]propose[#]regression[#]model[#]derive[#]estimator[#]This[#]based[#]approach[#]The[#]asymptotic[#]normality[#]also[#]established[#]A[#]small[#]simulation[#]study[#]provided[#]order[#]prove[#]efficiency" />
        </attvalues>
      </node>
      <node id="S. H. Alizadeh" label="S. H. Alizadeh">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]introduces[#]new[#]parsimonious[#]structure[#]mixture[#]autoregressive[#]models[#]weighting[#]coefficients[#]determined[#]latent[#]random[#]variables[#]following[#]hidden[#]Markov[#]model[#]We[#]propose[#]dynamic[#]programming[#]algorithm[#]application[#]forecasting[#]also[#]derive[#]limiting[#]behavior[#]unconditional[#]first[#]moment[#]process[#]appropriate[#]upper[#]bound[#]value[#]variance[#]considered[#]long[#]run[#]Finally[#]show[#]convergence[#]stability[#]second[#]Further[#]illustrate[#]efficacy[#]proposed[#]simulation" />
        </attvalues>
      </node>
      <node id="S. Rezakhah" label="S. Rezakhah">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]introduces[#]new[#]parsimonious[#]structure[#]mixture[#]autoregressive[#]models[#]weighting[#]coefficients[#]determined[#]latent[#]random[#]variables[#]following[#]hidden[#]Markov[#]model[#]We[#]propose[#]dynamic[#]programming[#]algorithm[#]application[#]forecasting[#]also[#]derive[#]limiting[#]behavior[#]unconditional[#]first[#]moment[#]process[#]appropriate[#]upper[#]bound[#]value[#]variance[#]considered[#]long[#]run[#]Finally[#]show[#]convergence[#]stability[#]second[#]Further[#]illustrate[#]efficacy[#]proposed[#]simulation" />
        </attvalues>
      </node>
      <node id="Suprateek Kundu" label="Suprateek Kundu">
        <attvalues>
          <attvalue for="0" value="Although[#]discrete[#]mixture[#]modeling[#]formed[#]backbone[#]literature[#]Bayesian[#]density[#]estimation[#]well[#]known[#]disadvantages[#]We[#]propose[#]alternative[#]class[#]priors[#]based[#]random[#]nonlinear[#]functions[#]uniform[#]latent[#]variable[#]additive[#]residual[#]The[#]induced[#]prior[#]shown[#]desirable[#]properties[#]including[#]ease[#]centering[#]initial[#]guess[#]large[#]support[#]posterior[#]consistency[#]straightforward[#]computation[#]via[#]Gibbs[#]sampling[#]Some[#]advantages[#]mixtures[#]Dirichlet[#]process[#]Gaussian[#]kernels[#]discussed[#]illustrated[#]simulations[#]epidemiology[#]application" />
        </attvalues>
      </node>
      <node id="Jorge Carlos Román" label="Jorge Carlos Román">
        <attvalues>
          <attvalue for="0" value="Bayesian[#]analysis[#]data[#]general[#]linear[#]mixed[#]model[#]challenging[#]nontrivial[#]prior[#]leads[#]intractable[#]posterior[#]density[#]However[#]conditionally[#]conjugate[#]adopted[#]simple[#]Gibbs[#]sampler[#]employed[#]explore[#]A[#]popular[#]default[#]among[#]priors[#]improper[#]takes[#]product[#]form[#]flat[#]regression[#]parameter[#]power[#]variance[#]components[#]In[#]paper[#]convergence[#]rate[#]corresponding[#]undertaken[#]The[#]main[#]result[#]sufficient[#]condition[#]geometric[#]ergodicity[#]chain[#]This[#]close[#]best[#]possible[#]sense[#]slightly[#]stronger[#]required[#]ensure[#]propriety[#]theory[#]developed[#]extremely[#]important[#]practical[#]standpoint[#]guarantees[#]existence[#]central[#]limit[#]theorems[#]allow[#]computation[#]valid[#]asymptotic[#]standard[#]errors[#]estimates[#]computed[#]using" />
        </attvalues>
      </node>
      <node id="James P. Hobert" label="James P. Hobert">
        <attvalues>
          <attvalue for="0" value="Bayesian[#]analysis[#]data[#]general[#]linear[#]mixed[#]model[#]challenging[#]nontrivial[#]prior[#]leads[#]intractable[#]posterior[#]density[#]However[#]conditionally[#]conjugate[#]adopted[#]simple[#]Gibbs[#]sampler[#]employed[#]explore[#]A[#]popular[#]default[#]among[#]priors[#]improper[#]takes[#]product[#]form[#]flat[#]regression[#]parameter[#]power[#]variance[#]components[#]In[#]paper[#]convergence[#]rate[#]corresponding[#]undertaken[#]The[#]main[#]result[#]sufficient[#]condition[#]geometric[#]ergodicity[#]chain[#]This[#]close[#]best[#]possible[#]sense[#]slightly[#]stronger[#]required[#]ensure[#]propriety[#]theory[#]developed[#]extremely[#]important[#]practical[#]standpoint[#]guarantees[#]existence[#]central[#]limit[#]theorems[#]allow[#]computation[#]valid[#]asymptotic[#]standard[#]errors[#]estimates[#]computed[#]using" />
        </attvalues>
      </node>
      <node id="Gaëlle Chastaing" label="Gaëlle Chastaing">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]regression[#]model[#]built[#]dependent[#]variables[#]This[#]modelizes[#]input[#]output[#]relationship[#]Under[#]boundedness[#]assumptions[#]joint[#]distribution[#]function[#]show[#]generalized[#]decomposition[#]available[#]leads[#]new[#]indices[#]measuring[#]sensitivity[#]respect[#]We[#]also[#]study[#]discuss[#]estimation" />
        </attvalues>
      </node>
      <node id="Fabrice Gamboa" label="Fabrice Gamboa">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]regression[#]model[#]built[#]dependent[#]variables[#]This[#]modelizes[#]input[#]output[#]relationship[#]Under[#]boundedness[#]assumptions[#]joint[#]distribution[#]function[#]show[#]generalized[#]decomposition[#]available[#]leads[#]new[#]indices[#]measuring[#]sensitivity[#]respect[#]We[#]also[#]study[#]discuss[#]estimation[#]using[#]spectral[#]theory[#]Hilbertian[#]operators[#]ARMA[#]Gaussian[#]processes[#]indexed[#]graphs[#]extend[#]Whittle[#]maximum[#]likelihood[#]parameters[#]corresponding[#]density[#]asymptotic[#]optimality[#]expansion[#]certain[#]class[#]characterized[#]case[#]x[#]slowly[#]varying[#]prove[#]LAN[#]property[#]models[#]include[#]particular[#]fractional[#]Brownian[#]motion[#]ARFIMA" />
        </attvalues>
      </node>
      <node id="Clémentine Prieur" label="Clémentine Prieur">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]consider[#]regression[#]model[#]built[#]dependent[#]variables[#]This[#]modelizes[#]input[#]output[#]relationship[#]Under[#]boundedness[#]assumptions[#]joint[#]distribution[#]function[#]show[#]generalized[#]decomposition[#]available[#]leads[#]new[#]indices[#]measuring[#]sensitivity[#]respect[#]We[#]also[#]study[#]discuss[#]estimation" />
        </attvalues>
      </node>
      <node id="Junbum Lee" label="Junbum Lee">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Suhasini Subba Rao" label="Suhasini Subba Rao">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Dan Shen" label="Dan Shen">
        <attvalues>
          <attvalue for="0" value="Sparse[#]Principal[#]Component[#]Analysis[#]PCA[#]methods[#]efficient[#]tools[#]reduce[#]dimension[#]number[#]variables[#]complex[#]data[#]principal[#]components[#]PCs[#]easier[#]interpret[#]conventional[#]loadings[#]zero[#]We[#]study[#]asymptotic[#]properties[#]sparse[#]PC[#]directions[#]scenarios[#]fixed[#]sample[#]size[#]increasing[#]High[#]Dimension[#]Low[#]Sample[#]Size[#]HDLSS[#]Under[#]previously[#]studied[#]spike[#]covariance[#]assumption[#]show[#]remains[#]consistent[#]large[#]condition[#]established[#]broad[#]range[#]small[#]conditions[#]find[#]set[#]sparsity[#]assumptions[#]strongly[#]inconsistent[#]The[#]boundaries[#]region[#]clarified[#]using[#]oracle[#]result" />
        </attvalues>
      </node>
      <node id="Haipeng Shen" label="Haipeng Shen">
        <attvalues>
          <attvalue for="0" value="Sparse[#]Principal[#]Component[#]Analysis[#]PCA[#]methods[#]efficient[#]tools[#]reduce[#]dimension[#]number[#]variables[#]complex[#]data[#]principal[#]components[#]PCs[#]easier[#]interpret[#]conventional[#]loadings[#]zero[#]We[#]study[#]asymptotic[#]properties[#]sparse[#]PC[#]directions[#]scenarios[#]fixed[#]sample[#]size[#]increasing[#]High[#]Dimension[#]Low[#]Sample[#]Size[#]HDLSS[#]Under[#]previously[#]studied[#]spike[#]covariance[#]assumption[#]show[#]remains[#]consistent[#]large[#]condition[#]established[#]broad[#]range[#]small[#]conditions[#]find[#]set[#]sparsity[#]assumptions[#]strongly[#]inconsistent[#]The[#]boundaries[#]region[#]clarified[#]using[#]oracle[#]result" />
        </attvalues>
      </node>
      <node id="J. S. Marron" label="J. S. Marron">
        <attvalues>
          <attvalue for="0" value="Sparse[#]Principal[#]Component[#]Analysis[#]PCA[#]methods[#]efficient[#]tools[#]reduce[#]dimension[#]number[#]variables[#]complex[#]data[#]principal[#]components[#]PCs[#]easier[#]interpret[#]conventional[#]loadings[#]zero[#]We[#]study[#]asymptotic[#]properties[#]sparse[#]PC[#]directions[#]scenarios[#]fixed[#]sample[#]size[#]increasing[#]High[#]Dimension[#]Low[#]Sample[#]Size[#]HDLSS[#]Under[#]previously[#]studied[#]spike[#]covariance[#]assumption[#]show[#]remains[#]consistent[#]large[#]condition[#]established[#]broad[#]range[#]small[#]conditions[#]find[#]set[#]sparsity[#]assumptions[#]strongly[#]inconsistent[#]The[#]boundaries[#]region[#]clarified[#]using[#]oracle[#]result" />
        </attvalues>
      </node>
      <node id="Ludovic Menneteau" label="Ludovic Menneteau">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]give[#]sufficient[#]conditions[#]establish[#]central[#]limit[#]theorems[#]boundary[#]estimates[#]Poisson[#]point[#]processes[#]The[#]considered[#]obtained[#]smoothing[#]bias[#]corrected[#]extreme[#]values[#]process[#]We[#]show[#]leads[#]Gaussian[#]asymptotic[#]distributions[#]therefore[#]pointwise[#]confidence[#]intervals[#]Some[#]new[#]unidimensional[#]multidimensional[#]examples[#]provided" />
        </attvalues>
      </node>
      <node id="Zhan Wang" label="Zhan Wang">
        <attvalues>
          <attvalue for="0" value="Given[#]dictionary[#]initial[#]estimates[#]unknown[#]true[#]regression[#]function[#]aim[#]construct[#]linearly[#]aggregated[#]estimators[#]target[#]best[#]performance[#]among[#]linear[#]combinations[#]sparse[#]q[#]constraint[#]coefficients[#]Besides[#]identifying[#]optimal[#]rates[#]aggregation[#]problems[#]universal[#]strategies[#]model[#]mixing[#]selection[#]achieve[#]simultaneously[#]full[#]range[#]general[#]upper[#]bound[#]Both[#]random[#]fixed[#]designs[#]known[#]error[#]variance[#]handled[#]examined[#]work[#]cover[#]major[#]types[#]previously[#]studied[#]literature[#]Consequences[#]adaptive[#]also[#]provided[#]Our[#]results[#]show[#]minimax[#]rate[#]basically[#]determined[#]effective[#]size[#]sparsity[#]index[#]depends[#]sample[#]n[#]easily[#]interpretable[#]way[#]based[#]classical[#]theory[#]deals[#]large[#]number[#]models[#]In[#]addition[#]design[#]case[#]approach[#]seen[#]yield[#]convergence[#]expectation[#]exponential[#]decay[#]deviation[#]probability[#]contrast[#]leading[#]constant[#]one[#]front[#]risk[#]oracle[#]inequality[#]offering[#]optimality" />
        </attvalues>
      </node>
      <node id="Sandra Paterlini" label="Sandra Paterlini">
        <attvalues>
          <attvalue for="0" value="Given[#]dictionary[#]initial[#]estimates[#]unknown[#]true[#]regression[#]function[#]aim[#]construct[#]linearly[#]aggregated[#]estimators[#]target[#]best[#]performance[#]among[#]linear[#]combinations[#]sparse[#]q[#]constraint[#]coefficients[#]Besides[#]identifying[#]optimal[#]rates[#]aggregation[#]problems[#]universal[#]strategies[#]model[#]mixing[#]selection[#]achieve[#]simultaneously[#]full[#]range[#]general[#]upper[#]bound[#]Both[#]random[#]fixed[#]designs[#]known[#]error[#]variance[#]handled[#]examined[#]work[#]cover[#]major[#]types[#]previously[#]studied[#]literature[#]Consequences[#]adaptive[#]also[#]provided[#]Our[#]results[#]show[#]minimax[#]rate[#]basically[#]determined[#]effective[#]size[#]sparsity[#]index[#]depends[#]sample[#]n[#]easily[#]interpretable[#]way[#]based[#]classical[#]theory[#]deals[#]large[#]number[#]models[#]In[#]addition[#]design[#]case[#]approach[#]seen[#]yield[#]convergence[#]expectation[#]exponential[#]decay[#]deviation[#]probability[#]contrast[#]leading[#]constant[#]one[#]front[#]risk[#]oracle[#]inequality[#]offering[#]optimality" />
        </attvalues>
      </node>
      <node id="Frank Gao" label="Frank Gao">
        <attvalues>
          <attvalue for="0" value="Given[#]dictionary[#]initial[#]estimates[#]unknown[#]true[#]regression[#]function[#]aim[#]construct[#]linearly[#]aggregated[#]estimators[#]target[#]best[#]performance[#]among[#]linear[#]combinations[#]sparse[#]q[#]constraint[#]coefficients[#]Besides[#]identifying[#]optimal[#]rates[#]aggregation[#]problems[#]universal[#]strategies[#]model[#]mixing[#]selection[#]achieve[#]simultaneously[#]full[#]range[#]general[#]upper[#]bound[#]Both[#]random[#]fixed[#]designs[#]known[#]error[#]variance[#]handled[#]examined[#]work[#]cover[#]major[#]types[#]previously[#]studied[#]literature[#]Consequences[#]adaptive[#]also[#]provided[#]Our[#]results[#]show[#]minimax[#]rate[#]basically[#]determined[#]effective[#]size[#]sparsity[#]index[#]depends[#]sample[#]n[#]easily[#]interpretable[#]way[#]based[#]classical[#]theory[#]deals[#]large[#]number[#]models[#]In[#]addition[#]design[#]case[#]approach[#]seen[#]yield[#]convergence[#]expectation[#]exponential[#]decay[#]deviation[#]probability[#]contrast[#]leading[#]constant[#]one[#]front[#]risk[#]oracle[#]inequality[#]offering[#]optimality" />
        </attvalues>
      </node>
      <node id="Yuhong Yang" label="Yuhong Yang">
        <attvalues>
          <attvalue for="0" value="Given[#]dictionary[#]initial[#]estimates[#]unknown[#]true[#]regression[#]function[#]aim[#]construct[#]linearly[#]aggregated[#]estimators[#]target[#]best[#]performance[#]among[#]linear[#]combinations[#]sparse[#]q[#]constraint[#]coefficients[#]Besides[#]identifying[#]optimal[#]rates[#]aggregation[#]problems[#]universal[#]strategies[#]model[#]mixing[#]selection[#]achieve[#]simultaneously[#]full[#]range[#]general[#]upper[#]bound[#]Both[#]random[#]fixed[#]designs[#]known[#]error[#]variance[#]handled[#]examined[#]work[#]cover[#]major[#]types[#]previously[#]studied[#]literature[#]Consequences[#]adaptive[#]also[#]provided[#]Our[#]results[#]show[#]minimax[#]rate[#]basically[#]determined[#]effective[#]size[#]sparsity[#]index[#]depends[#]sample[#]n[#]easily[#]interpretable[#]way[#]based[#]classical[#]theory[#]deals[#]large[#]number[#]models[#]In[#]addition[#]design[#]case[#]approach[#]seen[#]yield[#]convergence[#]expectation[#]exponential[#]decay[#]deviation[#]probability[#]contrast[#]leading[#]constant[#]one[#]front[#]risk[#]oracle[#]inequality[#]offering[#]optimality" />
        </attvalues>
      </node>
      <node id="Thibault Espinasse" label="Thibault Espinasse">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]using[#]spectral[#]theory[#]Hilbertian[#]operators[#]study[#]ARMA[#]Gaussian[#]processes[#]indexed[#]graphs[#]We[#]extend[#]Whittle[#]maximum[#]likelihood[#]estimation[#]parameters[#]corresponding[#]density[#]show[#]asymptotic[#]optimality" />
        </attvalues>
      </node>
      <node id="Jean-Michel Loubes" label="Jean-Michel Loubes">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]using[#]spectral[#]theory[#]Hilbertian[#]operators[#]study[#]ARMA[#]Gaussian[#]processes[#]indexed[#]graphs[#]We[#]extend[#]Whittle[#]maximum[#]likelihood[#]estimation[#]parameters[#]corresponding[#]density[#]show[#]asymptotic[#]optimality[#]expansion[#]certain[#]class[#]characterized[#]consider[#]case[#]x[#]slowly[#]varying[#]function[#]prove[#]LAN[#]property[#]models[#]include[#]particular[#]fractional[#]Brownian[#]motion[#]ARFIMA[#]model[#]selection[#]estimator[#]covariance[#]random[#]process[#]Using[#]Unbiased[#]Risk[#]Estimation[#]URE[#]method[#]build[#]risk[#]allows[#]select[#]collection[#]Then[#]present[#]oracle[#]inequality[#]ensures[#]selected[#]close[#]Simulations[#]efficiency[#]methodology[#]tackle[#]problem[#]comparing[#]distributions[#]variables[#]defining[#]mean[#]pattern[#]sample[#]events[#]barycenters[#]measures[#]Wasserstein[#]space[#]propose[#]iterative[#]version[#]distribution[#]Moreover[#]common[#]measure[#]warped[#]centered[#]operator[#]barycenter[#]enables[#]recover[#]template" />
        </attvalues>
      </node>
      <node id="Fuqing Gao" label="Fuqing Gao">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Xingqiu Zhao" label="Xingqiu Zhao">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Christophe Andrieu" label="Christophe Andrieu">
        <attvalues>
          <attvalue for="0" value="Let[#]P[#]E[#]space[#]probability[#]measures[#]measurable[#]In[#]paper[#]introduce[#]class[#]nonlinear[#]Markov[#]chain[#]Monte[#]Carlo[#]MCMC[#]methods[#]simulating[#]measure[#]Nonlinear[#]kernels[#]see[#]Feynman[#]Kac[#]Formulae[#]Genealogical[#]Interacting[#]Particle[#]Systems[#]Applications[#]Springer[#]K[#]constructed[#]sense[#]improve[#]However[#]simulated[#]exactly[#]approximations[#]using[#]auxiliary[#]potentially[#]chains[#]Several[#]presented[#]demonstrated[#]conditions[#]associated[#]exhibit[#]strong[#]law[#]large[#]numbers[#]proof[#]technique[#]via[#]Poisson[#]equation[#]Foster[#]Lyapunov[#]We[#]investigate[#]performance[#]simulations" />
        </attvalues>
      </node>
      <node id="Ajay Jasra" label="Ajay Jasra">
        <attvalues>
          <attvalue for="0" value="Let[#]P[#]E[#]space[#]probability[#]measures[#]measurable[#]In[#]paper[#]introduce[#]class[#]nonlinear[#]Markov[#]chain[#]Monte[#]Carlo[#]MCMC[#]methods[#]simulating[#]measure[#]Nonlinear[#]kernels[#]see[#]Feynman[#]Kac[#]Formulae[#]Genealogical[#]Interacting[#]Particle[#]Systems[#]Applications[#]Springer[#]K[#]constructed[#]sense[#]improve[#]However[#]simulated[#]exactly[#]approximations[#]using[#]auxiliary[#]potentially[#]chains[#]Several[#]presented[#]demonstrated[#]conditions[#]associated[#]exhibit[#]strong[#]law[#]large[#]numbers[#]proof[#]technique[#]via[#]Poisson[#]equation[#]Foster[#]Lyapunov[#]We[#]investigate[#]performance[#]simulations" />
        </attvalues>
      </node>
      <node id="Arnaud Doucet" label="Arnaud Doucet">
        <attvalues>
          <attvalue for="0" value="Let[#]P[#]E[#]space[#]probability[#]measures[#]measurable[#]In[#]paper[#]introduce[#]class[#]nonlinear[#]Markov[#]chain[#]Monte[#]Carlo[#]MCMC[#]methods[#]simulating[#]measure[#]Nonlinear[#]kernels[#]see[#]Feynman[#]Kac[#]Formulae[#]Genealogical[#]Interacting[#]Particle[#]Systems[#]Applications[#]Springer[#]K[#]constructed[#]sense[#]improve[#]However[#]simulated[#]exactly[#]approximations[#]using[#]auxiliary[#]potentially[#]chains[#]Several[#]presented[#]demonstrated[#]conditions[#]associated[#]exhibit[#]strong[#]law[#]large[#]numbers[#]proof[#]technique[#]via[#]Poisson[#]equation[#]Foster[#]Lyapunov[#]We[#]investigate[#]performance[#]simulations" />
        </attvalues>
      </node>
      <node id="Pierre Del Moral" label="Pierre Del Moral">
        <attvalues>
          <attvalue for="0" value="Let[#]P[#]E[#]space[#]probability[#]measures[#]measurable[#]In[#]paper[#]introduce[#]class[#]nonlinear[#]Markov[#]chain[#]Monte[#]Carlo[#]MCMC[#]methods[#]simulating[#]measure[#]Nonlinear[#]kernels[#]see[#]Feynman[#]Kac[#]Formulae[#]Genealogical[#]Interacting[#]Particle[#]Systems[#]Applications[#]Springer[#]K[#]constructed[#]sense[#]improve[#]However[#]simulated[#]exactly[#]approximations[#]using[#]auxiliary[#]potentially[#]chains[#]Several[#]presented[#]demonstrated[#]conditions[#]associated[#]exhibit[#]strong[#]law[#]large[#]numbers[#]proof[#]technique[#]via[#]Poisson[#]equation[#]Foster[#]Lyapunov[#]We[#]investigate[#]performance[#]simulations" />
        </attvalues>
      </node>
      <node id="Christophe Giraud" label="Christophe Giraud">
        <attvalues>
          <attvalue for="0" value="We[#]review[#]recent[#]results[#]sparse[#]linear[#]regression[#]practical[#]case[#]unknown[#]variance[#]Different[#]sparsity[#]settings[#]covered[#]including[#]The[#]emphasis[#]put[#]analyses[#]feasible[#]procedures[#]In[#]addition[#]small[#]numerical[#]study[#]compares[#]performance[#]three[#]schemes[#]tuning[#]Lasso[#]estimator[#]references[#]collected[#]general[#]models[#]multivariate[#]nonparametric" />
        </attvalues>
      </node>
      <node id="Sylvie Huet" label="Sylvie Huet">
        <attvalues>
          <attvalue for="0" value="We[#]review[#]recent[#]results[#]sparse[#]linear[#]regression[#]practical[#]case[#]unknown[#]variance[#]Different[#]sparsity[#]settings[#]covered[#]including[#]The[#]emphasis[#]put[#]analyses[#]feasible[#]procedures[#]In[#]addition[#]small[#]numerical[#]study[#]compares[#]performance[#]three[#]schemes[#]tuning[#]Lasso[#]estimator[#]references[#]collected[#]general[#]models[#]multivariate[#]nonparametric" />
        </attvalues>
      </node>
      <node id="Nicolas Verzelen" label="Nicolas Verzelen">
        <attvalues>
          <attvalue for="0" value="We[#]review[#]recent[#]results[#]sparse[#]linear[#]regression[#]practical[#]case[#]unknown[#]variance[#]Different[#]sparsity[#]settings[#]covered[#]including[#]The[#]emphasis[#]put[#]analyses[#]feasible[#]procedures[#]In[#]addition[#]small[#]numerical[#]study[#]compares[#]performance[#]three[#]schemes[#]tuning[#]Lasso[#]estimator[#]references[#]collected[#]general[#]models[#]multivariate[#]nonparametric" />
        </attvalues>
      </node>
      <node id="Yichao Wu" label="Yichao Wu">
        <attvalues>
          <attvalue for="0" value="Functional[#]linear[#]regression[#]analysis[#]aims[#]model[#]relations[#]include[#]functional[#]predictor[#]The[#]analog[#]parameter[#]vector[#]matrix[#]conventional[#]multivariate[#]models[#]function[#]one[#]two[#]arguments[#]If[#]addition[#]scalar[#]predictors[#]often[#]case[#]applications[#]longitudinal[#]studies[#]question[#]arises[#]incorporate[#]We[#]study[#]approach[#]covariates[#]modeled[#]additional[#]This[#]extension[#]analogous[#]shares[#]advantages[#]increased[#]flexibility[#]however[#]details[#]challenging[#]Our[#]methodology[#]combines[#]smoothing[#]methods[#]regularization[#]truncation[#]finite[#]number[#]principal[#]components[#]A[#]practical[#]version[#]developed[#]shown[#]perform[#]better[#]data[#]investigate[#]asymptotic[#]properties[#]establish[#]consistency" />
        </attvalues>
      </node>
      <node id="Hans-Georg Müller" label="Hans-Georg Müller">
        <attvalues>
          <attvalue for="0" value="Functional[#]linear[#]regression[#]analysis[#]aims[#]model[#]relations[#]include[#]functional[#]predictor[#]The[#]analog[#]parameter[#]vector[#]matrix[#]conventional[#]multivariate[#]models[#]function[#]one[#]two[#]arguments[#]If[#]addition[#]scalar[#]predictors[#]often[#]case[#]applications[#]longitudinal[#]studies[#]question[#]arises[#]incorporate[#]We[#]study[#]approach[#]covariates[#]modeled[#]additional[#]This[#]extension[#]analogous[#]shares[#]advantages[#]increased[#]flexibility[#]however[#]details[#]challenging[#]Our[#]methodology[#]combines[#]smoothing[#]methods[#]regularization[#]truncation[#]finite[#]number[#]principal[#]components[#]A[#]practical[#]version[#]developed[#]shown[#]perform[#]better[#]data[#]investigate[#]asymptotic[#]properties[#]establish[#]consistency[#]situation[#]dependent[#]independent[#]variables[#]stochastic[#]processes[#]Questions[#]concerning[#]definition[#]existence[#]corresponding[#]basic[#]explored[#]derive[#]representation[#]terms[#]canonical[#]involved[#]establishes[#]connection[#]suggests[#]alternative[#]approaches[#]implementation[#]specific[#]procedure[#]estimation[#]using[#]expansions[#]proposed[#]compared[#]established[#]component[#]As[#]example[#]application[#]present[#]mortality[#]cohorts[#]medflies[#]obtained[#]experimental[#]aging[#]longevity" />
        </attvalues>
      </node>
      <node id="B. T. Knapik" label="B. T. Knapik">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]Bayesian[#]approach[#]recovering[#]initial[#]condition[#]heat[#]equation[#]noisy[#]observations[#]solution[#]later[#]time[#]consider[#]class[#]prior[#]distributions[#]indexed[#]parameter[#]quantifying[#]smoothness[#]show[#]corresponding[#]posterior[#]contract[#]around[#]true[#]rate[#]depends[#]scale[#]Correct[#]combinations[#]characteristics[#]lead[#]optimal[#]minimax[#]One[#]type[#]priors[#]leads[#]procedure[#]The[#]frequentist[#]coverage[#]credible[#]sets[#]shown[#]depend[#]combination[#]well[#]smoother[#]leading[#]zero[#]rougher[#]extremely[#]conservative[#]results[#]In[#]latter[#]case[#]much[#]larger[#]confidence[#]ratio[#]diameters[#]diverges[#]infinity[#]numerically[#]illustrated[#]simulated[#]data[#]example" />
        </attvalues>
      </node>
      <node id="A. W. van der Vaart" label="A. W. van der Vaart">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]Bayesian[#]approach[#]recovering[#]initial[#]condition[#]heat[#]equation[#]noisy[#]observations[#]solution[#]later[#]time[#]consider[#]class[#]prior[#]distributions[#]indexed[#]parameter[#]quantifying[#]smoothness[#]show[#]corresponding[#]posterior[#]contract[#]around[#]true[#]rate[#]depends[#]scale[#]Correct[#]combinations[#]characteristics[#]lead[#]optimal[#]minimax[#]One[#]type[#]priors[#]leads[#]procedure[#]The[#]frequentist[#]coverage[#]credible[#]sets[#]shown[#]depend[#]combination[#]well[#]smoother[#]leading[#]zero[#]rougher[#]extremely[#]conservative[#]results[#]In[#]latter[#]case[#]much[#]larger[#]confidence[#]ratio[#]diameters[#]diverges[#]infinity[#]numerically[#]illustrated[#]simulated[#]data[#]example" />
        </attvalues>
      </node>
      <node id="J. H. van Zanten" label="J. H. van Zanten">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]Bayesian[#]approach[#]recovering[#]initial[#]condition[#]heat[#]equation[#]noisy[#]observations[#]solution[#]later[#]time[#]consider[#]class[#]prior[#]distributions[#]indexed[#]parameter[#]quantifying[#]smoothness[#]show[#]corresponding[#]posterior[#]contract[#]around[#]true[#]rate[#]depends[#]scale[#]Correct[#]combinations[#]characteristics[#]lead[#]optimal[#]minimax[#]One[#]type[#]priors[#]leads[#]procedure[#]The[#]frequentist[#]coverage[#]credible[#]sets[#]shown[#]depend[#]combination[#]well[#]smoother[#]leading[#]zero[#]rougher[#]extremely[#]conservative[#]results[#]In[#]latter[#]case[#]much[#]larger[#]confidence[#]ratio[#]diameters[#]diverges[#]infinity[#]numerically[#]illustrated[#]simulated[#]data[#]example" />
        </attvalues>
      </node>
      <node id="Azaïs Romain" label="Azaïs Romain">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gégout-Petit Anne" label="Gégout-Petit Anne">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Saracco Jérôme" label="Saracco Jérôme">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Nicolai Meinshausen" label="Nicolai Meinshausen">
        <attvalues>
          <attvalue for="0" value="Test[#]statistics[#]often[#]strongly[#]dependent[#]multiple[#]testing[#]applications[#]Most[#]corrections[#]multiplicity[#]unduly[#]conservative[#]correlated[#]test[#]resulting[#]loss[#]power[#]detect[#]true[#]positives[#]We[#]show[#]Westfall[#]Young[#]permutation[#]method[#]asymptotically[#]optimal[#]broad[#]class[#]problems[#]sparsity[#]structure[#]among[#]tests[#]number[#]tends[#]infinity" />
        </attvalues>
      </node>
      <node id="Marloes H. Maathuis" label="Marloes H. Maathuis">
        <attvalues>
          <attvalue for="0" value="Test[#]statistics[#]often[#]strongly[#]dependent[#]multiple[#]testing[#]applications[#]Most[#]corrections[#]multiplicity[#]unduly[#]conservative[#]correlated[#]test[#]resulting[#]loss[#]power[#]detect[#]true[#]positives[#]We[#]show[#]Westfall[#]Young[#]permutation[#]method[#]asymptotically[#]optimal[#]broad[#]class[#]problems[#]sparsity[#]structure[#]among[#]tests[#]number[#]tends[#]infinity" />
        </attvalues>
      </node>
      <node id="Peter Bühlmann" label="Peter Bühlmann">
        <attvalues>
          <attvalue for="0" value="Test[#]statistics[#]often[#]strongly[#]dependent[#]multiple[#]testing[#]applications[#]Most[#]corrections[#]multiplicity[#]unduly[#]conservative[#]correlated[#]test[#]resulting[#]loss[#]power[#]detect[#]true[#]positives[#]We[#]show[#]Westfall[#]Young[#]permutation[#]method[#]asymptotically[#]optimal[#]broad[#]class[#]problems[#]sparsity[#]structure[#]among[#]tests[#]number[#]tends[#]infinity" />
        </attvalues>
      </node>
      <node id="Guillermo Henry" label="Guillermo Henry">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Andrés Muñoz" label="Andrés Muñoz">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Daniela Rodriguez" label="Daniela Rodriguez">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexandre Lung-Yut-Fong" label="Alexandre Lung-Yut-Fong">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Olivier Cappé" label="Olivier Cappé">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Matthieu Solnon" label="Matthieu Solnon">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]kernel[#]multiple[#]ridge[#]regression[#]framework[#]refer[#]using[#]penalization[#]techniques[#]The[#]theoretical[#]analysis[#]problem[#]shows[#]key[#]element[#]appearing[#]optimal[#]calibration[#]covariance[#]matrix[#]noise[#]different[#]tasks[#]We[#]present[#]new[#]algorithm[#]estimate[#]based[#]concept[#]minimal[#]penalty[#]previously[#]used[#]variance[#]show[#]setting[#]mild[#]assumptions[#]target[#]function[#]estimator[#]converges[#]towards[#]Then[#]plugging[#]corresponding[#]ideal[#]leads[#]oracle[#]inequality[#]illustrate[#]behavior[#]synthetic[#]examples" />
        </attvalues>
      </node>
      <node id="Sylvain Arlot" label="Sylvain Arlot">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]kernel[#]multiple[#]ridge[#]regression[#]framework[#]refer[#]using[#]penalization[#]techniques[#]The[#]theoretical[#]analysis[#]problem[#]shows[#]key[#]element[#]appearing[#]optimal[#]calibration[#]covariance[#]matrix[#]noise[#]different[#]tasks[#]We[#]present[#]new[#]algorithm[#]estimate[#]based[#]concept[#]minimal[#]penalty[#]previously[#]used[#]variance[#]show[#]setting[#]mild[#]assumptions[#]target[#]function[#]estimator[#]converges[#]towards[#]Then[#]plugging[#]corresponding[#]ideal[#]leads[#]oracle[#]inequality[#]illustrate[#]behavior[#]synthetic[#]examples" />
        </attvalues>
      </node>
      <node id="Francis Bach" label="Francis Bach">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]study[#]kernel[#]multiple[#]ridge[#]regression[#]framework[#]refer[#]using[#]penalization[#]techniques[#]The[#]theoretical[#]analysis[#]problem[#]shows[#]key[#]element[#]appearing[#]optimal[#]calibration[#]covariance[#]matrix[#]noise[#]different[#]tasks[#]We[#]present[#]new[#]algorithm[#]estimate[#]based[#]concept[#]minimal[#]penalty[#]previously[#]used[#]variance[#]show[#]setting[#]mild[#]assumptions[#]target[#]function[#]estimator[#]converges[#]towards[#]Then[#]plugging[#]corresponding[#]ideal[#]leads[#]oracle[#]inequality[#]illustrate[#]behavior[#]synthetic[#]examples" />
        </attvalues>
      </node>
      <node id="Servane Gey" label="Servane Gey">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Tristan Mary-Huard" label="Tristan Mary-Huard">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Han Xiao" label="Han Xiao">
        <attvalues>
          <attvalue for="0" value="We[#]obtain[#]sharp[#]convergence[#]rate[#]banded[#]covariance[#]matrix[#]estimates[#]stationary[#]processes[#]A[#]precise[#]order[#]magnitude[#]derived[#]spectral[#]radius[#]sample[#]matrices[#]also[#]consider[#]thresholded[#]estimator[#]better[#]characterize[#]sparsity[#]true[#]sparse[#]As[#]main[#]tool[#]implement[#]Toeplitz[#]Math[#]Ann[#]idea[#]relate[#]eigenvalues[#]densities[#]Fourier[#]transforms[#]covariances[#]develop[#]large[#]deviation[#]result[#]quadratic[#]forms[#]using[#]approximation[#]framework[#]causal[#]representation[#]physical[#]dependence[#]measures" />
        </attvalues>
      </node>
      <node id="Wei Biao Wu" label="Wei Biao Wu">
        <attvalues>
          <attvalue for="0" value="We[#]obtain[#]sharp[#]convergence[#]rate[#]banded[#]covariance[#]matrix[#]estimates[#]stationary[#]processes[#]A[#]precise[#]order[#]magnitude[#]derived[#]spectral[#]radius[#]sample[#]matrices[#]also[#]consider[#]thresholded[#]estimator[#]better[#]characterize[#]sparsity[#]true[#]sparse[#]As[#]main[#]tool[#]implement[#]Toeplitz[#]Math[#]Ann[#]idea[#]relate[#]eigenvalues[#]densities[#]Fourier[#]transforms[#]covariances[#]develop[#]large[#]deviation[#]result[#]quadratic[#]forms[#]using[#]approximation[#]framework[#]causal[#]representation[#]physical[#]dependence[#]measures[#]Large[#]moderate[#]probabilities[#]play[#]important[#]role[#]many[#]applied[#]areas[#]insurance[#]risk[#]analysis[#]This[#]paper[#]studies[#]exact[#]asymptotics[#]form[#]linear[#]independent[#]innovations[#]The[#]analyze[#]general[#]therefore[#]include[#]long[#]memory[#]case[#]give[#]asymptotic[#]probability[#]tail[#]normalized[#]sums[#]specify[#]zones[#]approximated[#]either[#]standard[#]normal[#]distribution[#]marginal[#]innovation[#]process[#]results[#]regression[#]moving[#]averages[#]fractionally[#]integrated[#]regularly[#]varying[#]exponents[#]functions[#]computation[#]value[#]expected[#]shortfall[#]fundamental[#]quantities[#]theory[#]finance" />
        </attvalues>
      </node>
      <node id="Christoph Breunig" label="Christoph Breunig">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]problem[#]estimating[#]value[#]l[#]linear[#]functional[#]structural[#]function[#]models[#]nonparametric[#]relationship[#]presence[#]instrumental[#]variables[#]propose[#]estimator[#]based[#]dimension[#]reduction[#]technique[#]additional[#]thresholding[#]It[#]shown[#]consistent[#]attain[#]minimax[#]optimal[#]rate[#]convergence[#]regularity[#]conditions[#]This[#]however[#]requires[#]choice[#]parameter[#]depending[#]certain[#]characteristics[#]joint[#]distribution[#]regressor[#]instrument[#]unknown[#]practice[#]fully[#]data[#]driven[#]combines[#]model[#]selection[#]Lepski[#]method[#]show[#]adaptive[#]attains[#]logarithmic[#]factor[#]The[#]theory[#]paper[#]illustrated[#]considering[#]classical[#]smoothness[#]assumptions[#]discuss[#]examples[#]pointwise[#]estimation[#]averages" />
        </attvalues>
      </node>
      <node id="Laurens de Haan" label="Laurens de Haan">
        <attvalues>
          <attvalue for="0" value="The[#]climate[#]change[#]dispute[#]changes[#]time[#]environmental[#]characteristics[#]rainfall[#]Some[#]people[#]say[#]possible[#]much[#]mean[#]rather[#]extreme[#]phenomena[#]average[#]may[#]heavy[#]storms[#]become[#]less[#]frequent[#]paper[#]studies[#]probability[#]high[#]threshold[#]exceeded[#]model[#]need[#]specified[#]results[#]hold[#]For[#]simplicity[#]certain[#]linear[#]trend[#]studied[#]depending[#]one[#]real[#]parameter[#]Estimation[#]testing[#]procedures[#]developed[#]Simulation[#]presented[#]method[#]applied[#]trends[#]gauging[#]stations[#]across[#]Germany[#]Netherlands[#]A[#]tentative[#]conclusion[#]seems[#]depend[#]whether[#]station[#]close[#]sea" />
        </attvalues>
      </node>
      <node id="Albert Klein Tank" label="Albert Klein Tank">
        <attvalues>
          <attvalue for="0" value="The[#]climate[#]change[#]dispute[#]changes[#]time[#]environmental[#]characteristics[#]rainfall[#]Some[#]people[#]say[#]possible[#]much[#]mean[#]rather[#]extreme[#]phenomena[#]average[#]may[#]heavy[#]storms[#]become[#]less[#]frequent[#]paper[#]studies[#]probability[#]high[#]threshold[#]exceeded[#]model[#]need[#]specified[#]results[#]hold[#]For[#]simplicity[#]certain[#]linear[#]trend[#]studied[#]depending[#]one[#]real[#]parameter[#]Estimation[#]testing[#]procedures[#]developed[#]Simulation[#]presented[#]method[#]applied[#]trends[#]gauging[#]stations[#]across[#]Germany[#]Netherlands[#]A[#]tentative[#]conclusion[#]seems[#]depend[#]whether[#]station[#]close[#]sea" />
        </attvalues>
      </node>
      <node id="Cláudia Neves" label="Cláudia Neves">
        <attvalues>
          <attvalue for="0" value="The[#]climate[#]change[#]dispute[#]changes[#]time[#]environmental[#]characteristics[#]rainfall[#]Some[#]people[#]say[#]possible[#]much[#]mean[#]rather[#]extreme[#]phenomena[#]average[#]may[#]heavy[#]storms[#]become[#]less[#]frequent[#]paper[#]studies[#]probability[#]high[#]threshold[#]exceeded[#]model[#]need[#]specified[#]results[#]hold[#]For[#]simplicity[#]certain[#]linear[#]trend[#]studied[#]depending[#]one[#]real[#]parameter[#]Estimation[#]testing[#]procedures[#]developed[#]Simulation[#]presented[#]method[#]applied[#]trends[#]gauging[#]stations[#]across[#]Germany[#]Netherlands[#]A[#]tentative[#]conclusion[#]seems[#]depend[#]whether[#]station[#]close[#]sea" />
        </attvalues>
      </node>
      <node id="Maugis Cathy" label="Maugis Cathy">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Michel Bertrand" label="Michel Bertrand">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Paweł Hitczenko" label="Paweł Hitczenko">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jacek Wesołowski" label="Jacek Wesołowski">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Florian Gach" label="Florian Gach">
        <attvalues>
          <attvalue for="0" value="Given[#]random[#]sample[#]unknown[#]density[#]R[#]devise[#]Haar[#]wavelet[#]estimators[#]variable[#]resolution[#]levels[#]constructed[#]localised[#]test[#]procedures[#]Lepski[#]Mammen[#]Spokoiny[#]Ann[#]We[#]show[#]adapt[#]spatially[#]heterogeneous[#]smoothness[#]simultaneously[#]every[#]point[#]x[#]fixed[#]interval[#]loss[#]The[#]thresholding[#]constants[#]involved[#]chosen[#]practice[#]idealised[#]assumption[#]true[#]locally[#]constant[#]neighborhood[#]estimation[#]information[#]theoretic[#]justification[#]given" />
        </attvalues>
      </node>
      <node id="Vladimir Spokoiny" label="Vladimir Spokoiny">
        <attvalues>
          <attvalue for="0" value="Given[#]random[#]sample[#]unknown[#]density[#]R[#]devise[#]Haar[#]wavelet[#]estimators[#]variable[#]resolution[#]levels[#]constructed[#]localised[#]test[#]procedures[#]Lepski[#]Mammen[#]Spokoiny[#]Ann[#]We[#]show[#]adapt[#]spatially[#]heterogeneous[#]smoothness[#]simultaneously[#]every[#]point[#]x[#]fixed[#]interval[#]loss[#]The[#]thresholding[#]constants[#]involved[#]chosen[#]practice[#]idealised[#]assumption[#]true[#]locally[#]constant[#]neighborhood[#]estimation[#]information[#]theoretic[#]justification[#]given" />
        </attvalues>
      </node>
      <node id="Karthik Bharath" label="Karthik Bharath">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Vladimir Pozdnyakov" label="Vladimir Pozdnyakov">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Dipak Dey" label="Dipak Dey">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Reman Abu-Shanab" label="Reman Abu-Shanab">
        <attvalues>
          <attvalue for="0" value="Consider[#]estimating[#]n[#]p[#]matrix[#]means[#]independent[#]normally[#]distributed[#]observations[#]constant[#]variance[#]performance[#]estimator[#]judged[#]using[#]quadratic[#]error[#]loss[#]function[#]A[#]version[#]proposed[#]depending[#]tuning[#]It[#]shown[#]dominate[#]usual[#]maximum[#]likelihood[#]choices[#]greater[#]equal[#]This[#]result[#]also[#]extends[#]shrinkage[#]estimators[#]settings" />
        </attvalues>
      </node>
      <node id="John T. Kent" label="John T. Kent">
        <attvalues>
          <attvalue for="0" value="Consider[#]estimating[#]n[#]p[#]matrix[#]means[#]independent[#]normally[#]distributed[#]observations[#]constant[#]variance[#]performance[#]estimator[#]judged[#]using[#]quadratic[#]error[#]loss[#]function[#]A[#]version[#]proposed[#]depending[#]tuning[#]It[#]shown[#]dominate[#]usual[#]maximum[#]likelihood[#]choices[#]greater[#]equal[#]This[#]result[#]also[#]extends[#]shrinkage[#]estimators[#]settings" />
        </attvalues>
      </node>
      <node id="William E. Strawderman" label="William E. Strawderman">
        <attvalues>
          <attvalue for="0" value="Consider[#]estimating[#]n[#]p[#]matrix[#]means[#]independent[#]normally[#]distributed[#]observations[#]constant[#]variance[#]performance[#]estimator[#]judged[#]using[#]quadratic[#]error[#]loss[#]function[#]A[#]version[#]proposed[#]depending[#]tuning[#]It[#]shown[#]dominate[#]usual[#]maximum[#]likelihood[#]choices[#]greater[#]equal[#]This[#]result[#]also[#]extends[#]shrinkage[#]estimators[#]settings" />
        </attvalues>
      </node>
      <node id="Hongyuan Cao" label="Hongyuan Cao">
        <attvalues>
          <attvalue for="0" value="This[#]article[#]considers[#]problem[#]multiple[#]hypothesis[#]testing[#]using[#]The[#]observed[#]data[#]assumed[#]independently[#]generated[#]conditional[#]underlying[#]unknown[#]hidden[#]model[#]We[#]propose[#]asymptotically[#]valid[#]procedure[#]find[#]critical[#]values[#]rejection[#]regions[#]controlling[#]k[#]error[#]rate[#]false[#]discovery[#]FDR[#]tail[#]probability[#]proportion[#]FDTP[#]require[#]finite[#]fourth[#]moment[#]plus[#]general[#]conditions[#]mean[#]variance[#]population[#]virtue[#]moderate[#]deviations[#]properties[#]A[#]new[#]consistent[#]estimator[#]alternative[#]hypotheses[#]developed[#]Simulation[#]studies[#]support[#]theoretical[#]results[#]demonstrate[#]power[#]substantially[#]improved[#]directly[#]opposed[#]conventional[#]p[#]approach[#]Our[#]method[#]applied[#]analysis[#]microarray[#]leukemia[#]cancer[#]study[#]involves[#]large[#]number[#]simultaneously" />
        </attvalues>
      </node>
      <node id="Michael R. Kosorok" label="Michael R. Kosorok">
        <attvalues>
          <attvalue for="0" value="This[#]article[#]considers[#]problem[#]multiple[#]hypothesis[#]testing[#]using[#]The[#]observed[#]data[#]assumed[#]independently[#]generated[#]conditional[#]underlying[#]unknown[#]hidden[#]model[#]We[#]propose[#]asymptotically[#]valid[#]procedure[#]find[#]critical[#]values[#]rejection[#]regions[#]controlling[#]k[#]error[#]rate[#]false[#]discovery[#]FDR[#]tail[#]probability[#]proportion[#]FDTP[#]require[#]finite[#]fourth[#]moment[#]plus[#]general[#]conditions[#]mean[#]variance[#]population[#]virtue[#]moderate[#]deviations[#]properties[#]A[#]new[#]consistent[#]estimator[#]alternative[#]hypotheses[#]developed[#]Simulation[#]studies[#]support[#]theoretical[#]results[#]demonstrate[#]power[#]substantially[#]improved[#]directly[#]opposed[#]conventional[#]p[#]approach[#]Our[#]method[#]applied[#]analysis[#]microarray[#]leukemia[#]cancer[#]study[#]involves[#]large[#]number[#]simultaneously" />
        </attvalues>
      </node>
      <node id="Shalosh B. Ekhad" label="Shalosh B. Ekhad">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Doron Zeilberger" label="Doron Zeilberger">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Emilio Seijo" label="Emilio Seijo">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]introduces[#]version[#]argmax[#]continuous[#]mapping[#]theorem[#]applies[#]problems[#]objective[#]functions[#]converge[#]limiting[#]process[#]multiple[#]maximizers[#]The[#]concept[#]smallest[#]maximizer[#]function[#]Skorohod[#]space[#]introduced[#]main[#]properties[#]studied[#]resulting[#]applied[#]three[#]arising[#]regression[#]analysis[#]Some[#]results[#]proved[#]connection[#]also[#]independent[#]interest" />
        </attvalues>
      </node>
      <node id="Bodhisattva Sen" label="Bodhisattva Sen">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]introduces[#]version[#]argmax[#]continuous[#]mapping[#]theorem[#]applies[#]problems[#]objective[#]functions[#]converge[#]limiting[#]process[#]multiple[#]maximizers[#]The[#]concept[#]smallest[#]maximizer[#]function[#]Skorohod[#]space[#]introduced[#]main[#]properties[#]studied[#]resulting[#]applied[#]three[#]arising[#]regression[#]analysis[#]Some[#]results[#]proved[#]connection[#]also[#]independent[#]interest" />
        </attvalues>
      </node>
      <node id="Matthieu Lerasle" label="Matthieu Lerasle">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Daniel Y. Takahashi" label="Daniel Y. Takahashi">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Hedi Kortas" label="Hedi Kortas">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]introduce[#]new[#]class[#]estimators[#]Hurst[#]exponent[#]fractional[#]Brownian[#]motion[#]fBm[#]process[#]These[#]based[#]sample[#]expectiles[#]discrete[#]variations[#]path[#]order[#]derive[#]statistical[#]properties[#]proposed[#]establish[#]asymptotic[#]results[#]subordinated[#]stationary[#]Gaussian[#]processes[#]unit[#]variance[#]correlation[#]function[#]satisfying[#]Via[#]simulation[#]study[#]demonstrate[#]relevance[#]estimation[#]method[#]show[#]suggested[#]robust[#]data[#]rounding[#]counterparts" />
        </attvalues>
      </node>
      <node id="Sylvain Le Corff" label="Sylvain Le Corff">
        <attvalues>
          <attvalue for="0" value="The[#]Expectation[#]Maximization[#]EM[#]algorithm[#]versatile[#]tool[#]model[#]parameter[#]estimation[#]latent[#]data[#]models[#]When[#]processing[#]large[#]sets[#]stream[#]however[#]becomes[#]intractable[#]since[#]requires[#]whole[#]set[#]available[#]iteration[#]In[#]contribution[#]new[#]generic[#]online[#]inference[#]general[#]Hidden[#]Markov[#]Model[#]proposed[#]This[#]updates[#]estimate[#]block[#]observations[#]processed[#]convergence[#]established[#]rate[#]studied[#]showing[#]impact[#]size[#]An[#]averaging[#]procedure[#]also[#]improve[#]Finally[#]practical[#]illustrations[#]presented[#]highlight[#]performance[#]algorithms[#]comparison[#]maximum[#]likelihood[#]procedures" />
        </attvalues>
      </node>
      <node id="Djalel Eddine Meskaldji" label="Djalel Eddine Meskaldji">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Dimitri Van De Ville" label="Dimitri Van De Ville">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jean-Philippe Thiran" label="Jean-Philippe Thiran">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Stephan Morgenthaler" label="Stephan Morgenthaler">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Magda Peligrad" label="Magda Peligrad">
        <attvalues>
          <attvalue for="0" value="Large[#]moderate[#]deviation[#]probabilities[#]play[#]important[#]role[#]many[#]applied[#]areas[#]insurance[#]risk[#]analysis[#]This[#]paper[#]studies[#]exact[#]large[#]asymptotics[#]form[#]linear[#]processes[#]independent[#]innovations[#]The[#]analyze[#]general[#]therefore[#]include[#]long[#]memory[#]case[#]We[#]give[#]asymptotic[#]representation[#]probability[#]tail[#]normalized[#]sums[#]specify[#]zones[#]approximated[#]either[#]standard[#]normal[#]distribution[#]marginal[#]innovation[#]process[#]results[#]regression[#]estimates[#]moving[#]averages[#]fractionally[#]integrated[#]regularly[#]varying[#]exponents[#]functions[#]also[#]consider[#]computation[#]value[#]expected[#]shortfall[#]fundamental[#]quantities[#]theory[#]finance" />
        </attvalues>
      </node>
      <node id="Hailin Sang" label="Hailin Sang">
        <attvalues>
          <attvalue for="0" value="Large[#]moderate[#]deviation[#]probabilities[#]play[#]important[#]role[#]many[#]applied[#]areas[#]insurance[#]risk[#]analysis[#]This[#]paper[#]studies[#]exact[#]large[#]asymptotics[#]form[#]linear[#]processes[#]independent[#]innovations[#]The[#]analyze[#]general[#]therefore[#]include[#]long[#]memory[#]case[#]We[#]give[#]asymptotic[#]representation[#]probability[#]tail[#]normalized[#]sums[#]specify[#]zones[#]approximated[#]either[#]standard[#]normal[#]distribution[#]marginal[#]innovation[#]process[#]results[#]regression[#]estimates[#]moving[#]averages[#]fractionally[#]integrated[#]regularly[#]varying[#]exponents[#]functions[#]also[#]consider[#]computation[#]value[#]expected[#]shortfall[#]fundamental[#]quantities[#]theory[#]finance" />
        </attvalues>
      </node>
      <node id="Yunda Zhong" label="Yunda Zhong">
        <attvalues>
          <attvalue for="0" value="Large[#]moderate[#]deviation[#]probabilities[#]play[#]important[#]role[#]many[#]applied[#]areas[#]insurance[#]risk[#]analysis[#]This[#]paper[#]studies[#]exact[#]large[#]asymptotics[#]form[#]linear[#]processes[#]independent[#]innovations[#]The[#]analyze[#]general[#]therefore[#]include[#]long[#]memory[#]case[#]We[#]give[#]asymptotic[#]representation[#]probability[#]tail[#]normalized[#]sums[#]specify[#]zones[#]approximated[#]either[#]standard[#]normal[#]distribution[#]marginal[#]innovation[#]process[#]results[#]regression[#]estimates[#]moving[#]averages[#]fractionally[#]integrated[#]regularly[#]varying[#]exponents[#]functions[#]also[#]consider[#]computation[#]value[#]expected[#]shortfall[#]fundamental[#]quantities[#]theory[#]finance" />
        </attvalues>
      </node>
      <node id="Antonio Cuevas" label="Antonio Cuevas">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ricardo Fraiman" label="Ricardo Fraiman">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Beatriz Pateiro-López" label="Beatriz Pateiro-López">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Peter Brockwell" label="Peter Brockwell">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Vincenzo Ferrazzano" label="Vincenzo Ferrazzano">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Tilmann Gneiting" label="Tilmann Gneiting">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Roopesh Ranjan" label="Roopesh Ranjan">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yutao Ma" label="Yutao Ma">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Shi Shen" label="Shi Shen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Xinyu Wang" label="Xinyu Wang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Liming Wu" label="Liming Wu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Paul Kabaila" label="Paul Kabaila">
        <attvalues>
          <attvalue for="0" value="Freeman[#]considered[#]following[#]procedure[#]finding[#]confidence[#]interval[#]treatment[#]difference[#]theta[#]using[#]data[#]crossover[#]trial[#]In[#]first[#]stage[#]preliminary[#]test[#]null[#]hypothesis[#]differential[#]carryover[#]zero[#]carried[#]If[#]accepted[#]constructed[#]assuming[#]hand[#]rejected[#]period[#]shown[#]minimum[#]coverage[#]probability[#]far[#]nominal[#]He[#]therefore[#]concludes[#]used[#]present[#]paper[#]analyse[#]performance[#]similar[#]This[#]differs[#]significant[#]ways[#]including[#]fact[#]unbiased[#]estimator[#]unaffected[#]variation[#]Despite[#]great[#]differences[#]arrive[#]conclusion[#]Namely[#]resulting" />
        </attvalues>
      </node>
      <node id="Matthew Vicendese" label="Matthew Vicendese">
        <attvalues>
          <attvalue for="0" value="Freeman[#]considered[#]following[#]procedure[#]finding[#]confidence[#]interval[#]treatment[#]difference[#]theta[#]using[#]data[#]crossover[#]trial[#]In[#]first[#]stage[#]preliminary[#]test[#]null[#]hypothesis[#]differential[#]carryover[#]zero[#]carried[#]If[#]accepted[#]constructed[#]assuming[#]hand[#]rejected[#]period[#]shown[#]minimum[#]coverage[#]probability[#]far[#]nominal[#]He[#]therefore[#]concludes[#]used[#]present[#]paper[#]analyse[#]performance[#]similar[#]This[#]differs[#]significant[#]ways[#]including[#]fact[#]unbiased[#]estimator[#]unaffected[#]variation[#]Despite[#]great[#]differences[#]arrive[#]conclusion[#]Namely[#]resulting" />
        </attvalues>
      </node>
      <node id="Karim Benhenni" label="Karim Benhenni">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="David Degras" label="David Degras">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Iryna Kyrychynska" label="Iryna Kyrychynska">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ostap Okhrin" label="Ostap Okhrin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yaroslav Yeleyko" label="Yaroslav Yeleyko">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gérard Biau" label="Gérard Biau">
        <attvalues>
          <attvalue for="0" value="Let[#]Y[#]random[#]pair[#]taking[#]values[#]R[#]In[#]model[#]one[#]T[#]unknown[#]univariate[#]measurable[#]function[#]vector[#]W[#]denotes[#]noise[#]satisfying[#]E[#]The[#]known[#]offer[#]flexible[#]way[#]variety[#]phenomena[#]However[#]despite[#]relative[#]simplicity[#]dimension[#]reduction[#]scheme[#]faced[#]severe[#]complications[#]soon[#]underlying[#]becomes[#]larger[#]number[#]observations[#]p[#]n[#]paradigm[#]To[#]circumvent[#]difficulty[#]consider[#]estimation[#]problem[#]sparsity[#]perspective[#]using[#]approach[#]On[#]theoretical[#]side[#]sharp[#]oracle[#]inequality[#]powerful[#]best[#]inequalities[#]common[#]procedures[#]recovery[#]proposed[#]method[#]implemented[#]means[#]reversible[#]jump[#]Markov[#]chain[#]Monte[#]Carlo[#]technique[#]performance[#]compared[#]standard" />
        </attvalues>
      </node>
      <node id="David Azriel" label="David Azriel">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Micha Mandel" label="Micha Mandel">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yosef Rinott" label="Yosef Rinott">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Piotr Fryzlewicz" label="Piotr Fryzlewicz">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Artur J. Lemonte" label="Artur J. Lemonte">
        <attvalues>
          <attvalue for="0" value="We[#]derive[#]asymptotic[#]expansions[#]order[#]nonnull[#]distribution[#]functions[#]likelihood[#]ratio[#]Wald[#]score[#]gradient[#]test[#]statistics[#]class[#]dispersion[#]models[#]sequence[#]Pitman[#]alternatives[#]The[#]distributions[#]obtained[#]testing[#]subset[#]regression[#]parameters[#]precision[#]parameter[#]Based[#]shown[#]uniform[#]superiority[#]one[#]respect[#]others[#]Furthermore[#]compare[#]performance[#]tests[#]Monte[#]Carlo[#]simulations[#]presented[#]An[#]empirical[#]application[#]real[#]data[#]set[#]considered[#]illustrative[#]purposes" />
        </attvalues>
      </node>
      <node id="Caroline Bernard-Michel" label="Caroline Bernard-Michel">
        <attvalues>
          <attvalue for="0" value="In[#]Li[#]Yin[#]X[#]Sliced[#]Inverse[#]Regression[#]Regularizations[#]Biometrics[#]ridge[#]SIR[#]estimator[#]introduced[#]solution[#]minimization[#]problem[#]computed[#]thanks[#]alternating[#]algorithm[#]This[#]methodology[#]reveals[#]good[#]performance[#]practice[#]note[#]focus[#]theoretical[#]properties[#]Is[#]shown[#]degenerated[#]sense[#]two[#]situations[#]occur[#]Either[#]exist[#]zero" />
        </attvalues>
      </node>
      <node id="Odalric-Ambrym Maillard" label="Odalric-Ambrym Maillard">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]algorithm[#]stochastic[#]bandit[#]problem[#]case[#]distributions[#]finite[#]supports[#]necessarily[#]known[#]beforehand[#]whose[#]asymptotic[#]regret[#]matches[#]lower[#]bound[#]Our[#]contribution[#]provide[#]analysis[#]get[#]bounds[#]main[#]terms[#]smaller[#]ones[#]previously[#]algorithms[#]analyses[#]like" />
        </attvalues>
      </node>
      <node id="Rémi Munos" label="Rémi Munos">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]algorithm[#]stochastic[#]bandit[#]problem[#]case[#]distributions[#]finite[#]supports[#]necessarily[#]known[#]beforehand[#]whose[#]asymptotic[#]regret[#]matches[#]lower[#]bound[#]Our[#]contribution[#]provide[#]analysis[#]get[#]bounds[#]main[#]terms[#]smaller[#]ones[#]previously[#]algorithms[#]analyses[#]like" />
        </attvalues>
      </node>
      <node id="Daniel Bruynooghe" label="Daniel Bruynooghe">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Henry P. Wynn" label="Henry P. Wynn">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="G. Jogesh Babu" label="G. Jogesh Babu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Zhidong Bai" label="Zhidong Bai">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Kwok Pui Choi" label="Kwok Pui Choi">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Vasudevan Mangalam" label="Vasudevan Mangalam">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Theofanis Sapatinas" label="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]detection[#]problem[#]function[#]noisy[#]observations[#]integrals[#]lines[#]study[#]rate[#]sharp[#]asymptotics[#]error[#]probabilities[#]minimax[#]setup[#]By[#]construction[#]derived[#]tests[#]also[#]construct[#]adaptive[#]test[#]rather[#]simple[#]structure[#]estimating[#]unknown[#]response[#]multichannel[#]deconvolution[#]model[#]kernel[#]particular[#]interest[#]signal[#]processing[#]It[#]known[#]number[#]channels[#]finite[#]precision[#]reconstruction[#]increases[#]M[#]grow[#]even[#]total[#]n[#]remains[#]constant[#]requires[#]parameter[#]form[#]Badly[#]Approximable[#]Recent[#]advances[#]data[#]collection[#]recording[#]techniques[#]made[#]urgent[#]case[#]However[#]situations[#]usually[#]refers[#]physical[#]devices[#]consequently[#]may[#]infinity[#]slow[#]When[#]grows[#]slowly[#]develop[#]procedure[#]specified[#]interval[#]length[#]together[#]lower[#]bound[#]associated[#]explicitly[#]shows[#]dependence[#]growing[#]This[#]result[#]used[#]evaluation[#]suggested[#]wavelet[#]thresholding[#]estimator[#]furthermore[#]choice[#]optimal[#]minimizes" />
        </attvalues>
      </node>
      <node id="Irina A. Suslina" label="Irina A. Suslina">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]detection[#]problem[#]function[#]noisy[#]observations[#]integrals[#]lines[#]study[#]rate[#]sharp[#]asymptotics[#]error[#]probabilities[#]minimax[#]setup[#]By[#]construction[#]derived[#]tests[#]also[#]construct[#]adaptive[#]test[#]rather[#]simple[#]structure" />
        </attvalues>
      </node>
      <node id="Xiaolong Luo" label="Xiaolong Luo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gongjun Xu" label="Gongjun Xu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Zhiliang Ying" label="Zhiliang Ying">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="B. G. Manjunath" label="B. G. Manjunath">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="K. R. Parthasarathy" label="K. R. Parthasarathy">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mingyuan Zhang" label="Mingyuan Zhang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marshall M. Joffe" label="Marshall M. Joffe">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Dylan S. Small" label="Dylan S. Small">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Xinyu Zhang" label="Xinyu Zhang">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]model[#]selection[#]averaging[#]generalized[#]additive[#]partial[#]linear[#]models[#]GAPLMs[#]Polynomial[#]spline[#]used[#]approximate[#]nonparametric[#]functions[#]The[#]corresponding[#]estimators[#]parameters[#]shown[#]asymptotically[#]normal[#]develop[#]focused[#]information[#]criterion[#]FIC[#]frequentist[#]average[#]FMA[#]estimator[#]basis[#]principle[#]examine[#]theoretical[#]properties[#]major[#]advantages[#]proposed[#]procedures[#]existing[#]ones[#]computational[#]expediency[#]reliability[#]Simulation[#]experiments[#]provided[#]evidence[#]superiority[#]approach[#]applied[#]data[#]example" />
        </attvalues>
      </node>
      <node id="Hua Liang" label="Hua Liang">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]model[#]selection[#]averaging[#]generalized[#]additive[#]partial[#]linear[#]models[#]GAPLMs[#]Polynomial[#]spline[#]used[#]approximate[#]nonparametric[#]functions[#]The[#]corresponding[#]estimators[#]parameters[#]shown[#]asymptotically[#]normal[#]develop[#]focused[#]information[#]criterion[#]FIC[#]frequentist[#]average[#]FMA[#]estimator[#]basis[#]principle[#]examine[#]theoretical[#]properties[#]major[#]advantages[#]proposed[#]procedures[#]existing[#]ones[#]computational[#]expediency[#]reliability[#]Simulation[#]experiments[#]provided[#]evidence[#]superiority[#]approach[#]applied[#]data[#]example[#]proposing[#]use[#]polynomial[#]smoothing[#]estimation[#]deriving[#]based[#]establish[#]asymptotic[#]normality[#]parametric[#]components[#]procedure[#]avoids[#]solving[#]large[#]systems[#]equations[#]thus[#]results[#]gains[#]simplicity[#]class[#]variable[#]employing[#]nonconcave[#]penalized[#]oracle[#]property[#]Monte[#]Carlo[#]simulations[#]empirical[#]presented[#]illustration" />
        </attvalues>
      </node>
      <node id="Martina Mincheva" label="Martina Mincheva">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]deals[#]estimation[#]covariance[#]conditional[#]sparsity[#]structure[#]eigenvalues[#]By[#]assuming[#]sparse[#]error[#]matrix[#]approximate[#]factor[#]model[#]allow[#]presence[#]correlation[#]even[#]taking[#]common[#]unobservable[#]factors[#]We[#]introduce[#]Principal[#]Orthogonal[#]complEment[#]Thresholding[#]POET[#]method[#]explore[#]The[#]estimator[#]includes[#]sample[#]Fan[#]Lv[#]thresholding[#]Bickel[#]Levina[#]adaptive[#]Cai[#]Liu[#]specific[#]examples[#]provide[#]mathematical[#]insights[#]analysis[#]approximately[#]principal[#]component[#]data[#]rates[#]convergence[#]residual[#]studied[#]various[#]norms[#]It[#]shown[#]impact[#]estimating[#]unknown[#]vanishes[#]dimensionality[#]increases[#]uniform[#]unobserved[#]loadings[#]derived[#]asymptotic[#]results[#]also[#]verified[#]extensive[#]simulation[#]studies[#]Finally[#]real[#]application[#]portfolio[#]allocation[#]presented" />
        </attvalues>
      </node>
      <node id="Philipp Arbenz" label="Philipp Arbenz">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Paul Embrechts" label="Paul Embrechts">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Giovanni Puccetti" label="Giovanni Puccetti">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexandre Tsybakov" label="Alexandre Tsybakov">
        <attvalues>
          <attvalue for="0" value="This[#]article[#]considers[#]inference[#]linear[#]models[#]K[#]regressors[#]many[#]could[#]endogenous[#]L[#]instruments[#]range[#]less[#]order[#]smaller[#]exponential[#]sample[#]size[#]arbitrary[#]For[#]moderate[#]identification[#]robust[#]confidence[#]sets[#]obtained[#]solving[#]hierarchy[#]semidefinite[#]programs[#]larger[#]propose[#]STIV[#]estimator[#]The[#]analysis[#]error[#]uses[#]sensitivity[#]characteristics[#]sharper[#]literature[#]sparsity[#]bounds[#]Results[#]rates[#]convergence[#]variable[#]selection[#]adapt[#]given[#]We[#]generalize[#]approach[#]approximation[#]errors[#]systems[#]bands[#]vectors[#]functionals[#]functions[#]application[#]demand[#]system" />
        </attvalues>
      </node>
      <node id="Christiern Rose" label="Christiern Rose">
        <attvalues>
          <attvalue for="0" value="This[#]article[#]considers[#]inference[#]linear[#]models[#]K[#]regressors[#]many[#]could[#]endogenous[#]L[#]instruments[#]range[#]less[#]order[#]smaller[#]exponential[#]sample[#]size[#]arbitrary[#]For[#]moderate[#]identification[#]robust[#]confidence[#]sets[#]obtained[#]solving[#]hierarchy[#]semidefinite[#]programs[#]larger[#]propose[#]STIV[#]estimator[#]The[#]analysis[#]error[#]uses[#]sensitivity[#]characteristics[#]sharper[#]literature[#]sparsity[#]bounds[#]Results[#]rates[#]convergence[#]variable[#]selection[#]adapt[#]given[#]We[#]generalize[#]approach[#]approximation[#]errors[#]systems[#]bands[#]vectors[#]functionals[#]functions[#]application[#]demand[#]system" />
        </attvalues>
      </node>
      <node id="Luai Al Labadi" label="Luai Al Labadi">
        <attvalues>
          <attvalue for="0" value="Ferguson[#]Dirichlet[#]process[#]plays[#]important[#]role[#]nonparametric[#]Bayesian[#]inference[#]Let[#]R[#]base[#]probability[#]measure[#]H[#]concentration[#]parameter[#]In[#]paper[#]show[#]converges[#]certain[#]Brownian[#]bridge[#]We[#]also[#]derive[#]theorem[#]Using[#]functional[#]delta[#]method[#]weak[#]convergence[#]quantile[#]obtained[#]A[#]large[#]occurs[#]statistician[#]puts[#]much[#]emphasize[#]prior[#]guess[#]This[#]scenario[#]happens[#]sample[#]size[#]posterior[#]used[#]make" />
        </attvalues>
      </node>
      <node id="Mahmoud Zarepour" label="Mahmoud Zarepour">
        <attvalues>
          <attvalue for="0" value="Ferguson[#]Dirichlet[#]process[#]plays[#]important[#]role[#]nonparametric[#]Bayesian[#]inference[#]Let[#]R[#]base[#]probability[#]measure[#]H[#]concentration[#]parameter[#]In[#]paper[#]show[#]converges[#]certain[#]Brownian[#]bridge[#]We[#]also[#]derive[#]theorem[#]Using[#]functional[#]delta[#]method[#]weak[#]convergence[#]quantile[#]obtained[#]A[#]large[#]occurs[#]statistician[#]puts[#]much[#]emphasize[#]prior[#]guess[#]This[#]scenario[#]happens[#]sample[#]size[#]posterior[#]used[#]make" />
        </attvalues>
      </node>
      <node id="Tom Ketelaars" label="Tom Ketelaars">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]three[#]estimators[#]interval[#]censoring[#]case[#]problem[#]estimator[#]proposed[#]e[#]maximum[#]likelihood[#]MLE[#]smoothed[#]using[#]smoothing[#]kernel[#]Our[#]focus[#]asymptotic[#]distribution[#]fixed[#]point[#]The[#]compared[#]simulation" />
        </attvalues>
      </node>
      <node id="Tobias Kley" label="Tobias Kley">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]present[#]alternative[#]method[#]spectral[#]analysis[#]univariate[#]strictly[#]stationary[#]time[#]series[#]Z[#]We[#]define[#]new[#]spectrum[#]Fourier[#]transform[#]differences[#]copulas[#]pairs[#]independence[#]copula[#]This[#]object[#]called[#]density[#]kernel[#]allows[#]separate[#]marginal[#]serial[#]aspects[#]show[#]closely[#]related[#]concept[#]quantile[#]regression[#]Like[#]provides[#]much[#]information[#]conditional[#]distributions[#]classical[#]models[#]kernels[#]informative[#]traditional[#]densities[#]obtained[#]autocovariances[#]particular[#]population[#]versions[#]provide[#]asymptotically[#]sample[#]complete[#]description[#]Moreover[#]inherit[#]robustness[#]properties[#]require[#]distributional[#]assumptions[#]existence[#]finite[#]moments[#]order[#]estimate[#]introduce[#]Laplace[#]periodograms[#]calculated[#]bilinear[#]forms[#]weighted[#]ranks[#]observed[#]onto[#]harmonic[#]model[#]establish[#]asymptotic[#]distribution[#]consistency[#]adequately[#]smoothed[#]The[#]methodology[#]potential[#]applications[#]briefly[#]investigated[#]simulations[#]short[#]empirical[#]example" />
        </attvalues>
      </node>
      <node id="Abdelhakim Necir" label="Abdelhakim Necir">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Ričardas Zitikis" label="Ričardas Zitikis">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mehdi Fhima" label="Mehdi Fhima">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Arnaud Guillin" label="Arnaud Guillin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Pierre R. Bertrand" label="Pierre R. Bertrand">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Benoîte de Saporta" label="Benoîte de Saporta">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]symmetry[#]tests[#]bifurcating[#]autoregressive[#]processes[#]BAR[#]data[#]missing[#]typically[#]model[#]cell[#]division[#]Each[#]one[#]two[#]types[#]odd[#]even[#]The[#]goal[#]paper[#]study[#]possible[#]asymmetry[#]cells[#]single[#]observed[#]lineage[#]first[#]derive[#]modeled[#]process[#]applications[#]simulated[#]real" />
        </attvalues>
      </node>
      <node id="Anne Gégout-Petit" label="Anne Gégout-Petit">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]symmetry[#]tests[#]bifurcating[#]autoregressive[#]processes[#]BAR[#]data[#]missing[#]typically[#]model[#]cell[#]division[#]Each[#]one[#]two[#]types[#]odd[#]even[#]The[#]goal[#]paper[#]study[#]possible[#]asymmetry[#]cells[#]single[#]observed[#]lineage[#]first[#]derive[#]modeled[#]process[#]applications[#]simulated[#]real" />
        </attvalues>
      </node>
      <node id="Laurence Marsalle" label="Laurence Marsalle">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]symmetry[#]tests[#]bifurcating[#]autoregressive[#]processes[#]BAR[#]data[#]missing[#]typically[#]model[#]cell[#]division[#]Each[#]one[#]two[#]types[#]odd[#]even[#]The[#]goal[#]paper[#]study[#]possible[#]asymmetry[#]cells[#]single[#]observed[#]lineage[#]first[#]derive[#]modeled[#]process[#]applications[#]simulated[#]real" />
        </attvalues>
      </node>
      <node id="Zuofeng Shang" label="Zuofeng Shang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Murray K. Clayton" label="Murray K. Clayton">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mathieu Rosenbaum" label="Mathieu Rosenbaum">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]regression[#]model[#]observation[#]error[#]design[#]e[#]Here[#]random[#]vector[#]matrix[#]Z[#]observed[#]X[#]unknown[#]N[#]noise[#]parameters[#]estimated[#]setting[#]dimension[#]p[#]much[#]larger[#]sample[#]size[#]n[#]sparse[#]Because[#]presence[#]commonly[#]used[#]Lasso[#]Dantzig[#]selector[#]unstable[#]An[#]alternative[#]procedure[#]called[#]Matrix[#]Uncertainty[#]MU[#]proposed[#]Rosenbaum[#]Tsybakov[#]order[#]account[#]The[#]properties[#]studied[#]assumption[#]deterministic[#]values[#]small[#]In[#]paper[#]propose[#]modification[#]entries[#]variances[#]This[#]example[#]case[#]missing[#]show[#]theoretically[#]numerically[#]conditions[#]new[#]estimator[#]Compensated[#]achieves[#]better[#]accuracy[#]estimation[#]original" />
        </attvalues>
      </node>
      <node id="Alexandre B. Tsybakov" label="Alexandre B. Tsybakov">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]regression[#]model[#]observation[#]error[#]design[#]e[#]Here[#]random[#]vector[#]matrix[#]Z[#]observed[#]X[#]unknown[#]N[#]noise[#]parameters[#]estimated[#]setting[#]dimension[#]p[#]much[#]larger[#]sample[#]size[#]n[#]sparse[#]Because[#]presence[#]commonly[#]used[#]Lasso[#]Dantzig[#]selector[#]unstable[#]An[#]alternative[#]procedure[#]called[#]Matrix[#]Uncertainty[#]MU[#]proposed[#]Rosenbaum[#]Tsybakov[#]order[#]account[#]The[#]properties[#]studied[#]assumption[#]deterministic[#]values[#]small[#]In[#]paper[#]propose[#]modification[#]entries[#]variances[#]This[#]example[#]case[#]missing[#]show[#]theoretically[#]numerically[#]conditions[#]new[#]estimator[#]Compensated[#]achieves[#]better[#]accuracy[#]estimation[#]original" />
        </attvalues>
      </node>
      <node id="A. Murillo-Salas" label="A. Murillo-Salas">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="F. J. Rubio" label="F. J. Rubio">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Antonio Galves" label="Antonio Galves">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]problem[#]model[#]selection[#]data[#]produced[#]two[#]different[#]context[#]tree[#]sources[#]Motivated[#]linguistic[#]questions[#]consider[#]case[#]probabilistic[#]trees[#]corresponding[#]finite[#]share[#]many[#]contexts[#]In[#]order[#]understand[#]differences[#]important[#]identify[#]transition[#]probabilities[#]specific[#]source[#]class[#]models[#]three[#]types[#]appear[#]one[#]use[#]BIC[#]penalized[#]maximum[#]likelihood[#]procedure[#]jointly[#]estimates[#]propose[#]new[#]algorithm[#]efficiently[#]computes[#]estimated[#]prove[#]strongly[#]consistent[#]also[#]present[#]simulation[#]showing[#]practical[#]advantage[#]works[#]separately[#]dataset" />
        </attvalues>
      </node>
      <node id="Aurélien Garivier" label="Aurélien Garivier">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]problem[#]model[#]selection[#]data[#]produced[#]two[#]different[#]context[#]tree[#]sources[#]Motivated[#]linguistic[#]questions[#]consider[#]case[#]probabilistic[#]trees[#]corresponding[#]finite[#]share[#]many[#]contexts[#]In[#]order[#]understand[#]differences[#]important[#]identify[#]transition[#]probabilities[#]specific[#]source[#]class[#]models[#]three[#]types[#]appear[#]one[#]use[#]BIC[#]penalized[#]maximum[#]likelihood[#]procedure[#]jointly[#]estimates[#]propose[#]new[#]algorithm[#]efficiently[#]computes[#]estimated[#]prove[#]strongly[#]consistent[#]also[#]present[#]simulation[#]showing[#]practical[#]advantage[#]works[#]separately[#]dataset" />
        </attvalues>
      </node>
      <node id="Elisabeth Gassiat" label="Elisabeth Gassiat">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]problem[#]model[#]selection[#]data[#]produced[#]two[#]different[#]context[#]tree[#]sources[#]Motivated[#]linguistic[#]questions[#]consider[#]case[#]probabilistic[#]trees[#]corresponding[#]finite[#]share[#]many[#]contexts[#]In[#]order[#]understand[#]differences[#]important[#]identify[#]transition[#]probabilities[#]specific[#]source[#]class[#]models[#]three[#]types[#]appear[#]one[#]use[#]BIC[#]penalized[#]maximum[#]likelihood[#]procedure[#]jointly[#]estimates[#]propose[#]new[#]algorithm[#]efficiently[#]computes[#]estimated[#]prove[#]strongly[#]consistent[#]also[#]present[#]simulation[#]showing[#]practical[#]advantage[#]works[#]separately[#]dataset" />
        </attvalues>
      </node>
      <node id="Xavier Brossat" label="Xavier Brossat">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Georges Oppenheim" label="Georges Oppenheim">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marie-Claude Viano" label="Marie-Claude Viano">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Junya Honda" label="Junya Honda">
        <attvalues>
          <attvalue for="0" value="In[#]multiarmed[#]bandit[#]problem[#]gambler[#]chooses[#]arm[#]slot[#]machine[#]pull[#]considering[#]tradeoff[#]exploration[#]exploitation[#]We[#]study[#]stochastic[#]reward[#]distribution[#]supported[#]known[#]bounded[#]interval[#]For[#]model[#]policies[#]take[#]account[#]empirical[#]variances[#]second[#]moments[#]arms[#]perform[#]effectively[#]paper[#]generalize[#]idea[#]propose[#]policy[#]exploits[#]first[#]arbitrary[#]fixed[#]advance[#]The[#]asymptotic[#]upper[#]bound[#]regret[#]approaches[#]theoretical[#]Burnetas[#]Katehakis[#]increases[#]By[#]choosing[#]appropriate[#]proposed[#]realizes[#]computational[#]complexity[#]expected" />
        </attvalues>
      </node>
      <node id="Marianna Pensky" label="Marianna Pensky">
        <attvalues>
          <attvalue for="0" value="In[#]present[#]paper[#]consider[#]Laplace[#]deconvolution[#]discrete[#]noisy[#]data[#]observed[#]interval[#]whose[#]length[#]may[#]increase[#]sample[#]size[#]Although[#]problem[#]arises[#]variety[#]applications[#]best[#]knowledge[#]given[#]little[#]attention[#]statistical[#]community[#]Our[#]objective[#]fill[#]gap[#]provide[#]treatment[#]The[#]main[#]contribution[#]explicit[#]construction[#]asymptotically[#]minimax[#]sense[#]estimator[#]adaptive[#]regularity[#]unknown[#]function[#]We[#]show[#]original[#]reduced[#]nonparametric[#]estimation[#]regression[#]derivatives[#]growing[#]Whereas[#]forms[#]estimators[#]remain[#]standard[#]choices[#]parameters[#]convergence[#]rates[#]expressed[#]terms[#]case[#]affected[#]asymptotic[#]growth[#]derive[#]kernel[#]interest[#]establish[#]minimaxity[#]range[#]Sobolev[#]classes[#]illustrate[#]theory[#]examples[#]expressions[#]A[#]simulation[#]study[#]shows[#]addition[#]providing[#]optimality[#]number[#]observations[#]turns[#]infinity[#]proposed[#]demonstrates[#]good[#]performance[#]finite[#]estimating[#]response[#]multichannel[#]model[#]particular[#]signal[#]processing[#]It[#]known[#]channels[#]precision[#]reconstruction[#]increases[#]M[#]grow[#]even[#]total[#]n[#]remains[#]constant[#]requires[#]parameter[#]form[#]Badly[#]Approximable[#]Recent[#]advances[#]collection[#]recording[#]techniques[#]made[#]urgent[#]However[#]situations[#]usually[#]refers[#]physical[#]devices[#]consequently[#]slow[#]rate[#]When[#]grows[#]slowly[#]develop[#]procedure[#]specified[#]together[#]lower[#]bound[#]associated[#]explicitly[#]dependence[#]This[#]result[#]used[#]evaluation[#]suggested[#]wavelet[#]thresholding[#]furthermore[#]choice[#]optimal[#]minimizes" />
        </attvalues>
      </node>
      <node id="Kinga Sikolya" label="Kinga Sikolya">
        <attvalues>
          <attvalue for="0" value="The[#]problem[#]estimating[#]parameters[#]linear[#]regression[#]model[#]Z[#]based[#]observations[#]spatial[#]domain[#]G[#]special[#]shape[#]considered[#]driving[#]process[#]U[#]Gaussian[#]random[#]field[#]known[#]functions[#]Explicit[#]forms[#]maximum[#]likelihood[#]estimators[#]derived[#]cases[#]either[#]Wiener[#]stationary[#]nonstationary[#]sheet[#]Simulation[#]results[#]also[#]presented[#]sheets[#]simulated[#]help[#]eve[#]expansions" />
        </attvalues>
      </node>
      <node id="Gábor Lugosi" label="Gábor Lugosi">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]hypothesis[#]testing[#]problem[#]deciding[#]whether[#]observed[#]vector[#]independent[#]normal[#]components[#]alternatively[#]small[#]subset[#]correlated[#]The[#]may[#]certain[#]combinatorial[#]structure[#]known[#]statistician[#]establish[#]upper[#]lower[#]bounds[#]minimax[#]risk[#]terms[#]size[#]level[#]correlation[#]class[#]possibly[#]sets[#]show[#]simple[#]tests[#]performance[#]many[#]cases[#]generalized[#]likelihood[#]ratio[#]test[#]suboptimal[#]important" />
        </attvalues>
      </node>
      <node id="A. Philip Dawid" label="A. Philip Dawid">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Steffen Lauritzen" label="Steffen Lauritzen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Matthew Parry" label="Matthew Parry">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="John H. J. Einmahl" label="John H. J. Einmahl">
        <attvalues>
          <attvalue for="0" value="We[#]define[#]local[#]empirical[#]process[#]based[#]n[#]random[#]vectors[#]dimension[#]neighborhood[#]boundary[#]fixed[#]set[#]Under[#]natural[#]conditions[#]shrinking[#]show[#]processes[#]indexed[#]classes[#]sets[#]vary[#]satisfy[#]certain[#]appropriately[#]defined[#]uniform[#]central[#]limit[#]theorem[#]holds[#]The[#]concept[#]differentiation[#]measure[#]convenient[#]developing[#]results[#]Some[#]examples[#]statistical[#]applications[#]also[#]presented[#]Consider[#]sample[#]attraction[#]multivariate[#]extreme[#]value[#]distribution[#]dependence[#]structure[#]attractor[#]belongs[#]parametric[#]model[#]A[#]new[#]estimator[#]unknown[#]parameter[#]minimizes[#]distance[#]vector[#]weighted[#]integrals[#]tail[#]function[#]counterparts[#]minimization[#]problem[#]probability[#]tending[#]one[#]unique[#]global[#]solution[#]consistent[#]asymptotically[#]normal[#]spectral[#]measures[#]models[#]method[#]applies[#]discrete[#]continuous[#]Examples[#]demonstrate[#]applicability[#]performance" />
        </attvalues>
      </node>
      <node id="Estáte V. Khmaladze" label="Estáte V. Khmaladze">
        <attvalues>
          <attvalue for="0" value="We[#]define[#]local[#]empirical[#]process[#]based[#]n[#]random[#]vectors[#]dimension[#]neighborhood[#]boundary[#]fixed[#]set[#]Under[#]natural[#]conditions[#]shrinking[#]show[#]processes[#]indexed[#]classes[#]sets[#]vary[#]satisfy[#]certain[#]appropriately[#]defined[#]uniform[#]central[#]limit[#]theorem[#]holds[#]The[#]concept[#]differentiation[#]measure[#]convenient[#]developing[#]results[#]Some[#]examples[#]statistical[#]applications[#]also[#]presented" />
        </attvalues>
      </node>
      <node id="Francesco Bartolucci" label="Francesco Bartolucci">
        <attvalues>
          <attvalue for="0" value="Many[#]relevant[#]statistical[#]econometric[#]models[#]analysis[#]longitudinal[#]data[#]include[#]latent[#]process[#]account[#]unobserved[#]heterogeneity[#]subjects[#]dynamic[#]fashion[#]Such[#]may[#]continuous[#]typically[#]AR[#]discrete[#]Markov[#]chain[#]In[#]paper[#]propose[#]model[#]based[#]mixture[#]processes[#]different[#]means[#]correlation[#]coefficients[#]equal[#]variances[#]This[#]belongs[#]class[#]natural[#]interpretation[#]many[#]contexts[#]application[#]flexible[#]reaching[#]similar[#]reduced[#]number[#]parameters[#]We[#]show[#]perform[#]maximum[#]likelihood[#]estimation[#]proposed[#]joint[#]use[#]algorithm[#]implemented[#]recursions[#]developed[#]hidden[#]literature[#]also[#]introduce[#]simple[#]method[#]obtain[#]standard[#]errors[#]parameter[#]estimates[#]criterion[#]choose[#]components[#]The[#]approach[#]illustrated[#]dataset[#]coming[#]Health[#]Retirement[#]Study[#]health[#]status[#]sample[#]response[#]variable[#]ordinal[#]individual[#]covariates[#]available" />
        </attvalues>
      </node>
      <node id="Silvia Bacci" label="Silvia Bacci">
        <attvalues>
          <attvalue for="0" value="Many[#]relevant[#]statistical[#]econometric[#]models[#]analysis[#]longitudinal[#]data[#]include[#]latent[#]process[#]account[#]unobserved[#]heterogeneity[#]subjects[#]dynamic[#]fashion[#]Such[#]may[#]continuous[#]typically[#]AR[#]discrete[#]Markov[#]chain[#]In[#]paper[#]propose[#]model[#]based[#]mixture[#]processes[#]different[#]means[#]correlation[#]coefficients[#]equal[#]variances[#]This[#]belongs[#]class[#]natural[#]interpretation[#]many[#]contexts[#]application[#]flexible[#]reaching[#]similar[#]reduced[#]number[#]parameters[#]We[#]show[#]perform[#]maximum[#]likelihood[#]estimation[#]proposed[#]joint[#]use[#]algorithm[#]implemented[#]recursions[#]developed[#]hidden[#]literature[#]also[#]introduce[#]simple[#]method[#]obtain[#]standard[#]errors[#]parameter[#]estimates[#]criterion[#]choose[#]components[#]The[#]approach[#]illustrated[#]dataset[#]coming[#]Health[#]Retirement[#]Study[#]health[#]status[#]sample[#]response[#]variable[#]ordinal[#]individual[#]covariates[#]available" />
        </attvalues>
      </node>
      <node id="Fulvia Pennoni" label="Fulvia Pennoni">
        <attvalues>
          <attvalue for="0" value="Many[#]relevant[#]statistical[#]econometric[#]models[#]analysis[#]longitudinal[#]data[#]include[#]latent[#]process[#]account[#]unobserved[#]heterogeneity[#]subjects[#]dynamic[#]fashion[#]Such[#]may[#]continuous[#]typically[#]AR[#]discrete[#]Markov[#]chain[#]In[#]paper[#]propose[#]model[#]based[#]mixture[#]processes[#]different[#]means[#]correlation[#]coefficients[#]equal[#]variances[#]This[#]belongs[#]class[#]natural[#]interpretation[#]many[#]contexts[#]application[#]flexible[#]reaching[#]similar[#]reduced[#]number[#]parameters[#]We[#]show[#]perform[#]maximum[#]likelihood[#]estimation[#]proposed[#]joint[#]use[#]algorithm[#]implemented[#]recursions[#]developed[#]hidden[#]literature[#]also[#]introduce[#]simple[#]method[#]obtain[#]standard[#]errors[#]parameter[#]estimates[#]criterion[#]choose[#]components[#]The[#]approach[#]illustrated[#]dataset[#]coming[#]Health[#]Retirement[#]Study[#]health[#]status[#]sample[#]response[#]variable[#]ordinal[#]individual[#]covariates[#]available" />
        </attvalues>
      </node>
      <node id="Koshi Yamada" label="Koshi Yamada">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Sumio Watanabe" label="Sumio Watanabe">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Hélène Lescornel" label="Hélène Lescornel">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]model[#]selection[#]estimator[#]covariance[#]random[#]process[#]Using[#]Unbiased[#]Risk[#]Estimation[#]URE[#]method[#]build[#]risk[#]allows[#]select[#]collection[#]Then[#]present[#]oracle[#]inequality[#]ensures[#]selected[#]close[#]Simulations[#]show[#]efficiency[#]methodology" />
        </attvalues>
      </node>
      <node id="Claudie Chabriac" label="Claudie Chabriac">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]model[#]selection[#]estimator[#]covariance[#]random[#]process[#]Using[#]Unbiased[#]Risk[#]Estimation[#]URE[#]method[#]build[#]risk[#]allows[#]select[#]collection[#]Then[#]present[#]oracle[#]inequality[#]ensures[#]selected[#]close[#]Simulations[#]show[#]efficiency[#]methodology" />
        </attvalues>
      </node>
      <node id="Angelika Franke" label="Angelika Franke">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gerhard Osius" label="Gerhard Osius">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jun Shao" label="Jun Shao">
        <attvalues>
          <attvalue for="0" value="In[#]many[#]social[#]economical[#]biological[#]medical[#]studies[#]one[#]objective[#]classify[#]subject[#]several[#]classes[#]based[#]set[#]variables[#]observed[#]Because[#]probability[#]distribution[#]usually[#]unknown[#]rule[#]classification[#]constructed[#]using[#]training[#]sample[#]The[#]linear[#]discriminant[#]analysis[#]LDA[#]works[#]well[#]situation[#]number[#]used[#]much[#]smaller[#]size[#]advance[#]technologies[#]modern[#]statistical[#]often[#]face[#]problems[#]larger[#]may[#]perform[#]poorly[#]We[#]explore[#]poor[#]performance[#]propose[#]sparse[#]asymptotically[#]optimal[#]sparsity[#]conditions[#]parameters[#]For[#]illustration[#]application[#]discuss[#]example[#]classifying[#]human[#]cancer[#]two[#]leukemia[#]genes[#]A[#]simulation[#]also[#]conducted[#]check[#]proposed[#]method" />
        </attvalues>
      </node>
      <node id="Yazhen Wang" label="Yazhen Wang">
        <attvalues>
          <attvalue for="0" value="In[#]many[#]social[#]economical[#]biological[#]medical[#]studies[#]one[#]objective[#]classify[#]subject[#]several[#]classes[#]based[#]set[#]variables[#]observed[#]Because[#]probability[#]distribution[#]usually[#]unknown[#]rule[#]classification[#]constructed[#]using[#]training[#]sample[#]The[#]linear[#]discriminant[#]analysis[#]LDA[#]works[#]well[#]situation[#]number[#]used[#]much[#]smaller[#]size[#]advance[#]technologies[#]modern[#]statistical[#]often[#]face[#]problems[#]larger[#]may[#]perform[#]poorly[#]We[#]explore[#]poor[#]performance[#]propose[#]sparse[#]asymptotically[#]optimal[#]sparsity[#]conditions[#]parameters[#]For[#]illustration[#]application[#]discuss[#]example[#]classifying[#]human[#]cancer[#]two[#]leukemia[#]genes[#]A[#]simulation[#]also[#]conducted[#]check[#]proposed[#]method" />
        </attvalues>
      </node>
      <node id="Xinwei Deng" label="Xinwei Deng">
        <attvalues>
          <attvalue for="0" value="In[#]many[#]social[#]economical[#]biological[#]medical[#]studies[#]one[#]objective[#]classify[#]subject[#]several[#]classes[#]based[#]set[#]variables[#]observed[#]Because[#]probability[#]distribution[#]usually[#]unknown[#]rule[#]classification[#]constructed[#]using[#]training[#]sample[#]The[#]linear[#]discriminant[#]analysis[#]LDA[#]works[#]well[#]situation[#]number[#]used[#]much[#]smaller[#]size[#]advance[#]technologies[#]modern[#]statistical[#]often[#]face[#]problems[#]larger[#]may[#]perform[#]poorly[#]We[#]explore[#]poor[#]performance[#]propose[#]sparse[#]asymptotically[#]optimal[#]sparsity[#]conditions[#]parameters[#]For[#]illustration[#]application[#]discuss[#]example[#]classifying[#]human[#]cancer[#]two[#]leukemia[#]genes[#]A[#]simulation[#]also[#]conducted[#]check[#]proposed[#]method" />
        </attvalues>
      </node>
      <node id="Sijian Wang" label="Sijian Wang">
        <attvalues>
          <attvalue for="0" value="In[#]many[#]social[#]economical[#]biological[#]medical[#]studies[#]one[#]objective[#]classify[#]subject[#]several[#]classes[#]based[#]set[#]variables[#]observed[#]Because[#]probability[#]distribution[#]usually[#]unknown[#]rule[#]classification[#]constructed[#]using[#]training[#]sample[#]The[#]linear[#]discriminant[#]analysis[#]LDA[#]works[#]well[#]situation[#]number[#]used[#]much[#]smaller[#]size[#]advance[#]technologies[#]modern[#]statistical[#]often[#]face[#]problems[#]larger[#]may[#]perform[#]poorly[#]We[#]explore[#]poor[#]performance[#]propose[#]sparse[#]asymptotically[#]optimal[#]sparsity[#]conditions[#]parameters[#]For[#]illustration[#]application[#]discuss[#]example[#]classifying[#]human[#]cancer[#]two[#]leukemia[#]genes[#]A[#]simulation[#]also[#]conducted[#]check[#]proposed[#]method" />
        </attvalues>
      </node>
      <node id="Henrik Ohlsson" label="Henrik Ohlsson">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Allen Y. Yang" label="Allen Y. Yang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Roy Dong" label="Roy Dong">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="S. Shankar Sastry" label="S. Shankar Sastry">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="David Dunson" label="David Dunson">
        <attvalues>
          <attvalue for="0" value="In[#]nonparametric[#]regression[#]problems[#]involving[#]multiple[#]predictors[#]typically[#]interest[#]estimating[#]anisotropic[#]multivariate[#]surface[#]important[#]discarding[#]unimportant[#]ones[#]Our[#]focus[#]defining[#]Bayesian[#]procedure[#]leads[#]minimax[#]optimal[#]rate[#]posterior[#]contraction[#]log[#]factor[#]adapting[#]unknown[#]dimension[#]smoothness[#]true[#]We[#]propose[#]approach[#]based[#]Gaussian[#]process[#]prior[#]scalings[#]assigned[#]hyperpriors[#]additionally[#]show[#]using[#]homogenous[#]single[#]bandwidth[#]cases" />
        </attvalues>
      </node>
      <node id="Andrew M. Stuart" label="Andrew M. Stuart">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]inverse[#]problem[#]determining[#]permeability[#]pressure[#]Darcy[#]model[#]flow[#]porous[#]medium[#]Mathematically[#]find[#]diffusion[#]coefficient[#]linear[#]uniformly[#]elliptic[#]partial[#]differential[#]equation[#]divergence[#]form[#]bounded[#]domain[#]dimension[#]measurements[#]solution[#]interior[#]adopt[#]Bayesian[#]approach[#]place[#]prior[#]random[#]field[#]measure[#]log[#]specified[#]eve[#]expansion[#]draws[#]Gaussian[#]measures[#]constructed[#]way[#]study[#]regularity[#]functions[#]drawn[#]also[#]Lipschitz[#]properties[#]observation[#]operator[#]mapping[#]observations[#]Combining[#]continuity[#]estimates[#]show[#]posterior[#]suitable[#]Banach[#]space[#]Furthermore[#]shown[#]respect[#]data[#]Hellinger[#]metric[#]giving[#]rise[#]Determining[#]given[#]solves[#]uncertainty[#]quantification[#]In[#]practice[#]must[#]approximated[#]finite[#]dimensional[#]quantify[#]errors[#]incurred[#]employing[#]truncated[#]represent[#]meausure[#]particular[#]weak[#]convergence[#]general[#]class[#]locally[#]apply[#]theory[#]estimate[#]mean[#]covariance[#]refinement[#]truncation" />
        </attvalues>
      </node>
      <node id="Yoav Benjamini" label="Yoav Benjamini">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marina Bogomolov" label="Marina Bogomolov">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Serge Cohen" label="Serge Cohen">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]asymptotic[#]expansion[#]likelihood[#]certain[#]class[#]Gaussian[#]processes[#]characterized[#]spectral[#]density[#]consider[#]case[#]x[#]slowly[#]varying[#]function[#]prove[#]LAN[#]property[#]models[#]include[#]particular[#]fractional[#]Brownian[#]motion[#]ARFIMA" />
        </attvalues>
      </node>
      <node id="Céline Lacaux" label="Céline Lacaux">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]asymptotic[#]expansion[#]likelihood[#]certain[#]class[#]Gaussian[#]processes[#]characterized[#]spectral[#]density[#]consider[#]case[#]x[#]slowly[#]varying[#]function[#]prove[#]LAN[#]property[#]models[#]include[#]particular[#]fractional[#]Brownian[#]motion[#]ARFIMA" />
        </attvalues>
      </node>
      <node id="Anil Aswani" label="Anil Aswani">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Peter Bickel" label="Peter Bickel">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Claire Tomlin" label="Claire Tomlin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gerard Kerkyacharian" label="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="0" value="Let[#]random[#]sample[#]unknown[#]probability[#]density[#]f[#]defined[#]compact[#]homogeneous[#]manifold[#]M[#]dimension[#]Consider[#]frame[#]j[#]describing[#]localised[#]projection[#]onto[#]space[#]eigenfunctions[#]Laplace[#]operator[#]corresponding[#]eigenvalues[#]less[#]constructed[#]We[#]prove[#]concentration[#]inequalities[#]uniform[#]deviations[#]linear[#]needlet[#]estimator[#]obtained[#]empirical[#]estimate[#]apply[#]results[#]construct[#]estimators[#]nonasymptotic[#]confidence[#]bands[#]The[#]adaptive[#]classes[#]differentiable[#]older[#]functions[#]attain[#]lder[#]exponents" />
        </attvalues>
      </node>
      <node id="Dominique Picard" label="Dominique Picard">
        <attvalues>
          <attvalue for="0" value="Let[#]random[#]sample[#]unknown[#]probability[#]density[#]f[#]defined[#]compact[#]homogeneous[#]manifold[#]M[#]dimension[#]Consider[#]frame[#]j[#]describing[#]localised[#]projection[#]onto[#]space[#]eigenfunctions[#]Laplace[#]operator[#]corresponding[#]eigenvalues[#]less[#]constructed[#]We[#]prove[#]concentration[#]inequalities[#]uniform[#]deviations[#]linear[#]needlet[#]estimator[#]obtained[#]empirical[#]estimate[#]apply[#]results[#]construct[#]estimators[#]nonasymptotic[#]confidence[#]bands[#]The[#]adaptive[#]classes[#]differentiable[#]older[#]functions[#]attain[#]lder[#]exponents[#]This[#]paper[#]investigates[#]problem[#]selecting[#]variables[#]models[#]instrumental[#]setting[#]Our[#]study[#]motivated[#]empirically[#]verifying[#]conditional[#]convergence[#]hypothesis[#]used[#]economical[#]literature[#]concerning[#]growth[#]rate[#]To[#]avoid[#]unnecessary[#]discussion[#]choice[#]pertinence[#]embed[#]model[#]high[#]dimensional[#]propose[#]selection[#]procedure[#]optimization[#]step[#]called[#]LOLA[#]Learning[#]Out[#]Leaders[#]Adaptation[#]algorithm[#]two[#]thresholding[#]steps[#]consistency[#]proved[#]sparsity[#]conditions[#]simulations[#]conducted[#]illustrate[#]practical[#]good[#]performances[#]behavior[#]studied[#]artificially[#]added[#]without[#]priori[#]significant[#]connection[#]Using[#]provide[#]solution[#]modeling[#]link[#]initial[#]level[#]gross[#]domestic[#]product" />
        </attvalues>
      </node>
      <node id="Ci-Ren Jiang" label="Ci-Ren Jiang">
        <attvalues>
          <attvalue for="0" value="A[#]new[#]model[#]reflects[#]effects[#]single[#]index[#]proposed[#]longitudinal[#]functional[#]response[#]data[#]possibly[#]measured[#]errors[#]covariates[#]With[#]appropriate[#]initial[#]estimates[#]parametric[#]estimator[#]shown[#]n[#]asymptotically[#]normally[#]distributed[#]We[#]also[#]address[#]nonparametric[#]estimation[#]regression[#]functions[#]provide[#]optimal[#]convergence[#]rates[#]One[#]advantage[#]approach[#]bandwidth[#]used[#]estimate[#]mean[#]function[#]parameter[#]The[#]performance[#]procedure[#]studied[#]numerically" />
        </attvalues>
      </node>
      <node id="Jane-Ling Wang" label="Jane-Ling Wang">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]regression[#]models[#]situation[#]dependent[#]independent[#]variables[#]stochastic[#]processes[#]Questions[#]concerning[#]definition[#]existence[#]corresponding[#]functional[#]linear[#]basic[#]properties[#]explored[#]derive[#]representation[#]parameter[#]function[#]terms[#]canonical[#]components[#]involved[#]This[#]establishes[#]connection[#]analysis[#]suggests[#]alternative[#]approaches[#]implementation[#]A[#]specific[#]procedure[#]estimation[#]using[#]expansions[#]proposed[#]compared[#]established[#]principal[#]component[#]approach[#]As[#]example[#]application[#]present[#]mortality[#]data[#]cohorts[#]medflies[#]obtained[#]experimental[#]studies[#]aging[#]longevity[#]new[#]model[#]reflects[#]effects[#]single[#]index[#]longitudinal[#]response[#]possibly[#]measured[#]errors[#]covariates[#]With[#]appropriate[#]initial[#]estimates[#]parametric[#]estimator[#]shown[#]n[#]asymptotically[#]normally[#]distributed[#]also[#]address[#]nonparametric[#]functions[#]provide[#]optimal[#]convergence[#]rates[#]One[#]advantage[#]bandwidth[#]used[#]estimate[#]mean[#]The[#]performance[#]studied[#]numerically" />
        </attvalues>
      </node>
      <node id="Guozhong He" label="Guozhong He">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]regression[#]models[#]situation[#]dependent[#]independent[#]variables[#]stochastic[#]processes[#]Questions[#]concerning[#]definition[#]existence[#]corresponding[#]functional[#]linear[#]basic[#]properties[#]explored[#]derive[#]representation[#]parameter[#]function[#]terms[#]canonical[#]components[#]involved[#]This[#]establishes[#]connection[#]analysis[#]suggests[#]alternative[#]approaches[#]implementation[#]A[#]specific[#]procedure[#]estimation[#]using[#]expansions[#]proposed[#]compared[#]established[#]principal[#]component[#]approach[#]As[#]example[#]application[#]present[#]mortality[#]data[#]cohorts[#]medflies[#]obtained[#]experimental[#]studies[#]aging[#]longevity" />
        </attvalues>
      </node>
      <node id="Wenjing Yang" label="Wenjing Yang">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]regression[#]models[#]situation[#]dependent[#]independent[#]variables[#]stochastic[#]processes[#]Questions[#]concerning[#]definition[#]existence[#]corresponding[#]functional[#]linear[#]basic[#]properties[#]explored[#]derive[#]representation[#]parameter[#]function[#]terms[#]canonical[#]components[#]involved[#]This[#]establishes[#]connection[#]analysis[#]suggests[#]alternative[#]approaches[#]implementation[#]A[#]specific[#]procedure[#]estimation[#]using[#]expansions[#]proposed[#]compared[#]established[#]principal[#]component[#]approach[#]As[#]example[#]application[#]present[#]mortality[#]data[#]cohorts[#]medflies[#]obtained[#]experimental[#]studies[#]aging[#]longevity" />
        </attvalues>
      </node>
      <node id="Zheng-Yan Lin" label="Zheng-Yan Lin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yu-Ping Song" label="Yu-Ping Song">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Han-Chao Wang" label="Han-Chao Wang">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Nadir Maaroufi" label="Nadir Maaroufi">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]concerned[#]comparison[#]van[#]der[#]Waerden[#]scores" />
        </attvalues>
      </node>
      <node id="Camille Sabbah" label="Camille Sabbah">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]concerned[#]comparison[#]van[#]der[#]Waerden[#]scores" />
        </attvalues>
      </node>
      <node id="Yvik Swan" label="Yvik Swan">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]concerned[#]comparison[#]van[#]der[#]Waerden[#]scores" />
        </attvalues>
      </node>
      <node id="Thomas Verdebout" label="Thomas Verdebout">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]concerned[#]comparison[#]van[#]der[#]Waerden[#]scores" />
        </attvalues>
      </node>
      <node id="Lutz Duembgen" label="Lutz Duembgen">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Perla Zerial" label="Perla Zerial">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Michael Falk" label="Michael Falk">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Diana Tichy" label="Diana Tichy">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Christophe Chesneau" label="Christophe Chesneau">
        <attvalues>
          <attvalue for="0" value="A[#]nonparametric[#]additive[#]regression[#]model[#]dependent[#]observations[#]considered[#]Using[#]marginal[#]integration[#]technique[#]wavelets[#]methodology[#]develop[#]new[#]adaptive[#]estimator[#]component[#]function[#]Its[#]asymptotic[#]properties[#]investigated[#]via[#]minimax[#]approach[#]L[#]risk[#]Besov[#]balls[#]We[#]prove[#]attains[#]sharp[#]rate[#]convergence[#]turns[#]one[#]obtained[#]case[#]standard[#]univariate[#]estimation[#]problem" />
        </attvalues>
      </node>
      <node id="Jalal M. Fadili" label="Jalal M. Fadili">
        <attvalues>
          <attvalue for="0" value="A[#]nonparametric[#]additive[#]regression[#]model[#]dependent[#]observations[#]considered[#]Using[#]marginal[#]integration[#]technique[#]wavelets[#]methodology[#]develop[#]new[#]adaptive[#]estimator[#]component[#]function[#]Its[#]asymptotic[#]properties[#]investigated[#]via[#]minimax[#]approach[#]L[#]risk[#]Besov[#]balls[#]We[#]prove[#]attains[#]sharp[#]rate[#]convergence[#]turns[#]one[#]obtained[#]case[#]standard[#]univariate[#]estimation[#]problem" />
        </attvalues>
      </node>
      <node id="Bertrand Maillot" label="Bertrand Maillot">
        <attvalues>
          <attvalue for="0" value="A[#]nonparametric[#]additive[#]regression[#]model[#]dependent[#]observations[#]considered[#]Using[#]marginal[#]integration[#]technique[#]wavelets[#]methodology[#]develop[#]new[#]adaptive[#]estimator[#]component[#]function[#]Its[#]asymptotic[#]properties[#]investigated[#]via[#]minimax[#]approach[#]L[#]risk[#]Besov[#]balls[#]We[#]prove[#]attains[#]sharp[#]rate[#]convergence[#]turns[#]one[#]obtained[#]case[#]standard[#]univariate[#]estimation[#]problem" />
        </attvalues>
      </node>
      <node id="M. Lerasle" label="M. Lerasle">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="R. I. Oliveira" label="R. I. Oliveira">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Cécile Durot" label="Cécile Durot">
        <attvalues>
          <attvalue for="0" value="Let[#]f[#]nonincreasing[#]function[#]defined[#]Under[#]standard[#]regularity[#]conditions[#]derive[#]asymptotic[#]distribution[#]supremum[#]norm[#]difference[#]estimator[#]The[#]rate[#]convergence[#]found[#]order[#]n[#]limiting[#]Gumbel" />
        </attvalues>
      </node>
      <node id="Vladimir N. Kulikov" label="Vladimir N. Kulikov">
        <attvalues>
          <attvalue for="0" value="Let[#]f[#]nonincreasing[#]function[#]defined[#]Under[#]standard[#]regularity[#]conditions[#]derive[#]asymptotic[#]distribution[#]supremum[#]norm[#]difference[#]estimator[#]The[#]rate[#]convergence[#]found[#]order[#]n[#]limiting[#]Gumbel" />
        </attvalues>
      </node>
      <node id="Hendrik P. Lopuhaä" label="Hendrik P. Lopuhaä">
        <attvalues>
          <attvalue for="0" value="Let[#]f[#]nonincreasing[#]function[#]defined[#]Under[#]standard[#]regularity[#]conditions[#]derive[#]asymptotic[#]distribution[#]supremum[#]norm[#]difference[#]estimator[#]The[#]rate[#]convergence[#]found[#]order[#]n[#]limiting[#]Gumbel" />
        </attvalues>
      </node>
      <node id="Rajen D. Shah" label="Rajen D. Shah">
        <attvalues>
          <attvalue for="0" value="Stability[#]Selection[#]recently[#]introduced[#]Meinshausen[#]Buhlmann[#]general[#]technique[#]designed[#]improve[#]performance[#]variable[#]selection[#]algorithm[#]It[#]based[#]aggregating[#]results[#]applying[#]procedure[#]subsamples[#]data[#]We[#]introduce[#]variant[#]called[#]Complementary[#]Pairs[#]CPSS[#]derive[#]bounds[#]expected[#]number[#]variables[#]included[#]low[#]probability[#]original[#]high[#]excluded[#]These[#]require[#]exchangeability[#]assumptions[#]underlying[#]model[#]quality[#]Under[#]reasonable[#]shape[#]restrictions[#]tightened[#]yielding[#]improved[#]error[#]control[#]therefore[#]increasing[#]applicability[#]methodology" />
        </attvalues>
      </node>
      <node id="Richard J. Samworth" label="Richard J. Samworth">
        <attvalues>
          <attvalue for="0" value="Stability[#]Selection[#]recently[#]introduced[#]Meinshausen[#]Buhlmann[#]general[#]technique[#]designed[#]improve[#]performance[#]variable[#]selection[#]algorithm[#]It[#]based[#]aggregating[#]results[#]applying[#]procedure[#]subsamples[#]data[#]We[#]introduce[#]variant[#]called[#]Complementary[#]Pairs[#]CPSS[#]derive[#]bounds[#]expected[#]number[#]variables[#]included[#]low[#]probability[#]original[#]high[#]excluded[#]These[#]require[#]exchangeability[#]assumptions[#]underlying[#]model[#]quality[#]Under[#]reasonable[#]shape[#]restrictions[#]tightened[#]yielding[#]improved[#]error[#]control[#]therefore[#]increasing[#]applicability[#]methodology" />
        </attvalues>
      </node>
      <node id="Aleksey S. Polunchenko" label="Aleksey S. Polunchenko">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexander G. Tartakovsky" label="Alexander G. Tartakovsky">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="I. Bairamov" label="I. Bairamov">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="K. Bayramoglu" label="K. Bayramoglu">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Alexander Meister" label="Alexander Meister">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Yannick Baraud" label="Yannick Baraud">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Lucien Birgé" label="Lucien Birgé">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mireille Gettler Summa" label="Mireille Gettler Summa">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Francesco Palumbo" label="Francesco Palumbo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Cristina Tortora" label="Cristina Tortora">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Guillaume Lecué" label="Guillaume Lecué">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]argument[#]based[#]multidimensional[#]uniform[#]central[#]limit[#]theorems[#]proving[#]geometrical[#]assumptions[#]target[#]function[#]T[#]learning[#]class[#]F[#]excess[#]risk[#]empirical[#]minimization[#]algorithm[#]lower[#]bounded[#]E[#]Q[#]n[#]canonical[#]Gaussian[#]process[#]associated[#]well[#]chosen[#]subset[#]parameter[#]governing[#]oscillations[#]small[#]ball" />
        </attvalues>
      </node>
      <node id="Shahar Mendelson" label="Shahar Mendelson">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]argument[#]based[#]multidimensional[#]uniform[#]central[#]limit[#]theorems[#]proving[#]geometrical[#]assumptions[#]target[#]function[#]T[#]learning[#]class[#]F[#]excess[#]risk[#]empirical[#]minimization[#]algorithm[#]lower[#]bounded[#]E[#]Q[#]n[#]canonical[#]Gaussian[#]process[#]associated[#]well[#]chosen[#]subset[#]parameter[#]governing[#]oscillations[#]small[#]ball" />
        </attvalues>
      </node>
      <node id="Werner Ehm" label="Werner Ehm">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Mahdi Bashiri" label="Mahdi Bashiri">
        <attvalues>
          <attvalue for="0" value="The[#]multiresponse[#]surface[#]problem[#]modelled[#]one[#]multiobjective[#]stochastic[#]optimisation[#]diverse[#]solutions[#]proposed[#]Several[#]crucial[#]differences[#]highlighted[#]approach[#]others[#]Finally[#]numerical[#]example[#]particular[#]applied[#]described[#]detail" />
        </attvalues>
      </node>
      <node id="Yves Rozenholc" label="Yves Rozenholc">
        <attvalues>
          <attvalue for="0" value="In[#]present[#]paper[#]consider[#]Laplace[#]deconvolution[#]discrete[#]noisy[#]data[#]observed[#]interval[#]whose[#]length[#]may[#]increase[#]sample[#]size[#]Although[#]problem[#]arises[#]variety[#]applications[#]best[#]knowledge[#]given[#]little[#]attention[#]statistical[#]community[#]Our[#]objective[#]fill[#]gap[#]provide[#]treatment[#]The[#]main[#]contribution[#]explicit[#]construction[#]asymptotically[#]minimax[#]sense[#]estimator[#]adaptive[#]regularity[#]unknown[#]function[#]We[#]show[#]original[#]reduced[#]nonparametric[#]estimation[#]regression[#]derivatives[#]growing[#]Whereas[#]forms[#]estimators[#]remain[#]standard[#]choices[#]parameters[#]convergence[#]rates[#]expressed[#]terms[#]case[#]affected[#]asymptotic[#]growth[#]derive[#]kernel[#]interest[#]establish[#]minimaxity[#]range[#]Sobolev[#]classes[#]illustrate[#]theory[#]examples[#]expressions[#]A[#]simulation[#]study[#]shows[#]addition[#]providing[#]optimality[#]number[#]observations[#]turns[#]infinity[#]proposed[#]demonstrates[#]good[#]performance[#]finite" />
        </attvalues>
      </node>
      <node id="Maxime Lenormand" label="Maxime Lenormand">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Floriana Gargiulo" label="Floriana Gargiulo">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Siegfried Hörmann" label="Siegfried Hörmann">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]develops[#]framework[#]estimation[#]functional[#]mean[#]principal[#]components[#]functions[#]form[#]random[#]field[#]More[#]specifically[#]data[#]study[#]consist[#]curves[#]X[#]T[#]observed[#]spatial[#]points[#]We[#]establish[#]conditions[#]sample[#]average[#]space[#]consistent[#]estimator[#]population[#]function[#]usual[#]empirical[#]covariance[#]operator[#]These[#]involve[#]interplay[#]assumptions[#]appropriately[#]defined[#]dependence[#]distribution[#]The[#]rates[#]convergence[#]may[#]samples[#]generally[#]depend[#]strength[#]quantified[#]distances[#]also[#]formulate[#]lack[#]consistency" />
        </attvalues>
      </node>
      <node id="Péter Kevei" label="Péter Kevei">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="David M. Mason" label="David M. Mason">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Oliver Linton" label="Oliver Linton">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]general[#]principle[#]estimating[#]regression[#]function[#]nonparametrically[#]allowing[#]wide[#]variety[#]data[#]filtering[#]example[#]repeated[#]left[#]truncation[#]right[#]censoring[#]Both[#]mean[#]median[#]cases[#]considered[#]The[#]method[#]works[#]first[#]conditional[#]hazard[#]survivor[#]integrating[#]also[#]investigate[#]improved[#]methods[#]take[#]account[#]model[#]structure[#]independent[#]errors[#]show[#]improve[#]performance[#]true[#]establish[#]pointwise[#]asymptotic[#]normality[#]estimators" />
        </attvalues>
      </node>
      <node id="Jens Perch Nielsen" label="Jens Perch Nielsen">
        <attvalues>
          <attvalue for="0" value="We[#]present[#]general[#]principle[#]estimating[#]regression[#]function[#]nonparametrically[#]allowing[#]wide[#]variety[#]data[#]filtering[#]example[#]repeated[#]left[#]truncation[#]right[#]censoring[#]Both[#]mean[#]median[#]cases[#]considered[#]The[#]method[#]works[#]first[#]conditional[#]hazard[#]survivor[#]integrating[#]also[#]investigate[#]improved[#]methods[#]take[#]account[#]model[#]structure[#]independent[#]errors[#]show[#]improve[#]performance[#]true[#]establish[#]pointwise[#]asymptotic[#]normality[#]estimators" />
        </attvalues>
      </node>
      <node id="Taisei Kudo" label="Taisei Kudo">
        <attvalues>
          <attvalue for="0" value="We[#]give[#]exponential[#]lower[#]bound[#]Graver[#]complexity[#]incidence[#]matrix[#]complete[#]bipartite[#]graph[#]arbitrary[#]size[#]Our[#]result[#]generalization[#]Berstein[#]Onn[#]graphs[#]r" />
        </attvalues>
      </node>
      <node id="Ting Yan" label="Ting Yan">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]show[#]Wilks[#]type[#]results[#]model[#]Specifically[#]simple[#]composite[#]null[#]hypotheses[#]interest[#]likelihood[#]ratio[#]test[#]statistic[#]enjoys[#]approximation[#]sense[#]L[#]N[#]p[#]goes[#]infinity[#]corresponding[#]degrees[#]freedom[#]Simulation[#]studies[#]application[#]NBA[#]data[#]illustrate[#]theoretical" />
        </attvalues>
      </node>
      <node id="Yuanzhang Li" label="Yuanzhang Li">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]show[#]Wilks[#]type[#]results[#]model[#]Specifically[#]simple[#]composite[#]null[#]hypotheses[#]interest[#]likelihood[#]ratio[#]test[#]statistic[#]enjoys[#]approximation[#]sense[#]L[#]N[#]p[#]goes[#]infinity[#]corresponding[#]degrees[#]freedom[#]Simulation[#]studies[#]application[#]NBA[#]data[#]illustrate[#]theoretical" />
        </attvalues>
      </node>
      <node id="Jinfeng Xu" label="Jinfeng Xu">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]show[#]Wilks[#]type[#]results[#]model[#]Specifically[#]simple[#]composite[#]null[#]hypotheses[#]interest[#]likelihood[#]ratio[#]test[#]statistic[#]enjoys[#]approximation[#]sense[#]L[#]N[#]p[#]goes[#]infinity[#]corresponding[#]degrees[#]freedom[#]Simulation[#]studies[#]application[#]NBA[#]data[#]illustrate[#]theoretical" />
        </attvalues>
      </node>
      <node id="Yaning Yang" label="Yaning Yang">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]show[#]Wilks[#]type[#]results[#]model[#]Specifically[#]simple[#]composite[#]null[#]hypotheses[#]interest[#]likelihood[#]ratio[#]test[#]statistic[#]enjoys[#]approximation[#]sense[#]L[#]N[#]p[#]goes[#]infinity[#]corresponding[#]degrees[#]freedom[#]Simulation[#]studies[#]application[#]NBA[#]data[#]illustrate[#]theoretical" />
        </attvalues>
      </node>
      <node id="Ji Zhu" label="Ji Zhu">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]show[#]Wilks[#]type[#]results[#]model[#]Specifically[#]simple[#]composite[#]null[#]hypotheses[#]interest[#]likelihood[#]ratio[#]test[#]statistic[#]enjoys[#]approximation[#]sense[#]L[#]N[#]p[#]goes[#]infinity[#]corresponding[#]degrees[#]freedom[#]Simulation[#]studies[#]application[#]NBA[#]data[#]illustrate[#]theoretical" />
        </attvalues>
      </node>
      <node id="Giorgio Celant" label="Giorgio Celant">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Marco Di Battista" label="Marco Di Battista">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Samuela Leoni-Aubin" label="Samuela Leoni-Aubin">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="C. Bernard-Michel" label="C. Bernard-Michel">
        <attvalues>
          <attvalue for="0" value="Sliced[#]Inverse[#]Regression[#]SIR[#]effective[#]method[#]dimension[#]reduction[#]regression[#]problems[#]The[#]original[#]however[#]requires[#]inversion[#]predictors[#]covariance[#]matrix[#]In[#]case[#]collinearity[#]small[#]sample[#]sizes[#]compared[#]possible[#]regularization[#]technique[#]used[#]Our[#]approach[#]based[#]Fisher[#]Lecture[#]given[#]Cook[#]shown[#]axes[#]interpreted[#]solutions[#]inverse[#]problem[#]paper[#]Gaussian[#]prior[#]distribution[#]introduced[#]unknown[#]parameters[#]order[#]regularize[#]estimation[#]We[#]show[#]existing[#]regularizations[#]enter[#]framework[#]permits[#]global[#]understanding[#]methods[#]Three[#]new[#]priors[#]proposed[#]leading[#]A[#]comparison[#]simulated[#]data[#]provided" />
        </attvalues>
      </node>
      <node id="Kanti V. Mardia" label="Kanti V. Mardia">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Jochen Voss" label="Jochen Voss">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Kshitij Khare" label="Kshitij Khare">
        <attvalues>
          <attvalue for="0" value="Gaussian[#]covariance[#]graph[#]models[#]encode[#]marginal[#]independence[#]among[#]components[#]multivariate[#]random[#]vector[#]means[#]G[#]These[#]distinctly[#]different[#]traditional[#]concentration[#]often[#]also[#]referred[#]graphical[#]selection[#]since[#]zeros[#]parameter[#]reflected[#]matrix[#]compared[#]The[#]space[#]interest[#]cone[#]positive[#]definite[#]matrices[#]fixed[#]corresponding[#]missing[#]edges[#]As[#]Letac[#]Massam[#]Ann[#]Statist[#]consider[#]case[#]decomposable[#]In[#]paper[#]construct[#]family[#]Wishart[#]distributions[#]serve[#]similar[#]purpose[#]setting[#]constructed[#]Dawid[#]Lauritzen[#]We[#]proceed[#]undertake[#]rigorous[#]study[#]derive[#]several[#]deep[#]useful[#]properties[#]class" />
        </attvalues>
      </node>
      <node id="Gustavo Didier" label="Gustavo Didier">
        <attvalues>
          <attvalue for="0" value="Operator[#]fractional[#]Brownian[#]motions[#]OFBMs[#]Gaussian[#]ii[#]operator[#]iii[#]stationary[#]increment[#]processes[#]They[#]natural[#]multivariate[#]generalizations[#]Because[#]possible[#]lack[#]defining[#]properties[#]general[#]characterize[#]covariance[#]structure[#]To[#]circumvent[#]problem[#]class[#]characterized[#]means[#]integral[#]representations[#]spectral[#]time[#]domains[#]For[#]domain[#]involves[#]showing[#]shapes[#]density[#]representation[#]The[#]derived[#]using[#]primary[#]matrix[#]functions[#]taking[#]Fourier[#]transforms[#]deterministic[#]kernels[#]Necessary[#]sufficient[#]conditions[#]established[#]terms[#]It[#]also[#]shown[#]increments[#]OFBM[#]rigid[#]called[#]dichotomy[#]principle[#]notion[#]explored" />
        </attvalues>
      </node>
      <node id="Vladas Pipiras" label="Vladas Pipiras">
        <attvalues>
          <attvalue for="0" value="Operator[#]fractional[#]Brownian[#]motions[#]OFBMs[#]Gaussian[#]ii[#]operator[#]iii[#]stationary[#]increment[#]processes[#]They[#]natural[#]multivariate[#]generalizations[#]Because[#]possible[#]lack[#]defining[#]properties[#]general[#]characterize[#]covariance[#]structure[#]To[#]circumvent[#]problem[#]class[#]characterized[#]means[#]integral[#]representations[#]spectral[#]time[#]domains[#]For[#]domain[#]involves[#]showing[#]shapes[#]density[#]representation[#]The[#]derived[#]using[#]primary[#]matrix[#]functions[#]taking[#]Fourier[#]transforms[#]deterministic[#]kernels[#]Necessary[#]sufficient[#]conditions[#]established[#]terms[#]It[#]also[#]shown[#]increments[#]OFBM[#]rigid[#]called[#]dichotomy[#]principle[#]notion[#]explored" />
        </attvalues>
      </node>
      <node id="Andrea Krajina" label="Andrea Krajina">
        <attvalues>
          <attvalue for="0" value="Consider[#]random[#]sample[#]attraction[#]multivariate[#]extreme[#]value[#]distribution[#]dependence[#]structure[#]attractor[#]belongs[#]parametric[#]model[#]A[#]new[#]estimator[#]unknown[#]parameter[#]defined[#]minimizes[#]distance[#]vector[#]weighted[#]integrals[#]tail[#]function[#]empirical[#]counterparts[#]The[#]minimization[#]problem[#]probability[#]tending[#]one[#]unique[#]global[#]solution[#]consistent[#]asymptotically[#]normal[#]spectral[#]measures[#]models[#]method[#]applies[#]discrete[#]continuous[#]Examples[#]demonstrate[#]applicability[#]performance" />
        </attvalues>
      </node>
      <node id="Johan Segers" label="Johan Segers">
        <attvalues>
          <attvalue for="0" value="Consider[#]random[#]sample[#]attraction[#]multivariate[#]extreme[#]value[#]distribution[#]dependence[#]structure[#]attractor[#]belongs[#]parametric[#]model[#]A[#]new[#]estimator[#]unknown[#]parameter[#]defined[#]minimizes[#]distance[#]vector[#]weighted[#]integrals[#]tail[#]function[#]empirical[#]counterparts[#]The[#]minimization[#]problem[#]probability[#]tending[#]one[#]unique[#]global[#]solution[#]consistent[#]asymptotically[#]normal[#]spectral[#]measures[#]models[#]method[#]applies[#]discrete[#]continuous[#]Examples[#]demonstrate[#]applicability[#]performance" />
        </attvalues>
      </node>
      <node id="Nathan Huntley" label="Nathan Huntley">
        <attvalues>
          <attvalue for="0" value="We[#]revisit[#]reinterpret[#]Selten[#]concept[#]subgame[#]perfectness[#]context[#]single[#]agent[#]normal[#]form[#]sequential[#]decision[#]making[#]leads[#]us[#]subtree[#]Thereby[#]extend[#]characterization[#]extensive[#]consequentialist[#]consistent[#]behaviour[#]norms[#]arbitrary[#]choice[#]functions[#]assumptions[#]In[#]particular[#]need[#]assume[#]probabilities[#]event[#]utilities[#]reward[#]show[#]equivalent[#]equivalence[#]sufficient[#]perhaps[#]surprisingly[#]necessary[#]backward[#]induction[#]work" />
        </attvalues>
      </node>
      <node id="Matthias C. M. Troffaes" label="Matthias C. M. Troffaes">
        <attvalues>
          <attvalue for="0" value="We[#]revisit[#]reinterpret[#]Selten[#]concept[#]subgame[#]perfectness[#]context[#]single[#]agent[#]normal[#]form[#]sequential[#]decision[#]making[#]leads[#]us[#]subtree[#]Thereby[#]extend[#]characterization[#]extensive[#]consequentialist[#]consistent[#]behaviour[#]norms[#]arbitrary[#]choice[#]functions[#]assumptions[#]In[#]particular[#]need[#]assume[#]probabilities[#]event[#]utilities[#]reward[#]show[#]equivalent[#]equivalence[#]sufficient[#]perhaps[#]surprisingly[#]necessary[#]backward[#]induction[#]work" />
        </attvalues>
      </node>
      <node id="Alessandro Felluga" label="Alessandro Felluga">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Stefano Tiziani" label="Stefano Tiziani">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Rémi Gribonval" label="Rémi Gribonval">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Gilles Chardon" label="Gilles Chardon">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Laurent Daudet" label="Laurent Daudet">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Olivier Collier" label="Olivier Collier">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Arnak S. Dalalyan" label="Arnak S. Dalalyan">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="A. Garivier" label="A. Garivier">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Rama Cont" label="Rama Cont">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Cecilia Mancini" label="Cecilia Mancini">
        <attvalues>
          <attvalue for="0" value="" />
        </attvalues>
      </node>
      <node id="Li Wang" label="Li Wang">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]generalized[#]additive[#]partial[#]linear[#]models[#]proposing[#]use[#]polynomial[#]spline[#]smoothing[#]estimation[#]nonparametric[#]functions[#]deriving[#]based[#]estimators[#]parameters[#]establish[#]asymptotic[#]normality[#]parametric[#]components[#]The[#]procedure[#]avoids[#]solving[#]large[#]systems[#]equations[#]procedures[#]thus[#]results[#]gains[#]computational[#]simplicity[#]develop[#]class[#]variable[#]selection[#]employing[#]nonconcave[#]penalized[#]shown[#]oracle[#]property[#]Monte[#]Carlo[#]simulations[#]empirical[#]example[#]presented[#]illustration" />
        </attvalues>
      </node>
      <node id="Xiang Liu" label="Xiang Liu">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]generalized[#]additive[#]partial[#]linear[#]models[#]proposing[#]use[#]polynomial[#]spline[#]smoothing[#]estimation[#]nonparametric[#]functions[#]deriving[#]based[#]estimators[#]parameters[#]establish[#]asymptotic[#]normality[#]parametric[#]components[#]The[#]procedure[#]avoids[#]solving[#]large[#]systems[#]equations[#]procedures[#]thus[#]results[#]gains[#]computational[#]simplicity[#]develop[#]class[#]variable[#]selection[#]employing[#]nonconcave[#]penalized[#]shown[#]oracle[#]property[#]Monte[#]Carlo[#]simulations[#]empirical[#]example[#]presented[#]illustration" />
        </attvalues>
      </node>
      <node id="Raymond J. Carroll" label="Raymond J. Carroll">
        <attvalues>
          <attvalue for="0" value="We[#]study[#]generalized[#]additive[#]partial[#]linear[#]models[#]proposing[#]use[#]polynomial[#]spline[#]smoothing[#]estimation[#]nonparametric[#]functions[#]deriving[#]based[#]estimators[#]parameters[#]establish[#]asymptotic[#]normality[#]parametric[#]components[#]The[#]procedure[#]avoids[#]solving[#]large[#]systems[#]equations[#]procedures[#]thus[#]results[#]gains[#]computational[#]simplicity[#]develop[#]class[#]variable[#]selection[#]employing[#]nonconcave[#]penalized[#]shown[#]oracle[#]property[#]Monte[#]Carlo[#]simulations[#]empirical[#]example[#]presented[#]illustration" />
        </attvalues>
      </node>
      <node id="Rafał Kulik" label="Rafał Kulik">
        <attvalues>
          <attvalue for="0" value="We[#]consider[#]Stochastic[#]Volatility[#]processes[#]heavy[#]tails[#]possible[#]long[#]memory[#]volatility[#]study[#]limiting[#]conditional[#]distribution[#]future[#]events[#]given[#]present[#]past[#]event[#]extreme[#]level[#]tends[#]infinity[#]Even[#]though[#]extremes[#]stochastic[#]asymptotically[#]independent[#]sense[#]value[#]theory[#]distributions[#]differ[#]case[#]introduce[#]estimators[#]asymptotic[#]properties[#]If[#]rate[#]convergence[#]centered[#]depend[#]parameter[#]Hurst[#]index" />
        </attvalues>
      </node>
      <node id="Emmanuel Boissard" label="Emmanuel Boissard">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]tackle[#]problem[#]comparing[#]distributions[#]random[#]variables[#]defining[#]mean[#]pattern[#]sample[#]events[#]Using[#]barycenters[#]measures[#]Wasserstein[#]space[#]propose[#]iterative[#]version[#]estimation[#]distribution[#]Moreover[#]common[#]measure[#]warped[#]centered[#]operator[#]barycenter[#]enables[#]recover[#]template" />
        </attvalues>
      </node>
      <node id="Thibaut Le Gouic" label="Thibaut Le Gouic">
        <attvalues>
          <attvalue for="0" value="In[#]paper[#]tackle[#]problem[#]comparing[#]distributions[#]random[#]variables[#]defining[#]mean[#]pattern[#]sample[#]events[#]Using[#]barycenters[#]measures[#]Wasserstein[#]space[#]propose[#]iterative[#]version[#]estimation[#]distribution[#]Moreover[#]common[#]measure[#]warped[#]centered[#]operator[#]barycenter[#]enables[#]recover[#]template" />
        </attvalues>
      </node>
      <node id="Mathilde Mougeot" label="Mathilde Mougeot">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]problem[#]selecting[#]variables[#]models[#]instrumental[#]setting[#]Our[#]study[#]motivated[#]empirically[#]verifying[#]conditional[#]convergence[#]hypothesis[#]used[#]economical[#]literature[#]concerning[#]growth[#]rate[#]To[#]avoid[#]unnecessary[#]discussion[#]choice[#]pertinence[#]embed[#]model[#]high[#]dimensional[#]We[#]propose[#]selection[#]procedure[#]optimization[#]step[#]called[#]LOLA[#]Learning[#]Out[#]Leaders[#]Adaptation[#]algorithm[#]two[#]thresholding[#]steps[#]The[#]consistency[#]proved[#]sparsity[#]conditions[#]simulations[#]conducted[#]illustrate[#]practical[#]good[#]performances[#]behavior[#]studied[#]artificially[#]added[#]without[#]priori[#]significant[#]connection[#]Using[#]provide[#]solution[#]modeling[#]link[#]initial[#]level[#]gross[#]domestic[#]product[#]prove" />
        </attvalues>
      </node>
      <node id="Karine Tribouley" label="Karine Tribouley">
        <attvalues>
          <attvalue for="0" value="This[#]paper[#]investigates[#]problem[#]selecting[#]variables[#]models[#]instrumental[#]setting[#]Our[#]study[#]motivated[#]empirically[#]verifying[#]conditional[#]convergence[#]hypothesis[#]used[#]economical[#]literature[#]concerning[#]growth[#]rate[#]To[#]avoid[#]unnecessary[#]discussion[#]choice[#]pertinence[#]embed[#]model[#]high[#]dimensional[#]We[#]propose[#]selection[#]procedure[#]optimization[#]step[#]called[#]LOLA[#]Learning[#]Out[#]Leaders[#]Adaptation[#]algorithm[#]two[#]thresholding[#]steps[#]The[#]consistency[#]proved[#]sparsity[#]conditions[#]simulations[#]conducted[#]illustrate[#]practical[#]good[#]performances[#]behavior[#]studied[#]artificially[#]added[#]without[#]priori[#]significant[#]connection[#]Using[#]provide[#]solution[#]modeling[#]link[#]initial[#]level[#]gross[#]domestic[#]product[#]prove" />
        </attvalues>
      </node>
      <node id="Daniel Berend" label="Daniel Berend">
        <attvalues>
          <attvalue for="0" value="We[#]give[#]tight[#]lower[#]upper[#]bounds[#]expected[#]missing[#]mass[#]distributions[#]finite[#]countably[#]infinite[#]spaces[#]An[#]essential[#]characterization[#]extremal[#]given[#]also[#]provide[#]extension[#]totally[#]bounded[#]metric[#]may[#]independent[#]interest" />
        </attvalues>
      </node>
      <node id="Aryeh Kontorovich" label="Aryeh Kontorovich">
        <attvalues>
          <attvalue for="0" value="We[#]give[#]tight[#]lower[#]upper[#]bounds[#]expected[#]missing[#]mass[#]distributions[#]finite[#]countably[#]infinite[#]spaces[#]An[#]essential[#]characterization[#]extremal[#]given[#]also[#]provide[#]extension[#]totally[#]bounded[#]metric[#]may[#]independent[#]interest" />
        </attvalues>
      </node>
      <node id="T. Tony Cai" label="T. Tony Cai">
        <attvalues>
          <attvalue for="0" value="A[#]general[#]lower[#]bound[#]developed[#]minimax[#]risk[#]estimating[#]arbitrary[#]functional[#]The[#]based[#]testing[#]two[#]composite[#]hypotheses[#]shown[#]effective[#]nonsmooth[#]n[#]observation[#]N[#]This[#]problem[#]exhibits[#]features[#]significantly[#]different[#]occur[#]conventional[#]smooth[#]functionals[#]setting[#]standard[#]techniques[#]fail[#]yield[#]sharp[#]results[#]established[#]applying[#]technique[#]key[#]step[#]construction[#]special[#]priors[#]bounding[#]distance[#]normal[#]mixtures[#]An[#]estimator[#]constructed[#]using[#]approximation[#]theory[#]Hermite[#]polynomials[#]asymptotically[#]means[#]bounded[#]given[#]value[#]M[#]It[#]equals[#]Bernstein[#]constant[#]present[#]paper[#]also[#]used[#]solve[#]related[#]problems" />
        </attvalues>
      </node>
      <node id="Mark G. Low" label="Mark G. Low">
        <attvalues>
          <attvalue for="0" value="A[#]general[#]lower[#]bound[#]developed[#]minimax[#]risk[#]estimating[#]arbitrary[#]functional[#]The[#]based[#]testing[#]two[#]composite[#]hypotheses[#]shown[#]effective[#]nonsmooth[#]n[#]observation[#]N[#]This[#]problem[#]exhibits[#]features[#]significantly[#]different[#]occur[#]conventional[#]smooth[#]functionals[#]setting[#]standard[#]techniques[#]fail[#]yield[#]sharp[#]results[#]established[#]applying[#]technique[#]key[#]step[#]construction[#]special[#]priors[#]bounding[#]distance[#]normal[#]mixtures[#]An[#]estimator[#]constructed[#]using[#]approximation[#]theory[#]Hermite[#]polynomials[#]asymptotically[#]means[#]bounded[#]given[#]value[#]M[#]It[#]equals[#]Bernstein[#]constant[#]present[#]paper[#]also[#]used[#]solve[#]related[#]problems" />
        </attvalues>
      </node>
    </nodes>
    <edges>
      <edge id="0" source="Fabienne Comte" target="Jan Johannes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2509v2" />
          <attvalue for="2" value="Adaptive functional linear regression" />
          <attvalue for="3" value="We consider the estimation of the slope function in functional linear&#10;regression, where scalar responses are modeled in dependence of random&#10;functions. Cardot and Johannes [J. Multivariate Anal. 101 (2010) 395-408] have&#10;shown that a thresholded projection estimator can attain up to a constant&#10;minimax-rates of convergence in a general framework which allows us to cover&#10;the prediction problem with respect to the mean squared prediction error as&#10;well as the estimation of the slope function and its derivatives. This&#10;estimation procedure, however, requires an optimal choice of a tuning parameter&#10;with regard to certain characteristics of the slope function and the covariance&#10;operator associated with the functional regressor. As this information is&#10;usually inaccessible in practice, we investigate a fully data-driven choice of&#10;the tuning parameter which combines model selection and Lepski's method. It is&#10;inspired by the recent work of Goldenshluger and Lepski [Ann. Statist. 39&#10;(2011) 1608-1632]. The tuning parameter is selected as minimizer of a&#10;stochastic penalized contrast function imitating Lepski's method among a random&#10;collection of admissible values. This choice of the tuning parameter depends&#10;only on the data and we show that within the general framework the resulting&#10;data-driven thresholded projection estimator can attain minimax-rates up to a&#10;constant over a variety of classes of slope functions and covariance operators.&#10;The results are illustrated considering different configurations which cover in&#10;particular the prediction problem as well as the estimation of the slope and&#10;its derivatives. A simulation study shows the reasonable performance of the&#10;fully data-driven estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="1" source="Fabienne Comte" target="Valentine Genon-Catalot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2424v1" />
          <attvalue for="2" value="Estimation for Lévy processes from high frequency data within a long&#10;  time interval" />
          <attvalue for="3" value="In this paper, we study nonparametric estimation of the L\'{e}vy density for&#10;L\'{e}vy processes, with and without Brownian component. For this, we consider&#10;$n$ discrete time observations with step $\Delta$. The asymptotic framework is:&#10;$n$ tends to infinity, $\Delta=\Delta_n$ tends to zero while $n\Delta_n$ tends&#10;to infinity. We use a Fourier approach to construct an adaptive nonparametric&#10;estimator of the L\'{e}vy density and to provide a bound for the global&#10;${\mathbb{L}}^2$-risk. Estimators of the drift and of the variance of the&#10;Gaussian component are also studied. We discuss rates of convergence and give&#10;examples and simulation results for processes fitting in our framework." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="2" source="Jan Johannes" target="Rudolf Schenk">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2855v1" />
          <attvalue for="2" value="Adaptive estimation of linear functionals in functional linear models" />
          <attvalue for="3" value="We consider the estimation of the value of a linear functional of the slope&#10;parameter in functional linear regression, where scalar responses are modeled&#10;in dependence of random functions. In Johannes and Schenk [2010] it has been&#10;shown that a plug-in estimator based on dimension reduction and additional&#10;thresholding can attain minimax optimal rates of convergence up to a constant.&#10;However, this estimation procedure requires an optimal choice of a tuning&#10;parameter with regard to certain characteristics of the slope function and the&#10;covariance operator associated with the functional regressor. As these are&#10;unknown in practice, we investigate a fully data-driven choice of the tuning&#10;parameter based on a combination of model selection and Lepski's method, which&#10;is inspired by the recent work of Goldenshluger and Lepski [2011]. The tuning&#10;parameter is selected as the minimizer of a stochastic penalized contrast&#10;function imitating Lepski's method among a random collection of admissible&#10;values. We show that this adaptive procedure attains the lower bound for the&#10;minimax risk up to a logarithmic factor over a wide range of classes of slope&#10;functions and covariance operators. In particular, our theory covers point-wise&#10;estimation as well as the estimation of local averages of the slope parameter." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="3" source="Jan Johannes" target="Christoph Breunig">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.0961v1" />
          <attvalue for="2" value="Adaptive estimation of functionals in nonparametric instrumental&#10;  regression" />
          <attvalue for="3" value="We consider the problem of estimating the value l({\phi}) of a linear&#10;functional, where the structural function {\phi} models a nonparametric&#10;relationship in presence of instrumental variables. We propose a plug-in&#10;estimator which is based on a dimension reduction technique and additional&#10;thresholding. It is shown that this estimator is consistent and can attain the&#10;minimax optimal rate of convergence under additional regularity conditions.&#10;This, however, requires an optimal choice of the dimension parameter m&#10;depending on certain characteristics of the structural function {\phi} and the&#10;joint distribution of the regressor and the instrument, which are unknown in&#10;practice. We propose a fully data driven choice of m which combines model&#10;selection and Lepski's method. We show that the adaptive estimator attains the&#10;optimal rate of convergence up to a logarithmic factor. The theory in this&#10;paper is illustrated by considering classical smoothness assumptions and we&#10;discuss examples such as pointwise estimation or estimation of averages of the&#10;structural function {\phi}." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="4" source="Brice Franke" target="Tatsuhiko Saigo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5241v1" />
          <attvalue for="2" value="A self-similar process arising from a random walk with random&#10;  environment in random scenery" />
          <attvalue for="3" value="In this article, we merge celebrated results of Kesten and Spitzer [Z.&#10;Wahrsch. Verw. Gebiete 50 (1979) 5-25] and Kawazu and Kesten [J. Stat. Phys. 37&#10;(1984) 561-575]. A random walk performs a motion in an i.i.d. environment and&#10;observes an i.i.d. scenery along its path. We assume that the scenery is in the&#10;domain of attraction of a stable distribution and prove that the resulting&#10;observations satisfy a limit theorem. The resulting limit process is a&#10;self-similar stochastic process with non-trivial dependencies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="5" source="Michel Broniatowski" target="Aida Toma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.1541v1" />
          <attvalue for="2" value="Decomposable Pseudodistances and Applications in Statistical Estimation" />
          <attvalue for="3" value="The aim of this paper is to introduce new statistical criterions for&#10;estimation, suitable for inference in models with common continuous support.&#10;This proposal is in the direct line of a renewed interest for divergence based&#10;inference tools imbedding the most classical ones, such as maximum likelihood,&#10;Chi-square or Kullback Leibler. General pseudodistances with decomposable&#10;structure are considered, they allowing to define minimum pseudodistance&#10;estimators, without using nonparametric density estimators. A special class of&#10;pseudodistances indexed by {\alpha}&gt;0, leading for {\alpha}\downarrow0 to the&#10;Kulback Leibler divergence, is presented in detail. Corresponding estimation&#10;criteria are developed and asymptotic properties are studied. The estimation&#10;method is then extended to regression models. Finally, some examples based on&#10;Monte Carlo simulations are discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="6" source="Michel Broniatowski" target="Igor Vajda">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.1541v1" />
          <attvalue for="2" value="Decomposable Pseudodistances and Applications in Statistical Estimation" />
          <attvalue for="3" value="The aim of this paper is to introduce new statistical criterions for&#10;estimation, suitable for inference in models with common continuous support.&#10;This proposal is in the direct line of a renewed interest for divergence based&#10;inference tools imbedding the most classical ones, such as maximum likelihood,&#10;Chi-square or Kullback Leibler. General pseudodistances with decomposable&#10;structure are considered, they allowing to define minimum pseudodistance&#10;estimators, without using nonparametric density estimators. A special class of&#10;pseudodistances indexed by {\alpha}&gt;0, leading for {\alpha}\downarrow0 to the&#10;Kulback Leibler divergence, is presented in detail. Corresponding estimation&#10;criteria are developed and asymptotic properties are studied. The estimation&#10;method is then extended to regression models. Finally, some examples based on&#10;Monte Carlo simulations are discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="7" source="Michel Broniatowski" target="Samantha Leorato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4353v1" />
          <attvalue for="2" value="An estimation method for the chi-square divergence with application to&#10;  test of hypotheses" />
          <attvalue for="3" value="We propose a new definition of the chi-square divergence between&#10;distributions. Based on convexity properties and duality, this version of the&#10;{\chi}^2 is well suited both for the classical applications of the {\chi}^2 for&#10;the analysis of contingency tables and for the statistical tests for parametric&#10;models, for which it has been advocated to be robust against inliers. We&#10;present two applications in testing. In the first one we deal with tests for&#10;finite and infinite numbers of linear constraints, while, in the second one, we&#10;apply {\chi}^2-methodology for parametric testing against contamination." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="8" source="Michel Broniatowski" target="Giorgio Celant">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="9" source="Michel Broniatowski" target="Marco Di Battista">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="10" source="Michel Broniatowski" target="Samuela Leoni-Aubin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="11" source="Aida Toma" target="Igor Vajda">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.1541v1" />
          <attvalue for="2" value="Decomposable Pseudodistances and Applications in Statistical Estimation" />
          <attvalue for="3" value="The aim of this paper is to introduce new statistical criterions for&#10;estimation, suitable for inference in models with common continuous support.&#10;This proposal is in the direct line of a renewed interest for divergence based&#10;inference tools imbedding the most classical ones, such as maximum likelihood,&#10;Chi-square or Kullback Leibler. General pseudodistances with decomposable&#10;structure are considered, they allowing to define minimum pseudodistance&#10;estimators, without using nonparametric density estimators. A special class of&#10;pseudodistances indexed by {\alpha}&gt;0, leading for {\alpha}\downarrow0 to the&#10;Kulback Leibler divergence, is presented in detail. Corresponding estimation&#10;criteria are developed and asymptotic properties are studied. The estimation&#10;method is then extended to regression models. Finally, some examples based on&#10;Monte Carlo simulations are discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="12" source="John T. Flam" target="Saikat Chatterjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="13" source="John T. Flam" target="Kimmo Kansanen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="14" source="John T. Flam" target="Torbjorn Ekman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="15" source="Saikat Chatterjee" target="Kimmo Kansanen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="16" source="Saikat Chatterjee" target="Torbjorn Ekman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="17" source="Kimmo Kansanen" target="Torbjorn Ekman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3410v1" />
          <attvalue for="2" value="Minimum Mean Square Error Estimation Under Gaussian Mixture Statistics" />
          <attvalue for="3" value="This paper investigates the minimum mean square error (MMSE) estimation of x,&#10;given the observation y = Hx+n, when x and n are independent and Gaussian&#10;Mixture (GM) distributed. The introduction of GM distributions, represents a&#10;generalization of the more familiar and simpler Gaussian signal and Gaussian&#10;noise instance. We present the necessary theoretical foundation and derive the&#10;MMSE estimator for x in a closed form. Furthermore, we provide upper and lower&#10;bounds for its mean square error (MSE). These bounds are validated through&#10;Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="18" source="S. Chen" target="J. Dick">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1896v1" />
          <attvalue for="2" value="Consistency of Markov chain quasi-Monte Carlo on continuous state spaces" />
          <attvalue for="3" value="The random numbers driving Markov chain Monte Carlo (MCMC) simulation are&#10;usually modeled as independent U(0,1) random variables. Tribble [Markov chain&#10;Monte Carlo algorithms using completely uniformly distributed driving sequences&#10;(2007) Stanford Univ.] reports substantial improvements when those random&#10;numbers are replaced by carefully balanced inputs from completely uniformly&#10;distributed sequences. The previous theoretical justification for using&#10;anything other than i.i.d. U(0,1) points shows consistency for estimated means,&#10;but only applies for discrete stationary distributions. We extend those results&#10;to some MCMC algorithms for continuous stationary distributions. The main&#10;motivation is the search for quasi-Monte Carlo versions of MCMC. As a side&#10;benefit, the results also establish consistency for the usual method of using&#10;pseudo-random numbers in place of random ones." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="19" source="S. Chen" target="A. B. Owen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1896v1" />
          <attvalue for="2" value="Consistency of Markov chain quasi-Monte Carlo on continuous state spaces" />
          <attvalue for="3" value="The random numbers driving Markov chain Monte Carlo (MCMC) simulation are&#10;usually modeled as independent U(0,1) random variables. Tribble [Markov chain&#10;Monte Carlo algorithms using completely uniformly distributed driving sequences&#10;(2007) Stanford Univ.] reports substantial improvements when those random&#10;numbers are replaced by carefully balanced inputs from completely uniformly&#10;distributed sequences. The previous theoretical justification for using&#10;anything other than i.i.d. U(0,1) points shows consistency for estimated means,&#10;but only applies for discrete stationary distributions. We extend those results&#10;to some MCMC algorithms for continuous stationary distributions. The main&#10;motivation is the search for quasi-Monte Carlo versions of MCMC. As a side&#10;benefit, the results also establish consistency for the usual method of using&#10;pseudo-random numbers in place of random ones." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="20" source="J. Dick" target="A. B. Owen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1896v1" />
          <attvalue for="2" value="Consistency of Markov chain quasi-Monte Carlo on continuous state spaces" />
          <attvalue for="3" value="The random numbers driving Markov chain Monte Carlo (MCMC) simulation are&#10;usually modeled as independent U(0,1) random variables. Tribble [Markov chain&#10;Monte Carlo algorithms using completely uniformly distributed driving sequences&#10;(2007) Stanford Univ.] reports substantial improvements when those random&#10;numbers are replaced by carefully balanced inputs from completely uniformly&#10;distributed sequences. The previous theoretical justification for using&#10;anything other than i.i.d. U(0,1) points shows consistency for estimated means,&#10;but only applies for discrete stationary distributions. We extend those results&#10;to some MCMC algorithms for continuous stationary distributions. The main&#10;motivation is the search for quasi-Monte Carlo versions of MCMC. As a side&#10;benefit, the results also establish consistency for the usual method of using&#10;pseudo-random numbers in place of random ones." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="21" source="Heping He" target="Thomas A. Severini">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5224v1" />
          <attvalue for="2" value="Asymptotic properties of maximum likelihood estimators in models with&#10;  multiple change points" />
          <attvalue for="3" value="Models with multiple change points are used in many fields; however, the&#10;theoretical properties of maximum likelihood estimators of such models have&#10;received relatively little attention. The goal of this paper is to establish&#10;the asymptotic properties of maximum likelihood estimators of the parameters of&#10;a multiple change-point model for a general class of models in which the form&#10;of the distribution can change from segment to segment and in which, possibly,&#10;there are parameters that are common to all segments. Consistency of the&#10;maximum likelihood estimators of the change points is established and the rate&#10;of convergence is determined; the asymptotic distribution of the maximum&#10;likelihood estimators of the parameters of the within-segment distributions is&#10;also derived. Since the approach used in single change-point models is not&#10;easily extended to multiple change-point models, these results require the&#10;introduction of those tools for analyzing the likelihood function in a multiple&#10;change-point model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="22" source="Laëtitia Comminges" target="Arnak Dalalyan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.3616v1" />
          <attvalue for="2" value="Tight conditions for consistent variable selection in high dimensional&#10;  nonparametric regression" />
          <attvalue for="3" value="We address the issue of variable selection in the regression model with very&#10;high ambient dimension, i.e., when the number of covariates is very large. The&#10;main focus is on the situation where the number of relevant covariates, called&#10;intrinsic dimension, is much smaller than the ambient dimension. Without&#10;assuming any parametric form of the underlying regression function, we get&#10;tight conditions making it possible to consistently estimate the set of&#10;relevant variables. These conditions relate the intrinsic dimension to the&#10;ambient dimension and to the sample size. The procedure that is provably&#10;consistent under these tight conditions is simple and is based on comparing the&#10;empirical Fourier coefficients with an appropriately chosen threshold value." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="23" source="Arnak Dalalyan" target="Joseph Salmon">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3969v4" />
          <attvalue for="2" value="Sharp Oracle Inequalities for Aggregation of Affine Estimators" />
          <attvalue for="3" value="We consider the problem of combining a (possibly uncountably infinite) set of&#10;affine estimators in non-parametric regression model with heteroscedastic&#10;Gaussian noise. Focusing on the exponentially weighted aggregate, we prove a&#10;PAC-Bayesian type inequality that leads to sharp oracle inequalities in&#10;discrete but also in continuous settings. The framework is general enough to&#10;cover the combinations of various procedures such as least square regression,&#10;kernel ridge regression, shrinking estimators and many other estimators used in&#10;the literature on statistical inverse problems. As a consequence, we show that&#10;the proposed aggregate provides an adaptive estimator in the exact minimax&#10;sense without neither discretizing the range of tuning parameters nor splitting&#10;the set of observations. We also illustrate numerically the good performance&#10;achieved by the exponentially weighted aggregate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="24" source="Eric Gautier" target="Stefan Hoderlein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.0362v4" />
          <attvalue for="2" value="A triangular treatment effect model with random coefficients in the&#10;  selection equation" />
          <attvalue for="3" value="This paper considers treatment effects under endogeneity with complex&#10;heterogeneity in the selection equation. We model the outcome of an endogenous&#10;treatment as a triangular system, where both the outcome and first-stage&#10;equations consist of a random coefficients model. The first-stage specifically&#10;allows for nonmonotone selection into treatment. We provide conditions under&#10;which marginal distributions of potential outcomes, average and quantile&#10;treatment effects, all conditional on first-stage random coefficients, are&#10;identified. Under the same conditions, we derive bounds on the (conditional)&#10;joint distributions of potential outcomes and gains from treatment, and provide&#10;additional conditions for their point identification. All conditional&#10;quantities yield unconditional effects (\emph{e.g.}, the average treatment&#10;effect) by weighted integration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="25" source="Eric Gautier" target="Erwan Le Pennec">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.3503v3" />
          <attvalue for="2" value="Adaptive estimation in the nonparametric random coefficients binary&#10;  choice model by needlet thresholding" />
          <attvalue for="3" value="In the random coefficients binary choice model, a binary variable equals 1&#10;iff an index $X^\top\beta$ is positive.The vectors $X$ and $\beta$ are&#10;independent and belong to the sphere $\mathbb{S}^{d-1}$ in $\mathbb{R}^{d}$.We&#10;prove lower bounds on the minimax risk for estimation of the density&#10;$f\_{\beta}$ over Besov bodies where the loss is a power of the&#10;$L^p(\mathbb{S}^{d-1})$ norm for $1\le p\le \infty$. We show that a hard&#10;thresholding estimator based on a needlet expansion with data-driven thresholds&#10;achieves these lower bounds up to logarithmic factors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="26" source="Eric Gautier" target="Alexandre Tsybakov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2454v5" />
          <attvalue for="2" value="High-dimensional instrumental variables regression and confidence sets" />
          <attvalue for="3" value="This article considers inference in linear models with K regressors, some or&#10;many could be endogenous, and L instruments. L can range from less than K to&#10;any order smaller than an exponential in the sample size and K is arbitrary.&#10;For moderate K, identification robust confidence sets are obtained by solving a&#10;hierarchy of semidefinite programs. For larger K, we propose the STIV&#10;estimator. The analysis of its error uses sensitivity characteristics which are&#10;sharper than those in the literature on sparsity. Data-driven bounds on them&#10;and robust confidence sets are obtained by solving K linear programs. Results&#10;on rates of convergence, variable selection, and confidence sets which &quot;adapt&quot;&#10;to the sparsity are given. We generalize our approach to models with&#10;approximation errors, systems, endogenous instruments, and two-stage for&#10;confidence bands for vectors of linear functionals and functions. The&#10;application is to a demand system with many endogenous regressors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="27" source="Eric Gautier" target="Christiern Rose">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2454v5" />
          <attvalue for="2" value="High-dimensional instrumental variables regression and confidence sets" />
          <attvalue for="3" value="This article considers inference in linear models with K regressors, some or&#10;many could be endogenous, and L instruments. L can range from less than K to&#10;any order smaller than an exponential in the sample size and K is arbitrary.&#10;For moderate K, identification robust confidence sets are obtained by solving a&#10;hierarchy of semidefinite programs. For larger K, we propose the STIV&#10;estimator. The analysis of its error uses sensitivity characteristics which are&#10;sharper than those in the literature on sparsity. Data-driven bounds on them&#10;and robust confidence sets are obtained by solving K linear programs. Results&#10;on rates of convergence, variable selection, and confidence sets which &quot;adapt&quot;&#10;to the sparsity are given. We generalize our approach to models with&#10;approximation errors, systems, endogenous instruments, and two-stage for&#10;confidence bands for vectors of linear functionals and functions. The&#10;application is to a demand system with many endogenous regressors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="28" source="E. Ostrovsky" target="L. Sirota">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1386v1" />
          <attvalue for="2" value="Adaptive Optimal Signal (Cardiogram) Processing, with boundary values&#10;  and energy precisely measurement" />
          <attvalue for="3" value="We construct an adaptive asymptotically optimal in order in the weight&#10;Hilbert space norms signal denoising on the background noise and its energy&#10;measurement, with hight precision near the boundary of the signal. An offered&#10;method used the Fourier-Riesz expansion on the orthonormal polynomials, for&#10;instance, Jacobi's polynomials, relative unbounded near the boundary weight&#10;function. An applications: technical and medical, in particular, cardiac&#10;diagnosis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="29" source="Debdeep Pati" target="Anirban Bhattacharya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1044v4" />
          <attvalue for="2" value="Anisotropic function estimation using multi-bandwidth Gaussian processes" />
          <attvalue for="3" value="In nonparametric regression problems involving multiple predictors, there is&#10;typically interest in estimating an anisotropic multivariate regression surface&#10;in the important predictors while discarding the unimportant ones. Our focus is&#10;on defining a Bayesian procedure that leads to the minimax optimal rate of&#10;posterior contraction (up to a log factor) adapting to the unknown dimension&#10;and anisotropic smoothness of the true surface. We propose such an approach&#10;based on a Gaussian process prior with dimension-specific scalings, which are&#10;assigned carefully-chosen hyperpriors. We additionally show that using a&#10;homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate&#10;in anisotropic cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="30" source="Debdeep Pati" target="David B. Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5000v1" />
          <attvalue for="2" value="Posterior convergence rates in non-linear latent variable models" />
          <attvalue for="3" value="Non-linear latent variable models have become increasingly popular in a&#10;variety of applications. However, there has been little study on theoretical&#10;properties of these models. In this article, we study rates of posterior&#10;contraction in univariate density estimation for a class of non-linear latent&#10;variable models where unobserved U(0,1) latent variables are related to the&#10;response variables via a random non-linear regression with an additive error.&#10;Our approach relies on characterizing the space of densities induced by the&#10;above model as kernel convolutions with a general class of continuous mixing&#10;measures. The literature on posterior rates of contraction in density&#10;estimation almost entirely focuses on finite or countably infinite mixture&#10;models. We develop approximation results for our class of continuous mixing&#10;measures. Using an appropriate Gaussian process prior on the unknown regression&#10;function, we obtain the optimal frequentist rate up to a logarithmic factor&#10;under standard regularity conditions on the true density." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="31" source="Debdeep Pati" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1044v4" />
          <attvalue for="2" value="Anisotropic function estimation using multi-bandwidth Gaussian processes" />
          <attvalue for="3" value="In nonparametric regression problems involving multiple predictors, there is&#10;typically interest in estimating an anisotropic multivariate regression surface&#10;in the important predictors while discarding the unimportant ones. Our focus is&#10;on defining a Bayesian procedure that leads to the minimax optimal rate of&#10;posterior contraction (up to a log factor) adapting to the unknown dimension&#10;and anisotropic smoothness of the true surface. We propose such an approach&#10;based on a Gaussian process prior with dimension-specific scalings, which are&#10;assigned carefully-chosen hyperpriors. We additionally show that using a&#10;homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate&#10;in anisotropic cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="32" source="Anirban Bhattacharya" target="David B. Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5000v1" />
          <attvalue for="2" value="Posterior convergence rates in non-linear latent variable models" />
          <attvalue for="3" value="Non-linear latent variable models have become increasingly popular in a&#10;variety of applications. However, there has been little study on theoretical&#10;properties of these models. In this article, we study rates of posterior&#10;contraction in univariate density estimation for a class of non-linear latent&#10;variable models where unobserved U(0,1) latent variables are related to the&#10;response variables via a random non-linear regression with an additive error.&#10;Our approach relies on characterizing the space of densities induced by the&#10;above model as kernel convolutions with a general class of continuous mixing&#10;measures. The literature on posterior rates of contraction in density&#10;estimation almost entirely focuses on finite or countably infinite mixture&#10;models. We develop approximation results for our class of continuous mixing&#10;measures. Using an appropriate Gaussian process prior on the unknown regression&#10;function, we obtain the optimal frequentist rate up to a logarithmic factor&#10;under standard regularity conditions on the true density." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="33" source="Anirban Bhattacharya" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1044v4" />
          <attvalue for="2" value="Anisotropic function estimation using multi-bandwidth Gaussian processes" />
          <attvalue for="3" value="In nonparametric regression problems involving multiple predictors, there is&#10;typically interest in estimating an anisotropic multivariate regression surface&#10;in the important predictors while discarding the unimportant ones. Our focus is&#10;on defining a Bayesian procedure that leads to the minimax optimal rate of&#10;posterior contraction (up to a log factor) adapting to the unknown dimension&#10;and anisotropic smoothness of the true surface. We propose such an approach&#10;based on a Gaussian process prior with dimension-specific scalings, which are&#10;assigned carefully-chosen hyperpriors. We additionally show that using a&#10;homogenous Gaussian process with a single bandwidth leads to a sub-optimal rate&#10;in anisotropic cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="34" source="David B. Dunson" target="Suprateek Kundu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.2720v2" />
          <attvalue for="2" value="Latent Factor Models for Density Estimation" />
          <attvalue for="3" value="Although discrete mixture modeling has formed the backbone of the literature&#10;on Bayesian density estimation, there are some well known disadvantages. We&#10;propose an alternative class of priors based on random nonlinear functions of a&#10;uniform latent variable with an additive residual. The induced prior for the&#10;density is shown to have desirable properties including ease of centering on an&#10;initial guess for the density, large support, posterior consistency and&#10;straightforward computation via Gibbs sampling. Some advantages over discrete&#10;mixtures, such as Dirichlet process mixtures of Gaussian kernels, are discussed&#10;and illustrated via simulations and an epidemiology application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="35" source="David B. Dunson" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="36" source="Rina Foygel" target="Nathan Srebro">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.0373v1" />
          <attvalue for="2" value="Fast-rate and optimistic-rate error bounds for L1-regularized regression" />
          <attvalue for="3" value="We consider the prediction error of linear regression with L1 regularization&#10;when the number of covariates p is large relative to the sample size n. When&#10;the model is k-sparse and well-specified, and restricted isometry or similar&#10;conditions hold, the excess squared-error in prediction can be bounded on the&#10;order of sigma^2*(k*log(p)/n), where sigma^2 is the noise variance. Although&#10;these conditions are close to necessary for accurate recovery of the true&#10;coefficient vector, it is possible to guarantee good predictive accuracy under&#10;much milder conditions, avoiding the restricted isometry condition, but only&#10;ensuring an excess error bound of order (k*log(p)/n)+sigma*\surd(k*log(p)/n).&#10;Here we show that this is indeed the best bound possible (up to logarithmic&#10;factors) without introducing stronger assumptions similar to restricted&#10;isometry." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="37" source="Rina Foygel" target="Mathias Drton">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.5552v2" />
          <attvalue for="2" value="Half-trek criterion for generic identifiability of linear structural&#10;  equation models" />
          <attvalue for="3" value="A linear structural equation model relates random variables of interest and&#10;corresponding Gaussian noise terms via a linear equation system. Each such&#10;model can be represented by a mixed graph in which directed edges encode the&#10;linear equations and bidirected edges indicate possible correlations among&#10;noise terms. We study parameter identifiability in these models, that is, we&#10;ask for conditions that ensure that the edge coefficients and correlations&#10;appearing in a linear structural equation model can be uniquely recovered from&#10;the covariance matrix of the associated distribution. We treat the case of&#10;generic identifiability, where unique recovery is possible for almost every&#10;choice of parameters. We give a new graphical condition that is sufficient for&#10;generic identifiability and can be verified in time that is polynomial in the&#10;size of the graph. It improves criteria from prior work and does not require&#10;the directed part of the graph to be acyclic. We also develop a related&#10;necessary condition and examine the &quot;gap&quot; between sufficient and necessary&#10;conditions through simulations on graphs with 25 or 50 nodes, as well as&#10;exhaustive algebraic computations for graphs with up to five nodes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="38" source="Rina Foygel" target="Jan Draisma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.5552v2" />
          <attvalue for="2" value="Half-trek criterion for generic identifiability of linear structural&#10;  equation models" />
          <attvalue for="3" value="A linear structural equation model relates random variables of interest and&#10;corresponding Gaussian noise terms via a linear equation system. Each such&#10;model can be represented by a mixed graph in which directed edges encode the&#10;linear equations and bidirected edges indicate possible correlations among&#10;noise terms. We study parameter identifiability in these models, that is, we&#10;ask for conditions that ensure that the edge coefficients and correlations&#10;appearing in a linear structural equation model can be uniquely recovered from&#10;the covariance matrix of the associated distribution. We treat the case of&#10;generic identifiability, where unique recovery is possible for almost every&#10;choice of parameters. We give a new graphical condition that is sufficient for&#10;generic identifiability and can be verified in time that is polynomial in the&#10;size of the graph. It improves criteria from prior work and does not require&#10;the directed part of the graph to be acyclic. We also develop a related&#10;necessary condition and examine the &quot;gap&quot; between sufficient and necessary&#10;conditions through simulations on graphs with 25 or 50 nodes, as well as&#10;exhaustive algebraic computations for graphs with up to five nodes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="39" source="Erwan Le Pennec" target="Serge Cohen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.2021v5" />
          <attvalue for="2" value="Conditional Density Estimation by Penalized Likelihood Model Selection&#10;  and Applications" />
          <attvalue for="3" value="In this technical report, we consider conditional density estimation with a&#10;maximum likelihood approach. Under weak assumptions, we obtain a theoretical&#10;bound for a Kullback-Leibler type loss for a single model maximum likelihood&#10;estimate. We use a penalized model selection technique to select a best model&#10;within a collection. We give a general condition on penalty choice that leads&#10;to oracle type inequality for the resulting estimate. This construction is&#10;applied to two examples of partition-based conditional density models, models&#10;in which the conditional density depends only in a piecewise manner from the&#10;covariate. The first example relies on classical piecewise polynomial densities&#10;while the second uses Gaussian mixtures with varying mixing proportion but same&#10;mixture components. We show how this last case is related to an unsupervised&#10;segmentation application that has been the source of our motivation to this&#10;study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="40" source="Yizao Wang" target="Michael Woodroofe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0238v1" />
          <attvalue for="2" value="On the asymptotic normality of kernel density estimators for linear&#10;  random fields" />
          <attvalue for="3" value="We establish sufficient conditions for the asymptotic normality of kernel&#10;density estimators, applied to causal linear random fields. Our conditions on&#10;the coefficients of linear random fields are weaker than known results,&#10;although our assumption on the bandwidth is not minimal. The proof is based on&#10;the $m$-approximation method. As a key step, we prove a central limit theorem&#10;for triangular arrays of stationary $m$-dependent random fields with unbounded&#10;$m$. We also apply a moment inequality recently established for stationary&#10;random fields." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="41" source="Ery Arias-Castro" target="Geoffrey R. Grimmett">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0338v2" />
          <attvalue for="2" value="Cluster detection in networks using percolation" />
          <attvalue for="3" value="We consider the task of detecting a salient cluster in a sensor network, that&#10;is, an undirected graph with a random variable attached to each node. Motivated&#10;by recent research in environmental statistics and the drive to compete with&#10;the reigning scan statistic, we explore alternatives based on the percolative&#10;properties of the network. The first method is based on the size of the largest&#10;connected component after removing the nodes in the network with a value below&#10;a given threshold. The second method is the upper level set scan test&#10;introduced by Patil and Taillie [Statist. Sci. 18 (2003) 457-465]. We establish&#10;the performance of these methods in an asymptotic decision- theoretic framework&#10;in which the network size increases. These tests have two advantages over the&#10;more conventional scan statistic: they do not require previous information&#10;about cluster shape, and they are computationally more feasible. We make&#10;abundant use of percolation theory to derive our theoretical results, and&#10;complement our theory with some numerical experiments." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="42" source="Ery Arias-Castro" target="Sébastien Bubeck">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1193v2" />
          <attvalue for="2" value="Detection of correlations" />
          <attvalue for="3" value="We consider the hypothesis testing problem of deciding whether an observed&#10;high-dimensional vector has independent normal components or, alternatively, if&#10;it has a small subset of correlated components. The correlated components may&#10;have a certain combinatorial structure known to the statistician. We establish&#10;upper and lower bounds for the worst-case (minimax) risk in terms of the size&#10;of the correlated subset, the level of correlation, and the structure of the&#10;class of possibly correlated sets. We show that some simple tests have&#10;near-optimal performance in many cases, while the generalized likelihood ratio&#10;test is suboptimal in some important cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="43" source="Ery Arias-Castro" target="Gábor Lugosi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1193v2" />
          <attvalue for="2" value="Detection of correlations" />
          <attvalue for="3" value="We consider the hypothesis testing problem of deciding whether an observed&#10;high-dimensional vector has independent normal components or, alternatively, if&#10;it has a small subset of correlated components. The correlated components may&#10;have a certain combinatorial structure known to the statistician. We establish&#10;upper and lower bounds for the worst-case (minimax) risk in terms of the size&#10;of the correlated subset, the level of correlation, and the structure of the&#10;class of possibly correlated sets. We show that some simple tests have&#10;near-optimal performance in many cases, while the generalized likelihood ratio&#10;test is suboptimal in some important cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="44" source="Axel Bücher" target="Holger Dette">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0405v2" />
          <attvalue for="2" value="New estimators of the Pickands dependence function and a test for&#10;  extreme-value dependence" />
          <attvalue for="3" value="We propose a new class of estimators for Pickands dependence function which&#10;is based on the concept of minimum distance estimation. An explicit integral&#10;representation of the function $A^*(t)$, which minimizes a weighted&#10;$L^2$-distance between the logarithm of the copula $C(y^{1-t},y^t)$ and&#10;functions of the form $A(t)\log(y)$ is derived. If the unknown copula is an&#10;extreme-value copula, the function $A^*(t)$ coincides with Pickands dependence&#10;function. Moreover, even if this is not the case, the function $A^*(t)$ always&#10;satisfies the boundary conditions of a Pickands dependence function. The&#10;estimators are obtained by replacing the unknown copula by its empirical&#10;counterpart and weak convergence of the corresponding process is shown. A&#10;comparison with the commonly used estimators is performed from a theoretical&#10;point of view and by means of a simulation study. Our asymptotic and numerical&#10;results indicate that some of the new estimators outperform the estimators,&#10;which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009)&#10;2990--3022]. As a by-product of our results, we obtain a simple test for the&#10;hypothesis of an extreme-value copula, which is consistent against all positive&#10;quadrant dependent alternatives satisfying weak differentiability assumptions&#10;of first order." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="45" source="Axel Bücher" target="Stanislav Volgushev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0405v2" />
          <attvalue for="2" value="New estimators of the Pickands dependence function and a test for&#10;  extreme-value dependence" />
          <attvalue for="3" value="We propose a new class of estimators for Pickands dependence function which&#10;is based on the concept of minimum distance estimation. An explicit integral&#10;representation of the function $A^*(t)$, which minimizes a weighted&#10;$L^2$-distance between the logarithm of the copula $C(y^{1-t},y^t)$ and&#10;functions of the form $A(t)\log(y)$ is derived. If the unknown copula is an&#10;extreme-value copula, the function $A^*(t)$ coincides with Pickands dependence&#10;function. Moreover, even if this is not the case, the function $A^*(t)$ always&#10;satisfies the boundary conditions of a Pickands dependence function. The&#10;estimators are obtained by replacing the unknown copula by its empirical&#10;counterpart and weak convergence of the corresponding process is shown. A&#10;comparison with the commonly used estimators is performed from a theoretical&#10;point of view and by means of a simulation study. Our asymptotic and numerical&#10;results indicate that some of the new estimators outperform the estimators,&#10;which were recently proposed by Genest and Segers [Ann. Statist. 37 (2009)&#10;2990--3022]. As a by-product of our results, we obtain a simple test for the&#10;hypothesis of an extreme-value copula, which is consistent against all positive&#10;quadrant dependent alternatives satisfying weak differentiability assumptions&#10;of first order." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="46" source="Axel Bücher" target="Viatcheslav B. Melas">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="47" source="Axel Bücher" target="Marc Hallin">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="48" source="Holger Dette" target="Stanislav Volgushev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="49" source="Holger Dette" target="Viatcheslav B. Melas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3575v1" />
          <attvalue for="2" value="A note on the de la Garza phenomenon for locally optimal designs" />
          <attvalue for="3" value="The celebrated de la Garza phenomenon states that for a polynomial regression&#10;model of degree $p-1$ any optimal design can be based on at most $p$ design&#10;points. In a remarkable paper, Yang [Ann. Statist. 38 (2010) 2499--2524] showed&#10;that this phenomenon exists in many locally optimal design problems for&#10;nonlinear models. In the present note, we present a different view point on&#10;these findings using results about moment theory and Chebyshev systems. In&#10;particular, we show that this phenomenon occurs in an even larger class of&#10;models than considered so far." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="50" source="Holger Dette" target="Marc Hallin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="51" source="Holger Dette" target="Tobias Kley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="52" source="Holger Dette" target="Davy Paindaveine">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="53" source="Cristina Butucea" target="Yuri I. Ingster">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.0898v2" />
          <attvalue for="2" value="Detection of a sparse submatrix of a high-dimensional noisy matrix" />
          <attvalue for="3" value="We observe a $N\times M$ matrix $Y_{ij}=s_{ij}+\xi_{ij}$ with $\xi_{ij}\sim&#10;{\mathcal {N}}(0,1)$ i.i.d. in $i,j$, and $s_{ij}\in \mathbb {R}$. We test the&#10;null hypothesis $s_{ij}=0$ for all $i,j$ against the alternative that there&#10;exists some submatrix of size $n\times m$ with significant elements in the&#10;sense that $s_{ij}\ge a&gt;0$. We propose a test procedure and compute the&#10;asymptotical detection boundary $a$ so that the maximal testing risk tends to 0&#10;as $M\to\infty$, $N\to\infty$, $p=n/N\to0$, $q=m/M\to0$. We prove that this&#10;boundary is asymptotically sharp minimax under some additional constraints.&#10;Relations with other testing problems are discussed. We propose a testing&#10;procedure which adapts to unknown $(n,m)$ within some given set and compute the&#10;adaptive sharp rates. The implementation of our test procedure on synthetic&#10;data shows excellent behavior for sparse, not necessarily squared matrices. We&#10;extend our sharp minimax results in different directions: first, to Gaussian&#10;matrices with unknown variance, next, to matrices of random variables having a&#10;distribution from an exponential family (non-Gaussian) and, finally, to a&#10;two-sided alternative for matrices with Gaussian elements." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="54" source="Cristina Butucea" target="Pierre Vandekerkhove">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2247v1" />
          <attvalue for="2" value="Semiparametric mixtures of symmetric distributions" />
          <attvalue for="3" value="We consider in this paper the semiparametric mixture of two distributions&#10;equal up to a shift parameter. The model is said to be semiparametric in the&#10;sense that the mixed distribution is not supposed to belong to a parametric&#10;family. In order to insure the identifiability of the model it is assumed that&#10;the mixed distribution is symmetric, the model being then defined by the mixing&#10;proportion, two location parameters, and the probability density function of&#10;the mixed distribution. We propose a new class of M-estimators of these&#10;parameters based on a Fourier approach, and prove that they are square root&#10;consistent under mild regularity conditions. Their finite-sample properties are&#10;illustrated by a Monte Carlo study and a benchmark real dataset is also studied&#10;with our method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="55" source="Cristina Butucea" target="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="56" source="Cristina Butucea" target="Gersende Fort">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="57" source="Cristina Butucea" target="Eric Moulines">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="58" source="Yuri I. Ingster" target="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3442v3" />
          <attvalue for="2" value="Minimax nonparametric testing in a problem related to the Radon&#10;  transform" />
          <attvalue for="3" value="We consider the detection problem of a two-dimensional function from noisy&#10;observations of its integrals over lines. We study both rate and sharp&#10;asymptotics for the error probabilities in the minimax setup. By construction,&#10;the derived tests are non-adaptive. We also construct a minimax rate-optimal&#10;adaptive test of rather simple structure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="59" source="Yuri I. Ingster" target="Irina A. Suslina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3442v3" />
          <attvalue for="2" value="Minimax nonparametric testing in a problem related to the Radon&#10;  transform" />
          <attvalue for="3" value="We consider the detection problem of a two-dimensional function from noisy&#10;observations of its integrals over lines. We study both rate and sharp&#10;asymptotics for the error probabilities in the minimax setup. By construction,&#10;the derived tests are non-adaptive. We also construct a minimax rate-optimal&#10;adaptive test of rather simple structure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="60" source="Masayuki Kumon" target="Akimichi Takemura">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.3471v1" />
          <attvalue for="2" value="Conformal geometry of statistical manifold with application to&#10;  sequential estimation" />
          <attvalue for="3" value="We present a geometrical method for analyzing sequential estimating&#10;procedures. It is based on the design principle of the second-order efficient&#10;sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By&#10;introducing a dual conformal curvature quantity, we clarify the conditions for&#10;the covariance minimization of sequential estimators. These conditions are&#10;further elabolated for the multidimensional curved exponential family. The&#10;theoretical results are then numerically examined by using typical statistical&#10;models, von Mises-Fisher and hyperboloid models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="61" source="Masayuki Kumon" target="Kei Takeuchi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.3471v1" />
          <attvalue for="2" value="Conformal geometry of statistical manifold with application to&#10;  sequential estimation" />
          <attvalue for="3" value="We present a geometrical method for analyzing sequential estimating&#10;procedures. It is based on the design principle of the second-order efficient&#10;sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By&#10;introducing a dual conformal curvature quantity, we clarify the conditions for&#10;the covariance minimization of sequential estimators. These conditions are&#10;further elabolated for the multidimensional curved exponential family. The&#10;theoretical results are then numerically examined by using typical statistical&#10;models, von Mises-Fisher and hyperboloid models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="62" source="Akimichi Takemura" target="Kei Takeuchi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.3471v1" />
          <attvalue for="2" value="Conformal geometry of statistical manifold with application to&#10;  sequential estimation" />
          <attvalue for="3" value="We present a geometrical method for analyzing sequential estimating&#10;procedures. It is based on the design principle of the second-order efficient&#10;sequential estimation provided in Okamoto, Amari and Takeuchi (1991). By&#10;introducing a dual conformal curvature quantity, we clarify the conditions for&#10;the covariance minimization of sequential estimators. These conditions are&#10;further elabolated for the multidimensional curved exponential family. The&#10;theoretical results are then numerically examined by using typical statistical&#10;models, von Mises-Fisher and hyperboloid models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="63" source="Akimichi Takemura" target="Takuya Kashimura">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="64" source="Akimichi Takemura" target="Tomonari Sei">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="65" source="Akimichi Takemura" target="Kentaro Tanaka">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="66" source="Akimichi Takemura" target="Junya Honda">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2879v1" />
          <attvalue for="2" value="Stochastic Bandit Based on Empirical Moments" />
          <attvalue for="3" value="In the multiarmed bandit problem a gambler chooses an arm of a slot machine&#10;to pull considering a tradeoff between exploration and exploitation. We study&#10;the stochastic bandit problem where each arm has a reward distribution&#10;supported in a known bounded interval, e.g. [0,1]. For this model, policies&#10;which take into account the empirical variances (i.e. second moments) of the&#10;arms are known to perform effectively. In this paper, we generalize this idea&#10;and we propose a policy which exploits the first d empirical moments for&#10;arbitrary d fixed in advance. The asymptotic upper bound of the regret of the&#10;policy approaches the theoretical bound by Burnetas and Katehakis as d&#10;increases. By choosing appropriate d, the proposed policy realizes a tradeoff&#10;between the computational complexity and the expected regret." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="67" source="Akimichi Takemura" target="Taisei Kudo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4674v1" />
          <attvalue for="2" value="A lower bound for the Graver complexity of the incidence matrix of a&#10;  complete bipartite graph" />
          <attvalue for="3" value="We give an exponential lower bound for the Graver complexity of the incidence&#10;matrix of a complete bipartite graph of arbitrary size. Our result is a&#10;generalization of the result by Berstein and Onn (2009) for 3xr complete&#10;bipartite graphs, r \ge 3." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="68" source="Alain Celisse" target="J. -J. Daudin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3288v3" />
          <attvalue for="2" value="Consistency of maximum-likelihood and variational estimators in the&#10;  Stochastic Block Model" />
          <attvalue for="3" value="The stochastic block model (SBM) is a probabilistic model de- signed to&#10;describe heterogeneous directed and undirected graphs. In this paper, we&#10;address the asymptotic inference on SBM by use of maximum- likelihood and&#10;variational approaches. The identi ability of SBM is proved, while asymptotic&#10;properties of maximum-likelihood and variational esti- mators are provided. In&#10;particular, the consistency of these estimators is settled, which is, to the&#10;best of our knowledge, the rst result of this type for variational estimators&#10;with random graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="69" source="Alain Celisse" target="Laurent Pierre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3288v3" />
          <attvalue for="2" value="Consistency of maximum-likelihood and variational estimators in the&#10;  Stochastic Block Model" />
          <attvalue for="3" value="The stochastic block model (SBM) is a probabilistic model de- signed to&#10;describe heterogeneous directed and undirected graphs. In this paper, we&#10;address the asymptotic inference on SBM by use of maximum- likelihood and&#10;variational approaches. The identi ability of SBM is proved, while asymptotic&#10;properties of maximum-likelihood and variational esti- mators are provided. In&#10;particular, the consistency of these estimators is settled, which is, to the&#10;best of our knowledge, the rst result of this type for variational estimators&#10;with random graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="70" source="J. -J. Daudin" target="Laurent Pierre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3288v3" />
          <attvalue for="2" value="Consistency of maximum-likelihood and variational estimators in the&#10;  Stochastic Block Model" />
          <attvalue for="3" value="The stochastic block model (SBM) is a probabilistic model de- signed to&#10;describe heterogeneous directed and undirected graphs. In this paper, we&#10;address the asymptotic inference on SBM by use of maximum- likelihood and&#10;variational approaches. The identi ability of SBM is proved, while asymptotic&#10;properties of maximum-likelihood and variational esti- mators are provided. In&#10;particular, the consistency of these estimators is settled, which is, to the&#10;best of our knowledge, the rst result of this type for variational estimators&#10;with random graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="71" source="Lajos Horvath" target="Piotr Kokoszka">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0019v1" />
          <attvalue for="2" value="Estimation of the mean of functional time series and a two sample&#10;  problem" />
          <attvalue for="3" value="This paper is concerned with inference based on the mean function of a&#10;functional time series, which is defined as a collection of curves obtained by&#10;splitting a continuous time record, e.g. into daily or annual curves. We&#10;develop a normal approximation for the functional sample mean, and then focus&#10;on the estimation of the asymptotic variance kernel. Using these results, we&#10;develop and asymptotically justify a testing procedure for the equality of&#10;means in two functional samples exhibiting temporal dependence. Evaluated by&#10;means of a simulations study and application to real data sets, this two sample&#10;procedure enjoys good size and power in finite samples. We provide the details&#10;of its numerical implementation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="72" source="Lajos Horvath" target="Ron Reeder">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0015v2" />
          <attvalue for="2" value="Detecting changes in functional linear models" />
          <attvalue for="3" value="We observe two sequences of curve which are connected via an integral&#10;operator. Our model includes linear models as well as autoregressive models in&#10;Hilbert spaces. We wish to test the null hypothesis that the operator did not&#10;change during the observation period. Our method is based on projecting the&#10;observations onto a suitably chosen finite dimensional space. The testing&#10;procedure is based on functionals of the weighted residuals of the projections.&#10;Since the quadratic form is based on estimating the long-term covariance matrix&#10;of the residuals, we also provide some results on Bartlett-type estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="73" source="Lajos Horvath" target="Siegfried Hormann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0343v1" />
          <attvalue for="2" value="A Functional Version of the ARCH Model" />
          <attvalue for="3" value="Improvements in data acquisition and processing techniques have lead to an&#10;almost continuous flow of information for financial data. High resolution tick&#10;data are available and can be quite conveniently described by a continuous time&#10;process. It is therefore natural to ask for possible extensions of financial&#10;time series models to a functional setup. In this paper we propose a functional&#10;version of the popular ARCH model. We will establish conditions for the&#10;existence of a strictly stationary solution, derive weak dependence and moment&#10;conditions, show consistency of the estimators and perform a small empirical&#10;study demonstrating how our model matches with real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="74" source="Piotr Kokoszka" target="Ron Reeder">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0019v1" />
          <attvalue for="2" value="Estimation of the mean of functional time series and a two sample&#10;  problem" />
          <attvalue for="3" value="This paper is concerned with inference based on the mean function of a&#10;functional time series, which is defined as a collection of curves obtained by&#10;splitting a continuous time record, e.g. into daily or annual curves. We&#10;develop a normal approximation for the functional sample mean, and then focus&#10;on the estimation of the asymptotic variance kernel. Using these results, we&#10;develop and asymptotically justify a testing procedure for the equality of&#10;means in two functional samples exhibiting temporal dependence. Evaluated by&#10;means of a simulations study and application to real data sets, this two sample&#10;procedure enjoys good size and power in finite samples. We provide the details&#10;of its numerical implementation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="75" source="Piotr Kokoszka" target="Siegfried Hörmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3074v2" />
          <attvalue for="2" value="Consistency of the mean and the principal components of spatially&#10;  distributed functional data" />
          <attvalue for="3" value="This paper develops a framework for the estimation of the functional mean and&#10;the functional principal components when the functions form a random field.&#10;More specifically, the data we study consist of curves&#10;$X(\mathbf{s}_k;t),t\in[0,T]$, observed at spatial points&#10;$\mathbf{s}_1,\mathbf{s}_2,\ldots,\mathbf{s}_N$. We establish conditions for&#10;the sample average (in space) of the $X(\mathbf{s}_k)$ to be a consistent&#10;estimator of the population mean function, and for the usual empirical&#10;covariance operator to be a consistent estimator of the population covariance&#10;operator. These conditions involve an interplay of the assumptions on an&#10;appropriately defined dependence between the functions $X(\mathbf{s}_k)$ and&#10;the assumptions on the spatial distribution of the points $\mathbf{s}_k$. The&#10;rates of convergence may be the same as for i.i.d. functional samples, but&#10;generally depend on the strength of dependence and appropriately quantified&#10;distances between the points $\mathbf{s}_k$. We also formulate conditions for&#10;the lack of consistency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="76" source="Ron Reeder" target="Siegfried Hormann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0343v1" />
          <attvalue for="2" value="A Functional Version of the ARCH Model" />
          <attvalue for="3" value="Improvements in data acquisition and processing techniques have lead to an&#10;almost continuous flow of information for financial data. High resolution tick&#10;data are available and can be quite conveniently described by a continuous time&#10;process. It is therefore natural to ask for possible extensions of financial&#10;time series models to a functional setup. In this paper we propose a functional&#10;version of the popular ARCH model. We will establish conditions for the&#10;existence of a strictly stationary solution, derive weak dependence and moment&#10;conditions, show consistency of the estimators and perform a small empirical&#10;study demonstrating how our model matches with real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="77" source="Ron Reeder" target="Lajos Horváth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0014v2" />
          <attvalue for="2" value="A test of significance in functional quadratic regression" />
          <attvalue for="3" value="We consider a quadratic functional regression model in which a scalar&#10;response depends on a functional predictor; the common functional linear model&#10;is a special case. We wish to test the significance of the nonlinear term in&#10;the model. We develop a testing method which is based on projecting the&#10;observations onto a suitably chosen finite dimensional space using functional&#10;principal component analysis. The asymptotic behavior of our testing procedure&#10;is established. A simulation study shows that the testing procedure has good&#10;size and power with finite sample sizes. We then apply our test to a data set&#10;provided by Tecator, which consists of near-infrared absorbance spectra and fat&#10;content of meat." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="78" source="Höpfner Reinhard" target="Yury A Kutoyants">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.5314v1" />
          <attvalue for="2" value="On frequency estimation of periodic ergodic diffusion process" />
          <attvalue for="3" value="We consider the problem of frequency estimation by observations of the&#10;periodic diffusion process possesing ergodic properties in two different&#10;situations. The first one corresponds to continuously differentiable with&#10;respect to parameter trend coefficient and the second - to discontinuous trend&#10;coefficient. It is shown that in the first case the maximum likelihood and&#10;bayesian estimators are asymptotically normal with rate $T^{3/2}$ and in the&#10;second case these estimators have different limit distributions with the rate&#10;$T^2$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="79" source="T. Yoshida" target="K. Naito">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.5136v1" />
          <attvalue for="2" value="Asymptotics for penalized additive B-spline regression" />
          <attvalue for="3" value="This paper is concerned with asymptotic theory for penalized spline estimator&#10;in bivariate additive model. The focus of this paper is put upon the penalized&#10;spline estimator obtained by the backfitting algorithm. The convergence of the&#10;algorithm as well as the uniqueness of its solution are shown. The asymptotic&#10;bias and variance of penalized spline estimator are derived by an efficient use&#10;of the asymptotic results for the penalized spline estimator in marginal&#10;univariate model. Asymptotic normality of estimator is also developed, by which&#10;an approximate confidence interval can be obtained. Some numerical experiments&#10;confirming theoretical results are provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="80" source="Daniel J. McDonald" target="Cosma Rohilla Shalizi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5998v3" />
          <attvalue for="2" value="Estimating beta-mixing coefficients via histograms" />
          <attvalue for="3" value="The literature on statistical learning for time series often assumes&#10;asymptotic independence or &quot;mixing&quot; of the data-generating process. These&#10;mixing assumptions are never tested, nor are there methods for estimating&#10;mixing coefficients from data. Additionally, for many common classes of&#10;processes (Markov processes, ARMA processes, etc.) general functional forms for&#10;various mixing rates are known, but not specific coefficients. We present the&#10;first estimator for beta-mixing coefficients based on a single stationary&#10;sample path and show that it is risk consistent. Since mixing rates depend on&#10;infinite-dimensional dependence, we use a Markov approximation based on only a&#10;finite memory length $d$. We present convergence rates for the Markov&#10;approximation and show that as $d\rightarrow\infty$, the Markov approximation&#10;converges to the true mixing coefficient. Our estimator is constructed using&#10;$d$-dimensional histogram density estimates. Allowing asymptotics in the&#10;bandwidth as well as the dimension, we prove $L^1$ concentration for the&#10;histogram as an intermediate step. Simulations wherein the mixing rates are&#10;calculable and a real-data example demonstrate our methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="81" source="Daniel J. McDonald" target="Mark Schervish">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5998v3" />
          <attvalue for="2" value="Estimating beta-mixing coefficients via histograms" />
          <attvalue for="3" value="The literature on statistical learning for time series often assumes&#10;asymptotic independence or &quot;mixing&quot; of the data-generating process. These&#10;mixing assumptions are never tested, nor are there methods for estimating&#10;mixing coefficients from data. Additionally, for many common classes of&#10;processes (Markov processes, ARMA processes, etc.) general functional forms for&#10;various mixing rates are known, but not specific coefficients. We present the&#10;first estimator for beta-mixing coefficients based on a single stationary&#10;sample path and show that it is risk consistent. Since mixing rates depend on&#10;infinite-dimensional dependence, we use a Markov approximation based on only a&#10;finite memory length $d$. We present convergence rates for the Markov&#10;approximation and show that as $d\rightarrow\infty$, the Markov approximation&#10;converges to the true mixing coefficient. Our estimator is constructed using&#10;$d$-dimensional histogram density estimates. Allowing asymptotics in the&#10;bandwidth as well as the dimension, we prove $L^1$ concentration for the&#10;histogram as an intermediate step. Simulations wherein the mixing rates are&#10;calculable and a real-data example demonstrate our methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="82" source="Cosma Rohilla Shalizi" target="Mark Schervish">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5998v3" />
          <attvalue for="2" value="Estimating beta-mixing coefficients via histograms" />
          <attvalue for="3" value="The literature on statistical learning for time series often assumes&#10;asymptotic independence or &quot;mixing&quot; of the data-generating process. These&#10;mixing assumptions are never tested, nor are there methods for estimating&#10;mixing coefficients from data. Additionally, for many common classes of&#10;processes (Markov processes, ARMA processes, etc.) general functional forms for&#10;various mixing rates are known, but not specific coefficients. We present the&#10;first estimator for beta-mixing coefficients based on a single stationary&#10;sample path and show that it is risk consistent. Since mixing rates depend on&#10;infinite-dimensional dependence, we use a Markov approximation based on only a&#10;finite memory length $d$. We present convergence rates for the Markov&#10;approximation and show that as $d\rightarrow\infty$, the Markov approximation&#10;converges to the true mixing coefficient. Our estimator is constructed using&#10;$d$-dimensional histogram density estimates. Allowing asymptotics in the&#10;bandwidth as well as the dimension, we prove $L^1$ concentration for the&#10;histogram as an intermediate step. Simulations wherein the mixing rates are&#10;calculable and a real-data example demonstrate our methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="83" source="Cosma Rohilla Shalizi" target="Alessandro Rinaldo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3054v4" />
          <attvalue for="2" value="Consistency under sampling of exponential random graph models" />
          <attvalue for="3" value="The growing availability of network data and of scientific interest in&#10;distributed systems has led to the rapid development of statistical models of&#10;network structure. Typically, however, these are models for the entire network,&#10;while the data consists only of a sampled sub-network. Parameters for the whole&#10;network, which is what is of interest, are estimated by applying the model to&#10;the sub-network. This assumes that the model is consistent under sampling, or,&#10;in terms of the theory of stochastic processes, that it defines a projective&#10;family. Focusing on the popular class of exponential random graph models&#10;(ERGMs), we show that this apparently trivial condition is in fact violated by&#10;many popular and scientifically appealing models, and that satisfying it&#10;drastically limits ERGM's expressive power. These results are actually special&#10;cases of more general results about exponential families of dependent random&#10;variables, which we also prove. Using such results, we offer easily checked&#10;conditions for the consistency of maximum likelihood estimation in ERGMs, and&#10;discuss some possible constructive responses." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="84" source="Min Qian" target="Susan A. Murphy">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3369v1" />
          <attvalue for="2" value="Performance guarantees for individualized treatment rules" />
          <attvalue for="3" value="Because many illnesses show heterogeneous response to treatment, there is&#10;increasing interest in individualizing treatment to patients [Arch. Gen.&#10;Psychiatry 66 (2009) 128--133]. An individualized treatment rule is a decision&#10;rule that recommends treatment according to patient characteristics. We&#10;consider the use of clinical trial data in the construction of an&#10;individualized treatment rule leading to highest mean response. This is a&#10;difficult computational problem because the objective function is the&#10;expectation of a weighted indicator function that is nonconcave in the&#10;parameters. Furthermore, there are frequently many pretreatment variables that&#10;may or may not be useful in constructing an optimal individualized treatment&#10;rule, yet cost and interpretability considerations imply that only a few&#10;variables should be used by the individualized treatment rule. To address these&#10;challenges, we consider estimation based on $l_1$-penalized least squares. This&#10;approach is justified via a finite sample upper bound on the difference between&#10;the mean response due to the estimated individualized treatment rule and the&#10;mean response due to the optimal individualized treatment rule." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="85" source="Yuan Liao" target="Wenxin Jiang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.4847v2" />
          <attvalue for="2" value="Posterior consistency of nonparametric conditional moment restricted&#10;  models" />
          <attvalue for="3" value="This paper addresses the estimation of the nonparametric conditional moment&#10;restricted model that involves an infinite-dimensional parameter $g_0$. We&#10;estimate it in a quasi-Bayesian way, based on the limited information&#10;likelihood, and investigate the impact of three types of priors on the&#10;posterior consistency: (i) truncated prior (priors supported on a bounded set),&#10;(ii) thin-tail prior (a prior that has very thin tail outside a growing bounded&#10;set) and (iii) normal prior with nonshrinking variance. In addition, $g_0$ is&#10;allowed to be only partially identified in the frequentist sense, and the&#10;parameter space does not need to be compact. The posterior is regularized using&#10;a slowly growing sieve dimension, and it is shown that the posterior converges&#10;to any small neighborhood of the identified region. We then apply our results&#10;to the nonparametric instrumental regression model. Finally, the posterior&#10;consistency using a random sieve dimension parameter is studied." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="86" source="Yuan Liao" target="Jianqing Fan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0175v2" />
          <attvalue for="2" value="Large Covariance Estimation by Thresholding Principal Orthogonal&#10;  Complements" />
          <attvalue for="3" value="This paper deals with the estimation of a high-dimensional covariance with a&#10;conditional sparsity structure and fast-diverging eigenvalues. By assuming&#10;sparse error covariance matrix in an approximate factor model, we allow for the&#10;presence of some cross-sectional correlation even after taking out common but&#10;unobservable factors. We introduce the Principal Orthogonal complEment&#10;Thresholding (POET) method to explore such an approximate factor structure with&#10;sparsity. The POET estimator includes the sample covariance matrix, the&#10;factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding&#10;estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator&#10;(Cai and Liu, 2011) as specific examples. We provide mathematical insights when&#10;the factor analysis is approximately the same as the principal component&#10;analysis for high-dimensional data. The rates of convergence of the sparse&#10;residual covariance matrix and the conditional sparse covariance matrix are&#10;studied under various norms. It is shown that the impact of estimating the&#10;unknown factors vanishes as the dimensionality increases. The uniform rates of&#10;convergence for the unobserved factors and their factor loadings are derived.&#10;The asymptotic results are also verified by extensive simulation studies.&#10;Finally, a real data application on portfolio allocation is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="87" source="Yuan Liao" target="Martina Mincheva">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0175v2" />
          <attvalue for="2" value="Large Covariance Estimation by Thresholding Principal Orthogonal&#10;  Complements" />
          <attvalue for="3" value="This paper deals with the estimation of a high-dimensional covariance with a&#10;conditional sparsity structure and fast-diverging eigenvalues. By assuming&#10;sparse error covariance matrix in an approximate factor model, we allow for the&#10;presence of some cross-sectional correlation even after taking out common but&#10;unobservable factors. We introduce the Principal Orthogonal complEment&#10;Thresholding (POET) method to explore such an approximate factor structure with&#10;sparsity. The POET estimator includes the sample covariance matrix, the&#10;factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding&#10;estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator&#10;(Cai and Liu, 2011) as specific examples. We provide mathematical insights when&#10;the factor analysis is approximately the same as the principal component&#10;analysis for high-dimensional data. The rates of convergence of the sparse&#10;residual covariance matrix and the conditional sparse covariance matrix are&#10;studied under various norms. It is shown that the impact of estimating the&#10;unknown factors vanishes as the dimensionality increases. The uniform rates of&#10;convergence for the unobserved factors and their factor loadings are derived.&#10;The asymptotic results are also verified by extensive simulation studies.&#10;Finally, a real data application on portfolio allocation is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="88" source="Yuan Liao" target="Hans-Georg Müller">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="89" source="Carenne Ludeña" target="Philippe Soulier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5176v3" />
          <attvalue for="2" value="Estimating the scaling function of multifractal measures and&#10;  multifractal random walks using ratios" />
          <attvalue for="3" value="In this paper, we prove central limit theorems for bias reduced estimators of&#10;the structure function of several multifractal processes, namely mutiplicative&#10;cascades, multifractal random measures, multifractal random walk and&#10;multifractal fractional random walk as defined by Lude\~{n}a [Ann. Appl.&#10;Probab. 18 (2008) 1138-1163]. Previous estimators of the structure functions&#10;considered in the literature were severely biased with a logarithmic rate of&#10;convergence, whereas the estimators considered here have a polynomial rate of&#10;convergence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="90" source="Carenne Ludeña" target="Alexander Aue">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="91" source="Carenne Ludeña" target="Lajos Horváth">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="92" source="Philippe Soulier" target="Rafal Kulik">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5298v2" />
          <attvalue for="2" value="Limit theorems for long memory stochastic volatility models with&#10;  infinite variance: Partial Sums and Sample Covariances" />
          <attvalue for="3" value="Long Memory Stochastic volatility (LMSV) models capture two standardized&#10;features of financial data: the log-returns are uncorrelated, but their&#10;squares, or absolute values are (highly) dependent and they may have heavy&#10;tails. EGARCH and related models were introduced to model leverage, i.e.&#10;negative dependence between previous returns and future volatility. Limit&#10;theorems for partial sums, sample variance and sample covariances are basic&#10;tools to investigate the presence of long memory and heavy tails and their&#10;consequences. In this paper we extend the existing literature on the asymptotic&#10;behaviour of the partial sums and the sample covariances of long memory&#10;stochastic volatility models in the case of infinite variance. We also consider&#10;models with leverage, for which our results are entirely new in the infinite&#10;variance case. Depending on the nterplay between the tail behaviour and the&#10;intensity of dependence, wo types of convergence rates and limiting&#10;distributions can arise. In articular, we show that the asymptotic behaviour of&#10;partial sums is the same for both LMSV and models with leverage, whereas there&#10;is a crucial difference when sample covariances are considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="93" source="Philippe Soulier" target="Alexander Aue">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="94" source="Philippe Soulier" target="Lajos Horváth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="95" source="Philippe Soulier" target="Clifford M. Hurvich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="96" source="Philippe Soulier" target="Rafał Kulik">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3136v1" />
          <attvalue for="2" value="Estimation of limiting conditional distributions for the heavy tailed&#10;  long memory stochastic volatility process" />
          <attvalue for="3" value="We consider Stochastic Volatility processes with heavy tails and possible&#10;long memory in volatility. We study the limiting conditional distribution of&#10;future events given that some present or past event was extreme (i.e. above a&#10;level which tends to infinity). Even though extremes of stochastic volatility&#10;processes are asymptotically independent (in the sense of extreme value&#10;theory), these limiting conditional distributions differ from the i.i.d. case.&#10;We introduce estimators of these limiting conditional distributions and study&#10;their asymptotic properties. If volatility has long memory, then the rate of&#10;convergence and the limiting distribution of the centered estimators can depend&#10;on the long memory parameter (Hurst index)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="97" source="Delphine Cassart" target="Marc Hallin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.2171v1" />
          <attvalue for="2" value="A class of optimal tests for symmetry based on local Edgeworth&#10;  approximations" />
          <attvalue for="3" value="The objective of this paper is to provide, for the problem of univariate&#10;symmetry (with respect to specified or unspecified location), a concept of&#10;optimality, and to construct tests achieving such optimality. This requires&#10;embedding symmetry into adequate families of asymmetric (local) alternatives.&#10;We construct such families by considering non-Gaussian generalizations of&#10;classical first-order Edgeworth expansions indexed by a measure of skewness&#10;such that (i) location, scale and skewness play well-separated roles&#10;(diagonality of the corresponding information matrices) and (ii) the classical&#10;tests based on the Pearson--Fisher coefficient of skewness are optimal in the&#10;vicinity of Gaussian densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="98" source="Delphine Cassart" target="Davy Paindaveine">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.2171v1" />
          <attvalue for="2" value="A class of optimal tests for symmetry based on local Edgeworth&#10;  approximations" />
          <attvalue for="3" value="The objective of this paper is to provide, for the problem of univariate&#10;symmetry (with respect to specified or unspecified location), a concept of&#10;optimality, and to construct tests achieving such optimality. This requires&#10;embedding symmetry into adequate families of asymmetric (local) alternatives.&#10;We construct such families by considering non-Gaussian generalizations of&#10;classical first-order Edgeworth expansions indexed by a measure of skewness&#10;such that (i) location, scale and skewness play well-separated roles&#10;(diagonality of the corresponding information matrices) and (ii) the classical&#10;tests based on the Pearson--Fisher coefficient of skewness are optimal in the&#10;vicinity of Gaussian densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="99" source="Marc Hallin" target="Davy Paindaveine">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.2171v1" />
          <attvalue for="2" value="A class of optimal tests for symmetry based on local Edgeworth&#10;  approximations" />
          <attvalue for="3" value="The objective of this paper is to provide, for the problem of univariate&#10;symmetry (with respect to specified or unspecified location), a concept of&#10;optimality, and to construct tests achieving such optimality. This requires&#10;embedding symmetry into adequate families of asymmetric (local) alternatives.&#10;We construct such families by considering non-Gaussian generalizations of&#10;classical first-order Edgeworth expansions indexed by a measure of skewness&#10;such that (i) location, scale and skewness play well-separated roles&#10;(diagonality of the corresponding information matrices) and (ii) the classical&#10;tests based on the Pearson--Fisher coefficient of skewness are optimal in the&#10;vicinity of Gaussian densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="100" source="Marc Hallin" target="Tobias Kley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="101" source="Marc Hallin" target="Stanislav Volgushev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="102" source="Marc Hallin" target="Viatcheslav B. Melas">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="103" source="Marius Hofert" target="Martin Mächler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.6032v1" />
          <attvalue for="2" value="Likelihood inference for Archimedean copulas" />
          <attvalue for="3" value="Explicit functional forms for the generator derivatives of well-known&#10;one-parameter Archimedean copulas are derived. These derivatives are essential&#10;for likelihood inference as they appear in the copula density, conditional&#10;distribution functions, or the Kendall distribution function. They are also&#10;required for several asymmetric extensions of Archimedean copulas such as&#10;Khoudraji-transformed Archimedean copulas. Access to the generator derivatives&#10;makes maximum-likelihood estimation for Archimedean copulas feasible in terms&#10;of both precision and run time, even in large dimensions. It is shown by&#10;simulation that the root mean squared error is decreasing in the dimension.&#10;This decrease is of the same order as the decrease in sample size. Furthermore,&#10;confidence intervals for the parameter vector are derived. Moreover, extensions&#10;to multi-parameter Archimedean families are given. All presented methods are&#10;implemented in the open-source R package nacopula and can thus easily be&#10;accessed and studied." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="104" source="Marius Hofert" target="Alexander J. McNeil">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.6032v1" />
          <attvalue for="2" value="Likelihood inference for Archimedean copulas" />
          <attvalue for="3" value="Explicit functional forms for the generator derivatives of well-known&#10;one-parameter Archimedean copulas are derived. These derivatives are essential&#10;for likelihood inference as they appear in the copula density, conditional&#10;distribution functions, or the Kendall distribution function. They are also&#10;required for several asymmetric extensions of Archimedean copulas such as&#10;Khoudraji-transformed Archimedean copulas. Access to the generator derivatives&#10;makes maximum-likelihood estimation for Archimedean copulas feasible in terms&#10;of both precision and run time, even in large dimensions. It is shown by&#10;simulation that the root mean squared error is decreasing in the dimension.&#10;This decrease is of the same order as the decrease in sample size. Furthermore,&#10;confidence intervals for the parameter vector are derived. Moreover, extensions&#10;to multi-parameter Archimedean families are given. All presented methods are&#10;implemented in the open-source R package nacopula and can thus easily be&#10;accessed and studied." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="105" source="Martin Mächler" target="Alexander J. McNeil">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.6032v1" />
          <attvalue for="2" value="Likelihood inference for Archimedean copulas" />
          <attvalue for="3" value="Explicit functional forms for the generator derivatives of well-known&#10;one-parameter Archimedean copulas are derived. These derivatives are essential&#10;for likelihood inference as they appear in the copula density, conditional&#10;distribution functions, or the Kendall distribution function. They are also&#10;required for several asymmetric extensions of Archimedean copulas such as&#10;Khoudraji-transformed Archimedean copulas. Access to the generator derivatives&#10;makes maximum-likelihood estimation for Archimedean copulas feasible in terms&#10;of both precision and run time, even in large dimensions. It is shown by&#10;simulation that the root mean squared error is decreasing in the dimension.&#10;This decrease is of the same order as the decrease in sample size. Furthermore,&#10;confidence intervals for the parameter vector are derived. Moreover, extensions&#10;to multi-parameter Archimedean families are given. All presented methods are&#10;implemented in the open-source R package nacopula and can thus easily be&#10;accessed and studied." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="106" source="Sílvia R. C. Lopes" target="Guilherme Pumi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2628v2" />
          <attvalue for="2" value="Copulas Related to Manneville-Pomeau Processes" />
          <attvalue for="3" value="In this work we derive the copulas related to Manneville-Pomeau processes. We&#10;examine both bidimensional and multidimensional cases and derive some&#10;properties for the related copulas. Computational issues, approximations and&#10;random variate generation problems are addressed and simple numerical&#10;experiments to test the approximations developed are also performed. In&#10;particular, we propose an approximation to the copulas derived which we show to&#10;converge uniformly to the true copula. To illustrate the usefulness of the&#10;theory, we derive a fast procedure to estimate the underlying parameter in&#10;Manneville-Pomeau processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="107" source="Jian Huang" target="Shuangge Ma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="108" source="Jian Huang" target="Hongzhe Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="109" source="Jian Huang" target="Cun-Hui Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="110" source="Shuangge Ma" target="Hongzhe Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="111" source="Shuangge Ma" target="Cun-Hui Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="112" source="Hongzhe Li" target="Cun-Hui Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3450v1" />
          <attvalue for="2" value="The sparse Laplacian shrinkage estimator for high-dimensional regression" />
          <attvalue for="3" value="We propose a new penalized method for variable selection and estimation that&#10;explicitly incorporates the correlation patterns among predictors. This method&#10;is based on a combination of the minimax concave penalty and Laplacian&#10;quadratic associated with a graph as the penalty function. We call it the&#10;sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave&#10;penalty for encouraging sparsity and Laplacian quadratic penalty for promoting&#10;smoothness among coefficients associated with the correlated predictors. The&#10;SLS has a generalized grouping property with respect to the graph represented&#10;by the Laplacian quadratic. We show that the SLS possesses an oracle property&#10;in the sense that it is selection consistent and equal to the oracle Laplacian&#10;shrinkage estimator with high probability. This result holds in sparse,&#10;high-dimensional settings with p &gt;&gt; n under reasonable conditions. We derive a&#10;coordinate descent algorithm for computing the SLS estimates. Simulation&#10;studies are conducted to evaluate the performance of the SLS method and a real&#10;data example is used to illustrate its application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="113" source="Damien Passemier" target="Jian-Feng Yao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2677v1" />
          <attvalue for="2" value="On determining the number of spikes in a high-dimensional spiked&#10;  population model" />
          <attvalue for="3" value="In a spiked population model, the population covariance matrix has all its&#10;eigenvalues equal to units except for a few fixed eigenvalues (spikes).&#10;Determining the number of spikes is a fundamental problem which appears in many&#10;scientific fields, including signal processing (linear mixture model) or&#10;economics (factor model). Several recent papers studied the asymptotic behavior&#10;of the eigenvalues of the sample covariance matrix (sample eigenvalues) when&#10;the dimension of the observations and the sample size both grow to infinity so&#10;that their ratio converges to a positive constant. Using these results, we&#10;propose a new estimator based on the difference between two consecutive sample&#10;eigenvalues." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="114" source="Guang Cheng" target="Xiao Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1304v1" />
          <attvalue for="2" value="Semiparametric Additive Transformation Model under Current Status Data" />
          <attvalue for="3" value="We consider the efficient estimation of the semiparametric additive&#10;transformation model with current status data. A wide range of survival models&#10;and econometric models can be incorporated into this general transformation&#10;framework. We apply the B-spline approach to simultaneously estimate the linear&#10;regression vector, the nondecreasing transformation function, and a set of&#10;nonparametric regression functions. We show that the parametric estimate is&#10;semiparametric efficient in the presence of multiple nonparametric nuisance&#10;functions. An explicit consistent B-spline estimate of the asymptotic variance&#10;is also provided. All nonparametric estimates are smooth, and shown to be&#10;uniformly consistent and have faster than cubic rate of convergence.&#10;Interestingly, we observe the convergence rate interfere phenomenon, i.e., the&#10;convergence rates of B-spline estimators are all slowed down to equal the&#10;slowest one. The constrained optimization is not required in our&#10;implementation. Numerical results are used to illustrate the finite sample&#10;performance of the proposed estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="115" source="Runlong Tang" target="Moulinath Banerjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3018v1" />
          <attvalue for="2" value="A two-stage hybrid procedure for estimating an inverse regression&#10;  function" />
          <attvalue for="3" value="We consider a two-stage procedure (TSP) for estimating an inverse regression&#10;function at a given point, where isotonic regression is used at stage one to&#10;obtain an initial estimate and a local linear approximation in the vicinity of&#10;this estimate is used at stage two. We establish that the convergence rate of&#10;the second-stage estimate can attain the parametric $n^{1/2}$ rate.&#10;Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its&#10;consistency properties studied. This variant manages to overcome the slow speed&#10;of the convergence in distribution and the estimation of the derivative of the&#10;regression function at the unknown target quantity. Finally, the finite sample&#10;performance of BTSP is studied through simulations and the method is&#10;illustrated on a data set." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="116" source="Runlong Tang" target="George Michailidis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3018v1" />
          <attvalue for="2" value="A two-stage hybrid procedure for estimating an inverse regression&#10;  function" />
          <attvalue for="3" value="We consider a two-stage procedure (TSP) for estimating an inverse regression&#10;function at a given point, where isotonic regression is used at stage one to&#10;obtain an initial estimate and a local linear approximation in the vicinity of&#10;this estimate is used at stage two. We establish that the convergence rate of&#10;the second-stage estimate can attain the parametric $n^{1/2}$ rate.&#10;Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its&#10;consistency properties studied. This variant manages to overcome the slow speed&#10;of the convergence in distribution and the estimation of the derivative of the&#10;regression function at the unknown target quantity. Finally, the finite sample&#10;performance of BTSP is studied through simulations and the method is&#10;illustrated on a data set." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="117" source="Moulinath Banerjee" target="George Michailidis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3018v1" />
          <attvalue for="2" value="A two-stage hybrid procedure for estimating an inverse regression&#10;  function" />
          <attvalue for="3" value="We consider a two-stage procedure (TSP) for estimating an inverse regression&#10;function at a given point, where isotonic regression is used at stage one to&#10;obtain an initial estimate and a local linear approximation in the vicinity of&#10;this estimate is used at stage two. We establish that the convergence rate of&#10;the second-stage estimate can attain the parametric $n^{1/2}$ rate.&#10;Furthermore, a bootstrapped variant of TSP (BTSP) is introduced and its&#10;consistency properties studied. This variant manages to overcome the slow speed&#10;of the convergence in distribution and the estimation of the derivative of the&#10;regression function at the unknown target quantity. Finally, the finite sample&#10;performance of BTSP is studied through simulations and the method is&#10;illustrated on a data set." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="118" source="Jérémie Bigot" target="Sébastien Gadat">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="119" source="Jérémie Bigot" target="Thierry Klein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="120" source="Jérémie Bigot" target="Clément Marteau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="121" source="Sébastien Gadat" target="Thierry Klein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="122" source="Sébastien Gadat" target="Clément Marteau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="123" source="Thierry Klein" target="Clément Marteau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3625v1" />
          <attvalue for="2" value="Intensity estimation of non-homogeneous Poisson processes from shifted&#10;  trajectories" />
          <attvalue for="3" value="This paper considers the problem of adaptive estimation of a non-homogeneous&#10;intensity function from the observation of n independent Poisson processes&#10;having a common intensity that is randomly shifted for each observed&#10;trajectory. We show that estimating this intensity is a deconvolution problem&#10;for which the density of the random shifts plays the role of the convolution&#10;operator. In an asymptotic setting where the number n of observed trajectories&#10;tends to infinity, we derive upper and lower bounds for the minimax quadratic&#10;risk over Besov balls. Non-linear thresholding in a Meyer wavelet basis is used&#10;to derive an adaptive estimator of the intensity. The proposed estimator is&#10;shown to achieve a near-minimax rate of convergence. This rate depends both on&#10;the smoothness of the intensity function and the density of the random shifts,&#10;which makes a connection between the classical deconvolution problem in&#10;nonparametric statistics and the estimation of a mean intensity from the&#10;observations of independent Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="124" source="Denys Pommeret" target="Mohamed Boutahar">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0205v1" />
          <attvalue for="2" value="Nonparametric test for detecting change in distribution with panel data" />
          <attvalue for="3" value="This paper considers the problem of comparing two processes with panel data.&#10;A nonparametric test is proposed for detecting a monotone change in the link&#10;between the two process distributions. The test statistic is of CUSUM type,&#10;based on the empirical distribution functions. The asymptotic distribution of&#10;the proposed statistic is derived and its finite sample property is examined by&#10;bootstrap procedures through Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="125" source="Denys Pommeret" target="Badih Ghattas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0205v1" />
          <attvalue for="2" value="Nonparametric test for detecting change in distribution with panel data" />
          <attvalue for="3" value="This paper considers the problem of comparing two processes with panel data.&#10;A nonparametric test is proposed for detecting a monotone change in the link&#10;between the two process distributions. The test statistic is of CUSUM type,&#10;based on the empirical distribution functions. The asymptotic distribution of&#10;the proposed statistic is derived and its finite sample property is examined by&#10;bootstrap procedures through Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="126" source="Mohamed Boutahar" target="Badih Ghattas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0205v1" />
          <attvalue for="2" value="Nonparametric test for detecting change in distribution with panel data" />
          <attvalue for="3" value="This paper considers the problem of comparing two processes with panel data.&#10;A nonparametric test is proposed for detecting a monotone change in the link&#10;between the two process distributions. The test statistic is of CUSUM type,&#10;based on the empirical distribution functions. The asymptotic distribution of&#10;the proposed statistic is derived and its finite sample property is examined by&#10;bootstrap procedures through Monte Carlo simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="127" source="Kyusang Yu" target="Enno Mammen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4410v1" />
          <attvalue for="2" value="Semi-parametric regression: Efficiency gains from modeling the&#10;  nonparametric part" />
          <attvalue for="3" value="It is widely admitted that structured nonparametric modeling that circumvents&#10;the curse of dimensionality is important in nonparametric estimation. In this&#10;paper we show that the same holds for semi-parametric estimation. We argue that&#10;estimation of the parametric component of a semi-parametric model can be&#10;improved essentially when more structure is put into the nonparametric part of&#10;the model. We illustrate this for the partially linear model, and investigate&#10;efficiency gains when the nonparametric part of the model has an additive&#10;structure. We present the semi-parametric Fisher information bound for&#10;estimating the parametric part of the partially linear additive model and&#10;provide semi-parametric efficient estimators for which we use a smooth&#10;backfitting technique to deal with the additive nonparametric part. We also&#10;present the finite sample performances of the proposed estimators and analyze&#10;Boston housing data as an illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="128" source="Kyusang Yu" target="Byeong U. Park">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4410v1" />
          <attvalue for="2" value="Semi-parametric regression: Efficiency gains from modeling the&#10;  nonparametric part" />
          <attvalue for="3" value="It is widely admitted that structured nonparametric modeling that circumvents&#10;the curse of dimensionality is important in nonparametric estimation. In this&#10;paper we show that the same holds for semi-parametric estimation. We argue that&#10;estimation of the parametric component of a semi-parametric model can be&#10;improved essentially when more structure is put into the nonparametric part of&#10;the model. We illustrate this for the partially linear model, and investigate&#10;efficiency gains when the nonparametric part of the model has an additive&#10;structure. We present the semi-parametric Fisher information bound for&#10;estimating the parametric part of the partially linear additive model and&#10;provide semi-parametric efficient estimators for which we use a smooth&#10;backfitting technique to deal with the additive nonparametric part. We also&#10;present the finite sample performances of the proposed estimators and analyze&#10;Boston housing data as an illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="129" source="Enno Mammen" target="Byeong U. Park">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4410v1" />
          <attvalue for="2" value="Semi-parametric regression: Efficiency gains from modeling the&#10;  nonparametric part" />
          <attvalue for="3" value="It is widely admitted that structured nonparametric modeling that circumvents&#10;the curse of dimensionality is important in nonparametric estimation. In this&#10;paper we show that the same holds for semi-parametric estimation. We argue that&#10;estimation of the parametric component of a semi-parametric model can be&#10;improved essentially when more structure is put into the nonparametric part of&#10;the model. We illustrate this for the partially linear model, and investigate&#10;efficiency gains when the nonparametric part of the model has an additive&#10;structure. We present the semi-parametric Fisher information bound for&#10;estimating the parametric part of the partially linear additive model and&#10;provide semi-parametric efficient estimators for which we use a smooth&#10;backfitting technique to deal with the additive nonparametric part. We also&#10;present the finite sample performances of the proposed estimators and analyze&#10;Boston housing data as an illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="130" source="Enno Mammen" target="Oliver Linton">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="131" source="Enno Mammen" target="Jens Perch Nielsen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="132" source="Enno Mammen" target="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="133" source="Enno Mammen" target="Valentin Patilea">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="134" source="Byeong U. Park" target="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="135" source="Naohiro Kato" target="Satoshi Kuriki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1033v4" />
          <attvalue for="2" value="Likelihood ratio tests for positivity in polynomial regressions" />
          <attvalue for="3" value="A polynomial that is nonnegative over a given interval is called a positive&#10;polynomial. The set of such positive polynomials forms a closed convex cone&#10;$K$. In this paper, we consider the likelihood ratio test for the hypothesis of&#10;positivity that the estimand polynomial regression curve is a positive&#10;polynomial. By considering hierarchical hypotheses including the hypothesis of&#10;positivity, we define nested likelihood ratio tests, and derive their null&#10;distributions as mixtures of chi-square distributions by using the&#10;volume-of-tubes method. The mixing probabilities are obtained by utilizing the&#10;parameterizations for the cone $K$ and its dual provided in the framework of&#10;Tchebycheff systems for polynomials of degree at most 4. For polynomials of&#10;degree greater than 4, the upper and lower bounds for the null distributions&#10;are provided. Moreover, we propose associated simultaneous confidence bounds&#10;for polynomial regression curves. Regarding computation, we demonstrate that&#10;symmetric cone programming is useful to obtain the test statistics. As an&#10;illustrative example, we conduct data analysis on growth curves of two groups.&#10;We examine the hypothesis that the growth rate (the derivative of growth curve)&#10;of one group is always higher than the other." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="136" source="Stanislav Volgushev" target="Tobias Kley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7205v3" />
          <attvalue for="2" value="Of copulas, quantiles, ranks and spectra: An $L_1$-approach to spectral&#10;  analysis" />
          <attvalue for="3" value="In this paper, we present an alternative method for the spectral analysis of&#10;a univariate, strictly stationary time series $\{Y_t\}_{t\in \mathbb {Z}}$. We&#10;define a &quot;new&quot; spectrum as the Fourier transform of the differences between&#10;copulas of the pairs $(Y_t,Y_{t-k})$ and the independence copula. This object&#10;is called a copula spectral density kernel and allows to separate the marginal&#10;and serial aspects of a time series. We show that this spectrum is closely&#10;related to the concept of quantile regression. Like quantile regression, which&#10;provides much more information about conditional distributions than classical&#10;location-scale regression models, copula spectral density kernels are more&#10;informative than traditional spectral densities obtained from classical&#10;autocovariances. In particular, copula spectral density kernels, in their&#10;population versions, provide (asymptotically provide, in their sample versions)&#10;a complete description of the copulas of all pairs $(Y_t,Y_{t-k})$. Moreover,&#10;they inherit the robustness properties of classical quantile regression, and do&#10;not require any distributional assumptions such as the existence of finite&#10;moments. In order to estimate the copula spectral density kernel, we introduce&#10;rank-based Laplace periodograms which are calculated as bilinear forms of&#10;weighted $L_1$-projections of the ranks of the observed time series onto a&#10;harmonic regression model. We establish the asymptotic distribution of those&#10;periodograms, and the consistency of adequately smoothed versions. The&#10;finite-sample properties of the new methodology, and its potential for&#10;applications are briefly investigated by simulations and a short empirical&#10;example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="137" source="Giorgio Dall'Aglio" target="Elisabetta Bona">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.1536v1" />
          <attvalue for="2" value="The Minimum of the Entropy of a Two-Dimensional Distribution with Given&#10;  Marginals" />
          <attvalue for="3" value="The paper search for the minimum of the entropy of a two- dimensional&#10;distribution in the Fr\'echet class, the class of distributions with given&#10;marginals. The main result for discrete distributions is an algorithm for&#10;building the minimizing distribution, which is given by the maximum&#10;distribution function of the Fr\'echet class after a suitable rearrangement of&#10;the rows and of the columns. For absolutely continuous distributions a minimum&#10;does not exists, and the infimum is equal to -\infty." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="138" source="Chunming Zhang" target="Jianqing Fan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1966v1" />
          <attvalue for="2" value="Multiple testing via $FDR_L$ for large-scale imaging data" />
          <attvalue for="3" value="The multiple testing procedure plays an important role in detecting the&#10;presence of spatial signals for large-scale imaging data. Typically, the&#10;spatial signals are sparse but clustered. This paper provides empirical&#10;evidence that for a range of commonly used control levels, the conventional&#10;$\operatorname {FDR}$ procedure can lack the ability to detect statistical&#10;significance, even if the $p$-values under the true null hypotheses are&#10;independent and uniformly distributed; more generally, ignoring the neighboring&#10;information of spatially structured data will tend to diminish the detection&#10;effectiveness of the $\operatorname {FDR}$ procedure. This paper first&#10;introduces a scalar quantity to characterize the extent to which the &quot;lack of&#10;identification phenomenon&quot; ($\operatorname {LIP}$) of the $\operatorname {FDR}$&#10;procedure occurs. Second, we propose a new multiple comparison procedure,&#10;called $\operatorname {FDR}_L$, to accommodate the spatial information of&#10;neighboring $p$-values, via a local aggregation of $p$-values. Theoretical&#10;properties of the $\operatorname {FDR}_L$ procedure are investigated under weak&#10;dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$&#10;procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$&#10;procedure, thus substantially facilitating the selection of more stringent&#10;control levels. Simulation evaluations indicate that the $\operatorname&#10;{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname&#10;{FDR}$ procedure with little loss in detection specificity. The computational&#10;simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure&#10;are illustrated through a real brain fMRI dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="139" source="Chunming Zhang" target="Tao Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1966v1" />
          <attvalue for="2" value="Multiple testing via $FDR_L$ for large-scale imaging data" />
          <attvalue for="3" value="The multiple testing procedure plays an important role in detecting the&#10;presence of spatial signals for large-scale imaging data. Typically, the&#10;spatial signals are sparse but clustered. This paper provides empirical&#10;evidence that for a range of commonly used control levels, the conventional&#10;$\operatorname {FDR}$ procedure can lack the ability to detect statistical&#10;significance, even if the $p$-values under the true null hypotheses are&#10;independent and uniformly distributed; more generally, ignoring the neighboring&#10;information of spatially structured data will tend to diminish the detection&#10;effectiveness of the $\operatorname {FDR}$ procedure. This paper first&#10;introduces a scalar quantity to characterize the extent to which the &quot;lack of&#10;identification phenomenon&quot; ($\operatorname {LIP}$) of the $\operatorname {FDR}$&#10;procedure occurs. Second, we propose a new multiple comparison procedure,&#10;called $\operatorname {FDR}_L$, to accommodate the spatial information of&#10;neighboring $p$-values, via a local aggregation of $p$-values. Theoretical&#10;properties of the $\operatorname {FDR}_L$ procedure are investigated under weak&#10;dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$&#10;procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$&#10;procedure, thus substantially facilitating the selection of more stringent&#10;control levels. Simulation evaluations indicate that the $\operatorname&#10;{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname&#10;{FDR}$ procedure with little loss in detection specificity. The computational&#10;simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure&#10;are illustrated through a real brain fMRI dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="140" source="Jianqing Fan" target="Tao Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1966v1" />
          <attvalue for="2" value="Multiple testing via $FDR_L$ for large-scale imaging data" />
          <attvalue for="3" value="The multiple testing procedure plays an important role in detecting the&#10;presence of spatial signals for large-scale imaging data. Typically, the&#10;spatial signals are sparse but clustered. This paper provides empirical&#10;evidence that for a range of commonly used control levels, the conventional&#10;$\operatorname {FDR}$ procedure can lack the ability to detect statistical&#10;significance, even if the $p$-values under the true null hypotheses are&#10;independent and uniformly distributed; more generally, ignoring the neighboring&#10;information of spatially structured data will tend to diminish the detection&#10;effectiveness of the $\operatorname {FDR}$ procedure. This paper first&#10;introduces a scalar quantity to characterize the extent to which the &quot;lack of&#10;identification phenomenon&quot; ($\operatorname {LIP}$) of the $\operatorname {FDR}$&#10;procedure occurs. Second, we propose a new multiple comparison procedure,&#10;called $\operatorname {FDR}_L$, to accommodate the spatial information of&#10;neighboring $p$-values, via a local aggregation of $p$-values. Theoretical&#10;properties of the $\operatorname {FDR}_L$ procedure are investigated under weak&#10;dependence of $p$-values. It is shown that the $\operatorname {FDR}_L$&#10;procedure alleviates the $\operatorname {LIP}$ of the $\operatorname {FDR}$&#10;procedure, thus substantially facilitating the selection of more stringent&#10;control levels. Simulation evaluations indicate that the $\operatorname&#10;{FDR}_L$ procedure improves the detection sensitivity of the $\operatorname&#10;{FDR}$ procedure with little loss in detection specificity. The computational&#10;simplicity and detection effectiveness of the $\operatorname {FDR}_L$ procedure&#10;are illustrated through a real brain fMRI dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="141" source="Jianqing Fan" target="Yichao Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5217v1" />
          <attvalue for="2" value="Varying-coefficient functional linear regression" />
          <attvalue for="3" value="Functional linear regression analysis aims to model regression relations&#10;which include a functional predictor. The analog of the regression parameter&#10;vector or matrix in conventional multivariate or multiple-response linear&#10;regression models is a regression parameter function in one or two arguments.&#10;If, in addition, one has scalar predictors, as is often the case in&#10;applications to longitudinal studies, the question arises how to incorporate&#10;these into a functional regression model. We study a varying-coefficient&#10;approach where the scalar covariates are modeled as additional arguments of the&#10;regression parameter function. This extension of the functional linear&#10;regression model is analogous to the extension of conventional linear&#10;regression models to varying-coefficient models and shares its advantages, such&#10;as increased flexibility; however, the details of this extension are more&#10;challenging in the functional case. Our methodology combines smoothing methods&#10;with regularization by truncation at a finite number of functional principal&#10;components. A practical version is developed and is shown to perform better&#10;than functional linear regression for longitudinal data. We investigate the&#10;asymptotic properties of varying-coefficient functional linear regression and&#10;establish consistency properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="142" source="Jianqing Fan" target="Hans-Georg Müller">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5217v1" />
          <attvalue for="2" value="Varying-coefficient functional linear regression" />
          <attvalue for="3" value="Functional linear regression analysis aims to model regression relations&#10;which include a functional predictor. The analog of the regression parameter&#10;vector or matrix in conventional multivariate or multiple-response linear&#10;regression models is a regression parameter function in one or two arguments.&#10;If, in addition, one has scalar predictors, as is often the case in&#10;applications to longitudinal studies, the question arises how to incorporate&#10;these into a functional regression model. We study a varying-coefficient&#10;approach where the scalar covariates are modeled as additional arguments of the&#10;regression parameter function. This extension of the functional linear&#10;regression model is analogous to the extension of conventional linear&#10;regression models to varying-coefficient models and shares its advantages, such&#10;as increased flexibility; however, the details of this extension are more&#10;challenging in the functional case. Our methodology combines smoothing methods&#10;with regularization by truncation at a finite number of functional principal&#10;components. A practical version is developed and is shown to perform better&#10;than functional linear regression for longitudinal data. We investigate the&#10;asymptotic properties of varying-coefficient functional linear regression and&#10;establish consistency properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="143" source="Jianqing Fan" target="Martina Mincheva">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0175v2" />
          <attvalue for="2" value="Large Covariance Estimation by Thresholding Principal Orthogonal&#10;  Complements" />
          <attvalue for="3" value="This paper deals with the estimation of a high-dimensional covariance with a&#10;conditional sparsity structure and fast-diverging eigenvalues. By assuming&#10;sparse error covariance matrix in an approximate factor model, we allow for the&#10;presence of some cross-sectional correlation even after taking out common but&#10;unobservable factors. We introduce the Principal Orthogonal complEment&#10;Thresholding (POET) method to explore such an approximate factor structure with&#10;sparsity. The POET estimator includes the sample covariance matrix, the&#10;factor-based covariance matrix (Fan, Fan, and Lv, 2008), the thresholding&#10;estimator (Bickel and Levina, 2008) and the adaptive thresholding estimator&#10;(Cai and Liu, 2011) as specific examples. We provide mathematical insights when&#10;the factor analysis is approximately the same as the principal component&#10;analysis for high-dimensional data. The rates of convergence of the sparse&#10;residual covariance matrix and the conditional sparse covariance matrix are&#10;studied under various norms. It is shown that the impact of estimating the&#10;unknown factors vanishes as the dimensionality increases. The uniform rates of&#10;convergence for the unobserved factors and their factor loadings are derived.&#10;The asymptotic results are also verified by extensive simulation studies.&#10;Finally, a real data application on portfolio allocation is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="144" source="Jianqing Fan" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="145" source="Rafal Kulik" target="Pawel Lorek">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4372v1" />
          <attvalue for="2" value="Some results on random design regression with long memory errors and&#10;  predictors" />
          <attvalue for="3" value="This paper studies nonparametric regression with long memory (LRD) errors and&#10;predictors. First, we formulate general conditions which guarantee the standard&#10;rate of convergence for a nonparametric kernel estimator. Second, we calculate&#10;the Mean Integrated Squared Error (MISE). In particular, we show that LRD of&#10;errors may influence MISE. On the other hand, an estimator for a shape function&#10;is typically not influenced by LRD in errors. Finally, we investigate&#10;properties of a data-driven bandwidth choice. We show that Averaged Squared&#10;Error (ASE) is a good approximation of MISE, however, this is not the case for&#10;a cross-validation criterion." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="146" source="Felix Abramovich" target="Vadim Grinshtein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.1771v2" />
          <attvalue for="2" value="Estimation of a sparse group of sparse vectors" />
          <attvalue for="3" value="We consider a problem of estimating a sparse group of sparse normal mean&#10;vectors. The proposed approach is based on penalized likelihood estimation with&#10;complexity penalties on the number of nonzero mean vectors and the numbers of&#10;their &quot;significant&quot; components, and can be performed by a computationally fast&#10;algorithm. The resulting estimators are developed within Bayesian framework and&#10;can be viewed as MAP estimators. We establish their adaptive minimaxity over a&#10;wide range of sparse and dense settings. The presented short simulation study&#10;demonstrates the efficiency of the proposed approach that successfully competes&#10;with the recently developed sparse group lasso estimator." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="147" source="Felix Abramovich" target="Marianna Pensky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2766v3" />
          <attvalue for="2" value="Laplace deconvolution with noisy observations" />
          <attvalue for="3" value="In the present paper we consider Laplace deconvolution for discrete noisy&#10;data observed on the interval whose length may increase with a sample size.&#10;Although this problem arises in a variety of applications, to the best of our&#10;knowledge, it has been given very little attention by the statistical&#10;community. Our objective is to fill this gap and provide statistical treatment&#10;of Laplace deconvolution problem with noisy discrete data. The main&#10;contribution of the paper is explicit construction of an asymptotically&#10;rate-optimal (in the minimax sense) Laplace deconvolution estimator which is&#10;adaptive to the regularity of the unknown function. We show that the original&#10;Laplace deconvolution problem can be reduced to nonparametric estimation of a&#10;regression function and its derivatives on the interval of growing length T_n.&#10;Whereas the forms of the estimators remain standard, the choices of the&#10;parameters and the minimax convergence rates, which are expressed in terms of&#10;T_n^2/n in this case, are affected by the asymptotic growth of the length of&#10;the interval.&#10;  We derive an adaptive kernel estimator of the function of interest, and&#10;establish its asymptotic minimaxity over a range of Sobolev classes. We&#10;illustrate the theory by examples of construction of explicit expressions of&#10;Laplace deconvolution estimators. A simulation study shows that, in addition to&#10;providing asymptotic optimality as the number of observations turns to&#10;infinity, the proposed estimator demonstrates good performance in finite sample&#10;examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="148" source="Felix Abramovich" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2766v3" />
          <attvalue for="2" value="Laplace deconvolution with noisy observations" />
          <attvalue for="3" value="In the present paper we consider Laplace deconvolution for discrete noisy&#10;data observed on the interval whose length may increase with a sample size.&#10;Although this problem arises in a variety of applications, to the best of our&#10;knowledge, it has been given very little attention by the statistical&#10;community. Our objective is to fill this gap and provide statistical treatment&#10;of Laplace deconvolution problem with noisy discrete data. The main&#10;contribution of the paper is explicit construction of an asymptotically&#10;rate-optimal (in the minimax sense) Laplace deconvolution estimator which is&#10;adaptive to the regularity of the unknown function. We show that the original&#10;Laplace deconvolution problem can be reduced to nonparametric estimation of a&#10;regression function and its derivatives on the interval of growing length T_n.&#10;Whereas the forms of the estimators remain standard, the choices of the&#10;parameters and the minimax convergence rates, which are expressed in terms of&#10;T_n^2/n in this case, are affected by the asymptotic growth of the length of&#10;the interval.&#10;  We derive an adaptive kernel estimator of the function of interest, and&#10;establish its asymptotic minimaxity over a range of Sobolev classes. We&#10;illustrate the theory by examples of construction of explicit expressions of&#10;Laplace deconvolution estimators. A simulation study shows that, in addition to&#10;providing asymptotic optimality as the number of observations turns to&#10;infinity, the proposed estimator demonstrates good performance in finite sample&#10;examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="149" source="Felix Abramovich" target="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="150" source="Vadim Grinshtein" target="Marianna Pensky">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="151" source="Vadim Grinshtein" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="152" source="Jean-Marc Bardet" target="Donatas Surgailis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4732v2" />
          <attvalue for="2" value="Moment bounds and central limit theorems for Gaussian subordinated&#10;  arrays" />
          <attvalue for="3" value="A general moment bound for sums of products of Gaussian vector's functions&#10;extending the moment bound in Taqqu (1977, Lemma 4.5) is established. A general&#10;central limit theorem for triangular arrays of nonlinear functionals of&#10;multidimensional non-stationary Gaussian sequences is proved. This theorem&#10;extends the previous results of Breuer and Major (1981), Arcones (1994) and&#10;others. A Berry-Esseen-type bound in the above-mentioned central limit theorem&#10;is derived following Nourdin, Peccati and Podolskij (2011). Two applications of&#10;the above results are discussed. The first one refers to the asymptotic&#10;behavior of a roughness statistic for continuous-time Gaussian processes and&#10;the second one is a central limit theorem satisfied by long memory locally&#10;stationary process." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="153" source="Piet Groeneboom" target="Geurt Jongbloed">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3333v2" />
          <attvalue for="2" value="Testing monotonicity of a hazard: asymptotic distribution theory" />
          <attvalue for="3" value="Two new test statistics are introduced to test the null hypotheses that the&#10;sampling distribution has an increasing hazard rate on a specified interval&#10;[0,a]. These statistics are empirical L_1-type distances between the isotonic&#10;estimates, which use the monotonicity constraint, and either the empirical&#10;distribution function or the empirical cumulative hazard. They measure the&#10;excursions of the empirical estimates with respect to the isotonic estimates,&#10;due to local non-monotonicity. Asymptotic normality of the test statistics, if&#10;the hazard is strictly increasing on [0,a], is established under mild&#10;conditions. This is done by first approximating the global empirical distance&#10;by an distance with respect to the underlying distribution function. The&#10;resulting integral is treated as sum of increasingly many local integrals to&#10;which a CLT can be applied. The behavior of the local integrals is determined&#10;by a canonical process: the difference between the stochastic process x -&gt;&#10;W(x)+x^2 where W is standard two-sided Brownian Motion, and its greatest convex&#10;minorant." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="154" source="Piet Groeneboom" target="Birgit Witte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1875v1" />
          <attvalue for="2" value="Smooth plug-in inverse estimators in the current status continuous mark&#10;  model" />
          <attvalue for="3" value="We consider the problem of estimating the joint distribution function of the&#10;event time and a continuous mark variable when the event time is subject to&#10;interval censoring case 1 and the continuous mark variable is only observed in&#10;case the event occurred before the time of inspection. The nonparametric&#10;maximum likelihood estimator in this model is known to be inconsistent. We&#10;study two alternative smooth estimators, based on the explicit (inverse)&#10;expression of the distribution function of interest in terms of the density of&#10;the observable vector. We derive the pointwise asymptotic distribution of both&#10;estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="155" source="Piet Groeneboom" target="Tom Ketelaars">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6176v2" />
          <attvalue for="2" value="Estimators for the interval censoring problem" />
          <attvalue for="3" value="We study three estimators for the interval censoring case 2 problem, a&#10;histogram-type estimator, proposed in Birg\'e (1999), the maximum likelihood&#10;estimator (MLE) and the smoothed MLE, using a smoothing kernel. Our focus is on&#10;the asymptotic distribution of the estimators at a fixed point. The estimators&#10;are compared in a simulation study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="156" source="Geurt Jongbloed" target="Birgit Witte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1875v1" />
          <attvalue for="2" value="Smooth plug-in inverse estimators in the current status continuous mark&#10;  model" />
          <attvalue for="3" value="We consider the problem of estimating the joint distribution function of the&#10;event time and a continuous mark variable when the event time is subject to&#10;interval censoring case 1 and the continuous mark variable is only observed in&#10;case the event occurred before the time of inspection. The nonparametric&#10;maximum likelihood estimator in this model is known to be inconsistent. We&#10;study two alternative smooth estimators, based on the explicit (inverse)&#10;expression of the distribution function of interest in terms of the density of&#10;the observable vector. We derive the pointwise asymptotic distribution of both&#10;estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="157" source="Thomas A. Dean" target="Sumeetpal S. Singh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3655v1" />
          <attvalue for="2" value="Asymptotic Behaviour of Approximate Bayesian Estimators" />
          <attvalue for="3" value="Although approximate Bayesian computation (ABC) has become a popular&#10;technique for performing parameter estimation when the likelihood functions are&#10;analytically intractable there has not as yet been a complete investigation of&#10;the theoretical properties of the resulting estimators. In this paper we give a&#10;theoretical analysis of the asymptotic properties of ABC based parameter&#10;estimators for hidden Markov models and show that ABC based estimators satisfy&#10;asymptotically biased versions of the standard results in the statistical&#10;literature." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="158" source="Takuya Kashimura" target="Tomonari Sei">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="159" source="Takuya Kashimura" target="Kentaro Tanaka">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="160" source="Jose A. Diaz-Garcia" target="Rogelio Raos-Quiroga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.6104v1" />
          <attvalue for="2" value="A modified Prékopa's approach in optimum allocation in multivariate&#10;  stratified random sampling" />
          <attvalue for="3" value="A modified Prekopa's approach is considered for the problem of optimum&#10;allocation in multivariate stratified random sampling. An example is solved by&#10;applying the proposed methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="161" source="Jose A. Diaz-Garcia" target="Rogelio Ramos-Quiroga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3224v1" />
          <attvalue for="2" value="Optimum allocation in multivariate stratified random sampling:&#10;  Stochastic matrix optimisation" />
          <attvalue for="3" value="The allocation problem for multivariate stratified random sampling as a&#10;problem of stochastic matrix integer mathematical programming is considered.&#10;With these aims the asymptotic normality of sample covariance matrices for each&#10;strata is established. Some alternative approaches are suggested for its&#10;solution. An example is solved by applying the proposed techniques." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="162" source="Jose A. Diaz-Garcia" target="Mahdi Bashiri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2911v1" />
          <attvalue for="2" value="Multiple response optimisation: Multiobjective stochastic programming&#10;  methods" />
          <attvalue for="3" value="The multiresponse surface problem is modelled as one of multiobjective&#10;stochastic optimisation, and diverse solutions are proposed. Several crucial&#10;differences are highlighted between this approach and others that have been&#10;proposed. Finally, in a numerical example, some particular solutions are&#10;applied and described in detail." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="163" source="Michael Evans" target="Gun Ho Jang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3258v1" />
          <attvalue for="2" value="Inferences from prior-based loss functions" />
          <attvalue for="3" value="Inferences that arise from loss functions determined by the prior are&#10;considered and it is shown that these lead to limiting Bayes rules that are&#10;closely connected with likelihood. The procedures obtained via these loss&#10;functions are invariant under reparameterizations and are Bayesian unbiased or&#10;limits of Bayesian unbiased inferences. These inferences serve as&#10;well-supported alternatives to MAP-based inferences." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="164" source="Elizabeth Gross" target="Mathias Drton">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3308v1" />
          <attvalue for="2" value="Maximum likelihood degree of variance component models" />
          <attvalue for="3" value="Most statistical software packages implement numerical strategies for&#10;computation of maximum likelihood estimates in random effects models. Little is&#10;known, however, about the algebraic complexity of this problem. For the one-way&#10;layout with random effects and unbalanced group sizes, we give formulas for the&#10;algebraic degree of the likelihood equations as well as the equations for&#10;restricted maximum likelihood estimation. In particular, the latter approach is&#10;shown to be algebraically less complex. The formulas are obtained by studying a&#10;univariate rational equation whose solutions correspond to the solutions of the&#10;likelihood equations. Applying techniques from computational algebra, we also&#10;show that balanced two-way layouts with or without interaction have likelihood&#10;equations of degree four. Our work suggests that algebraic methods allow one to&#10;reliably find global optima of likelihood functions of linear mixed models with&#10;a small number of variance components." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="165" source="Elizabeth Gross" target="Sonja Petrović">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3308v1" />
          <attvalue for="2" value="Maximum likelihood degree of variance component models" />
          <attvalue for="3" value="Most statistical software packages implement numerical strategies for&#10;computation of maximum likelihood estimates in random effects models. Little is&#10;known, however, about the algebraic complexity of this problem. For the one-way&#10;layout with random effects and unbalanced group sizes, we give formulas for the&#10;algebraic degree of the likelihood equations as well as the equations for&#10;restricted maximum likelihood estimation. In particular, the latter approach is&#10;shown to be algebraically less complex. The formulas are obtained by studying a&#10;univariate rational equation whose solutions correspond to the solutions of the&#10;likelihood equations. Applying techniques from computational algebra, we also&#10;show that balanced two-way layouts with or without interaction have likelihood&#10;equations of degree four. Our work suggests that algebraic methods allow one to&#10;reliably find global optima of likelihood functions of linear mixed models with&#10;a small number of variance components." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="166" source="Mathias Drton" target="Sonja Petrović">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3308v1" />
          <attvalue for="2" value="Maximum likelihood degree of variance component models" />
          <attvalue for="3" value="Most statistical software packages implement numerical strategies for&#10;computation of maximum likelihood estimates in random effects models. Little is&#10;known, however, about the algebraic complexity of this problem. For the one-way&#10;layout with random effects and unbalanced group sizes, we give formulas for the&#10;algebraic degree of the likelihood equations as well as the equations for&#10;restricted maximum likelihood estimation. In particular, the latter approach is&#10;shown to be algebraically less complex. The formulas are obtained by studying a&#10;univariate rational equation whose solutions correspond to the solutions of the&#10;likelihood equations. Applying techniques from computational algebra, we also&#10;show that balanced two-way layouts with or without interaction have likelihood&#10;equations of degree four. Our work suggests that algebraic methods allow one to&#10;reliably find global optima of likelihood functions of linear mixed models with&#10;a small number of variance components." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="167" source="Mathias Drton" target="Jan Draisma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.5552v2" />
          <attvalue for="2" value="Half-trek criterion for generic identifiability of linear structural&#10;  equation models" />
          <attvalue for="3" value="A linear structural equation model relates random variables of interest and&#10;corresponding Gaussian noise terms via a linear equation system. Each such&#10;model can be represented by a mixed graph in which directed edges encode the&#10;linear equations and bidirected edges indicate possible correlations among&#10;noise terms. We study parameter identifiability in these models, that is, we&#10;ask for conditions that ensure that the edge coefficients and correlations&#10;appearing in a linear structural equation model can be uniquely recovered from&#10;the covariance matrix of the associated distribution. We treat the case of&#10;generic identifiability, where unique recovery is possible for almost every&#10;choice of parameters. We give a new graphical condition that is sufficient for&#10;generic identifiability and can be verified in time that is polynomial in the&#10;size of the graph. It improves criteria from prior work and does not require&#10;the directed part of the graph to be acyclic. We also develop a related&#10;necessary condition and examine the &quot;gap&quot; between sufficient and necessary&#10;conditions through simulations on graphs with 25 or 50 nodes, as well as&#10;exhaustive algebraic computations for graphs with up to five nodes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="168" source="Raúl Jiménez" target="J. E. Yukich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1492v1" />
          <attvalue for="2" value="Nonparametric estimation of surface integrals" />
          <attvalue for="3" value="The estimation of surface integrals on the boundary of an unknown body is a&#10;challenge for nonparametric methods in statistics, with powerful applications&#10;to physics and image analysis, among other fields. Provided that one can&#10;determine whether random shots hit the body, Cuevas et al. [Ann. Statist. 35&#10;(2007) 1031--1051] estimate the boundary measure (the boundary length for&#10;planar sets and the surface area for 3-dimensional objects) via the&#10;consideration of shots at a box containing the body. The statistics considered&#10;by these authors, as well as those in subsequent papers, are based on the&#10;estimation of Minkowski content and depend on a smoothing parameter which must&#10;be carefully chosen. For the same sampling scheme, we introduce a new approach&#10;which bypasses this issue, providing strongly consistent estimators of both the&#10;boundary measure and the surface integrals of scalar functions, provided one&#10;can collect the function values at the sample points. Examples arise in&#10;experiments in which the density of the body can be measured by physical&#10;properties of the impacts, or in situations where such quantities as&#10;temperature and humidity are observed by randomly distributed sensors. Our&#10;method is based on random Delaunay triangulations and involves a simple&#10;procedure for surface reconstruction from a dense cloud of points inside and&#10;outside the body. We obtain basic asymptotics of the estimator, perform&#10;simulations and discuss, via Google Earth's data, an application to the image&#10;analysis of the Aral Sea coast and its cliffs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="169" source="Pierre Vandekerkhove" target="Gersende Fort">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="170" source="Pierre Vandekerkhove" target="Eric Moulines">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="171" source="Pierre Vandekerkhove" target="Pierre Priouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="172" source="Jie Yang" target="Abhyuday Mandal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5320v8" />
          <attvalue for="2" value="Optimal Designs for 2^k Factorial Experiments with Binary Response" />
          <attvalue for="3" value="We consider the problem of obtaining D-optimal designs for factorial&#10;experiments with a binary response and $k$ qualitative factors each at two&#10;levels. We obtain a characterization for a design to be locally D-optimal.&#10;Based on this characterization, we develop efficient numerical techniques to&#10;search for locally D-optimal designs. Using prior distributions on the&#10;parameters, we investigate EW D-optimal designs, which are designs that&#10;maximize the determinant of the expected information matrix. It turns out that&#10;these designs can be obtained very easily using our algorithm for locally&#10;D-optimal designs and are very good surrogates for Bayes D-optimal designs. We&#10;also investigate the properties of fractional factorial designs and study the&#10;robustness with respect to the assumed parameter values of locally D-optimal&#10;designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="173" source="Jie Yang" target="Dibyen Majumdar">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5320v8" />
          <attvalue for="2" value="Optimal Designs for 2^k Factorial Experiments with Binary Response" />
          <attvalue for="3" value="We consider the problem of obtaining D-optimal designs for factorial&#10;experiments with a binary response and $k$ qualitative factors each at two&#10;levels. We obtain a characterization for a design to be locally D-optimal.&#10;Based on this characterization, we develop efficient numerical techniques to&#10;search for locally D-optimal designs. Using prior distributions on the&#10;parameters, we investigate EW D-optimal designs, which are designs that&#10;maximize the determinant of the expected information matrix. It turns out that&#10;these designs can be obtained very easily using our algorithm for locally&#10;D-optimal designs and are very good surrogates for Bayes D-optimal designs. We&#10;also investigate the properties of fractional factorial designs and study the&#10;robustness with respect to the assumed parameter values of locally D-optimal&#10;designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="174" source="Abhyuday Mandal" target="Dibyen Majumdar">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5320v8" />
          <attvalue for="2" value="Optimal Designs for 2^k Factorial Experiments with Binary Response" />
          <attvalue for="3" value="We consider the problem of obtaining D-optimal designs for factorial&#10;experiments with a binary response and $k$ qualitative factors each at two&#10;levels. We obtain a characterization for a design to be locally D-optimal.&#10;Based on this characterization, we develop efficient numerical techniques to&#10;search for locally D-optimal designs. Using prior distributions on the&#10;parameters, we investigate EW D-optimal designs, which are designs that&#10;maximize the determinant of the expected information matrix. It turns out that&#10;these designs can be obtained very easily using our algorithm for locally&#10;D-optimal designs and are very good surrogates for Bayes D-optimal designs. We&#10;also investigate the properties of fractional factorial designs and study the&#10;robustness with respect to the assumed parameter values of locally D-optimal&#10;designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="175" source="Peter McCullagh" target="Han Han">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3228v1" />
          <attvalue for="2" value="On Bayes' theorem for improper mixtures" />
          <attvalue for="3" value="Although Bayes's theorem demands a prior that is a probability distribution&#10;on the parameter space, the calculus associated with Bayes's theorem sometimes&#10;generates sensible procedures from improper priors, Pitman's estimator being a&#10;good example. However, improper priors may also lead to Bayes procedures that&#10;are paradoxical or otherwise unsatisfactory, prompting some authors to insist&#10;that all priors be proper. This paper begins with the observation that an&#10;improper measure on Theta satisfying Kingman's countability condition is in&#10;fact a probability distribution on the power set. We show how to extend a model&#10;in such a way that the extended parameter space is the power set. Under an&#10;additional finiteness condition, which is needed for the existence of a&#10;sampling region, the conditions for Bayes's theorem are satisfied by the&#10;extension. Lack of interference ensures that the posterior distribution in the&#10;extended space is compatible with the original parameter space. Provided that&#10;the key finiteness condition is satisfied, this probabilistic analysis of the&#10;extended model may be interpreted as a vindication of improper Bayes procedures&#10;derived from the original model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="176" source="Dominique Guillot" target="Bala Rajaratnam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3325v1" />
          <attvalue for="2" value="Retaining positive definiteness in thresholded matrices" />
          <attvalue for="3" value="Positive definite (p.d.) matrices arise naturally in many areas within&#10;mathematics and also feature extensively in scientific applications. In modern&#10;high-dimensional applications, a common approach to finding sparse positive&#10;definite matrices is to threshold their small off-diagonal elements. This&#10;thresholding, sometimes referred to as hard-thresholding, sets small elements&#10;to zero. Thresholding has the attractive property that the resulting matrices&#10;are sparse, and are thus easier to interpret and work with. In many&#10;applications, it is often required, and thus implicitly assumed, that&#10;thresholded matrices retain positive definiteness. In this paper we formally&#10;investigate the algebraic properties of p.d. matrices which are thresholded. We&#10;demonstrate that for positive definiteness to be preserved, the pattern of&#10;elements to be set to zero has to necessarily correspond to a graph which is a&#10;union of disconnected complete components. This result rigorously demonstrates&#10;that, except in special cases, positive definiteness can be easily lost. We&#10;then proceed to demonstrate that the class of diagonally dominant matrices is&#10;not maximal in terms of retaining positive definiteness when thresholded.&#10;Consequently, we derive characterizations of matrices which retain positive&#10;definiteness when thresholded with respect to important classes of graphs. In&#10;particular, we demonstrate that retaining positive definiteness upon&#10;thresholding is governed by complex algebraic conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="177" source="Bala Rajaratnam" target="Alfred Hero">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6846v2" />
          <attvalue for="2" value="Hub discovery in partial correlation graphical models" />
          <attvalue for="3" value="This paper treats the problem of screening a p-variate sample for strongly&#10;and multiply connected vertices in the partial correlation graph associated&#10;with the the partial correlation matrix of the sample. This problem, called hub&#10;screening, is important in many applications ranging from network security to&#10;computational biology to finance to social networks. In the area of network&#10;security, a node that becomes a hub of high correlation with neighboring nodes&#10;might signal anomalous activity such as a coordinated flooding attack. In the&#10;area of computational biology the set of hubs of a gene expression correlation&#10;graph can serve as potential targets for drug treatment to block a pathway or&#10;modulate host response. In the area of finance a hub might indicate a&#10;vulnerable financial instrument or sector whose collapse might have major&#10;repercussions on the market. In the area of social networks a hub of observed&#10;interactions between criminal suspects could be an influential ringleader. The&#10;techniques and theory presented in this paper permit scalable and reliable&#10;screening for such hubs. This paper extends our previous work on correlation&#10;screening [arXiv:1102.1204] to the more challenging problem of partial&#10;correlation screening for variables with a high degree of connectivity. In&#10;particular we consider 1) extension to the more difficult problem of screening&#10;for partial correlations exceeding a specified magnitude; 2) extension to&#10;screening variables whose vertex degree in the associated partial correlation&#10;graph, often called the concentration graph, exceeds a specified degree." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="178" source="Bala Rajaratnam" target="Kshitij Khare">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1768v1" />
          <attvalue for="2" value="Wishart distributions for decomposable covariance graph models" />
          <attvalue for="3" value="Gaussian covariance graph models encode marginal independence among the&#10;components of a multivariate random vector by means of a graph $G$. These&#10;models are distinctly different from the traditional concentration graph models&#10;(often also referred to as Gaussian graphical models or covariance selection&#10;models) since the zeros in the parameter are now reflected in the covariance&#10;matrix $\Sigma$, as compared to the concentration matrix $\Omega =\Sigma^{-1}$.&#10;The parameter space of interest for covariance graph models is the cone $P_G$&#10;of positive definite matrices with fixed zeros corresponding to the missing&#10;edges of $G$. As in Letac and Massam [Ann. Statist. 35 (2007) 1278--1323], we&#10;consider the case where $G$ is decomposable. In this paper, we construct on the&#10;cone $P_G$ a family of Wishart distributions which serve a similar purpose in&#10;the covariance graph setting as those constructed by Letac and Massam [Ann.&#10;Statist. 35 (2007) 1278--1323] and Dawid and Lauritzen [Ann. Statist. 21 (1993)&#10;1272--1317] do in the concentration graph setting. We proceed to undertake a&#10;rigorous study of these &quot;covariance&quot; Wishart distributions and derive several&#10;deep and useful properties of this class." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="179" source="Helene Gehrmann" target="Steffen L. Lauritzen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3709v3" />
          <attvalue for="2" value="Estimation of means in graphical Gaussian models with symmetries" />
          <attvalue for="3" value="We study the problem of estimability of means in undirected graphical&#10;Gaussian models with symmetry restrictions represented by a colored graph.&#10;Following on from previous studies, we partition the variables into sets of&#10;vertices whose corresponding means are restricted to being identical. We find a&#10;necessary and sufficient condition on the partition to ensure equality between&#10;the maximum likelihood and least-squares estimators of the mean." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="180" source="Ryan J. Tibshirani" target="Jonathan Taylor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0653v4" />
          <attvalue for="2" value="Degrees of freedom in lasso problems" />
          <attvalue for="3" value="We derive the degrees of freedom of the lasso fit, placing no assumptions on&#10;the predictor matrix $X$. Like the well-known result of Zou, Hastie and&#10;Tibshirani [Ann. Statist. 35 (2007) 2173-2192], which gives the degrees of&#10;freedom of the lasso fit when $X$ has full column rank, we express our result&#10;in terms of the active set of a lasso solution. We extend this result to cover&#10;the degrees of freedom of the generalized lasso fit for an arbitrary predictor&#10;matrix $X$ (and an arbitrary penalty matrix $D$). Though our focus is degrees&#10;of freedom, we establish some intermediate results on the lasso and generalized&#10;lasso that may be interesting on their own." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="181" source="Pierre-Olivier Amblard" target="Jean-François Coeurjolly">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2872v1" />
          <attvalue for="2" value="Identification of the Multivariate Fractional Brownian Motion" />
          <attvalue for="3" value="This paper deals with the identification of the multivariate fractional&#10;Brownian motion, a recently developed extension of the fractional Brownian&#10;motion to the multivariate case. This process is a $p$-multivariate&#10;self-similar Gaussian process parameterized by $p$ different Hurst exponents&#10;$H_i$, $p$ scaling coefficients $\sigma_i$ (of each component) and also by&#10;$p(p-1)$ coefficients $\rho_{ij},\eta_{ij}$ (for $i,j=1,...,p$ with $j&gt;i$)&#10;allowing two components to be more or less strongly correlated and allowing the&#10;process to be time reversible or not. We investigate the use of discrete&#10;filtering techniques to estimate jointly or separately the different parameters&#10;and prove the efficiency of the methodology with a simulation study and the&#10;derivation of asymptotic results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="182" source="Jean-François Coeurjolly" target="Hedi Kortas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0540v1" />
          <attvalue for="2" value="Expectiles for subordinated Gaussian processes with applications" />
          <attvalue for="3" value="In this paper, we introduce a new class of estimators of the Hurst exponent&#10;of the fractional Brownian motion (fBm) process. These estimators are based on&#10;sample expectiles of discrete variations of a sample path of the fBm process.&#10;In order to derive the statistical properties of the proposed estimators, we&#10;establish asymptotic results for sample expectiles of subordinated stationary&#10;Gaussian processes with unit variance and correlation function satisfying&#10;$\rho(i)\sim \kappa|i|^{-\alpha}$ ($\kappa\in \RR$) with $\alpha&gt;0$. Via a&#10;simulation study, we demonstrate the relevance of the expectile-based&#10;estimation method and show that the suggested estimators are more robust to&#10;data rounding than their sample quantile-based counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="183" source="Fabrice Rossi" target="Nathalie Villa-Vialaneix">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0204v1" />
          <attvalue for="2" value="Consistency of functional learning methods based on derivatives" />
          <attvalue for="3" value="In some real world applications, such as spectrometry, functional models&#10;achieve better predictive performances if they work on the derivatives of order&#10;m of their inputs rather than on the original functions. As a consequence, the&#10;use of derivatives is a common practice in Functional Data Analysis, despite a&#10;lack of theoretical guarantees on the asymptotically achievable performances of&#10;a derivative based model. In this paper, we show that a smoothing spline&#10;approach can be used to preprocess multivariate observations obtained by&#10;sampling functions on a discrete and finite sampling grid in a way that leads&#10;to a consistent scheme on the original infinite dimensional functional problem.&#10;This work extends (Mas and Pumo, 2009) to nonparametric approaches and&#10;incomplete knowledge. To be more precise, the paper tackles two difficulties in&#10;a nonparametric framework: the information loss due to the use of the&#10;derivatives instead of the original functions and the information loss due to&#10;the fact that the functions are observed through a discrete sampling and are&#10;thus also unperfectly known: the use of a smoothing spline based approach&#10;solves these two problems. Finally, the proposed approach is tested on two real&#10;world datasets and the approach is experimentaly proven to be a good solution&#10;in the case of noisy functional predictors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="184" source="Séphane Gaïffas" target="Agathe Guilloux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4662v2" />
          <attvalue for="2" value="High-dimensional additive hazard models and the Lasso" />
          <attvalue for="3" value="We consider a general high-dimensional additive hazard model in a&#10;non-asymptotic setting, including regression for censored-data. In this&#10;context, we consider a Lasso estimator with a fully data-driven $\ell_1$&#10;penalization, which is tuned for the estimation problem at hand. We prove sharp&#10;oracle inequalities for this estimator. Our analysis involves a new&#10;&quot;data-driven&quot; Bernstein's inequality, that is of independent interest, where&#10;the predictable variation is replaced by the optional variation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="185" source="Helena Ferreira" target="Marta Ferreira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.1490v1" />
          <attvalue for="2" value="Fragility Index of block tailed vectors" />
          <attvalue for="3" value="Financial crises are a recurrent phenomenon with important effects on the&#10;real economy. The financial system is inherently fragile and it is therefore of&#10;great importance to be able to measure and characterize its systemic stability.&#10;Multivariate extreme value theory provide us such a framework through the&#10;\emph{fragility index} (Geluk \cite{gel+}, \emph{et al.}, 2007; Falk and Tichy,&#10;\cite{falk+tichy1,falk+tichy2} 2010, 2011). Here we generalize this concept and&#10;contribute to the modeling of the stability of a stochastic system divided into&#10;blocks. We will find several relations with well-known tail dependence measures&#10;in literature, which will provide us immediate estimators. We end with an&#10;application to financial data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="186" source="Helena Ferreira" target="Cecília Fonseca">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="187" source="Helena Ferreira" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="188" source="Helena Ferreira" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="189" source="Marta Ferreira" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="190" source="Marta Ferreira" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="191" source="Laurent Gardes" target="Stéphane Girard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0098v1" />
          <attvalue for="2" value="A Note on Sliced Inverse Regression with Regularizations" />
          <attvalue for="3" value="In &quot;Li, L. and Yin, X. (2008). Sliced Inverse Regression with&#10;Regularizations. Biometrics, 64(1):124--131&quot; a ridge SIR estimator is&#10;introduced as the solution of a minimization problem and computed thanks to an&#10;alternating least-squares algorithm. This methodology reveals good performance&#10;in practice. In this note, we focus on the theoretical properties of the&#10;estimator. Is it shown that the minimization problem is degenerated in the&#10;sense that only two situations can occur: Either the ridge SIR estimator does&#10;not exist or it is zero." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="192" source="Laurent Gardes" target="Caroline Bernard-Michel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0098v1" />
          <attvalue for="2" value="A Note on Sliced Inverse Regression with Regularizations" />
          <attvalue for="3" value="In &quot;Li, L. and Yin, X. (2008). Sliced Inverse Regression with&#10;Regularizations. Biometrics, 64(1):124--131&quot; a ridge SIR estimator is&#10;introduced as the solution of a minimization problem and computed thanks to an&#10;alternating least-squares algorithm. This methodology reveals good performance&#10;in practice. In this note, we focus on the theoretical properties of the&#10;estimator. Is it shown that the minimization problem is degenerated in the&#10;sense that only two situations can occur: Either the ridge SIR estimator does&#10;not exist or it is zero." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="193" source="Stéphane Girard" target="Cécile Amblard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5953v1" />
          <attvalue for="2" value="Symmetry and dependence properties within a semiparametric family of&#10;  bivariate copulas" />
          <attvalue for="3" value="In this paper, we study a semiparametric family of bivariate copulas. The&#10;family is generated by an univariate function, determining the symmetry (radial&#10;symmetry, joint symmetry) and dependence property (quadrant dependence, total&#10;positivity, ...) of the copulas. We provide bounds on different measures of&#10;association (such as Kendall's Tau, Spearman's Rho) for this family and several&#10;choices of generating functions allowing to reach these bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="194" source="Stéphane Girard" target="Ludovic Menneteau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5884v1" />
          <attvalue for="2" value="Central limit theorems for smoothed extreme value estimates of Poisson&#10;  point processes boundaries" />
          <attvalue for="3" value="In this paper, we give sufficient conditions to establish central limit&#10;theorems for boundary estimates of Poisson point processes. The considered&#10;estimates are obtained by smoothing some bias corrected extreme values of the&#10;point process. We show how the smoothing leads Gaussian asymptotic&#10;distributions and therefore pointwise confidence intervals. Some new&#10;unidimensional and multidimensional examples are provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="195" source="Stéphane Girard" target="Caroline Bernard-Michel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0098v1" />
          <attvalue for="2" value="A Note on Sliced Inverse Regression with Regularizations" />
          <attvalue for="3" value="In &quot;Li, L. and Yin, X. (2008). Sliced Inverse Regression with&#10;Regularizations. Biometrics, 64(1):124--131&quot; a ridge SIR estimator is&#10;introduced as the solution of a minimization problem and computed thanks to an&#10;alternating least-squares algorithm. This methodology reveals good performance&#10;in practice. In this note, we focus on the theoretical properties of the&#10;estimator. Is it shown that the minimization problem is degenerated in the&#10;sense that only two situations can occur: Either the ridge SIR estimator does&#10;not exist or it is zero." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="196" source="Fan Wei" target="Richard M Dudley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.5356v2" />
          <attvalue for="2" value="Dvoretzky--Kiefer--Wolfowitz Inequalities for the Two-sample Case" />
          <attvalue for="3" value="The Dvoretzky--Kiefer--Wolfowitz (DKW) inequality says that if $F_n$ is an&#10;empirical distribution function for variables i.i.d.\ with a distribution&#10;function $F$, and $K_n$ is the Kolmogorov statistic&#10;$\sqrt{n}\sup_x|(F_n-F)(x)|$, then there is a finite constant $C$ such that for&#10;any $M&gt;0$, $\Pr(K_n&gt;M) \leq C\exp(-2M^2).$ Massart proved that one can take C=2&#10;(DKWM inequality) which is sharp for $F$ continuous. We consider the analogous&#10;Kolmogorov--Smirnov statistic $KS_{m,n}$ for the two-sample case and show that&#10;for $m=n$, the DKW inequality holds with C=2 if and only if $n\geq 458$. For&#10;$n_0\leq n&lt;458$ it holds for some $C&gt;2$ depending on $n_0$.&#10;  For $m\neq n$, the DKWM inequality fails for the three pairs $(m,n)$ with&#10;$1\leq m &lt; n\leq 3$. We found by computer search that for $n\geq 4$, the DKWM&#10;inequality always holds for $1\leq m&lt; n\leq 200$, and further that it holds for&#10;$n=2m$ with $101\leq m\leq 300$. We conjecture that the DKWM inequality holds&#10;for pairs $m\leq n$ with the $457+3 =460$ exceptions mentioned." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="197" source="Holger Fink" target="Claudia Klüppelberg">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1830v1" />
          <attvalue for="2" value="Fractional Lévy-driven Ornstein--Uhlenbeck processes and stochastic&#10;  differential equations" />
          <attvalue for="3" value="Using Riemann-Stieltjes methods for integrators of bounded $p$-variation we&#10;define a pathwise integral driven by a fractional L\'{e}vy process (FLP). To&#10;explicitly solve general fractional stochastic differential equations (SDEs) we&#10;introduce an Ornstein-Uhlenbeck model by a stochastic integral representation,&#10;where the driving stochastic process is an FLP. To achieve the convergence of&#10;improper integrals, the long-time behavior of FLPs is derived. This is&#10;sufficient to define the fractional L\'{e}vy-Ornstein-Uhlenbeck process (FLOUP)&#10;pathwise as an improper Riemann-Stieltjes integral. We show further that the&#10;FLOUP is the unique stationary solution of the corresponding Langevin equation.&#10;Furthermore, we calculate the autocovariance function and prove that its&#10;increments exhibit long-range dependence. Exploiting the Langevin equation, we&#10;consider SDEs driven by FLPs of bounded $p$-variation for $p&lt;2$ and construct&#10;solutions using the corresponding FLOUP. Finally, we consider examples of such&#10;SDEs, including various state space transforms of the FLOUP and also fractional&#10;L\'{e}vy-driven Cox-Ingersoll-Ross (CIR) models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="198" source="Claudia Klüppelberg" target="Peter Brockwell">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4468v4" />
          <attvalue for="2" value="High-frequency sampling and kernel estimation for continuous-time moving&#10;  average processes" />
          <attvalue for="3" value="Interest in continuous-time processes has increased rapidly in recent years,&#10;largely because of high-frequency data available in many applications. We&#10;develop a method for estimating the kernel function $g$ of a second-order&#10;stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based&#10;on observations of the discrete-time process $Y^\Delta$ obtained by sampling&#10;$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by&#10;$g^\Delta$ based on the Wold representation and prove its pointwise convergence&#10;to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric&#10;estimators of $g^\Delta$, based on the innovations algorithm and the&#10;Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA&#10;process we give conditions on the sample size $n$ and the grid-spacing&#10;$\Delta(n)$ under which the innovations estimator is consistent and&#10;asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated&#10;from sampled observations of {\it any} CMA process and simulations suggest that&#10;they perform well even outside the class of CARMA processes. We illustrate&#10;their performance for simulated data and apply them to the Brookhaven turbulent&#10;wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled&#10;CARMA processes to a much wider class of CMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="199" source="Claudia Klüppelberg" target="Vincenzo Ferrazzano">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4468v4" />
          <attvalue for="2" value="High-frequency sampling and kernel estimation for continuous-time moving&#10;  average processes" />
          <attvalue for="3" value="Interest in continuous-time processes has increased rapidly in recent years,&#10;largely because of high-frequency data available in many applications. We&#10;develop a method for estimating the kernel function $g$ of a second-order&#10;stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based&#10;on observations of the discrete-time process $Y^\Delta$ obtained by sampling&#10;$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by&#10;$g^\Delta$ based on the Wold representation and prove its pointwise convergence&#10;to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric&#10;estimators of $g^\Delta$, based on the innovations algorithm and the&#10;Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA&#10;process we give conditions on the sample size $n$ and the grid-spacing&#10;$\Delta(n)$ under which the innovations estimator is consistent and&#10;asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated&#10;from sampled observations of {\it any} CMA process and simulations suggest that&#10;they perform well even outside the class of CARMA processes. We illustrate&#10;their performance for simulated data and apply them to the Brookhaven turbulent&#10;wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled&#10;CARMA processes to a much wider class of CMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="200" source="Milan Studeny" target="David Haws">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4708v3" />
          <attvalue for="2" value="On polyhedral approximations of polytopes for learning Bayes nets" />
          <attvalue for="3" value="We review three vector encodings of Bayesian network structures. The first&#10;one has recently been applied by Jaakkola 2010, the other two use special&#10;integral vectors formerly introduced, called imsets [Studeny 2005, Studeny&#10;2010]. The central topic is the comparison of outer polyhedral approximations&#10;of the corresponding polytopes. We show how to transform the inequalities&#10;suggested by Jaakkola et al. to the framework of imsets. The result of our&#10;comparison is the observation that the implicit polyhedral approximation of the&#10;standard imset polytope suggested in [Studeny 2011] gives a closer&#10;approximation than the (transformed) explicit polyhedral approximation from&#10;[Jaakkola 2010]. Finally, we confirm a conjecture from [Studeny 2011] that the&#10;above-mentioned implicit polyhedral approximation of the standard imset&#10;polytope is an LP relaxation of the polytope." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="201" source="Adam D. Bull" target="Richard Nickl">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5568v3" />
          <attvalue for="2" value="Adaptive confidence sets in L^2" />
          <attvalue for="3" value="The problem of constructing confidence sets that are adaptive in L^2-loss&#10;over a continuous scale of Sobolev classes of probability densities is&#10;considered. Adaptation holds, where possible, with respect to both the radius&#10;of the Sobolev ball and its smoothness degree, and over maximal parameter&#10;spaces for which adaptation is possible. Two key regimes of parameter&#10;constellations are identified: one where full adaptation is possible, and one&#10;where adaptation requires critical regions be removed. Techniques used to&#10;derive these results include a general nonparametric minimax test for&#10;infinite-dimensional null- and alternative hypotheses, and new lower bounds for&#10;L^2-adaptive confidence sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="202" source="Richard Nickl" target="Karim Lounici">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1489v1" />
          <attvalue for="2" value="Global uniform risk bounds for wavelet deconvolution estimators" />
          <attvalue for="3" value="We consider the statistical deconvolution problem where one observes $n$&#10;replications from the model $Y=X+\epsilon$, where $X$ is the unobserved random&#10;signal of interest and $\epsilon$ is an independent random error with&#10;distribution $\phi$. Under weak assumptions on the decay of the Fourier&#10;transform of $\phi,$ we derive upper bounds for the finite-sample sup-norm risk&#10;of wavelet deconvolution density estimators $f_n$ for the density $f$ of $X$,&#10;where $f:\mathbb{R}\to \mathbb{R}$ is assumed to be bounded. We then derive&#10;lower bounds for the minimax sup-norm risk over Besov balls in this estimation&#10;problem and show that wavelet deconvolution density estimators attain these&#10;bounds. We further show that linear estimators adapt to the unknown smoothness&#10;of $f$ if the Fourier transform of $\phi$ decays exponentially and that a&#10;corresponding result holds true for the hard thresholding wavelet estimator if&#10;$\phi$ decays polynomially. We also analyze the case where $f$ is a&#10;&quot;supersmooth&quot;/analytic density. We finally show how our results and recent&#10;techniques from Rademacher processes can be applied to construct global&#10;confidence bands for the density $f$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="203" source="Richard Nickl" target="Florian Gach">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2807v2" />
          <attvalue for="2" value="Spatially Adaptive Density Estimation by Localised Haar Projections" />
          <attvalue for="3" value="Given a random sample from some unknown density $f_0: \mathbb R \to [0,&#10;\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution&#10;levels constructed from localised test procedures (as in Lepski, Mammen, and&#10;Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to&#10;spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$&#10;in a fixed interval, in sup-norm loss. The thresholding constants involved in&#10;the test procedures can be chosen in practice under the idealised assumption&#10;that the true density is locally constant in a neighborhood of the point $x$ of&#10;estimation, and an information theoretic justification of this practice is&#10;given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="204" source="Richard Nickl" target="Vladimir Spokoiny">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2807v2" />
          <attvalue for="2" value="Spatially Adaptive Density Estimation by Localised Haar Projections" />
          <attvalue for="3" value="Given a random sample from some unknown density $f_0: \mathbb R \to [0,&#10;\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution&#10;levels constructed from localised test procedures (as in Lepski, Mammen, and&#10;Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to&#10;spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$&#10;in a fixed interval, in sup-norm loss. The thresholding constants involved in&#10;the test procedures can be chosen in practice under the idealised assumption&#10;that the true density is locally constant in a neighborhood of the point $x$ of&#10;estimation, and an information theoretic justification of this practice is&#10;given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="205" source="Richard Nickl" target="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2450v1" />
          <attvalue for="2" value="Concentration Inequalities and Confidence Bands for Needlet Density&#10;  Estimators on Compact Homogeneous Manifolds" />
          <attvalue for="3" value="Let $X_1,...,X_n$ be a random sample from some unknown probability density&#10;$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge&#10;1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised&#10;projection onto the space of eigenfunctions of the Laplace operator on $\mathbf&#10;M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in&#10;\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform&#10;deviations of the linear needlet density estimator $f_n(j)$ obtained from an&#10;empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f&#10;\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive&#10;estimators and nonasymptotic confidence bands for the unknown density $f$. The&#10;confidence bands are adaptive over classes of differentiable and&#10;H\&quot;{older}-continuous functions on $\mathbf M$ that attain their H\&quot;{o}lder&#10;exponents." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="206" source="Richard Nickl" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2450v1" />
          <attvalue for="2" value="Concentration Inequalities and Confidence Bands for Needlet Density&#10;  Estimators on Compact Homogeneous Manifolds" />
          <attvalue for="3" value="Let $X_1,...,X_n$ be a random sample from some unknown probability density&#10;$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge&#10;1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised&#10;projection onto the space of eigenfunctions of the Laplace operator on $\mathbf&#10;M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in&#10;\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform&#10;deviations of the linear needlet density estimator $f_n(j)$ obtained from an&#10;empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f&#10;\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive&#10;estimators and nonasymptotic confidence bands for the unknown density $f$. The&#10;confidence bands are adaptive over classes of differentiable and&#10;H\&quot;{older}-continuous functions on $\mathbf M$ that attain their H\&quot;{o}lder&#10;exponents." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="207" source="Richard Nickl" target="Mathilde Mougeot">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="208" source="Richard Nickl" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="209" source="Elchanan Mossel" target="Allan Sly">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.4765v5" />
          <attvalue for="2" value="From Agreement to Asymptotic Learning" />
          <attvalue for="3" value="We consider a group of Bayesian agents who are each given an independent&#10;signal about an unknown state of the world, and proceed to communicate with&#10;each other. We study the question of asymptotic learning: do agents learn the&#10;state of the world with probability that approaches one as the number of agents&#10;tends to infinity?&#10;  We show that under general conditions asymptotic learning follows from&#10;agreement on posterior actions or posterior beliefs, regardless of the&#10;communication dynamics. In particular, we prove that asymptotic learning holds&#10;for the Gale-Kariv model on undirected networks and non-atomic private beliefs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="210" source="Elchanan Mossel" target="Omer Tamuz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.4765v5" />
          <attvalue for="2" value="From Agreement to Asymptotic Learning" />
          <attvalue for="3" value="We consider a group of Bayesian agents who are each given an independent&#10;signal about an unknown state of the world, and proceed to communicate with&#10;each other. We study the question of asymptotic learning: do agents learn the&#10;state of the world with probability that approaches one as the number of agents&#10;tends to infinity?&#10;  We show that under general conditions asymptotic learning follows from&#10;agreement on posterior actions or posterior beliefs, regardless of the&#10;communication dynamics. In particular, we prove that asymptotic learning holds&#10;for the Gale-Kariv model on undirected networks and non-atomic private beliefs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="211" source="Allan Sly" target="Omer Tamuz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.4765v5" />
          <attvalue for="2" value="From Agreement to Asymptotic Learning" />
          <attvalue for="3" value="We consider a group of Bayesian agents who are each given an independent&#10;signal about an unknown state of the world, and proceed to communicate with&#10;each other. We study the question of asymptotic learning: do agents learn the&#10;state of the world with probability that approaches one as the number of agents&#10;tends to infinity?&#10;  We show that under general conditions asymptotic learning follows from&#10;agreement on posterior actions or posterior beliefs, regardless of the&#10;communication dynamics. In particular, we prove that asymptotic learning holds&#10;for the Gale-Kariv model on undirected networks and non-atomic private beliefs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="212" source="Oleg Lepski" target="Nora Serdyukova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3563v4" />
          <attvalue for="2" value="Adaptive estimation in the single-index model via oracle approach" />
          <attvalue for="3" value="In the framework of nonparametric multivariate function estimation we are&#10;interested in structural adaptation. We assume that the function to be&#10;estimated has the &quot;single-index&quot; structure where neither the link function nor&#10;the index vector is known. We suggest a novel procedure that adapts&#10;simultaneously to the unknown index and smoothness of the link function. For&#10;the proposed procedure, we prove a &quot;local&quot; oracle inequality (described by the&#10;pointwise seminorm), which is then used to obtain the upper bound on the&#10;maximal risk of the adaptive estimator under assumption that the link function&#10;belongs to a scale of H\&quot;{o}lder classes. The lower bound on the minimax risk&#10;shows that in the case of estimating at a given point the constructed estimator&#10;is optimally rate adaptive over the considered range of classes. For the same&#10;procedure we also establish a &quot;global&quot; oracle inequality (under the $ L_r $&#10;norm, $r&lt; \infty $) and examine its performance over the Nikol'skii classes.&#10;This study shows that the proposed method can be applied to estimating&#10;functions of inhomogeneous smoothness, that is whose smoothness may vary from&#10;point to point." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="213" source="Robert L. Wolpert" target="Merlise A. Clyde">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3149v1" />
          <attvalue for="2" value="Stochastic expansions using continuous dictionaries: Lévy adaptive&#10;  regression kernels" />
          <attvalue for="3" value="This article describes a new class of prior distributions for nonparametric&#10;function estimation. The unknown function is modeled as a limit of weighted&#10;sums of kernels or generator functions indexed by continuous parameters that&#10;control local and global features such as their translation, dilation,&#10;modulation and shape. L\'{e}vy random fields and their stochastic integrals are&#10;employed to induce prior distributions for the unknown functions or,&#10;equivalently, for the number of kernels and for the parameters governing their&#10;features. Scaling, shape, and other features of the generating functions are&#10;location-specific to allow quite different function properties in different&#10;parts of the space, as with wavelet bases and other methods employing&#10;overcomplete dictionaries. We provide conditions under which the stochastic&#10;expansions converge in specified Besov or Sobolev norms. Under a Gaussian error&#10;model, this may be viewed as a sparse regression problem, with regularization&#10;induced via the L\'{e}vy random field prior distribution. Posterior inference&#10;for the unknown functions is based on a reversible jump Markov chain Monte&#10;Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)&#10;method to wavelet-based methods using some of the standard test functions, and&#10;illustrate its flexibility and adaptability in nonstationary applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="214" source="Robert L. Wolpert" target="Chong Tu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3149v1" />
          <attvalue for="2" value="Stochastic expansions using continuous dictionaries: Lévy adaptive&#10;  regression kernels" />
          <attvalue for="3" value="This article describes a new class of prior distributions for nonparametric&#10;function estimation. The unknown function is modeled as a limit of weighted&#10;sums of kernels or generator functions indexed by continuous parameters that&#10;control local and global features such as their translation, dilation,&#10;modulation and shape. L\'{e}vy random fields and their stochastic integrals are&#10;employed to induce prior distributions for the unknown functions or,&#10;equivalently, for the number of kernels and for the parameters governing their&#10;features. Scaling, shape, and other features of the generating functions are&#10;location-specific to allow quite different function properties in different&#10;parts of the space, as with wavelet bases and other methods employing&#10;overcomplete dictionaries. We provide conditions under which the stochastic&#10;expansions converge in specified Besov or Sobolev norms. Under a Gaussian error&#10;model, this may be viewed as a sparse regression problem, with regularization&#10;induced via the L\'{e}vy random field prior distribution. Posterior inference&#10;for the unknown functions is based on a reversible jump Markov chain Monte&#10;Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)&#10;method to wavelet-based methods using some of the standard test functions, and&#10;illustrate its flexibility and adaptability in nonstationary applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="215" source="Merlise A. Clyde" target="Chong Tu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3149v1" />
          <attvalue for="2" value="Stochastic expansions using continuous dictionaries: Lévy adaptive&#10;  regression kernels" />
          <attvalue for="3" value="This article describes a new class of prior distributions for nonparametric&#10;function estimation. The unknown function is modeled as a limit of weighted&#10;sums of kernels or generator functions indexed by continuous parameters that&#10;control local and global features such as their translation, dilation,&#10;modulation and shape. L\'{e}vy random fields and their stochastic integrals are&#10;employed to induce prior distributions for the unknown functions or,&#10;equivalently, for the number of kernels and for the parameters governing their&#10;features. Scaling, shape, and other features of the generating functions are&#10;location-specific to allow quite different function properties in different&#10;parts of the space, as with wavelet bases and other methods employing&#10;overcomplete dictionaries. We provide conditions under which the stochastic&#10;expansions converge in specified Besov or Sobolev norms. Under a Gaussian error&#10;model, this may be viewed as a sparse regression problem, with regularization&#10;induced via the L\'{e}vy random field prior distribution. Posterior inference&#10;for the unknown functions is based on a reversible jump Markov chain Monte&#10;Carlo algorithm. We compare the L\'{e}vy Adaptive Regression Kernel (LARK)&#10;method to wavelet-based methods using some of the standard test functions, and&#10;illustrate its flexibility and adaptability in nonstationary applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="216" source="Sebastian Krumscheid" target="Grigorios A. Pavliotis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1916v3" />
          <attvalue for="2" value="Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions" />
          <attvalue for="3" value="We consider the problem of statistical inference for the effective dynamics&#10;of multiscale diffusion processes with (at least) two widely separated&#10;characteristic time scales. More precisely, we seek to determine parameters in&#10;the effective equation describing the dynamics on the longer diffusive time&#10;scale, i.e. in a homogenization framework. We examine the case where both the&#10;drift and the diffusion coefficients in the effective dynamics are&#10;space-dependent and depend on multiple unknown parameters. It is known that&#10;classical estimators, such as Maximum Likelihood and Quadratic Variation of the&#10;Path Estimators, fail to obtain reasonable estimates for parameters in the&#10;effective dynamics when based on observations of the underlying multiscale&#10;diffusion. We propose a novel algorithm for estimating both the drift and&#10;diffusion coefficients in the effective dynamics based on a semi-parametric&#10;framework. We demonstrate by means of extensive numerical simulations of a&#10;number of selected examples that the algorithm performs well when applied to&#10;data from a multiscale diffusion. These examples also illustrate that the&#10;algorithm can be used effectively to obtain accurate and unbiased estimates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="217" source="Sebastian Krumscheid" target="Serafim Kalliadasis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1916v3" />
          <attvalue for="2" value="Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions" />
          <attvalue for="3" value="We consider the problem of statistical inference for the effective dynamics&#10;of multiscale diffusion processes with (at least) two widely separated&#10;characteristic time scales. More precisely, we seek to determine parameters in&#10;the effective equation describing the dynamics on the longer diffusive time&#10;scale, i.e. in a homogenization framework. We examine the case where both the&#10;drift and the diffusion coefficients in the effective dynamics are&#10;space-dependent and depend on multiple unknown parameters. It is known that&#10;classical estimators, such as Maximum Likelihood and Quadratic Variation of the&#10;Path Estimators, fail to obtain reasonable estimates for parameters in the&#10;effective dynamics when based on observations of the underlying multiscale&#10;diffusion. We propose a novel algorithm for estimating both the drift and&#10;diffusion coefficients in the effective dynamics based on a semi-parametric&#10;framework. We demonstrate by means of extensive numerical simulations of a&#10;number of selected examples that the algorithm performs well when applied to&#10;data from a multiscale diffusion. These examples also illustrate that the&#10;algorithm can be used effectively to obtain accurate and unbiased estimates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="218" source="Grigorios A. Pavliotis" target="Serafim Kalliadasis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1916v3" />
          <attvalue for="2" value="Semi-Parametric Drift and Diffusion Estimation for Multiscale Diffusions" />
          <attvalue for="3" value="We consider the problem of statistical inference for the effective dynamics&#10;of multiscale diffusion processes with (at least) two widely separated&#10;characteristic time scales. More precisely, we seek to determine parameters in&#10;the effective equation describing the dynamics on the longer diffusive time&#10;scale, i.e. in a homogenization framework. We examine the case where both the&#10;drift and the diffusion coefficients in the effective dynamics are&#10;space-dependent and depend on multiple unknown parameters. It is known that&#10;classical estimators, such as Maximum Likelihood and Quadratic Variation of the&#10;Path Estimators, fail to obtain reasonable estimates for parameters in the&#10;effective dynamics when based on observations of the underlying multiscale&#10;diffusion. We propose a novel algorithm for estimating both the drift and&#10;diffusion coefficients in the effective dynamics based on a semi-parametric&#10;framework. We demonstrate by means of extensive numerical simulations of a&#10;number of selected examples that the algorithm performs well when applied to&#10;data from a multiscale diffusion. These examples also illustrate that the&#10;algorithm can be used effectively to obtain accurate and unbiased estimates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="219" source="Sándor Baran" target="Gyula Pap">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.3318v3" />
          <attvalue for="2" value="Parameter estimation in a spatial unit root autoregressive model" />
          <attvalue for="3" value="Spatial unilateral autoregressive model $X_{k,\ell}=\alpha X_{k-1,\ell}+\beta&#10;X_{k,\ell-1}+\gamma X_{k-1,\ell-1}+\epsilon_{k,\ell}$ is investigated in the&#10;unit root case, that is when the parameters are on the boundary of the domain&#10;of stability that forms a tetrahedron with vertices $(1,1,-1), \ (1,-1,1),\&#10;(-1,1,1)$ and $(-1,-1,-1)$. It is shown that the limiting distribution of the&#10;least squares estimator of the parameters is normal and the rate of convergence&#10;is $n$ when the parameters are in the faces or on the edges of the tetrahedron,&#10;while on the vertices the rate is $n^{3/2}$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="220" source="Sándor Baran" target="Kinga Sikolya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2205v1" />
          <attvalue for="2" value="Parameter estimation in linear regression driven by a Gaussian sheet" />
          <attvalue for="3" value="The problem of estimating the parameters of a linear regression model&#10;$Z(s,t)=m_1g_1(s,t)+ \cdots + m_pg_p(s,t)+U(s,t)$ based on observations of $Z$&#10;on a spatial domain $G$ of special shape is considered, where the driving&#10;process $U$ is a Gaussian random field and $g_1, \ldots, g_p$ are known&#10;functions. Explicit forms of the maximum likelihood estimators of the&#10;parameters are derived in the cases when $U$ is either a Wiener or a stationary&#10;or nonstationary Ornstein-Uhlenbeck sheet. Simulation results are also&#10;presented, where the driving random sheets are simulated with the help of their&#10;Karhunen-Lo\`eve expansions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="221" source="Gyula Pap" target="Kinga Sikolya">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="222" source="Tony Cai" target="Tiefeng Jiang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2925v1" />
          <attvalue for="2" value="Limiting Laws of Coherence of Random Matrices with Applications to&#10;  Testing Covariance Structure and Construction of Compressed Sensing Matrices" />
          <attvalue for="3" value="Testing covariance structure is of significant interest in many areas of&#10;statistical analysis and construction of compressed sensing matrices is an&#10;important problem in signal processing. Motivated by these applications, we&#10;study in this paper the limiting laws of the coherence of an $n\times p$ random&#10;matrix in the high-dimensional setting where $p$ can be much larger than $n$.&#10;Both the law of large numbers and the limiting distribution are derived. We&#10;then consider testing the bandedness of the covariance matrix of a high&#10;dimensional Gaussian distribution which includes testing for independence as a&#10;special case. The limiting laws of the coherence of the data matrix play a&#10;critical role in the construction of the test. We also apply the asymptotic&#10;results to the construction of compressed sensing matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="223" source="Weidong Liu" target="Shiqing Ling">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2802v1" />
          <attvalue for="2" value="On non-stationary threshold autoregressive models" />
          <attvalue for="3" value="In this paper we study the limiting distributions of the least-squares&#10;estimators for the non-stationary first-order threshold autoregressive (TAR(1))&#10;model. It is proved that the limiting behaviors of the TAR(1) process are very&#10;different from those of the classical unit root model and the explosive AR(1)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="224" source="Weidong Liu" target="Qi-Man Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2802v1" />
          <attvalue for="2" value="On non-stationary threshold autoregressive models" />
          <attvalue for="3" value="In this paper we study the limiting distributions of the least-squares&#10;estimators for the non-stationary first-order threshold autoregressive (TAR(1))&#10;model. It is proved that the limiting behaviors of the TAR(1) process are very&#10;different from those of the classical unit root model and the explosive AR(1)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="225" source="Shiqing Ling" target="Qi-Man Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2802v1" />
          <attvalue for="2" value="On non-stationary threshold autoregressive models" />
          <attvalue for="3" value="In this paper we study the limiting distributions of the least-squares&#10;estimators for the non-stationary first-order threshold autoregressive (TAR(1))&#10;model. It is proved that the limiting behaviors of the TAR(1) process are very&#10;different from those of the classical unit root model and the explosive AR(1)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="226" source="Zhengyan Lin" target="Hanchao Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3402v3" />
          <attvalue for="2" value="Weak Convergence to Stochastic Integrals Driven by $α-$Stable&#10;  Lévy Processes" />
          <attvalue for="3" value="We use the martingale convergence method to get the weak convergence theorem&#10;on general functionals of partial sums of independent heavy-tailed random&#10;variables. The limiting process is the stochastic integral driven by&#10;$\alpha-$stable L\'evy process. Our method is very powerful to obtain the limit&#10;behavior of heavy-tailed random variables." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="227" source="Alexander Aue" target="Lajos Horváth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="228" source="Alexander Aue" target="Clifford M. Hurvich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="229" source="Lajos Horváth" target="Clifford M. Hurvich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.0841v2" />
          <attvalue for="2" value="Limit Laws in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We consider pure-jump transaction-level models for asset prices in continuous&#10;time, driven by point processes. In a bivariate model that admits&#10;cointegration, we allow for time deformations to account for such effects as&#10;intraday seasonal patterns in volatility, and non-trading periods that may be&#10;different for the two assets. We also allow for asymmetries (leverage effects).&#10;We obtain the asymptotic distribution of the log-price process. We also obtain&#10;the asymptotic distribution of the ordinary least-squares estimator of the&#10;cointegrating parameter based on data sampled from an equally-spaced&#10;discretization of calendar time, in the case of weak fractional cointegration.&#10;For this same case, we obtain the asymptotic distribution for a tapered&#10;estimator under more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="230" source="Céline Duval" target="Marc Hoffmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1031v1" />
          <attvalue for="2" value="Statistical inference across time scales" />
          <attvalue for="3" value="We investigate statistical inference across time scales. We take as toy model&#10;the estimation of the intensity of a discretely observed compound Poisson&#10;process with symmetric Bernoulli jumps. We have data at different time scales:&#10;microscopic, intermediate and macroscopic. We quantify the smooth statistical&#10;transition from a microscopic Poissonian regime to a macroscopic Gaussian&#10;regime. The classical quadratic variation estimator is efficient in both&#10;microscopic and macroscopic scales but surprisingly shows a substantial loss of&#10;information in the intermediate scale that can be explicitly related to the&#10;sampling rate. We discuss the implications of these findings beyond this&#10;idealised framework." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="231" source="Shan Luo" target="Zehua Chen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2815v1" />
          <attvalue for="2" value="Selection Consistency of EBIC for GLIM with Non-canonical Links and&#10;  Diverging Number of Parameters" />
          <attvalue for="3" value="In this article, we investigate the properties of the EBIC in variable&#10;selection for generalized linear models with non-canonical links and diverging&#10;number of parameters in ultra-high dimensional feature space. The selection&#10;consistency of the EBIC in this situation is established under moderate&#10;conditions. The finite sample performance of the EBIC coupled with a forward&#10;selection procedure is demonstrated through simulation studies and a real data&#10;analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="232" source="J. E. Chacón" target="J. Montanero">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4542v1" />
          <attvalue for="2" value="A note on kernel density estimation at a parametric rate" />
          <attvalue for="3" value="In the context of kernel density estimation, we give a characterization of&#10;the kernels for which the parametric mean integrated squared error rate&#10;$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases&#10;where this rate is attainable, we give an asymptotic bandwidth choice that&#10;makes the kernel estimator consistent in mean integrated squared error at that&#10;rate and a numerical example showing the superior performance of the&#10;superkernel estimator when the bandwidth is properly chosen." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="233" source="J. E. Chacón" target="A. G. Nogales">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4542v1" />
          <attvalue for="2" value="A note on kernel density estimation at a parametric rate" />
          <attvalue for="3" value="In the context of kernel density estimation, we give a characterization of&#10;the kernels for which the parametric mean integrated squared error rate&#10;$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases&#10;where this rate is attainable, we give an asymptotic bandwidth choice that&#10;makes the kernel estimator consistent in mean integrated squared error at that&#10;rate and a numerical example showing the superior performance of the&#10;superkernel estimator when the bandwidth is properly chosen." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="234" source="J. Montanero" target="A. G. Nogales">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4542v1" />
          <attvalue for="2" value="A note on kernel density estimation at a parametric rate" />
          <attvalue for="3" value="In the context of kernel density estimation, we give a characterization of&#10;the kernels for which the parametric mean integrated squared error rate&#10;$n^{-1}$ may be obtained, where $n$ is the sample size. Also, for the cases&#10;where this rate is attainable, we give an asymptotic bandwidth choice that&#10;makes the kernel estimator consistent in mean integrated squared error at that&#10;rate and a numerical example showing the superior performance of the&#10;superkernel estimator when the bandwidth is properly chosen." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="235" source="Fasheng Sun" target="Dennis K. J. Lin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3816v1" />
          <attvalue for="2" value="On construction of optimal mixed-level supersaturated designs" />
          <attvalue for="3" value="Supersaturated design (SSD) has received much recent interest because of its&#10;potential in factor screening experiments. In this paper, we provide equivalent&#10;conditions for two columns to be fully aliased and consequently propose methods&#10;for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs&#10;without fully aliased columns, via equidistant designs and difference matrices.&#10;The methods can be easily performed and many new optimal mixed-level SSDs have&#10;been obtained. Furthermore, it is proved that the nonorthogonality between&#10;columns of the resulting design is well controlled by the source designs. A&#10;rather complete list of newly generated optimal mixed-level SSDs are tabulated&#10;for practical use." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="236" source="Fasheng Sun" target="Min-Qian Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3816v1" />
          <attvalue for="2" value="On construction of optimal mixed-level supersaturated designs" />
          <attvalue for="3" value="Supersaturated design (SSD) has received much recent interest because of its&#10;potential in factor screening experiments. In this paper, we provide equivalent&#10;conditions for two columns to be fully aliased and consequently propose methods&#10;for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs&#10;without fully aliased columns, via equidistant designs and difference matrices.&#10;The methods can be easily performed and many new optimal mixed-level SSDs have&#10;been obtained. Furthermore, it is proved that the nonorthogonality between&#10;columns of the resulting design is well controlled by the source designs. A&#10;rather complete list of newly generated optimal mixed-level SSDs are tabulated&#10;for practical use." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="237" source="Dennis K. J. Lin" target="Min-Qian Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3816v1" />
          <attvalue for="2" value="On construction of optimal mixed-level supersaturated designs" />
          <attvalue for="3" value="Supersaturated design (SSD) has received much recent interest because of its&#10;potential in factor screening experiments. In this paper, we provide equivalent&#10;conditions for two columns to be fully aliased and consequently propose methods&#10;for constructing $E(f_{\mathrm{NOD}})$- and $\chi^2$-optimal mixed-level SSDs&#10;without fully aliased columns, via equidistant designs and difference matrices.&#10;The methods can be easily performed and many new optimal mixed-level SSDs have&#10;been obtained. Furthermore, it is proved that the nonorthogonality between&#10;columns of the resulting design is well controlled by the source designs. A&#10;rather complete list of newly generated optimal mixed-level SSDs are tabulated&#10;for practical use." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="238" source="Piotr Graczyk" target="Ishi Hideyuki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0147v1" />
          <attvalue for="2" value="Riesz measures and Wishart laws associated to quadratic maps" />
          <attvalue for="3" value="We introduce a natural definition of Riesz measures and Wishart laws&#10;associated to an $\Omega$-positive (virtual) quadratic map, where $\Omega&#10;\subset \real^n$ is a regular open convex cone. We give a general formula for&#10;moments of the Wishart laws. Moreover, if the quadratic map has an equivariance&#10;property under the action of a linear group acting on the cone $\Omega$&#10;transitively, then the associated Riesz measure and Wishart law are described&#10;explicitly by making use of theory of relatively invariant distributions on&#10;homogeneous cones." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="239" source="Neal Madras" target="Deniz Sezer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5245v1" />
          <attvalue for="2" value="Quantitative bounds for Markov chain convergence: Wasserstein and total&#10;  variation distances" />
          <attvalue for="3" value="We present a framework for obtaining explicit bounds on the rate of&#10;convergence to equilibrium of a Markov chain on a general state space, with&#10;respect to both total variation and Wasserstein distances. For Wasserstein&#10;bounds, our main tool is Steinsaltz's convergence theorem for locally&#10;contractive random dynamical systems. We describe practical methods for finding&#10;Steinsaltz's &quot;drift functions&quot; that prove local contractivity. We then use the&#10;idea of &quot;one-shot coupling&quot; to derive criteria that give bounds for total&#10;variation distances in terms of Wasserstein distances. Our methods are applied&#10;to two examples: a two-component Gibbs sampler for the Normal distribution and&#10;a random logistic dynamical system." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="240" source="Markus Bibinger" target="Markus Reiß">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.0939v3" />
          <attvalue for="2" value="Spectral covolatility estimation from noisy observations using local&#10;  weights" />
          <attvalue for="3" value="We propose localized spectral estimators for the quadratic covariation and&#10;the spot covolatility of diffusion processes which are observed discretely with&#10;additive observation noise. The eligibility of this approach to lead to an&#10;appropriate estimation for time-varying volatilities stems from an asymptotic&#10;equivalence of the underlying statistical model to a white noise model with&#10;correlation and volatility processes being constant over small intervals. The&#10;asymptotic equivalence of the continuous-time and the discrete-time experiments&#10;are proved by a construction with linear interpolation in one direction and&#10;local means for the other. The new estimator outperforms earlier nonparametric&#10;approaches in the considered model. We investigate its finite sample size&#10;characteristics in simulations and draw a comparison between the various&#10;proposed methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="241" source="Markus Reiß" target="Alexander Meister">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.5248v1" />
          <attvalue for="2" value="Asymptotic Equivalence for Nonparametric Regression with Non-Regular&#10;  Errors" />
          <attvalue for="3" value="Asymptotic equivalence in Le Cam's sense for nonparametric regression&#10;experiments is extended to the case of non-regular error densities, which have&#10;jump discontinuities at their endpoints. We prove asymptotic equivalence of&#10;such regression models and the observation of two independent Poisson point&#10;processes which contain the target curve as the support boundary of its&#10;intensity function. The intensity of the point processes is of order of the&#10;sample size $n$ and involves the jump sizes as well as the design density. The&#10;statistical model significantly differs from regression problems with Gaussian&#10;or regular errors, which are known to be asymptotically equivalent to Gaussian&#10;white noise models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="242" source="Tatiane F. N. Melo" target="Silvia L. P. Ferrari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1098v1" />
          <attvalue for="2" value="Adjusted likelihood inference in an elliptical multivariate&#10;  errors-in-variables model" />
          <attvalue for="3" value="In this paper we obtain an adjusted version of the likelihood ratio test for&#10;errors-in-variables multivariate linear regression models. The error terms are&#10;allowed to follow a multivariate distribution in the class of the elliptical&#10;distributions, which has the multivariate normal distribution as a special&#10;case. We derive a modified likelihood ratio statistic that follows a&#10;chi-squared distribution with a high degree of accuracy. Our results generalize&#10;those in Melo and Ferrari(Advances in Statistical Analysis, 2010, 94, 75-87) by&#10;allowing the parameter of interest to be vector-valued in the multivariate&#10;errors-in-variables model. We report a simulation study which shows that the&#10;proposed test displays superior finite sample behavior relative to the standard&#10;likelihood ratio test." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="243" source="Tatiane F. N. Melo" target="Artur J. Lemonte">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="244" source="Silvia L. P. Ferrari" target="Artur J. Lemonte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4371v1" />
          <attvalue for="2" value="Local power of the LR, Wald, score and gradient tests in dispersion&#10;  models" />
          <attvalue for="3" value="We derive asymptotic expansions up to order $n^{-1/2}$ for the nonnull&#10;distribution functions of the likelihood ratio, Wald, score and gradient test&#10;statistics in the class of dispersion models, under a sequence of Pitman&#10;alternatives. The asymptotic distributions of these statistics are obtained for&#10;testing a subset of regression parameters and for testing the precision&#10;parameter. Based on these nonnull asymptotic expansions it is shown that there&#10;is no uniform superiority of one test with respect to the others for testing a&#10;subset of regression parameters. Furthermore, in order to compare the&#10;finite-sample performance of these tests in this class of models, Monte Carlo&#10;simulations are presented. An empirical application to a real data set is&#10;considered for illustrative purposes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="245" source="Cari Kaufman" target="Benjamin Shaby">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1851v2" />
          <attvalue for="2" value="The Role of the Range Parameter for Estimation and Prediction in&#10;  Geostatistics" />
          <attvalue for="3" value="Two canonical problems in geostatistics are estimating the parameters in a&#10;specified family of stochastic process models and predicting the process at new&#10;locations. A number of asymptotic results addressing these problems over a&#10;fixed spatial domain indicate that, for a Gaussian process with Mat\'ern&#10;covariance function, one can fix the range parameter controlling the rate of&#10;decay of the process and obtain results that are asymptotically equivalent to&#10;the case that the range parameter is known. In this paper we show that the same&#10;asymptotic results can be obtained by jointly estimating both the range and the&#10;variance of the process using maximum likelihood or maximum tapered likelihood.&#10;Moreover, we show that intuition and approximations derived from asymptotic&#10;arguments using a fixed range parameter can be problematic when applied to&#10;finite samples, even for moderate to large sample sizes. In contrast, we show&#10;via simulation that performance on a variety of metrics is improved and&#10;asymptotic approximations are applicable for smaller sample sizes when the&#10;range and variance parameters are jointly estimated. These effects are&#10;particularly apparent when the process is mean square differentiable or the&#10;effective range of spatial correlation is small." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="246" source="Parikshit Shah" target="Venkat Chandrasekaran">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7061v1" />
          <attvalue for="2" value="Group Symmetry and Covariance Regularization" />
          <attvalue for="3" value="Statistical models that possess symmetry arise in diverse settings such as&#10;random fields associated to geophysical phenomena, exchangeable processes in&#10;Bayesian statistics, and cyclostationary processes in engineering. We formalize&#10;the notion of a symmetric model via group invariance. We propose projection&#10;onto a group fixed point subspace as a fundamental way of regularizing&#10;covariance matrices in the high-dimensional regime. In terms of parameters&#10;associated to the group we derive precise rates of convergence of the&#10;regularized covariance matrix and demonstrate that significant statistical&#10;gains may be expected in terms of the sample complexity. We further explore the&#10;consequences of symmetry on related model-selection problems such as the&#10;learning of sparse covariance and inverse covariance matrices. We also verify&#10;our results with simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="247" source="Karim Lounici" target="Vladimir Spokoiny">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="248" source="Karim Lounici" target="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="249" source="Karim Lounici" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="250" source="Junlong Zhao" target="Chenlei Leng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3755v2" />
          <attvalue for="2" value="New Error Analysis for Lasso" />
          <attvalue for="3" value="The Lasso is one of the most important approaches for parameter estimation&#10;and variable selection in high dimensional linear regression. At the heart of&#10;its success is the attractive rate of convergence result even when $p$, the&#10;dimension of the problem, is much larger than the sample size $n$. In&#10;particular, Bickel et al. (2009) showed that this rate, in terms of the&#10;$\ell_1$ norm, is of the order $s\sqrt{(\log p)/n}$ for a sparsity index $s$.&#10;In this paper, we obtain a new bound on the convergence rate by taking&#10;advantage of the distributional information of the model. Under the normality&#10;or sub-Gaussian assumption, the rate can be improved to nearly $s/\sqrt{n}$ for&#10;certain design matrices. We further outline a general partitioning technique&#10;that helps to derive sharper convergence rate for the Lasso. The result is&#10;applicable to many covariance matrices suitable for high-dimensional data&#10;analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="251" source="Marianne Clausel" target="François Roueff">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="252" source="Marianne Clausel" target="Murad S. Taqqu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="253" source="Marianne Clausel" target="Ciprian A. Tudor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="254" source="François Roueff" target="Murad S. Taqqu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="255" source="François Roueff" target="Ciprian A. Tudor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="256" source="Murad S. Taqqu" target="Ciprian A. Tudor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1011v3" />
          <attvalue for="2" value="Wavelet estimation of the long memory parameter for Hermite polynomial&#10;  of Gaussian processes" />
          <attvalue for="3" value="We consider stationary processes with long memory which are non-Gaussian and&#10;represented as Hermite polynomials of a Gaussian process. We focus on the&#10;corresponding wavelet coefficients and study the asymptotic behavior of the sum&#10;of their squares since this sum is often used for estimating the long-memory&#10;parameter. We show that the limit is not Gaussian but can be expressed using&#10;the non-Gaussian Rosenblatt process defined as a Wiener It\^o integral of order&#10;2. This happens even if the original process is defined through a Hermite&#10;polynomial of order higher than 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="257" source="Masoumeh Dashti" target="Stephen Harris">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0889v2" />
          <attvalue for="2" value="Besov priors for Bayesian inverse problems" />
          <attvalue for="3" value="We consider the inverse problem of estimating a function $u$ from noisy,&#10;possibly nonlinear, observations. We adopt a Bayesian approach to the problem.&#10;This approach has a long history for inversion, dating back to 1970, and has,&#10;over the last decade, gained importance as a practical tool. However most of&#10;the existing theory has been developed for Gaussian prior measures. Recently&#10;Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct&#10;Besov prior measures, based on wavelet expansions with random coefficients, and&#10;used these prior measures to study linear inverse problems. In this paper we&#10;build on this development of Besov priors to include the case of nonlinear&#10;measurements. In doing so a key technical tool, established here, is a&#10;Fernique-like theorem for Besov measures. This theorem enables us to identify&#10;appropriate conditions on the forward solution operator which, when matched to&#10;properties of the prior Besov measure, imply the well-definedness and&#10;well-posedness of the posterior measure. We then consider the application of&#10;these results to the inverse problem of finding the diffusion coefficient of an&#10;elliptic partial differential equation, given noisy measurements of its&#10;solution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="258" source="Masoumeh Dashti" target="Andrew Stuart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0889v2" />
          <attvalue for="2" value="Besov priors for Bayesian inverse problems" />
          <attvalue for="3" value="We consider the inverse problem of estimating a function $u$ from noisy,&#10;possibly nonlinear, observations. We adopt a Bayesian approach to the problem.&#10;This approach has a long history for inversion, dating back to 1970, and has,&#10;over the last decade, gained importance as a practical tool. However most of&#10;the existing theory has been developed for Gaussian prior measures. Recently&#10;Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct&#10;Besov prior measures, based on wavelet expansions with random coefficients, and&#10;used these prior measures to study linear inverse problems. In this paper we&#10;build on this development of Besov priors to include the case of nonlinear&#10;measurements. In doing so a key technical tool, established here, is a&#10;Fernique-like theorem for Besov measures. This theorem enables us to identify&#10;appropriate conditions on the forward solution operator which, when matched to&#10;properties of the prior Besov measure, imply the well-definedness and&#10;well-posedness of the posterior measure. We then consider the application of&#10;these results to the inverse problem of finding the diffusion coefficient of an&#10;elliptic partial differential equation, given noisy measurements of its&#10;solution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="259" source="Masoumeh Dashti" target="Andrew M. Stuart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0143v1" />
          <attvalue for="2" value="Uncertainty quantification and weak approximation of an elliptic inverse&#10;  problem" />
          <attvalue for="3" value="We consider the inverse problem of determining the permeability from the&#10;pressure in a Darcy model of flow in a porous medium. Mathematically the&#10;problem is to find the diffusion coefficient for a linear uniformly elliptic&#10;partial differential equation in divergence form, in a bounded domain in&#10;dimension $d \le 3$, from measurements of the solution in the interior. We&#10;adopt a Bayesian approach to the problem. We place a prior random field measure&#10;on the log permeability, specified through the Karhunen-Lo\`eve expansion of&#10;its draws. We consider Gaussian measures constructed this way, and study the&#10;regularity of functions drawn from them. We also study the Lipschitz properties&#10;of the observation operator mapping the log permeability to the observations.&#10;Combining these regularity and continuity estimates, we show that the posterior&#10;measure is well-defined on a suitable Banach space. Furthermore the posterior&#10;measure is shown to be Lipschitz with respect to the data in the Hellinger&#10;metric, giving rise to a form of well-posedness of the inverse problem.&#10;Determining the posterior measure, given the data, solves the problem of&#10;uncertainty quantification for this inverse problem. In practice the posterior&#10;measure must be approximated in a finite dimensional space. We quantify the&#10;errors incurred by employing a truncated Karhunen-Lo\`eve expansion to&#10;represent this meausure. In particular we study weak convergence of a general&#10;class of locally Lipschitz functions of the log permeability, and apply this&#10;general theory to estimate errors in the posterior mean of the pressure and the&#10;pressure covariance, under refinement of the finite dimensional&#10;Karhunen-Lo\`eve truncation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="260" source="Stephen Harris" target="Andrew Stuart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0889v2" />
          <attvalue for="2" value="Besov priors for Bayesian inverse problems" />
          <attvalue for="3" value="We consider the inverse problem of estimating a function $u$ from noisy,&#10;possibly nonlinear, observations. We adopt a Bayesian approach to the problem.&#10;This approach has a long history for inversion, dating back to 1970, and has,&#10;over the last decade, gained importance as a practical tool. However most of&#10;the existing theory has been developed for Gaussian prior measures. Recently&#10;Lassas, Saksman and Siltanen (Inv. Prob. Imag. 2009) showed how to construct&#10;Besov prior measures, based on wavelet expansions with random coefficients, and&#10;used these prior measures to study linear inverse problems. In this paper we&#10;build on this development of Besov priors to include the case of nonlinear&#10;measurements. In doing so a key technical tool, established here, is a&#10;Fernique-like theorem for Besov measures. This theorem enables us to identify&#10;appropriate conditions on the forward solution operator which, when matched to&#10;properties of the prior Besov measure, imply the well-definedness and&#10;well-posedness of the posterior measure. We then consider the application of&#10;these results to the inverse problem of finding the diffusion coefficient of an&#10;elliptic partial differential equation, given noisy measurements of its&#10;solution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="261" source="Gersende Fort" target="Eric Moulines">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="262" source="Gersende Fort" target="Pierre Priouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="263" source="Gersende Fort" target="Sylvain Le Corff">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.3968v3" />
          <attvalue for="2" value="Online Expectation Maximization based algorithms for inference in hidden&#10;  Markov models" />
          <attvalue for="3" value="The Expectation Maximization (EM) algorithm is a versatile tool for model&#10;parameter estimation in latent data models. When processing large data sets or&#10;data stream however, EM becomes intractable since it requires the whole data&#10;set to be available at each iteration of the algorithm. In this contribution, a&#10;new generic online EM algorithm for model parameter inference in general Hidden&#10;Markov Model is proposed. This new algorithm updates the parameter estimate&#10;after a block of observations is processed (online). The convergence of this&#10;new algorithm is established, and the rate of convergence is studied showing&#10;the impact of the block size. An averaging procedure is also proposed to&#10;improve the rate of convergence. Finally, practical illustrations are presented&#10;to highlight the performance of these algorithms in comparison to other online&#10;maximum likelihood procedures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="264" source="Eric Moulines" target="Pierre Priouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2576v1" />
          <attvalue for="2" value="A simple variance inequality for U-statistics of a Markov chain with&#10;  applications" />
          <attvalue for="3" value="We establish a simple variance inequality for U-statistics whose underlying&#10;sequence of random variables is an ergodic Markov Chain. The constants in this&#10;inequality are explicit and depend on computable bounds on the mixing rate of&#10;the Markov Chain. We apply this result to derive the strong law of large number&#10;for U-statistics of a Markov Chain under conditions which are close from being&#10;optimal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="265" source="Eric Moulines" target="Sylvain Le Corff">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="266" source="Noureddine El Karoui" target="Holger Koesters">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1404v1" />
          <attvalue for="2" value="Geometric sensitivity of random matrix results: consequences for&#10;  shrinkage estimators of covariance and related statistical methods" />
          <attvalue for="3" value="Shrinkage estimators of covariance are an important tool in modern applied&#10;and theoretical statistics. They play a key role in regularized estimation&#10;problems, such as ridge regression (aka Tykhonov regularization), regularized&#10;discriminant analysis and a variety of optimization problems.&#10;  In this paper, we bring to bear the tools of random matrix theory to&#10;understand their behavior, and in particular, that of quadratic forms involving&#10;inverses of those estimators, which are important in practice.&#10;  We use very mild assumptions compared to the usual assumptions made in random&#10;matrix theory, requiring only mild conditions on the moments of linear and&#10;quadratic forms in our random vectors. In particular, we show that our results&#10;apply for instance to log-normal data, which are of interest in financial&#10;applications.&#10;  Our study highlights the relative sensitivity of random matrix results (and&#10;their practical consequences) to geometric assumptions which are often&#10;implicitly made by random matrix theorists and may not be relevant in data&#10;analytic practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="267" source="François Portier" target="Bernard Delyon">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1056v1" />
          <attvalue for="2" value="Test function: A new approach for covering the central subspace" />
          <attvalue for="3" value="In this paper we offer a complete methodology for sufficient dimension&#10;reduction called the test function (TF). TF provides a new family of methods&#10;for the estimation of the central subspace (CS) based on the introduction of a&#10;nonlinear transformation of the response. Theoretical background of TF is&#10;developed under weaker conditions than the existing methods. By considering&#10;order 1 and 2 conditional moments of the predictor given the response, we&#10;divide TF in two classes. In each class we provide conditions that guarantee an&#10;exhaustive estimation of the CS. Besides, the optimal members are calculated&#10;via the minimization of the asymptotic mean squared error deriving from the&#10;distance between the CS and its estimate. This leads us to two plug-in methods&#10;which are evaluated with several simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="268" source="Stephen E. Fienberg" target="Alessandro Rinaldo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3618v2" />
          <attvalue for="2" value="Maximum likelihood estimation in log-linear models" />
          <attvalue for="3" value="We study maximum likelihood estimation in log-linear models under conditional&#10;Poisson sampling schemes. We derive necessary and sufficient conditions for&#10;existence of the maximum likelihood estimator (MLE) of the model parameters and&#10;investigate estimability of the natural and mean-value parameters under a&#10;nonexistent MLE. Our conditions focus on the role of sampling zeros in the&#10;observed table. We situate our results within the framework of extended&#10;exponential families, and we exploit the geometric properties of log-linear&#10;models. We propose algorithms for extended maximum likelihood estimation that&#10;improve and correct the existing algorithms for log-linear model analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="269" source="Gerard Letac" target="Helene Massam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5381v3" />
          <attvalue for="2" value="Bayes factors and the geometry of discrete hierarchical loglinear models" />
          <attvalue for="3" value="A standard tool for model selection in a Bayesian framework is the Bayes&#10;factor which compares the marginal likelihood of the data under two given&#10;different models. In this paper, we consider the class of hierarchical&#10;loglinear models for discrete data given under the form of a contingency table&#10;with multinomial sampling. We assume that the Diaconis-Ylvisaker conjugate&#10;prior is the prior distribution on the loglinear parameters and the uniform is&#10;the prior distribution on the space of models. Under these conditions, the&#10;Bayes factor between two models is a function of their prior and posterior&#10;normalizing constants. These constants are functions of the hyperparameters&#10;$(m,\alpha)$ which can be interpreted respectively as marginal counts and the&#10;total count of a fictive contingency table.&#10;  We study the behaviour of the Bayes factor when $\alpha$ tends to zero. In&#10;this study two mathematical objects play a most important role. They are,&#10;first, the interior $C$ of the convex hull $\bar{C}$ of the support of the&#10;multinomial distribution for a given hierarchical loglinear model together with&#10;its faces and second, the characteristic function $\mathbb{J}_C$ of this convex&#10;set $C$.&#10;  We show that, when $\alpha$ tends to 0, if the data lies on a face $F_i$ of&#10;$\bar{C_i},i=1,2$ of dimension $k_i$, the Bayes factor behaves like&#10;$\alpha^{k_1-k_2}$. This implies in particular that when the data is in $C_1$&#10;and in $C_2$, i.e. when $k_i$ equals the dimension of model $J_i$, the sparser&#10;model is favored, thus confirming the idea of Bayesian regularization." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="270" source="Marian Hristache" target="Valentin Patilea">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6428v1" />
          <attvalue for="2" value="Semiparametric efficiency bounds for seemingly unrelated conditional&#10;  moment restrictions" />
          <attvalue for="3" value="This paper addresses the problem of semiparametric efficiency bounds for&#10;conditional moment restriction models with different conditioning variables. We&#10;characterize such an efficiency bound, that in general is not explicit, as a&#10;limit of explicit efficiency bounds for a decreasing sequence of unconditional&#10;(marginal) moment restriction models. An iterative procedure for approximating&#10;the efficient score when this is not explicit is provided. Our theoretical&#10;results complete and extend existing results in the literature, provide new&#10;insight for the theory of semiparametric efficiency bounds literature and open&#10;the door to new applications. In particular, we investigate a class of&#10;regression-like (mean regression, quantile regression,...) models with missing&#10;data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="271" source="Valentin Patilea" target="Olivier Lopez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6232v2" />
          <attvalue for="2" value="Single index regression models in the presence of censoring depending on&#10;  the covariates" />
          <attvalue for="3" value="Consider a random vector (X',Y)', where X is d-dimensional and Y is&#10;one-dimensional. We assume that Y is subject to random right censoring. The aim&#10;of this paper is twofold. First, we propose a new estimator of the joint&#10;distribution of (X',Y)'. This estimator overcomes the common&#10;curse-of-dimensionality problem, by using a new dimension reduction technique.&#10;Second, we assume that the relation between X and Y is given by a mean&#10;regression single index model, and propose a new estimator of the parameters in&#10;this model. The asymptotic properties of all proposed estimators are obtained." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="272" source="Valentin Patilea" target="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6232v2" />
          <attvalue for="2" value="Single index regression models in the presence of censoring depending on&#10;  the covariates" />
          <attvalue for="3" value="Consider a random vector (X',Y)', where X is d-dimensional and Y is&#10;one-dimensional. We assume that Y is subject to random right censoring. The aim&#10;of this paper is twofold. First, we propose a new estimator of the joint&#10;distribution of (X',Y)'. This estimator overcomes the common&#10;curse-of-dimensionality problem, by using a new dimension reduction technique.&#10;Second, we assume that the relation between X and Y is given by a mean&#10;regression single index model, and propose a new estimator of the parameters in&#10;this model. The asymptotic properties of all proposed estimators are obtained." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="273" source="Olga Klopp" target="Stéphane Gaiffas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3055v3" />
          <attvalue for="2" value="High dimensional matrix estimation with unknown variance of the noise" />
          <attvalue for="3" value="We propose a new pivotal method for estimating high-dimensional matrices.&#10;Assume that we observe a small set of entries or linear combinations of entries&#10;of an unknown matrix $A\_0$ corrupted by noise. We propose a new method for&#10;estimating $A\_0$ which does not rely on the knowledge or an estimation of the&#10;standard deviation of the noise $\sigma$. Our estimator achieves, up to a&#10;logarithmic factor, optimal rates of convergence under the Frobenius risk and,&#10;thus, has the same prediction performance as previously proposed estimators&#10;which rely on the knowledge of $\sigma$. Our method is based on the solution of&#10;a convex optimization problem which makes it computationally attractive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="274" source="Tomonari Sei" target="Kentaro Tanaka">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2408v1" />
          <attvalue for="2" value="Cones of elementary imsets and supermodular functions: a review and some&#10;  new results" />
          <attvalue for="3" value="In this paper we give a review of the method of imsets introduced by Studeny&#10;(2005) from a geometric point of view. Elementary imsets span a polyhedral cone&#10;and its dual cone is the cone of supermodular functions. We review basic facts&#10;on the structure of these cones. Then we derive some new results on the&#10;following topics: i) extreme rays of the cone of standardized supermodular&#10;functions, ii) faces of the cones, iii) small relations among elementary&#10;imsets, and iv) some computational results on Markov basis for the toric ideal&#10;defined by elementary imsets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="275" source="Ingo Steinwart" target="Andreas Christmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2101v1" />
          <attvalue for="2" value="Estimating conditional quantiles with the help of the pinball loss" />
          <attvalue for="3" value="The so-called pinball loss for estimating conditional quantiles is a&#10;well-known tool in both statistics and machine learning. So far, however, only&#10;little work has been done to quantify the efficiency of this tool for&#10;nonparametric approaches. We fill this gap by establishing inequalities that&#10;describe how close approximate pinball risk minimizers are to the corresponding&#10;conditional quantile. These inequalities, which hold under mild assumptions on&#10;the data-generating distribution, are then used to establish so-called variance&#10;bounds, which recently turned out to play an important role in the statistical&#10;analysis of (regularized) empirical risk minimization approaches. Finally, we&#10;use both types of inequalities to establish an oracle inequality for support&#10;vector machines that use the pinball loss. The resulting learning rates are&#10;min--max optimal under some standard regularity assumptions on the conditional&#10;quantile." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="276" source="Christian Genest" target="Ivan Kojadinovic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="277" source="Christian Genest" target="Johanna Nešlehová">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="278" source="Christian Genest" target="Jun Yan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="279" source="Ivan Kojadinovic" target="Johanna Nešlehová">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="280" source="Ivan Kojadinovic" target="Jun Yan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="281" source="Johanna Nešlehová" target="Jun Yan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2078v1" />
          <attvalue for="2" value="A goodness-of-fit test for bivariate extreme-value copulas" />
          <attvalue for="3" value="It is often reasonable to assume that the dependence structure of a bivariate&#10;continuous distribution belongs to the class of extreme-value copulas. The&#10;latter are characterized by their Pickands dependence function. In this paper,&#10;a procedure is proposed for testing whether this function belongs to a given&#10;parametric family. The test is based on a Cram\'{e}r--von Mises statistic&#10;measuring the distance between an estimate of the parametric Pickands&#10;dependence function and either one of two nonparametric estimators thereof&#10;studied by Genest and Segers [Ann. Statist. 37 (2009) 2990--3022]. As the&#10;limiting distribution of the test statistic depends on unknown parameters, it&#10;must be estimated via a parametric bootstrap procedure, the validity of which&#10;is established. Monte Carlo simulations are used to assess the power of the&#10;test and an extension to dependence structures that are left-tail decreasing in&#10;both variables is considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="282" source="Cecília Fonseca" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="283" source="Cecília Fonseca" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="284" source="Luísa Pereira" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.2637v2" />
          <attvalue for="2" value="Generalized madogram and pairwise dependence of maxima over two regions&#10;  of a random field" />
          <attvalue for="3" value="Spatial environmental processes often exhibit dependence in their large&#10;values. In order to model such processes their dependence properties must be&#10;characterized and quantified. In this paper we introduce a measure that&#10;evaluates the dependence among extreme observations located in two separated&#10;regions of locations of R^2. We compute the range of this new dependence&#10;measure, which extends the existing {\lambda}-madogram concept, and compare it&#10;with extremal coefficients, finding generalizations of the known relations in&#10;pairwise approach. Estimators for this measure are introduced and asymptotic&#10;normality and strong consistency are shown. An application to the annual maxima&#10;precipitation in Portuguese regions is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="285" source="Pierre Alquier" target="Paul Doukhan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1615v5" />
          <attvalue for="2" value="Sparsity considerations for dependent observations" />
          <attvalue for="3" value="The aim of this paper is to provide a comprehensive introduction for the&#10;study of L1-penalized estimators in the context of dependent observations. We&#10;define a general $\ell_{1}$-penalized estimator for solving problems of&#10;stochastic optimization. This estimator turns out to be the LASSO in the&#10;regression estimation setting. Powerful theoretical guarantees on the&#10;statistical performances of the LASSO were provided in recent papers, however,&#10;they usually only deal with the iid case. Here, we study our estimator under&#10;various dependence assumptions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="286" source="Pierre Alquier" target="Gérard Biau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3229v2" />
          <attvalue for="2" value="Sparse single-index model" />
          <attvalue for="3" value="Let $(\bX, Y)$ be a random pair taking values in $\mathbb R^p \times \mathbb&#10;R$. In the so-called single-index model, one has $Y=f^{\star}(\theta^{\star&#10;T}\bX)+\bW$, where $f^{\star}$ is an unknown univariate measurable function,&#10;$\theta^{\star}$ is an unknown vector in $\mathbb R^d$, and $W$ denotes a&#10;random noise satisfying $\mathbb E[\bW|\bX]=0$. The single-index model is known&#10;to offer a flexible way to model a variety of high-dimensional real-world&#10;phenomena. However, despite its relative simplicity, this dimension reduction&#10;scheme is faced with severe complications as soon as the underlying dimension&#10;becomes larger than the number of observations (&quot;$p$ larger than $n$&quot;&#10;paradigm). To circumvent this difficulty, we consider the single-index model&#10;estimation problem from a sparsity perspective using a PAC-Bayesian approach.&#10;On the theoretical side, we offer a sharp oracle inequality, which is more&#10;powerful than the best known oracle inequalities for other common procedures of&#10;single-index recovery. The proposed method is implemented by means of the&#10;reversible jump Markov chain Monte Carlo technique and its performance is&#10;compared with that of standard procedures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="287" source="Paul Doukhan" target="Gérard Biau">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="288" source="Jérôme Dedecker" target="Adeline Samson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1310v2" />
          <attvalue for="2" value="Estimation in autoregressive model with measurement error" />
          <attvalue for="3" value="Consider an autoregressive model with measurement error: we observe&#10;$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation&#10;$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is&#10;known up to a finite dimensional parameter $\theta^0$. The distributions of&#10;$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is&#10;completely known. We want to estimate the parameter $\theta^0$ by using the&#10;observations $Z_0,..,Z_n$. We propose an estimation procedure based on a&#10;modified least square criterion involving a weight function $w$, to be suitably&#10;chosen. We give upper bounds for the risk of the estimator, which depend on the&#10;smoothness of the errors density $f_\epsilon$ and on the smoothness properties&#10;of $w f_\theta$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="289" source="Jérôme Dedecker" target="Marie-Luce Taupin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1310v2" />
          <attvalue for="2" value="Estimation in autoregressive model with measurement error" />
          <attvalue for="3" value="Consider an autoregressive model with measurement error: we observe&#10;$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation&#10;$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is&#10;known up to a finite dimensional parameter $\theta^0$. The distributions of&#10;$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is&#10;completely known. We want to estimate the parameter $\theta^0$ by using the&#10;observations $Z_0,..,Z_n$. We propose an estimation procedure based on a&#10;modified least square criterion involving a weight function $w$, to be suitably&#10;chosen. We give upper bounds for the risk of the estimator, which depend on the&#10;smoothness of the errors density $f_\epsilon$ and on the smoothness properties&#10;of $w f_\theta$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="290" source="Adeline Samson" target="Marie-Luce Taupin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1310v2" />
          <attvalue for="2" value="Estimation in autoregressive model with measurement error" />
          <attvalue for="3" value="Consider an autoregressive model with measurement error: we observe&#10;$Z_i=X_i+\epsilon_i$, where $X_i$ is a stationary solution of the equation&#10;$X_i=f_{\theta^0}(X_{i-1})+\xi_i$. The regression function $f_{\theta^0}$ is&#10;known up to a finite dimensional parameter $\theta^0$. The distributions of&#10;$X_0$ and $\xi_1$ are unknown whereas the distribution of $\epsilon_1$ is&#10;completely known. We want to estimate the parameter $\theta^0$ by using the&#10;observations $Z_0,..,Z_n$. We propose an estimation procedure based on a&#10;modified least square criterion involving a weight function $w$, to be suitably&#10;chosen. We give upper bounds for the risk of the estimator, which depend on the&#10;smoothness of the errors density $f_\epsilon$ and on the smoothness properties&#10;of $w f_\theta$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="291" source="Takumi Saegusa" target="Jon A. Wellner">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4951v3" />
          <attvalue for="2" value="Weighted likelihood estimation under two-phase sampling" />
          <attvalue for="3" value="We develop asymptotic theory for weighted likelihood estimators (WLE) under&#10;two-phase stratified sampling without replacement. We also consider several&#10;variants of WLEs involving estimated weights and calibration. A set of&#10;empirical process tools are developed including a Glivenko-Cantelli theorem, a&#10;theorem for rates of convergence of M-estimators, and a Donsker theorem for the&#10;inverse probability weighted empirical processes under two-phase sampling and&#10;sampling without replacement at the second phase. Using these general results,&#10;we derive asymptotic distributions of the WLE of a finite-dimensional parameter&#10;in a general semiparametric model where an estimator of a nuisance parameter is&#10;estimable either at regular or nonregular rates. We illustrate these results&#10;and methods in the Cox model with right censoring and interval censoring. We&#10;compare the methods via their asymptotic variances under both sampling without&#10;replacement and the more usual (and easier to analyze) assumption of Bernoulli&#10;sampling at the second phase." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="292" source="Olivier Wintenberger" target="Sixiang Cai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4983v3" />
          <attvalue for="2" value="Parametric inference and forecasting in continuously invertible&#10;  volatility models" />
          <attvalue for="3" value="We introduce the notion of continuously invertible volatility models that&#10;relies on some Lyapunov condition and some regularity condition. We show that&#10;it is almost equivalent to the ability of the volatilities forecasting using&#10;the parametric inference approach based on the SRE given in [16]. Under very&#10;weak assumptions, we prove the strong consistency and the asymptotic normality&#10;of the parametric inference. Based on this parametric estimation, a natural&#10;strongly consistent forecast of the volatility is given. We apply successfully&#10;this approach to recover known results on univariate and multivariate GARCH&#10;type models and to the EGARCH(1,1) model. We prove the strong consistency of&#10;the forecasting as soon as the model is invertible and the asymptotic normality&#10;of the parametric inference as soon as the limiting variance exists. Finally,&#10;we give some encouraging empirical results of our approach on simulations and&#10;real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="293" source="Apostolos Batsidis" target="Nirian Martín">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="294" source="Apostolos Batsidis" target="Leandro Pardo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="295" source="Apostolos Batsidis" target="Konstantinos Zografos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="296" source="Nirian Martín" target="Leandro Pardo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="297" source="Nirian Martín" target="Konstantinos Zografos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="298" source="Leandro Pardo" target="Konstantinos Zografos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0864v1" />
          <attvalue for="2" value="A procedure for the change point problem in parametric models based on&#10;  phi-divergence test-statistics" />
          <attvalue for="3" value="This paper studies the change point problem for a general parametric,&#10;univariate or multivariate family of distributions. An information theoretic&#10;procedure is developed which is based on general divergence measures for&#10;testing the hypothesis of the existence of a change. For comparing the accuracy&#10;of the new test-statistic a simulation study is performed for the special case&#10;of a univariate discrete model. Finally, the procedure proposed in this paper&#10;is illustrated through a classical change-point example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="299" source="Bo Kai" target="Runze Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1525v1" />
          <attvalue for="2" value="New efficient estimation and variable selection methods for&#10;  semiparametric varying-coefficient partially linear models" />
          <attvalue for="3" value="The complexity of semiparametric models poses new challenges to statistical&#10;inference and model selection that frequently arise from real applications. In&#10;this work, we propose new estimation and variable selection procedures for the&#10;semiparametric varying-coefficient partially linear model. We first study&#10;quantile regression estimates for the nonparametric varying-coefficient&#10;functions and the parametric regression coefficients. To achieve nice&#10;efficiency properties, we further develop a semiparametric composite quantile&#10;regression procedure. We establish the asymptotic normality of proposed&#10;estimators for both the parametric and nonparametric parts and show that the&#10;estimators achieve the best convergence rate. Moreover, we show that the&#10;proposed method is much more efficient than the least-squares-based method for&#10;many non-normal errors and that it only loses a small amount of efficiency for&#10;normal errors. In addition, it is shown that the loss in efficiency is at most&#10;11.1% for estimating varying coefficient functions and is no greater than 13.6%&#10;for estimating parametric components. To achieve sparsity with high-dimensional&#10;covariates, we propose adaptive penalization methods for variable selection in&#10;the semiparametric varying-coefficient partially linear model and prove that&#10;the methods possess the oracle property. Extensive Monte Carlo simulation&#10;studies are conducted to examine the finite-sample performance of the proposed&#10;procedures. Finally, we apply the new methods to analyze the plasma&#10;beta-carotene level data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="300" source="Bo Kai" target="Hui Zou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1525v1" />
          <attvalue for="2" value="New efficient estimation and variable selection methods for&#10;  semiparametric varying-coefficient partially linear models" />
          <attvalue for="3" value="The complexity of semiparametric models poses new challenges to statistical&#10;inference and model selection that frequently arise from real applications. In&#10;this work, we propose new estimation and variable selection procedures for the&#10;semiparametric varying-coefficient partially linear model. We first study&#10;quantile regression estimates for the nonparametric varying-coefficient&#10;functions and the parametric regression coefficients. To achieve nice&#10;efficiency properties, we further develop a semiparametric composite quantile&#10;regression procedure. We establish the asymptotic normality of proposed&#10;estimators for both the parametric and nonparametric parts and show that the&#10;estimators achieve the best convergence rate. Moreover, we show that the&#10;proposed method is much more efficient than the least-squares-based method for&#10;many non-normal errors and that it only loses a small amount of efficiency for&#10;normal errors. In addition, it is shown that the loss in efficiency is at most&#10;11.1% for estimating varying coefficient functions and is no greater than 13.6%&#10;for estimating parametric components. To achieve sparsity with high-dimensional&#10;covariates, we propose adaptive penalization methods for variable selection in&#10;the semiparametric varying-coefficient partially linear model and prove that&#10;the methods possess the oracle property. Extensive Monte Carlo simulation&#10;studies are conducted to examine the finite-sample performance of the proposed&#10;procedures. Finally, we apply the new methods to analyze the plasma&#10;beta-carotene level data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="301" source="Runze Li" target="Hui Zou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1525v1" />
          <attvalue for="2" value="New efficient estimation and variable selection methods for&#10;  semiparametric varying-coefficient partially linear models" />
          <attvalue for="3" value="The complexity of semiparametric models poses new challenges to statistical&#10;inference and model selection that frequently arise from real applications. In&#10;this work, we propose new estimation and variable selection procedures for the&#10;semiparametric varying-coefficient partially linear model. We first study&#10;quantile regression estimates for the nonparametric varying-coefficient&#10;functions and the parametric regression coefficients. To achieve nice&#10;efficiency properties, we further develop a semiparametric composite quantile&#10;regression procedure. We establish the asymptotic normality of proposed&#10;estimators for both the parametric and nonparametric parts and show that the&#10;estimators achieve the best convergence rate. Moreover, we show that the&#10;proposed method is much more efficient than the least-squares-based method for&#10;many non-normal errors and that it only loses a small amount of efficiency for&#10;normal errors. In addition, it is shown that the loss in efficiency is at most&#10;11.1% for estimating varying coefficient functions and is no greater than 13.6%&#10;for estimating parametric components. To achieve sparsity with high-dimensional&#10;covariates, we propose adaptive penalization methods for variable selection in&#10;the semiparametric varying-coefficient partially linear model and prove that&#10;the methods possess the oracle property. Extensive Monte Carlo simulation&#10;studies are conducted to examine the finite-sample performance of the proposed&#10;procedures. Finally, we apply the new methods to analyze the plasma&#10;beta-carotene level data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="302" source="Shota Gugushvili" target="Peter Spreij">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1120v2" />
          <attvalue for="2" value="Parametric inference for stochastic differential equations: a smooth and&#10;  match approach" />
          <attvalue for="3" value="We study the problem of parameter estimation for a univariate discretely&#10;observed ergodic diffusion process given as a solution to a stochastic&#10;differential equation. The estimation procedure we propose consists of two&#10;steps. In the first step, which is referred to as a smoothing step, we smooth&#10;the data and construct a nonparametric estimator of the invariant density of&#10;the process. In the second step, which is referred to as a matching step, we&#10;exploit a characterisation of the invariant density as a solution of a certain&#10;ordinary differential equation, replace the invariant density in this equation&#10;by its nonparametric estimator from the smoothing step in order to arrive at an&#10;intuitively appealing criterion function, and next define our estimator of the&#10;parameter of interest as a minimiser of this criterion function. Our main&#10;results show that under suitable conditions our estimator is&#10;$\sqrt{n}$-consistent, and even asymptotically normal. We also discuss a way of&#10;improving its asymptotic performance through a one-step Newton-Raphson type&#10;procedure and present results of a small scale simulation study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="303" source="Olivier Lopez" target="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6232v2" />
          <attvalue for="2" value="Single index regression models in the presence of censoring depending on&#10;  the covariates" />
          <attvalue for="3" value="Consider a random vector (X',Y)', where X is d-dimensional and Y is&#10;one-dimensional. We assume that Y is subject to random right censoring. The aim&#10;of this paper is twofold. First, we propose a new estimator of the joint&#10;distribution of (X',Y)'. This estimator overcomes the common&#10;curse-of-dimensionality problem, by using a new dimension reduction technique.&#10;Second, we assume that the relation between X and Y is given by a mean&#10;regression single index model, and propose a new estimator of the parameters in&#10;this model. The asymptotic properties of all proposed estimators are obtained." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="304" source="Ingrid Van Keilegom" target="Oliver Linton">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="305" source="Ingrid Van Keilegom" target="Jens Perch Nielsen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="306" source="Weining Shen" target="Surya T. Tokdar">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6406v3" />
          <attvalue for="2" value="Adaptive Bayesian multivariate density estimation with Dirichlet&#10;  mixtures" />
          <attvalue for="3" value="We show that rate-adaptive multivariate density estimation can be performed&#10;using Bayesian methods based on Dirichlet mixtures of normal kernels with a&#10;prior distribution on the kernel's covariance matrix parameter. We derive&#10;sufficient conditions on the prior specification that guarantee convergence to&#10;a true density at a rate that is optimal minimax for the smoothness class to&#10;which the true density belongs. No prior knowledge of smoothness is assumed.&#10;The sufficient conditions are shown to hold for the Dirichlet location mixture&#10;of normals prior with a Gaussian base measure and an inverse-Wishart prior on&#10;the covariance matrix parameter. Locally H\&quot;older smoothness classes and their&#10;anisotropic extensions are considered. Our study involves several technical&#10;novelties, including sharp approximation of finitely differentiable&#10;multivariate densities by normal mixtures and a new sieve on the space of such&#10;densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="307" source="Weining Shen" target="Subhashis Ghosal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6406v3" />
          <attvalue for="2" value="Adaptive Bayesian multivariate density estimation with Dirichlet&#10;  mixtures" />
          <attvalue for="3" value="We show that rate-adaptive multivariate density estimation can be performed&#10;using Bayesian methods based on Dirichlet mixtures of normal kernels with a&#10;prior distribution on the kernel's covariance matrix parameter. We derive&#10;sufficient conditions on the prior specification that guarantee convergence to&#10;a true density at a rate that is optimal minimax for the smoothness class to&#10;which the true density belongs. No prior knowledge of smoothness is assumed.&#10;The sufficient conditions are shown to hold for the Dirichlet location mixture&#10;of normals prior with a Gaussian base measure and an inverse-Wishart prior on&#10;the covariance matrix parameter. Locally H\&quot;older smoothness classes and their&#10;anisotropic extensions are considered. Our study involves several technical&#10;novelties, including sharp approximation of finitely differentiable&#10;multivariate densities by normal mixtures and a new sieve on the space of such&#10;densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="308" source="Surya T. Tokdar" target="Subhashis Ghosal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6406v3" />
          <attvalue for="2" value="Adaptive Bayesian multivariate density estimation with Dirichlet&#10;  mixtures" />
          <attvalue for="3" value="We show that rate-adaptive multivariate density estimation can be performed&#10;using Bayesian methods based on Dirichlet mixtures of normal kernels with a&#10;prior distribution on the kernel's covariance matrix parameter. We derive&#10;sufficient conditions on the prior specification that guarantee convergence to&#10;a true density at a rate that is optimal minimax for the smoothness class to&#10;which the true density belongs. No prior knowledge of smoothness is assumed.&#10;The sufficient conditions are shown to hold for the Dirichlet location mixture&#10;of normals prior with a Gaussian base measure and an inverse-Wishart prior on&#10;the covariance matrix parameter. Locally H\&quot;older smoothness classes and their&#10;anisotropic extensions are considered. Our study involves several technical&#10;novelties, including sharp approximation of finitely differentiable&#10;multivariate densities by normal mixtures and a new sieve on the space of such&#10;densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="309" source="Tabea Rebafka" target="Céline Lévy-Leduc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5158v1" />
          <attvalue for="2" value="OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar&#10;  Signals" />
          <attvalue for="3" value="A transmitted, unknown radar signal is observed at the receiver through more&#10;than one path in additive noise. The aim is to recover the waveform of the&#10;intercepted signal and to simultaneously estimate the direction of arrival&#10;(DOA). We propose an approach exploiting the parsimonious time-frequency&#10;representation of the signal by applying a new OMP-type algorithm for&#10;structured sparsity patterns. An important issue is the scalability of the&#10;proposed algorithm since high-dimensional models shall be used for radar&#10;signals. Monte-Carlo simulations for modulated signals illustrate the good&#10;performance of the method even for low signal-to-noise ratios and a gain of 20&#10;dB for the DOA estimation compared to some elementary method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="310" source="Tabea Rebafka" target="Maurice Charbit">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5158v1" />
          <attvalue for="2" value="OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar&#10;  Signals" />
          <attvalue for="3" value="A transmitted, unknown radar signal is observed at the receiver through more&#10;than one path in additive noise. The aim is to recover the waveform of the&#10;intercepted signal and to simultaneously estimate the direction of arrival&#10;(DOA). We propose an approach exploiting the parsimonious time-frequency&#10;representation of the signal by applying a new OMP-type algorithm for&#10;structured sparsity patterns. An important issue is the scalability of the&#10;proposed algorithm since high-dimensional models shall be used for radar&#10;signals. Monte-Carlo simulations for modulated signals illustrate the good&#10;performance of the method even for low signal-to-noise ratios and a gain of 20&#10;dB for the DOA estimation compared to some elementary method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="311" source="Céline Lévy-Leduc" target="Maurice Charbit">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5158v1" />
          <attvalue for="2" value="OMP-type Algorithm with Structured Sparsity Patterns for Multipath Radar&#10;  Signals" />
          <attvalue for="3" value="A transmitted, unknown radar signal is observed at the receiver through more&#10;than one path in additive noise. The aim is to recover the waveform of the&#10;intercepted signal and to simultaneously estimate the direction of arrival&#10;(DOA). We propose an approach exploiting the parsimonious time-frequency&#10;representation of the signal by applying a new OMP-type algorithm for&#10;structured sparsity patterns. An important issue is the scalability of the&#10;proposed algorithm since high-dimensional models shall be used for radar&#10;signals. Monte-Carlo simulations for modulated signals illustrate the good&#10;performance of the method even for low signal-to-noise ratios and a gain of 20&#10;dB for the DOA estimation compared to some elementary method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="312" source="Céline Lévy-Leduc" target="Alexandre Lung-Yut-Fong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.1971v3" />
          <attvalue for="2" value="Homogeneity and change-point detection tests for multivariate data using&#10;  rank statistics" />
          <attvalue for="3" value="Detecting and locating changes in highly multivariate data is a major concern&#10;in several current statistical applications. In this context, the first&#10;contribution of the paper is a novel non-parametric two-sample homogeneity test&#10;for multivariate data based on the well-known Wilcoxon rank statistic. The&#10;proposed two-sample homogeneity test statistic can be extended to deal with&#10;ordinal or censored data as well as to test for the homogeneity of more than&#10;two samples. The second contribution of the paper concerns the use of the&#10;proposed test statistic to perform retrospective change-point analysis. It is&#10;first shown that the approach is computationally feasible even when looking for&#10;a large number of change-points thanks to the use of dynamic programming.&#10;Computable asymptotic $p$-values for the test are then provided in the case&#10;where a single potential change-point is to be detected. Compared to available&#10;alternatives, the proposed approach appears to be very reliable and robust.&#10;This is particularly true in situations where the data is contaminated by&#10;outliers or corrupted by noise and where the potential changes only affect&#10;subsets of the coordinates of the data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="313" source="Céline Lévy-Leduc" target="Olivier Cappé">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.1971v3" />
          <attvalue for="2" value="Homogeneity and change-point detection tests for multivariate data using&#10;  rank statistics" />
          <attvalue for="3" value="Detecting and locating changes in highly multivariate data is a major concern&#10;in several current statistical applications. In this context, the first&#10;contribution of the paper is a novel non-parametric two-sample homogeneity test&#10;for multivariate data based on the well-known Wilcoxon rank statistic. The&#10;proposed two-sample homogeneity test statistic can be extended to deal with&#10;ordinal or censored data as well as to test for the homogeneity of more than&#10;two samples. The second contribution of the paper concerns the use of the&#10;proposed test statistic to perform retrospective change-point analysis. It is&#10;first shown that the approach is computationally feasible even when looking for&#10;a large number of change-points thanks to the use of dynamic programming.&#10;Computable asymptotic $p$-values for the test are then provided in the case&#10;where a single potential change-point is to be detected. Compared to available&#10;alternatives, the proposed approach appears to be very reliable and robust.&#10;This is particularly true in situations where the data is contaminated by&#10;outliers or corrupted by noise and where the potential changes only affect&#10;subsets of the coordinates of the data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="314" source="Runchu Zhang" target="Frederick K. H. Phoa">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="315" source="Runchu Zhang" target="Rahul Mukerjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="316" source="Runchu Zhang" target="Hongquan Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="317" source="Frederick K. H. Phoa" target="Rahul Mukerjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="318" source="Frederick K. H. Phoa" target="Hongquan Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="319" source="Rahul Mukerjee" target="Hongquan Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2698v1" />
          <attvalue for="2" value="A trigonometric approach to quaternary code designs with application to&#10;  one-eighth and one-sixteenth fractions" />
          <attvalue for="3" value="The study of good nonregular fractional factorial designs has received&#10;significant attention over the last two decades. Recent research indicates that&#10;designs constructed from quaternary codes (QC) are very promising in this&#10;regard. The present paper shows how a trigonometric approach can facilitate a&#10;systematic understanding of such QC designs and lead to new theoretical results&#10;covering hitherto unexplored situations. We focus attention on one-eighth and&#10;one-sixteenth fractions of two-level factorials and show that optimal QC&#10;designs often have larger generalized resolution and projectivity than&#10;comparable regular designs. Moreover, some of these designs are found to have&#10;maximum projectivity among all designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="320" source="Sébastien Bubeck" target="Gilles Stoltz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5041v2" />
          <attvalue for="2" value="Lipschitz Bandits without the Lipschitz Constant" />
          <attvalue for="3" value="We consider the setting of stochastic bandit problems with a continuum of&#10;arms. We first point out that the strategies considered so far in the&#10;literature only provided theoretical guarantees of the form: given some tuning&#10;parameters, the regret is small with respect to a class of environments that&#10;depends on these parameters. This is however not the right perspective, as it&#10;is the strategy that should adapt to the specific bandit environment at hand,&#10;and not the other way round. Put differently, an adaptation issue is raised. We&#10;solve it for the special case of environments whose mean-payoff functions are&#10;globally Lipschitz. More precisely, we show that the minimax optimal orders of&#10;magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an&#10;environment $f$ with Lipschitz constant $L$ over $T$ time instances can be&#10;achieved without knowing $L$ or $T$ in advance. This is in contrast to all&#10;previously known strategies, which require to some extent the knowledge of $L$&#10;to achieve this performance guarantee." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="321" source="Sébastien Bubeck" target="Jia Yuan Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5041v2" />
          <attvalue for="2" value="Lipschitz Bandits without the Lipschitz Constant" />
          <attvalue for="3" value="We consider the setting of stochastic bandit problems with a continuum of&#10;arms. We first point out that the strategies considered so far in the&#10;literature only provided theoretical guarantees of the form: given some tuning&#10;parameters, the regret is small with respect to a class of environments that&#10;depends on these parameters. This is however not the right perspective, as it&#10;is the strategy that should adapt to the specific bandit environment at hand,&#10;and not the other way round. Put differently, an adaptation issue is raised. We&#10;solve it for the special case of environments whose mean-payoff functions are&#10;globally Lipschitz. More precisely, we show that the minimax optimal orders of&#10;magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an&#10;environment $f$ with Lipschitz constant $L$ over $T$ time instances can be&#10;achieved without knowing $L$ or $T$ in advance. This is in contrast to all&#10;previously known strategies, which require to some extent the knowledge of $L$&#10;to achieve this performance guarantee." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="322" source="Sébastien Bubeck" target="Gábor Lugosi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1193v2" />
          <attvalue for="2" value="Detection of correlations" />
          <attvalue for="3" value="We consider the hypothesis testing problem of deciding whether an observed&#10;high-dimensional vector has independent normal components or, alternatively, if&#10;it has a small subset of correlated components. The correlated components may&#10;have a certain combinatorial structure known to the statistician. We establish&#10;upper and lower bounds for the worst-case (minimax) risk in terms of the size&#10;of the correlated subset, the level of correlation, and the structure of the&#10;class of possibly correlated sets. We show that some simple tests have&#10;near-optimal performance in many cases, while the generalized likelihood ratio&#10;test is suboptimal in some important cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="323" source="Sébastien Bubeck" target="Rémi Munos">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="324" source="Gilles Stoltz" target="Jia Yuan Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5041v2" />
          <attvalue for="2" value="Lipschitz Bandits without the Lipschitz Constant" />
          <attvalue for="3" value="We consider the setting of stochastic bandit problems with a continuum of&#10;arms. We first point out that the strategies considered so far in the&#10;literature only provided theoretical guarantees of the form: given some tuning&#10;parameters, the regret is small with respect to a class of environments that&#10;depends on these parameters. This is however not the right perspective, as it&#10;is the strategy that should adapt to the specific bandit environment at hand,&#10;and not the other way round. Put differently, an adaptation issue is raised. We&#10;solve it for the special case of environments whose mean-payoff functions are&#10;globally Lipschitz. More precisely, we show that the minimax optimal orders of&#10;magnitude $L^{d/(d+2)} \, T^{(d+1)/(d+2)}$ of the regret bound against an&#10;environment $f$ with Lipschitz constant $L$ over $T$ time instances can be&#10;achieved without knowing $L$ or $T$ in advance. This is in contrast to all&#10;previously known strategies, which require to some extent the knowledge of $L$&#10;to achieve this performance guarantee." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="325" source="Gilles Stoltz" target="Odalric-Ambrym Maillard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5820v1" />
          <attvalue for="2" value="A Finite-Time Analysis of Multi-armed Bandits Problems with&#10;  Kullback-Leibler Divergences" />
          <attvalue for="3" value="We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed&#10;bandit problem in the case of distributions with finite supports (not&#10;necessarily known beforehand), whose asymptotic regret matches the lower bound&#10;of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of&#10;this algorithm; we get bounds whose main terms are smaller than the ones of&#10;previously known algorithms with finite-time analyses (like UCB-type&#10;algorithms)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="326" source="Gilles Stoltz" target="Rémi Munos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5820v1" />
          <attvalue for="2" value="A Finite-Time Analysis of Multi-armed Bandits Problems with&#10;  Kullback-Leibler Divergences" />
          <attvalue for="3" value="We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed&#10;bandit problem in the case of distributions with finite supports (not&#10;necessarily known beforehand), whose asymptotic regret matches the lower bound&#10;of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of&#10;this algorithm; we get bounds whose main terms are smaller than the ones of&#10;previously known algorithms with finite-time analyses (like UCB-type&#10;algorithms)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="327" source="Pierpaolo De Blasi" target="Lancelot F. James">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5008v1" />
          <attvalue for="2" value="Bayesian nonparametric estimation and consistency of mixed multinomial&#10;  logit choice models" />
          <attvalue for="3" value="This paper develops nonparametric estimation for discrete choice models based&#10;on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models&#10;encompass all discrete choice models derived under the assumption of random&#10;utility maximization, subject to the identification of an unknown distribution&#10;$G$. Noting the mixture model description of the MMNL, we employ a Bayesian&#10;nonparametric approach, using nonparametric priors on the unknown mixing&#10;distribution $G$, to estimate choice probabilities. We provide an important&#10;theoretical support for the use of the proposed methodology by investigating&#10;consistency of the posterior distribution for a general nonparametric prior on&#10;the mixing distribution. Consistency is defined according to an $L_1$-type&#10;distance on the space of choice probabilities and is achieved by extending to a&#10;regression model framework a recent approach to strong consistency based on the&#10;summability of square roots of prior probabilities. Moving to estimation,&#10;slightly different techniques for non-panel and panel data models are&#10;discussed. For practical implementation, we describe efficient and relatively&#10;easy-to-use blocked Gibbs sampling procedures. These procedures are based on&#10;approximations of the random probability measure by classes of finite&#10;stick-breaking processes. A simulation study is also performed to investigate&#10;the performance of the proposed methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="328" source="Pierpaolo De Blasi" target="John W. Lau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5008v1" />
          <attvalue for="2" value="Bayesian nonparametric estimation and consistency of mixed multinomial&#10;  logit choice models" />
          <attvalue for="3" value="This paper develops nonparametric estimation for discrete choice models based&#10;on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models&#10;encompass all discrete choice models derived under the assumption of random&#10;utility maximization, subject to the identification of an unknown distribution&#10;$G$. Noting the mixture model description of the MMNL, we employ a Bayesian&#10;nonparametric approach, using nonparametric priors on the unknown mixing&#10;distribution $G$, to estimate choice probabilities. We provide an important&#10;theoretical support for the use of the proposed methodology by investigating&#10;consistency of the posterior distribution for a general nonparametric prior on&#10;the mixing distribution. Consistency is defined according to an $L_1$-type&#10;distance on the space of choice probabilities and is achieved by extending to a&#10;regression model framework a recent approach to strong consistency based on the&#10;summability of square roots of prior probabilities. Moving to estimation,&#10;slightly different techniques for non-panel and panel data models are&#10;discussed. For practical implementation, we describe efficient and relatively&#10;easy-to-use blocked Gibbs sampling procedures. These procedures are based on&#10;approximations of the random probability measure by classes of finite&#10;stick-breaking processes. A simulation study is also performed to investigate&#10;the performance of the proposed methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="329" source="Lancelot F. James" target="John W. Lau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5008v1" />
          <attvalue for="2" value="Bayesian nonparametric estimation and consistency of mixed multinomial&#10;  logit choice models" />
          <attvalue for="3" value="This paper develops nonparametric estimation for discrete choice models based&#10;on the mixed multinomial logit (MMNL) model. It has been shown that MMNL models&#10;encompass all discrete choice models derived under the assumption of random&#10;utility maximization, subject to the identification of an unknown distribution&#10;$G$. Noting the mixture model description of the MMNL, we employ a Bayesian&#10;nonparametric approach, using nonparametric priors on the unknown mixing&#10;distribution $G$, to estimate choice probabilities. We provide an important&#10;theoretical support for the use of the proposed methodology by investigating&#10;consistency of the posterior distribution for a general nonparametric prior on&#10;the mixing distribution. Consistency is defined according to an $L_1$-type&#10;distance on the space of choice probabilities and is achieved by extending to a&#10;regression model framework a recent approach to strong consistency based on the&#10;summability of square roots of prior probabilities. Moving to estimation,&#10;slightly different techniques for non-panel and panel data models are&#10;discussed. For practical implementation, we describe efficient and relatively&#10;easy-to-use blocked Gibbs sampling procedures. These procedures are based on&#10;approximations of the random probability measure by classes of finite&#10;stick-breaking processes. A simulation study is also performed to investigate&#10;the performance of the proposed methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="330" source="J. Diebolt" target="L. Gardes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6172v1" />
          <attvalue for="2" value="Bias-reduced estimators of the Weibull tail-coefficient" />
          <attvalue for="3" value="In this paper, we consider the problem of the estimation of a Weibull&#10;tail-coefficient. In particular, we propose a regression model, from which we&#10;derive a bias-reduced estimator. This estimator is based on a least-squares&#10;approach. The asymptotic normality of this estimator is also established. A&#10;small simulation study is provided in order to prove its efficiency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="331" source="J. Diebolt" target="S. Girard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6172v1" />
          <attvalue for="2" value="Bias-reduced estimators of the Weibull tail-coefficient" />
          <attvalue for="3" value="In this paper, we consider the problem of the estimation of a Weibull&#10;tail-coefficient. In particular, we propose a regression model, from which we&#10;derive a bias-reduced estimator. This estimator is based on a least-squares&#10;approach. The asymptotic normality of this estimator is also established. A&#10;small simulation study is provided in order to prove its efficiency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="332" source="J. Diebolt" target="A. Guillou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6172v1" />
          <attvalue for="2" value="Bias-reduced estimators of the Weibull tail-coefficient" />
          <attvalue for="3" value="In this paper, we consider the problem of the estimation of a Weibull&#10;tail-coefficient. In particular, we propose a regression model, from which we&#10;derive a bias-reduced estimator. This estimator is based on a least-squares&#10;approach. The asymptotic normality of this estimator is also established. A&#10;small simulation study is provided in order to prove its efficiency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="333" source="L. Gardes" target="S. Girard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6118v1" />
          <attvalue for="2" value="Gaussian Regularized Sliced Inverse Regression" />
          <attvalue for="3" value="Sliced Inverse Regression (SIR) is an effective method for dimension&#10;reduction in high-dimensional regression problems. The original method,&#10;however, requires the inversion of the predictors covariance matrix. In case of&#10;collinearity between these predictors or small sample sizes compared to the&#10;dimension, the inversion is not possible and a regularization technique has to&#10;be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it&#10;is shown that SIR axes can be interpreted as solutions of an inverse regression&#10;problem. In this paper, a Gaussian prior distribution is introduced on the&#10;unknown parameters of the inverse regression problem in order to regularize&#10;their estimation. We show that some existing SIR regularizations can enter our&#10;framework, which permits a global understanding of these methods. Three new&#10;priors are proposed leading to new regularizations of the SIR method. A&#10;comparison on simulated data is provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="334" source="L. Gardes" target="A. Guillou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6172v1" />
          <attvalue for="2" value="Bias-reduced estimators of the Weibull tail-coefficient" />
          <attvalue for="3" value="In this paper, we consider the problem of the estimation of a Weibull&#10;tail-coefficient. In particular, we propose a regression model, from which we&#10;derive a bias-reduced estimator. This estimator is based on a least-squares&#10;approach. The asymptotic normality of this estimator is also established. A&#10;small simulation study is provided in order to prove its efficiency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="335" source="L. Gardes" target="C. Bernard-Michel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6118v1" />
          <attvalue for="2" value="Gaussian Regularized Sliced Inverse Regression" />
          <attvalue for="3" value="Sliced Inverse Regression (SIR) is an effective method for dimension&#10;reduction in high-dimensional regression problems. The original method,&#10;however, requires the inversion of the predictors covariance matrix. In case of&#10;collinearity between these predictors or small sample sizes compared to the&#10;dimension, the inversion is not possible and a regularization technique has to&#10;be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it&#10;is shown that SIR axes can be interpreted as solutions of an inverse regression&#10;problem. In this paper, a Gaussian prior distribution is introduced on the&#10;unknown parameters of the inverse regression problem in order to regularize&#10;their estimation. We show that some existing SIR regularizations can enter our&#10;framework, which permits a global understanding of these methods. Three new&#10;priors are proposed leading to new regularizations of the SIR method. A&#10;comparison on simulated data is provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="336" source="S. Girard" target="A. Guillou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6172v1" />
          <attvalue for="2" value="Bias-reduced estimators of the Weibull tail-coefficient" />
          <attvalue for="3" value="In this paper, we consider the problem of the estimation of a Weibull&#10;tail-coefficient. In particular, we propose a regression model, from which we&#10;derive a bias-reduced estimator. This estimator is based on a least-squares&#10;approach. The asymptotic normality of this estimator is also established. A&#10;small simulation study is provided in order to prove its efficiency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="337" source="S. Girard" target="C. Bernard-Michel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.6118v1" />
          <attvalue for="2" value="Gaussian Regularized Sliced Inverse Regression" />
          <attvalue for="3" value="Sliced Inverse Regression (SIR) is an effective method for dimension&#10;reduction in high-dimensional regression problems. The original method,&#10;however, requires the inversion of the predictors covariance matrix. In case of&#10;collinearity between these predictors or small sample sizes compared to the&#10;dimension, the inversion is not possible and a regularization technique has to&#10;be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it&#10;is shown that SIR axes can be interpreted as solutions of an inverse regression&#10;problem. In this paper, a Gaussian prior distribution is introduced on the&#10;unknown parameters of the inverse regression problem in order to regularize&#10;their estimation. We show that some existing SIR regularizations can enter our&#10;framework, which permits a global understanding of these methods. Three new&#10;priors are proposed leading to new regularizations of the SIR method. A&#10;comparison on simulated data is provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="338" source="S. H. Alizadeh" target="S. Rezakhah">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1212v2" />
          <attvalue for="2" value="Hidden Markov Mixture Autoregressive Models: Stability and Moments" />
          <attvalue for="3" value="This paper introduces a new parsimonious structure for mixture of&#10;autoregressive models. the weighting coefficients are determined through latent&#10;random variables, following a hidden Markov model. We propose a dynamic&#10;programming algorithm for the application of forecasting. We also derive the&#10;limiting behavior of unconditional first moment of the process and an&#10;appropriate upper bound for the limiting value of the variance. This can be&#10;considered as long run behavior of the process. Finally we show convergence and&#10;stability of the second moment. Further, we illustrate the efficacy of the&#10;proposed model by simulation and forecasting." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="339" source="Jorge Carlos Román" target="James P. Hobert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3210v2" />
          <attvalue for="2" value="Convergence analysis of the Gibbs sampler for Bayesian general linear&#10;  mixed models with improper priors" />
          <attvalue for="3" value="Bayesian analysis of data from the general linear mixed model is challenging&#10;because any nontrivial prior leads to an intractable posterior density.&#10;However, if a conditionally conjugate prior density is adopted, then there is a&#10;simple Gibbs sampler that can be employed to explore the posterior density. A&#10;popular default among the conditionally conjugate priors is an improper prior&#10;that takes a product form with a flat prior on the regression parameter, and&#10;so-called power priors on each of the variance components. In this paper, a&#10;convergence rate analysis of the corresponding Gibbs sampler is undertaken. The&#10;main result is a simple, easily-checked sufficient condition for geometric&#10;ergodicity of the Gibbs-Markov chain. This result is close to the best possible&#10;result in the sense that the sufficient condition is only slightly stronger&#10;than what is required to ensure posterior propriety. The theory developed in&#10;this paper is extremely important from a practical standpoint because it&#10;guarantees the existence of central limit theorems that allow for the&#10;computation of valid asymptotic standard errors for the estimates computed&#10;using the Gibbs sampler." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="340" source="Gaëlle Chastaing" target="Fabrice Gamboa">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.1788v3" />
          <attvalue for="2" value="Generalized Hoeffding-Sobol Decomposition for Dependent Variables&#10;  -Application to Sensitivity Analysis" />
          <attvalue for="3" value="In this paper, we consider a regression model built on dependent variables.&#10;This regression modelizes an input output relationship. Under boundedness&#10;assumptions on the joint distribution function of the input variables, we show&#10;that a generalized Hoeffding-Sobol decomposition is available. This leads to&#10;new indices measuring the sensitivity of the output with respect to the input&#10;variables. We also study and discuss the estimation of these new indices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="341" source="Gaëlle Chastaing" target="Clémentine Prieur">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.1788v3" />
          <attvalue for="2" value="Generalized Hoeffding-Sobol Decomposition for Dependent Variables&#10;  -Application to Sensitivity Analysis" />
          <attvalue for="3" value="In this paper, we consider a regression model built on dependent variables.&#10;This regression modelizes an input output relationship. Under boundedness&#10;assumptions on the joint distribution function of the input variables, we show&#10;that a generalized Hoeffding-Sobol decomposition is available. This leads to&#10;new indices measuring the sensitivity of the output with respect to the input&#10;variables. We also study and discuss the estimation of these new indices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="342" source="Fabrice Gamboa" target="Clémentine Prieur">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.1788v3" />
          <attvalue for="2" value="Generalized Hoeffding-Sobol Decomposition for Dependent Variables&#10;  -Application to Sensitivity Analysis" />
          <attvalue for="3" value="In this paper, we consider a regression model built on dependent variables.&#10;This regression modelizes an input output relationship. Under boundedness&#10;assumptions on the joint distribution function of the input variables, we show&#10;that a generalized Hoeffding-Sobol decomposition is available. This leads to&#10;new indices measuring the sensitivity of the output with respect to the input&#10;variables. We also study and discuss the estimation of these new indices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="343" source="Fabrice Gamboa" target="Thibault Espinasse">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3664v3" />
          <attvalue for="2" value="Gaussian stationary processes over graphs, general frame and maximum&#10;  likelihood identification" />
          <attvalue for="3" value="In this paper, using spectral theory of Hilbertian operators, we study ARMA&#10;Gaussian processes indexed by graphs. We extend Whittle maximum likelihood&#10;estimation of the parameters for the corresponding spectral density and show&#10;their asymptotic optimality." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="344" source="Fabrice Gamboa" target="Jean-Michel Loubes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="345" source="Fabrice Gamboa" target="Serge Cohen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="346" source="Fabrice Gamboa" target="Céline Lacaux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="347" source="Fabrice Gamboa" target="Hélène Lescornel">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="348" source="Junbum Lee" target="Suhasini Subba Rao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2759v2" />
          <attvalue for="2" value="The quantile spectral density and comparison based tests for nonlinear&#10;  time series" />
          <attvalue for="3" value="In this paper we consider tests for nonlinear time series, which are&#10;motivated by the notion of serial dependence. The proposed tests are based on&#10;comparisons with the quantile spectral density, which can be considered as a&#10;quantile version of the usual spectral density function. The quantile spectral&#10;density 'measures' sequential dependence structure of a time series, and is&#10;well defined under relatively weak mixing conditions. We propose an estimator&#10;for the quantile spectral density and derive its asympototic sampling&#10;properties. We use the quantile spectral density to construct a goodness of fit&#10;test for time series and explain how this test can also be used for comparing&#10;the sequential dependence structure of two time series. The method is&#10;illustrated with simulations and some real data examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="349" source="Suhasini Subba Rao" target="Piotr Fryzlewicz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2053v1" />
          <attvalue for="2" value="Mixing properties of ARCH and time-varying ARCH processes" />
          <attvalue for="3" value="There exist very few results on mixing for non-stationary processes. However,&#10;mixing is often required in statistical inference for non-stationary processes&#10;such as time-varying ARCH (tvARCH) models. In this paper, bounds for the mixing&#10;rates of a stochastic process are derived in terms of the conditional densities&#10;of the process. These bounds are used to obtain the $\alpha$, 2-mixing and&#10;$\beta$-mixing rates of the non-stationary time-varying $\operatorname&#10;{ARCH}(p)$ process and $\operatorname {ARCH}(\infty)$ process. It is shown that&#10;the mixing rate of the time-varying $\operatorname {ARCH}(p)$ process is&#10;geometric, whereas the bound on the mixing rate of the $\operatorname&#10;{ARCH}(\infty)$ process depends on the rate of decay of the $\operatorname&#10;{ARCH}(\infty)$ parameters. We note that the methodology given in this paper is&#10;applicable to other processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="350" source="Dan Shen" target="Haipeng Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4289v1" />
          <attvalue for="2" value="Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts" />
          <attvalue for="3" value="Sparse Principal Component Analysis (PCA) methods are efficient tools to&#10;reduce the dimension (or the number of variables) of complex data. Sparse&#10;principal components (PCs) are easier to interpret than conventional PCs,&#10;because most loadings are zero. We study the asymptotic properties of these&#10;sparse PC directions for scenarios with fixed sample size and increasing&#10;dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously&#10;studied spike covariance assumption, we show that Sparse PCA remains consistent&#10;under the same large spike condition that was previously established for&#10;conventional PCA. Under a broad range of small spike conditions, we find a&#10;large set of sparsity assumptions where Sparse PCA is consistent, but PCA is&#10;strongly inconsistent. The boundaries of the consistent region are clarified&#10;using an oracle result." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="351" source="Dan Shen" target="J. S. Marron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4289v1" />
          <attvalue for="2" value="Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts" />
          <attvalue for="3" value="Sparse Principal Component Analysis (PCA) methods are efficient tools to&#10;reduce the dimension (or the number of variables) of complex data. Sparse&#10;principal components (PCs) are easier to interpret than conventional PCs,&#10;because most loadings are zero. We study the asymptotic properties of these&#10;sparse PC directions for scenarios with fixed sample size and increasing&#10;dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously&#10;studied spike covariance assumption, we show that Sparse PCA remains consistent&#10;under the same large spike condition that was previously established for&#10;conventional PCA. Under a broad range of small spike conditions, we find a&#10;large set of sparsity assumptions where Sparse PCA is consistent, but PCA is&#10;strongly inconsistent. The boundaries of the consistent region are clarified&#10;using an oracle result." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="352" source="Haipeng Shen" target="J. S. Marron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4289v1" />
          <attvalue for="2" value="Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts" />
          <attvalue for="3" value="Sparse Principal Component Analysis (PCA) methods are efficient tools to&#10;reduce the dimension (or the number of variables) of complex data. Sparse&#10;principal components (PCs) are easier to interpret than conventional PCs,&#10;because most loadings are zero. We study the asymptotic properties of these&#10;sparse PC directions for scenarios with fixed sample size and increasing&#10;dimension (i.e. High Dimension, Low Sample Size (HDLSS)). Under the previously&#10;studied spike covariance assumption, we show that Sparse PCA remains consistent&#10;under the same large spike condition that was previously established for&#10;conventional PCA. Under a broad range of small spike conditions, we find a&#10;large set of sparsity assumptions where Sparse PCA is consistent, but PCA is&#10;strongly inconsistent. The boundaries of the consistent region are clarified&#10;using an oracle result." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="353" source="Zhan Wang" target="Sandra Paterlini">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="354" source="Zhan Wang" target="Frank Gao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="355" source="Zhan Wang" target="Yuhong Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="356" source="Sandra Paterlini" target="Frank Gao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="357" source="Sandra Paterlini" target="Yuhong Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="358" source="Frank Gao" target="Yuhong Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1961v4" />
          <attvalue for="2" value="Adaptive Minimax Estimation over Sparse $\ell_q$-Hulls" />
          <attvalue for="3" value="Given a dictionary of $M_n$ initial estimates of the unknown true regression&#10;function, we aim to construct linearly aggregated estimators that target the&#10;best performance among all the linear combinations under a sparse $q$-norm ($0&#10;\leq q \leq 1$) constraint on the linear coefficients. Besides identifying the&#10;optimal rates of aggregation for these $\ell_q$-aggregation problems, our&#10;multi-directional (or universal) aggregation strategies by model mixing or&#10;model selection achieve the optimal rates simultaneously over the full range of&#10;$0\leq q \leq 1$ for general $M_n$ and upper bound $t_n$ of the $q$-norm. Both&#10;random and fixed designs, with known or unknown error variance, are handled,&#10;and the $\ell_q$-aggregations examined in this work cover major types of&#10;aggregation problems previously studied in the literature. Consequences on&#10;minimax-rate adaptive regression under $\ell_q$-constrained true coefficients&#10;($0 \leq q \leq 1$) are also provided.&#10;  Our results show that the minimax rate of $\ell_q$-aggregation ($0 \leq q&#10;\leq 1$) is basically determined by an effective model size, which is a&#10;sparsity index that depends on $q$, $t_n$, $M_n$, and the sample size $n$ in an&#10;easily interpretable way based on a classical model selection theory that deals&#10;with a large number of models. In addition, in the fixed design case, the model&#10;selection approach is seen to yield optimal rates of convergence not only in&#10;expectation but also with exponential decay of deviation probability. In&#10;contrast, the model mixing approach can have leading constant one in front of&#10;the target risk in the oracle inequality while not offering optimality in&#10;deviation probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="359" source="Thibault Espinasse" target="Jean-Michel Loubes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.3664v3" />
          <attvalue for="2" value="Gaussian stationary processes over graphs, general frame and maximum&#10;  likelihood identification" />
          <attvalue for="3" value="In this paper, using spectral theory of Hilbertian operators, we study ARMA&#10;Gaussian processes indexed by graphs. We extend Whittle maximum likelihood&#10;estimation of the parameters for the corresponding spectral density and show&#10;their asymptotic optimality." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="360" source="Thibault Espinasse" target="Hélène Lescornel">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="361" source="Jean-Michel Loubes" target="Hélène Lescornel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4735v1" />
          <attvalue for="2" value="Unbiased risk estimation method for covariance estimation" />
          <attvalue for="3" value="We consider a model selection estimator of the covariance of a random&#10;process. Using the Unbiased Risk Estimation (URE) method, we build an estimator&#10;of the risk which allows to select an estimator in a collection of model. Then,&#10;we present an oracle inequality which ensures that the risk of the selected&#10;estimator is close to the risk of the oracle. Simulations show the efficiency&#10;of this methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="362" source="Jean-Michel Loubes" target="Claudie Chabriac">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4735v1" />
          <attvalue for="2" value="Unbiased risk estimation method for covariance estimation" />
          <attvalue for="3" value="We consider a model selection estimator of the covariance of a random&#10;process. Using the Unbiased Risk Estimation (URE) method, we build an estimator&#10;of the risk which allows to select an estimator in a collection of model. Then,&#10;we present an oracle inequality which ensures that the risk of the selected&#10;estimator is close to the risk of the oracle. Simulations show the efficiency&#10;of this methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="363" source="Jean-Michel Loubes" target="Serge Cohen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="364" source="Jean-Michel Loubes" target="Céline Lacaux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="365" source="Jean-Michel Loubes" target="Emmanuel Boissard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5927v2" />
          <attvalue for="2" value="Distribution's template estimate with Wasserstein metrics" />
          <attvalue for="3" value="In this paper we tackle the problem of comparing distributions of random&#10;variables and defining a mean pattern between a sample of random events. Using&#10;barycenters of measures in the Wasserstein space, we propose an iterative&#10;version as an estimation of the mean distribution. Moreover, when the&#10;distributions are a common measure warped by a centered random operator, then&#10;the barycenter enables to recover this distribution template." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="366" source="Jean-Michel Loubes" target="Thibaut Le Gouic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5927v2" />
          <attvalue for="2" value="Distribution's template estimate with Wasserstein metrics" />
          <attvalue for="3" value="In this paper we tackle the problem of comparing distributions of random&#10;variables and defining a mean pattern between a sample of random events. Using&#10;barycenters of measures in the Wasserstein space, we propose an iterative&#10;version as an estimation of the mean distribution. Moreover, when the&#10;distributions are a common measure warped by a centered random operator, then&#10;the barycenter enables to recover this distribution template." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="367" source="Fuqing Gao" target="Xingqiu Zhao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3552v1" />
          <attvalue for="2" value="Delta method in large deviations and moderate deviations for estimators" />
          <attvalue for="3" value="The delta method is a popular and elementary tool for deriving limiting&#10;distributions of transformed statistics, while applications of asymptotic&#10;distributions do not allow one to obtain desirable accuracy of approximation&#10;for tail probabilities. The large and moderate deviation theory can achieve&#10;this goal. Motivated by the delta method in weak convergence, a general delta&#10;method in large deviations is proposed. The new method can be widely applied to&#10;driving the moderate deviations of estimators and is illustrated by examples&#10;including the Wilcoxon statistic, the Kaplan--Meier estimator, the empirical&#10;quantile processes and the empirical copula function. We also improve the&#10;existing moderate deviations results for $M$-estimators and $L$-statistics by&#10;the new method. Some applications of moderate deviations to statistical&#10;hypothesis testing are provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="368" source="Christophe Andrieu" target="Ajay Jasra">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="369" source="Christophe Andrieu" target="Arnaud Doucet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="370" source="Christophe Andrieu" target="Pierre Del Moral">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="371" source="Ajay Jasra" target="Arnaud Doucet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="372" source="Ajay Jasra" target="Pierre Del Moral">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="373" source="Arnaud Doucet" target="Pierre Del Moral">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3046v1" />
          <attvalue for="2" value="On nonlinear Markov chain Monte Carlo" />
          <attvalue for="3" value="Let $\mathscr{P}(E)$ be the space of probability measures on a measurable&#10;space $(E,\mathcal{E})$. In this paper we introduce a class of nonlinear Markov&#10;chain Monte Carlo (MCMC) methods for simulating from a probability measure&#10;$\pi\in\mathscr{P}(E)$. Nonlinear Markov kernels (see [Feynman--Kac Formulae:&#10;Genealogical and Interacting Particle Systems with Applications (2004)&#10;Springer]) $K:\mathscr{P}(E)\times E\rightarrow\mathscr{P}(E)$ can be&#10;constructed to, in some sense, improve over MCMC methods. However, such&#10;nonlinear kernels cannot be simulated exactly, so approximations of the&#10;nonlinear kernels are constructed using auxiliary or potentially&#10;self-interacting chains. Several nonlinear kernels are presented and it is&#10;demonstrated that, under some conditions, the associated approximations exhibit&#10;a strong law of large numbers; our proof technique is via the Poisson equation&#10;and Foster--Lyapunov conditions. We investigate the performance of our&#10;approximations with some simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="374" source="Christophe Giraud" target="Sylvie Huet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5587v2" />
          <attvalue for="2" value="High-dimensional regression with unknown variance" />
          <attvalue for="3" value="We review recent results for high-dimensional sparse linear regression in the&#10;practical case of unknown variance. Different sparsity settings are covered,&#10;including coordinate-sparsity, group-sparsity and variation-sparsity. The&#10;emphasis is put on non-asymptotic analyses and feasible procedures. In&#10;addition, a small numerical study compares the practical performance of three&#10;schemes for tuning the Lasso estimator and some references are collected for&#10;some more general models, including multivariate regression and nonparametric&#10;regression." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="375" source="Christophe Giraud" target="Nicolas Verzelen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5587v2" />
          <attvalue for="2" value="High-dimensional regression with unknown variance" />
          <attvalue for="3" value="We review recent results for high-dimensional sparse linear regression in the&#10;practical case of unknown variance. Different sparsity settings are covered,&#10;including coordinate-sparsity, group-sparsity and variation-sparsity. The&#10;emphasis is put on non-asymptotic analyses and feasible procedures. In&#10;addition, a small numerical study compares the practical performance of three&#10;schemes for tuning the Lasso estimator and some references are collected for&#10;some more general models, including multivariate regression and nonparametric&#10;regression." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="376" source="Sylvie Huet" target="Nicolas Verzelen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5587v2" />
          <attvalue for="2" value="High-dimensional regression with unknown variance" />
          <attvalue for="3" value="We review recent results for high-dimensional sparse linear regression in the&#10;practical case of unknown variance. Different sparsity settings are covered,&#10;including coordinate-sparsity, group-sparsity and variation-sparsity. The&#10;emphasis is put on non-asymptotic analyses and feasible procedures. In&#10;addition, a small numerical study compares the practical performance of three&#10;schemes for tuning the Lasso estimator and some references are collected for&#10;some more general models, including multivariate regression and nonparametric&#10;regression." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="377" source="Sylvie Huet" target="Maxime Lenormand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6759v3" />
          <attvalue for="2" value="Generating French virtual commuting network at municipality level" />
          <attvalue for="3" value="We aim to generate virtual commuting networks in the French rural regions in&#10;order to study the dynamics of their municipalities. Since we have to model&#10;small commuting flows between municipalities with a few hundreds or thousands&#10;inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.&#10;It reproduces the various possible complete networks using an iterative&#10;process, stochastically choosing a workplace in the region for each commuter&#10;living in the municipality of a region. The choice is made considering the job&#10;offers in each municipality of the region and the distance to all the possible&#10;destinations. This paper presents how to adapt and implement this model to&#10;generate French regions commuting networks between municipalities. We address&#10;three different questions: How to generate a reliable virtual commuting network&#10;for a region highly dependant of other regions for the satisfaction of its&#10;resident's demand for employment? What about a convenient deterrence function?&#10;How to calibrate the model when detailed data is not available? We answer&#10;proposing an extended job search geographical base for commuters living in the&#10;municipalities, we compare two different deterrence functions and we show that&#10;the parameter is a constant for network linking French municipalities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="378" source="Sylvie Huet" target="Floriana Gargiulo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6759v3" />
          <attvalue for="2" value="Generating French virtual commuting network at municipality level" />
          <attvalue for="3" value="We aim to generate virtual commuting networks in the French rural regions in&#10;order to study the dynamics of their municipalities. Since we have to model&#10;small commuting flows between municipalities with a few hundreds or thousands&#10;inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.&#10;It reproduces the various possible complete networks using an iterative&#10;process, stochastically choosing a workplace in the region for each commuter&#10;living in the municipality of a region. The choice is made considering the job&#10;offers in each municipality of the region and the distance to all the possible&#10;destinations. This paper presents how to adapt and implement this model to&#10;generate French regions commuting networks between municipalities. We address&#10;three different questions: How to generate a reliable virtual commuting network&#10;for a region highly dependant of other regions for the satisfaction of its&#10;resident's demand for employment? What about a convenient deterrence function?&#10;How to calibrate the model when detailed data is not available? We answer&#10;proposing an extended job search geographical base for commuters living in the&#10;municipalities, we compare two different deterrence functions and we show that&#10;the parameter is a constant for network linking French municipalities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="379" source="Yichao Wu" target="Hans-Georg Müller">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5217v1" />
          <attvalue for="2" value="Varying-coefficient functional linear regression" />
          <attvalue for="3" value="Functional linear regression analysis aims to model regression relations&#10;which include a functional predictor. The analog of the regression parameter&#10;vector or matrix in conventional multivariate or multiple-response linear&#10;regression models is a regression parameter function in one or two arguments.&#10;If, in addition, one has scalar predictors, as is often the case in&#10;applications to longitudinal studies, the question arises how to incorporate&#10;these into a functional regression model. We study a varying-coefficient&#10;approach where the scalar covariates are modeled as additional arguments of the&#10;regression parameter function. This extension of the functional linear&#10;regression model is analogous to the extension of conventional linear&#10;regression models to varying-coefficient models and shares its advantages, such&#10;as increased flexibility; however, the details of this extension are more&#10;challenging in the functional case. Our methodology combines smoothing methods&#10;with regularization by truncation at a finite number of functional principal&#10;components. A practical version is developed and is shown to perform better&#10;than functional linear regression for longitudinal data. We investigate the&#10;asymptotic properties of varying-coefficient functional linear regression and&#10;establish consistency properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="380" source="Hans-Georg Müller" target="Guozhong He">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="381" source="Hans-Georg Müller" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="382" source="Hans-Georg Müller" target="Wenjing Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="383" source="B. T. Knapik" target="A. W. van der Vaart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5876v3" />
          <attvalue for="2" value="Bayesian recovery of the initial condition for the heat equation" />
          <attvalue for="3" value="We study a Bayesian approach to recovering the initial condition for the heat&#10;equation from noisy observations of the solution at a later time. We consider a&#10;class of prior distributions indexed by a parameter quantifying &quot;smoothness&quot;&#10;and show that the corresponding posterior distributions contract around the&#10;true parameter at a rate that depends on the smoothness of the true initial&#10;condition and the smoothness and scale of the prior. Correct combinations of&#10;these characteristics lead to the optimal minimax rate. One type of priors&#10;leads to a rate-adaptive Bayesian procedure. The frequentist coverage of&#10;credible sets is shown to depend on the combination of the prior and true&#10;parameter as well, with smoother priors leading to zero coverage and rougher&#10;priors to (extremely) conservative results. In the latter case credible sets&#10;are much larger than frequentist confidence sets, in that the ratio of&#10;diameters diverges to infinity. The results are numerically illustrated by a&#10;simulated data example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="384" source="B. T. Knapik" target="J. H. van Zanten">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5876v3" />
          <attvalue for="2" value="Bayesian recovery of the initial condition for the heat equation" />
          <attvalue for="3" value="We study a Bayesian approach to recovering the initial condition for the heat&#10;equation from noisy observations of the solution at a later time. We consider a&#10;class of prior distributions indexed by a parameter quantifying &quot;smoothness&quot;&#10;and show that the corresponding posterior distributions contract around the&#10;true parameter at a rate that depends on the smoothness of the true initial&#10;condition and the smoothness and scale of the prior. Correct combinations of&#10;these characteristics lead to the optimal minimax rate. One type of priors&#10;leads to a rate-adaptive Bayesian procedure. The frequentist coverage of&#10;credible sets is shown to depend on the combination of the prior and true&#10;parameter as well, with smoother priors leading to zero coverage and rougher&#10;priors to (extremely) conservative results. In the latter case credible sets&#10;are much larger than frequentist confidence sets, in that the ratio of&#10;diameters diverges to infinity. The results are numerically illustrated by a&#10;simulated data example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="385" source="A. W. van der Vaart" target="J. H. van Zanten">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5876v3" />
          <attvalue for="2" value="Bayesian recovery of the initial condition for the heat equation" />
          <attvalue for="3" value="We study a Bayesian approach to recovering the initial condition for the heat&#10;equation from noisy observations of the solution at a later time. We consider a&#10;class of prior distributions indexed by a parameter quantifying &quot;smoothness&quot;&#10;and show that the corresponding posterior distributions contract around the&#10;true parameter at a rate that depends on the smoothness of the true initial&#10;condition and the smoothness and scale of the prior. Correct combinations of&#10;these characteristics lead to the optimal minimax rate. One type of priors&#10;leads to a rate-adaptive Bayesian procedure. The frequentist coverage of&#10;credible sets is shown to depend on the combination of the prior and true&#10;parameter as well, with smoother priors leading to zero coverage and rougher&#10;priors to (extremely) conservative results. In the latter case credible sets&#10;are much larger than frequentist confidence sets, in that the ratio of&#10;diameters diverges to infinity. The results are numerically illustrated by a&#10;simulated data example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="386" source="Azaïs Romain" target="Gégout-Petit Anne">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.2121v2" />
          <attvalue for="2" value="Optimal quantization applied to Sliced Inverse Regression" />
          <attvalue for="3" value="In this paper we consider a semiparametric regression model involving a&#10;$d$-dimensional quantitative explanatory variable $X$ and including a dimension&#10;reduction of $X$ via an index $\beta'X$. In this model, the main goal is to&#10;estimate the euclidean parameter $\beta$ and to predict the real response&#10;variable $Y$ conditionally to $X$. Our approach is based on sliced inverse&#10;regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We&#10;obtain the convergence of the proposed estimators of $\beta$ and of the&#10;conditional distribution. Simulation studies show the good numerical behavior&#10;of the proposed estimators for finite sample size." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="387" source="Azaïs Romain" target="Saracco Jérôme">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.2121v2" />
          <attvalue for="2" value="Optimal quantization applied to Sliced Inverse Regression" />
          <attvalue for="3" value="In this paper we consider a semiparametric regression model involving a&#10;$d$-dimensional quantitative explanatory variable $X$ and including a dimension&#10;reduction of $X$ via an index $\beta'X$. In this model, the main goal is to&#10;estimate the euclidean parameter $\beta$ and to predict the real response&#10;variable $Y$ conditionally to $X$. Our approach is based on sliced inverse&#10;regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We&#10;obtain the convergence of the proposed estimators of $\beta$ and of the&#10;conditional distribution. Simulation studies show the good numerical behavior&#10;of the proposed estimators for finite sample size." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="388" source="Gégout-Petit Anne" target="Saracco Jérôme">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.2121v2" />
          <attvalue for="2" value="Optimal quantization applied to Sliced Inverse Regression" />
          <attvalue for="3" value="In this paper we consider a semiparametric regression model involving a&#10;$d$-dimensional quantitative explanatory variable $X$ and including a dimension&#10;reduction of $X$ via an index $\beta'X$. In this model, the main goal is to&#10;estimate the euclidean parameter $\beta$ and to predict the real response&#10;variable $Y$ conditionally to $X$. Our approach is based on sliced inverse&#10;regression (SIR) method and optimal quantization in $\mathbf{L}^p$-norm. We&#10;obtain the convergence of the proposed estimators of $\beta$ and of the&#10;conditional distribution. Simulation studies show the good numerical behavior&#10;of the proposed estimators for finite sample size." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="389" source="Nicolai Meinshausen" target="Marloes H. Maathuis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2068v2" />
          <attvalue for="2" value="Asymptotic optimality of the Westfall--Young permutation procedure for&#10;  multiple testing under dependence" />
          <attvalue for="3" value="Test statistics are often strongly dependent in large-scale multiple testing&#10;applications. Most corrections for multiplicity are unduly conservative for&#10;correlated test statistics, resulting in a loss of power to detect true&#10;positives. We show that the Westfall--Young permutation method has&#10;asymptotically optimal power for a broad class of testing problems with a&#10;block-dependence and sparsity structure among the tests, when the number of&#10;tests tends to infinity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="390" source="Nicolai Meinshausen" target="Peter Bühlmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2068v2" />
          <attvalue for="2" value="Asymptotic optimality of the Westfall--Young permutation procedure for&#10;  multiple testing under dependence" />
          <attvalue for="3" value="Test statistics are often strongly dependent in large-scale multiple testing&#10;applications. Most corrections for multiplicity are unduly conservative for&#10;correlated test statistics, resulting in a loss of power to detect true&#10;positives. We show that the Westfall--Young permutation method has&#10;asymptotically optimal power for a broad class of testing problems with a&#10;block-dependence and sparsity structure among the tests, when the number of&#10;tests tends to infinity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="391" source="Marloes H. Maathuis" target="Peter Bühlmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2068v2" />
          <attvalue for="2" value="Asymptotic optimality of the Westfall--Young permutation procedure for&#10;  multiple testing under dependence" />
          <attvalue for="3" value="Test statistics are often strongly dependent in large-scale multiple testing&#10;applications. Most corrections for multiplicity are unduly conservative for&#10;correlated test statistics, resulting in a loss of power to detect true&#10;positives. We show that the Westfall--Young permutation method has&#10;asymptotically optimal power for a broad class of testing problems with a&#10;block-dependence and sparsity structure among the tests, when the number of&#10;tests tends to infinity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="392" source="Guillermo Henry" target="Andrés Muñoz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4763v1" />
          <attvalue for="2" value="k-Nearest neighbor density estimation on Riemannian Manifolds" />
          <attvalue for="3" value="In this paper, we consider a k-nearest neighbor kernel type estimator when&#10;the random variables belong in a Riemannian manifolds. We study asymptotic&#10;properties such as the consistency and the asymptotic distribution. A&#10;simulation study is also consider to evaluate the performance of the proposal.&#10;Finally, to illustrate the potential applications of the proposed estimator, we&#10;analyzed two real example where two different manifolds are considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="393" source="Guillermo Henry" target="Daniela Rodriguez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4763v1" />
          <attvalue for="2" value="k-Nearest neighbor density estimation on Riemannian Manifolds" />
          <attvalue for="3" value="In this paper, we consider a k-nearest neighbor kernel type estimator when&#10;the random variables belong in a Riemannian manifolds. We study asymptotic&#10;properties such as the consistency and the asymptotic distribution. A&#10;simulation study is also consider to evaluate the performance of the proposal.&#10;Finally, to illustrate the potential applications of the proposed estimator, we&#10;analyzed two real example where two different manifolds are considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="394" source="Andrés Muñoz" target="Daniela Rodriguez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4763v1" />
          <attvalue for="2" value="k-Nearest neighbor density estimation on Riemannian Manifolds" />
          <attvalue for="3" value="In this paper, we consider a k-nearest neighbor kernel type estimator when&#10;the random variables belong in a Riemannian manifolds. We study asymptotic&#10;properties such as the consistency and the asymptotic distribution. A&#10;simulation study is also consider to evaluate the performance of the proposal.&#10;Finally, to illustrate the potential applications of the proposed estimator, we&#10;analyzed two real example where two different manifolds are considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="395" source="Alexandre Lung-Yut-Fong" target="Olivier Cappé">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.1971v3" />
          <attvalue for="2" value="Homogeneity and change-point detection tests for multivariate data using&#10;  rank statistics" />
          <attvalue for="3" value="Detecting and locating changes in highly multivariate data is a major concern&#10;in several current statistical applications. In this context, the first&#10;contribution of the paper is a novel non-parametric two-sample homogeneity test&#10;for multivariate data based on the well-known Wilcoxon rank statistic. The&#10;proposed two-sample homogeneity test statistic can be extended to deal with&#10;ordinal or censored data as well as to test for the homogeneity of more than&#10;two samples. The second contribution of the paper concerns the use of the&#10;proposed test statistic to perform retrospective change-point analysis. It is&#10;first shown that the approach is computationally feasible even when looking for&#10;a large number of change-points thanks to the use of dynamic programming.&#10;Computable asymptotic $p$-values for the test are then provided in the case&#10;where a single potential change-point is to be detected. Compared to available&#10;alternatives, the proposed approach appears to be very reliable and robust.&#10;This is particularly true in situations where the data is contaminated by&#10;outliers or corrupted by noise and where the potential changes only affect&#10;subsets of the coordinates of the data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="396" source="Matthieu Solnon" target="Sylvain Arlot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4512v3" />
          <attvalue for="2" value="Multi-task Regression using Minimal Penalties" />
          <attvalue for="3" value="In this paper we study the kernel multiple ridge regression framework, which&#10;we refer to as multi-task regression, using penalization techniques. The&#10;theoretical analysis of this problem shows that the key element appearing for&#10;an optimal calibration is the covariance matrix of the noise between the&#10;different tasks. We present a new algorithm to estimate this covariance matrix,&#10;based on the concept of minimal penalty, which was previously used in the&#10;single-task regression framework to estimate the variance of the noise. We&#10;show, in a non-asymptotic setting and under mild assumptions on the target&#10;function, that this estimator converges towards the covariance matrix. Then&#10;plugging this estimator into the corresponding ideal penalty leads to an oracle&#10;inequality. We illustrate the behavior of our algorithm on synthetic examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="397" source="Matthieu Solnon" target="Francis Bach">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4512v3" />
          <attvalue for="2" value="Multi-task Regression using Minimal Penalties" />
          <attvalue for="3" value="In this paper we study the kernel multiple ridge regression framework, which&#10;we refer to as multi-task regression, using penalization techniques. The&#10;theoretical analysis of this problem shows that the key element appearing for&#10;an optimal calibration is the covariance matrix of the noise between the&#10;different tasks. We present a new algorithm to estimate this covariance matrix,&#10;based on the concept of minimal penalty, which was previously used in the&#10;single-task regression framework to estimate the variance of the noise. We&#10;show, in a non-asymptotic setting and under mild assumptions on the target&#10;function, that this estimator converges towards the covariance matrix. Then&#10;plugging this estimator into the corresponding ideal penalty leads to an oracle&#10;inequality. We illustrate the behavior of our algorithm on synthetic examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="398" source="Sylvain Arlot" target="Francis Bach">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4512v3" />
          <attvalue for="2" value="Multi-task Regression using Minimal Penalties" />
          <attvalue for="3" value="In this paper we study the kernel multiple ridge regression framework, which&#10;we refer to as multi-task regression, using penalization techniques. The&#10;theoretical analysis of this problem shows that the key element appearing for&#10;an optimal calibration is the covariance matrix of the noise between the&#10;different tasks. We present a new algorithm to estimate this covariance matrix,&#10;based on the concept of minimal penalty, which was previously used in the&#10;single-task regression framework to estimate the variance of the noise. We&#10;show, in a non-asymptotic setting and under mild assumptions on the target&#10;function, that this estimator converges towards the covariance matrix. Then&#10;plugging this estimator into the corresponding ideal penalty leads to an oracle&#10;inequality. We illustrate the behavior of our algorithm on synthetic examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="399" source="Servane Gey" target="Tristan Mary-Huard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.0757v2" />
          <attvalue for="2" value="Risk Bounds for Embedded Variable Selection in Classification Trees" />
          <attvalue for="3" value="The problems of model and variable selections for classification trees are&#10;jointly considered. A penalized criterion is proposed which explicitly takes&#10;into account the number of variables, and a risk bound inequality is provided&#10;for the tree classifier minimizing this criterion. This penalized criterion is&#10;compared to the one used during the pruning step of the CART algorithm. It is&#10;shown that the two criteria are similar under some specific margin assumptions.&#10;In practice, the tuning parameter of the CART penalty has to be calibrated by&#10;hold-out. Simulation studies are performed which confirm that the hold-out&#10;procedure mimics the form of the proposed penalized criterion." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="400" source="Han Xiao" target="Wei Biao Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.4563v2" />
          <attvalue for="2" value="Covariance matrix estimation for stationary time series" />
          <attvalue for="3" value="We obtain a sharp convergence rate for banded covariance matrix estimates of&#10;stationary processes. A precise order of magnitude is derived for spectral&#10;radius of sample covariance matrices. We also consider a thresholded covariance&#10;matrix estimator that can better characterize sparsity if the true covariance&#10;matrix is sparse. As our main tool, we implement Toeplitz [Math. Ann. 70 (1911)&#10;351-376] idea and relate eigenvalues of covariance matrices to the spectral&#10;densities or Fourier transforms of the covariances. We develop a large&#10;deviation result for quadratic forms of stationary processes using m-dependence&#10;approximation, under the framework of causal representation and physical&#10;dependence measures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="401" source="Wei Biao Wu" target="Magda Peligrad">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="402" source="Wei Biao Wu" target="Hailin Sang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="403" source="Wei Biao Wu" target="Yunda Zhong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="404" source="Laurens de Haan" target="Albert Klein Tank">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4149v2" />
          <attvalue for="2" value="On tail trend detection: modeling relative risk" />
          <attvalue for="3" value="The climate change dispute is about changes over time of environmental&#10;characteristics (such as rainfall). Some people say that a possible change is&#10;not so much in the mean but rather in the extreme phenomena (that is, the&#10;average rainfall may not change much but heavy storms may become more or less&#10;frequent). The paper studies changes over time in the probability that some&#10;high threshold is exceeded. The model is such that the threshold does not need&#10;to be specified, the results hold for any high threshold. For simplicity a&#10;certain linear trend is studied depending on one real parameter. Estimation and&#10;testing procedures (is there a trend?) are developed. Simulation results are&#10;presented. The method is applied to trends in heavy rainfall at 18 gauging&#10;stations across Germany and The Netherlands. A tentative conclusion is that the&#10;trend seems to depend on whether or not a station is close to the sea." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="405" source="Laurens de Haan" target="Cláudia Neves">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4149v2" />
          <attvalue for="2" value="On tail trend detection: modeling relative risk" />
          <attvalue for="3" value="The climate change dispute is about changes over time of environmental&#10;characteristics (such as rainfall). Some people say that a possible change is&#10;not so much in the mean but rather in the extreme phenomena (that is, the&#10;average rainfall may not change much but heavy storms may become more or less&#10;frequent). The paper studies changes over time in the probability that some&#10;high threshold is exceeded. The model is such that the threshold does not need&#10;to be specified, the results hold for any high threshold. For simplicity a&#10;certain linear trend is studied depending on one real parameter. Estimation and&#10;testing procedures (is there a trend?) are developed. Simulation results are&#10;presented. The method is applied to trends in heavy rainfall at 18 gauging&#10;stations across Germany and The Netherlands. A tentative conclusion is that the&#10;trend seems to depend on whether or not a station is close to the sea." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="406" source="Albert Klein Tank" target="Cláudia Neves">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.4149v2" />
          <attvalue for="2" value="On tail trend detection: modeling relative risk" />
          <attvalue for="3" value="The climate change dispute is about changes over time of environmental&#10;characteristics (such as rainfall). Some people say that a possible change is&#10;not so much in the mean but rather in the extreme phenomena (that is, the&#10;average rainfall may not change much but heavy storms may become more or less&#10;frequent). The paper studies changes over time in the probability that some&#10;high threshold is exceeded. The model is such that the threshold does not need&#10;to be specified, the results hold for any high threshold. For simplicity a&#10;certain linear trend is studied depending on one real parameter. Estimation and&#10;testing procedures (is there a trend?) are developed. Simulation results are&#10;presented. The method is applied to trends in heavy rainfall at 18 gauging&#10;stations across Germany and The Netherlands. A tentative conclusion is that the&#10;trend seems to depend on whether or not a station is close to the sea." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="407" source="Maugis Cathy" target="Michel Bertrand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.4253v2" />
          <attvalue for="2" value="Adaptive density estimation for clustering with Gaussian mixtures" />
          <attvalue for="3" value="Gaussian mixture models are widely used to study clustering problems. These&#10;model-based clustering methods require an accurate estimation of the unknown&#10;data density by Gaussian mixtures. In Maugis and Michel (2009), a penalized&#10;maximum likelihood estimator is proposed for automatically selecting the number&#10;of mixture components. In the present paper, a collection of univariate&#10;densities whose logarithm is locally {\beta}-H\&quot;older with moment and tail&#10;conditions are considered. We show that this penalized estimator is minimax&#10;adaptive to the {\beta} regularity of such densities in the Hellinger sense." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="408" source="Paweł Hitczenko" target="Jacek Wesołowski">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2753v1" />
          <attvalue for="2" value="Renorming divergent perpetuities" />
          <attvalue for="3" value="We consider a sequence of random variables $(R_n)$ defined by the recurrence&#10;$R_n=Q_n+M_nR_{n-1}$, $n\ge1$, where $R_0$ is arbitrary and $(Q_n,M_n)$,&#10;$n\ge1$, are i.i.d. copies of a two-dimensional random vector $(Q,M)$, and&#10;$(Q_n,M_n)$ is independent of $R_{n-1}$. It is well known that if $E{\ln}|M|&lt;0$&#10;and $E{\ln^+}|Q|&lt;\infty$, then the sequence $(R_n)$ converges in distribution&#10;to a random variable $R$ given by&#10;$R\stackrel{d}{=}\sum_{k=1}^{\infty}Q_k\prod_{j=1}^{k-1}M_j$, and usually&#10;referred to as perpetuity. In this paper we consider a situation in which the&#10;sequence $(R_n)$ itself does not converge. We assume that $E{\ln}|M|$ exists&#10;but that it is non-negative and we ask if in this situation the sequence&#10;$(R_n)$, after suitable normalization, converges in distribution to a&#10;non-degenerate limit." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="409" source="Florian Gach" target="Vladimir Spokoiny">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2807v2" />
          <attvalue for="2" value="Spatially Adaptive Density Estimation by Localised Haar Projections" />
          <attvalue for="3" value="Given a random sample from some unknown density $f_0: \mathbb R \to [0,&#10;\infty)$ we devise Haar wavelet estimators for $f_0$ with variable resolution&#10;levels constructed from localised test procedures (as in Lepski, Mammen, and&#10;Spokoiny (1997, Ann. Statist.)). We show that these estimators adapt to&#10;spatially heterogeneous smoothness of $f_0$, simultaneously for every point $x$&#10;in a fixed interval, in sup-norm loss. The thresholding constants involved in&#10;the test procedures can be chosen in practice under the idealised assumption&#10;that the true density is locally constant in a neighborhood of the point $x$ of&#10;estimation, and an information theoretic justification of this practice is&#10;given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="410" source="Vladimir Spokoiny" target="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="411" source="Vladimir Spokoiny" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="412" source="Karthik Bharath" target="Vladimir Pozdnyakov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3427v4" />
          <attvalue for="2" value="Asymptotics of the Empirical Cross-over Function" />
          <attvalue for="3" value="We consider a combination of heavily trimmed sums and sample quantiles which&#10;arises when examining properties of clustering criteria and prove limit&#10;theorems. The object of interest, which we call the Empirical Cross-over&#10;Function, is an L-statistic whose weights do not comply with the requisite&#10;regularity conditions for usage of ex- isting limit results. The law of large&#10;numbers, CLT and a functional CLT are proven." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="413" source="Karthik Bharath" target="Dipak Dey">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3427v4" />
          <attvalue for="2" value="Asymptotics of the Empirical Cross-over Function" />
          <attvalue for="3" value="We consider a combination of heavily trimmed sums and sample quantiles which&#10;arises when examining properties of clustering criteria and prove limit&#10;theorems. The object of interest, which we call the Empirical Cross-over&#10;Function, is an L-statistic whose weights do not comply with the requisite&#10;regularity conditions for usage of ex- isting limit results. The law of large&#10;numbers, CLT and a functional CLT are proven." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="414" source="Vladimir Pozdnyakov" target="Dipak Dey">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3427v4" />
          <attvalue for="2" value="Asymptotics of the Empirical Cross-over Function" />
          <attvalue for="3" value="We consider a combination of heavily trimmed sums and sample quantiles which&#10;arises when examining properties of clustering criteria and prove limit&#10;theorems. The object of interest, which we call the Empirical Cross-over&#10;Function, is an L-statistic whose weights do not comply with the requisite&#10;regularity conditions for usage of ex- isting limit results. The law of large&#10;numbers, CLT and a functional CLT are proven." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="415" source="Reman Abu-Shanab" target="John T. Kent">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3412v1" />
          <attvalue for="2" value="Shrinkage estimation with a matrix loss function" />
          <attvalue for="3" value="Consider estimating the n by p matrix of means of an n by p matrix of&#10;independent normally distributed observations with constant variance, where the&#10;performance of an estimator is judged using a p by p matrix quadratic error&#10;loss function. A matrix version of the James-Stein estimator is proposed,&#10;depending on a tuning constant. It is shown to dominate the usual maximum&#10;likelihood estimator for some choices of of the tuning constant when n is&#10;greater than or equal to 3. This result also extends to other shrinkage&#10;estimators and settings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="416" source="Reman Abu-Shanab" target="William E. Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3412v1" />
          <attvalue for="2" value="Shrinkage estimation with a matrix loss function" />
          <attvalue for="3" value="Consider estimating the n by p matrix of means of an n by p matrix of&#10;independent normally distributed observations with constant variance, where the&#10;performance of an estimator is judged using a p by p matrix quadratic error&#10;loss function. A matrix version of the James-Stein estimator is proposed,&#10;depending on a tuning constant. It is shown to dominate the usual maximum&#10;likelihood estimator for some choices of of the tuning constant when n is&#10;greater than or equal to 3. This result also extends to other shrinkage&#10;estimators and settings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="417" source="John T. Kent" target="William E. Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.3412v1" />
          <attvalue for="2" value="Shrinkage estimation with a matrix loss function" />
          <attvalue for="3" value="Consider estimating the n by p matrix of means of an n by p matrix of&#10;independent normally distributed observations with constant variance, where the&#10;performance of an estimator is judged using a p by p matrix quadratic error&#10;loss function. A matrix version of the James-Stein estimator is proposed,&#10;depending on a tuning constant. It is shown to dominate the usual maximum&#10;likelihood estimator for some choices of of the tuning constant when n is&#10;greater than or equal to 3. This result also extends to other shrinkage&#10;estimators and settings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="418" source="Hongyuan Cao" target="Michael R. Kosorok">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2046v2" />
          <attvalue for="2" value="Simultaneous critical values for $t$-tests in very high dimensions" />
          <attvalue for="3" value="This article considers the problem of multiple hypothesis testing using&#10;$t$-tests. The observed data are assumed to be independently generated&#10;conditional on an underlying and unknown two-state hidden model. We propose an&#10;asymptotically valid data-driven procedure to find critical values for&#10;rejection regions controlling the $k$-familywise error rate ($k$-FWER), false&#10;discovery rate (FDR) and the tail probability of false discovery proportion&#10;(FDTP) by using one-sample and two-sample $t$-statistics. We only require a&#10;finite fourth moment plus some very general conditions on the mean and variance&#10;of the population by virtue of the moderate deviations properties of&#10;$t$-statistics. A new consistent estimator for the proportion of alternative&#10;hypotheses is developed. Simulation studies support our theoretical results and&#10;demonstrate that the power of a multiple testing procedure can be substantially&#10;improved by using critical values directly, as opposed to the conventional&#10;$p$-value approach. Our method is applied in an analysis of the microarray data&#10;from a leukemia cancer study that involves testing a large number of hypotheses&#10;simultaneously." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="419" source="Shalosh B. Ekhad" target="Doron Zeilberger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.5531v1" />
          <attvalue for="2" value="Balls in Boxes: Variations on a Theme of Warren Ewens and Herbert Wilf" />
          <attvalue for="3" value="We comment on, elaborate, and extend the work of Warren Ewens and Herbert&#10;Wilf, described in their http://www.pnas.org/content/104/27/11189.full.pdf&#10;about the maximum in balls-and-boxes problem. In particular we meta-apply their&#10;ingenious method to show that it is not really needed, and that one is better&#10;off using the so-called Poisson Approximation, at least in applications to the&#10;real world, because extremely unlikely events mever happen in real life. This&#10;article is accompanied by the Maple package&#10;http://www.math.rutgers.edu/~zeilberg/tokhniot/BallsInBoxes&quot;&gt;BallsInBoxes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="420" source="Emilio Seijo" target="Bodhisattva Sen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.1320v1" />
          <attvalue for="2" value="A continuous mapping theorem for the smallest argmax functional" />
          <attvalue for="3" value="This paper introduces a version of the argmax continuous mapping theorem that&#10;applies to M-estimation problems in which the objective functions converge to a&#10;limiting process with multiple maximizers. The concept of the smallest&#10;maximizer of a function in the d-dimensional Skorohod space is introduced and&#10;its main properties are studied. The resulting continuous mapping theorem is&#10;applied to three problems arising in change-point regression analysis. Some of&#10;the results proved in connection to the d-dimensional Skorohod space are also&#10;of independent interest." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="421" source="Matthieu Lerasle" target="Daniel Y. Takahashi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2467v2" />
          <attvalue for="2" value="Sharp oracle inequalities and slope heuristic for specification&#10;  probabilities estimation in discrete random fields" />
          <attvalue for="3" value="We study the problem of estimating the one-point specification probabilities&#10;in non-necessary finite discrete random fields from partially observed&#10;independent samples. Our procedures are based on model selection by&#10;minimization of a penalized empirical criterion. The selected estimators&#10;satisfy sharp oracle inequalities in $L_2$-risk. We also obtain theoretical&#10;results on the slope heuristic for this problem, justifying the slope algorithm&#10;to calibrate the leading constant in the penalty. The practical performances of&#10;our methods are investigated in two simulation studies. We illustrate the&#10;usefulness of our approach by applying the methods to a multi-unit neuronal&#10;data from a rat hippocampus." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="422" source="Djalel Eddine Meskaldji" target="Dimitri Van De Ville">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="423" source="Djalel Eddine Meskaldji" target="Jean-Philippe Thiran">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="424" source="Djalel Eddine Meskaldji" target="Stephan Morgenthaler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="425" source="Dimitri Van De Ville" target="Jean-Philippe Thiran">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="426" source="Dimitri Van De Ville" target="Stephan Morgenthaler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="427" source="Jean-Philippe Thiran" target="Stephan Morgenthaler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4519v5" />
          <attvalue for="2" value="A comprehensive error rate for multiple testing" />
          <attvalue for="3" value="The higher criticism of a family of tests starts with the individual&#10;uncorrected p-values of each test. It then requires a procedure for deciding&#10;whether the collection of p-values indicates the presence of a real effect and&#10;if possible selects the ones that deserve closer scrutiny. This paper&#10;investigates procedures in which the ordered p-values are compared to an&#10;arbitrary positive and non-decreasing threshold sequence." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="428" source="Magda Peligrad" target="Hailin Sang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="429" source="Magda Peligrad" target="Yunda Zhong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="430" source="Hailin Sang" target="Yunda Zhong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.0537v3" />
          <attvalue for="2" value="Exact Moderate and Large Deviations for Linear Processes" />
          <attvalue for="3" value="Large and moderate deviation probabilities play an important role in many&#10;applied areas, such as insurance and risk analysis. This paper studies the&#10;exact moderate and large deviation asymptotics in non-logarithmic form for&#10;linear processes with independent innovations. The linear processes we analyze&#10;are general and therefore they include the long memory case. We give an&#10;asymptotic representation for probability of the tail of the normalized sums&#10;and specify the zones in which it can be approximated either by a standard&#10;normal distribution or by the marginal distribution of the innovation process.&#10;The results are then applied to regression estimates, moving averages,&#10;fractionally integrated processes, linear processes with regularly varying&#10;exponents and functions of linear processes. We also consider the computation&#10;of value at risk and expected shortfall, fundamental quantities in risk theory&#10;and finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="431" source="Antonio Cuevas" target="Ricardo Fraiman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.6239v2" />
          <attvalue for="2" value="On statistical properties of sets fulfilling rolling-type conditions" />
          <attvalue for="3" value="Motivated by set estimation problems, we consider three closely related shape&#10;conditions for compact sets: positive reach, r-convexity and rolling condition.&#10;First, the relations between these shape conditions are analyzed. Second, we&#10;obtain for the estimation of sets fulfilling a rolling condition a result of&#10;&quot;full consistency&quot; (i.e., consistency with respect to the Hausdorff metric for&#10;the target set and for its boundary). Third, the class of uniformly bounded&#10;compact sets whose reach is not smaller than a given constant r is shown to be&#10;a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in&#10;particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the&#10;r-convex hull of the sample is proved to be a fully consistent estimator of an&#10;r-convex support in the two-dimensional case. Moreover, its boundary length is&#10;shown to converge (a.s.) to that of the underlying support. Fifth, the above&#10;results are applied to get new consistency statements for level set estimators&#10;based on the excess mass methodology (Polonik, 1995)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="432" source="Antonio Cuevas" target="Beatriz Pateiro-López">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.6239v2" />
          <attvalue for="2" value="On statistical properties of sets fulfilling rolling-type conditions" />
          <attvalue for="3" value="Motivated by set estimation problems, we consider three closely related shape&#10;conditions for compact sets: positive reach, r-convexity and rolling condition.&#10;First, the relations between these shape conditions are analyzed. Second, we&#10;obtain for the estimation of sets fulfilling a rolling condition a result of&#10;&quot;full consistency&quot; (i.e., consistency with respect to the Hausdorff metric for&#10;the target set and for its boundary). Third, the class of uniformly bounded&#10;compact sets whose reach is not smaller than a given constant r is shown to be&#10;a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in&#10;particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the&#10;r-convex hull of the sample is proved to be a fully consistent estimator of an&#10;r-convex support in the two-dimensional case. Moreover, its boundary length is&#10;shown to converge (a.s.) to that of the underlying support. Fifth, the above&#10;results are applied to get new consistency statements for level set estimators&#10;based on the excess mass methodology (Polonik, 1995)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="433" source="Ricardo Fraiman" target="Beatriz Pateiro-López">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.6239v2" />
          <attvalue for="2" value="On statistical properties of sets fulfilling rolling-type conditions" />
          <attvalue for="3" value="Motivated by set estimation problems, we consider three closely related shape&#10;conditions for compact sets: positive reach, r-convexity and rolling condition.&#10;First, the relations between these shape conditions are analyzed. Second, we&#10;obtain for the estimation of sets fulfilling a rolling condition a result of&#10;&quot;full consistency&quot; (i.e., consistency with respect to the Hausdorff metric for&#10;the target set and for its boundary). Third, the class of uniformly bounded&#10;compact sets whose reach is not smaller than a given constant r is shown to be&#10;a P-uniformity class (in Billingsley and Topsoe's (1967) sense) and, in&#10;particular, a Glivenko-Cantelli class. Fourth, under broad conditions, the&#10;r-convex hull of the sample is proved to be a fully consistent estimator of an&#10;r-convex support in the two-dimensional case. Moreover, its boundary length is&#10;shown to converge (a.s.) to that of the underlying support. Fifth, the above&#10;results are applied to get new consistency statements for level set estimators&#10;based on the excess mass methodology (Polonik, 1995)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="434" source="Peter Brockwell" target="Vincenzo Ferrazzano">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4468v4" />
          <attvalue for="2" value="High-frequency sampling and kernel estimation for continuous-time moving&#10;  average processes" />
          <attvalue for="3" value="Interest in continuous-time processes has increased rapidly in recent years,&#10;largely because of high-frequency data available in many applications. We&#10;develop a method for estimating the kernel function $g$ of a second-order&#10;stationary L\'evy-driven continuous-time moving average (CMA) process $Y$ based&#10;on observations of the discrete-time process $Y^\Delta$ obtained by sampling&#10;$Y$ at $\Delta, 2\Delta,...,n\Delta$ for small $\Delta$. We approximate $g$ by&#10;$g^\Delta$ based on the Wold representation and prove its pointwise convergence&#10;to $g$ as $\Delta\rightarrow 0$ for $\CARMA(p,q)$ processes. Two non-parametric&#10;estimators of $g^\Delta$, based on the innovations algorithm and the&#10;Durbin-Levinson algorithm, are proposed to estimate $g$. For a Gaussian CARMA&#10;process we give conditions on the sample size $n$ and the grid-spacing&#10;$\Delta(n)$ under which the innovations estimator is consistent and&#10;asymptotically normal as $n\rightarrow\infty$. The estimators can be calculated&#10;from sampled observations of {\it any} CMA process and simulations suggest that&#10;they perform well even outside the class of CARMA processes. We illustrate&#10;their performance for simulated data and apply them to the Brookhaven turbulent&#10;wind speed data. Finally we extend results of \citet{bfk:2011:1} for sampled&#10;CARMA processes to a much wider class of CMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="435" source="Tilmann Gneiting" target="Roopesh Ranjan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.1638v1" />
          <attvalue for="2" value="Combining Predictive Distributions" />
          <attvalue for="3" value="Predictive distributions need to be aggregated when probabilistic forecasts&#10;are merged, or when expert opinions expressed in terms of probability&#10;distributions are fused. We take a prediction space approach that applies to&#10;discrete, mixed discrete-continuous and continuous predictive distributions&#10;alike, and study combination formulas for cumulative distribution functions&#10;from the perspectives of coherence, probabilistic and conditional calibration,&#10;and dispersion. Both linear and non-linear aggregation methods are&#10;investigated, including generalized, spread-adjusted and beta-transformed&#10;linear pools. The effects and techniques are demonstrated theoretically, in&#10;simulation examples, and in case studies on density forecasts for S&amp;P 500&#10;returns and daily maximum temperature at Seattle-Tacoma Airport." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="436" source="Tilmann Gneiting" target="Werner Ehm">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5031v2" />
          <attvalue for="2" value="Local proper scoring rules of order two" />
          <attvalue for="3" value="Scoring rules assess the quality of probabilistic forecasts, by assigning a&#10;numerical score based on the predictive distribution and on the event or value&#10;that materializes. A scoring rule is proper if it encourages truthful&#10;reporting. It is local of order $k$ if the score depends on the predictive&#10;density only through its value and the values of its derivatives of order up to&#10;$k$ at the realizing event. Complementing fundamental recent work by Parry,&#10;Dawid and Lauritzen, we characterize the local proper scoring rules of order 2&#10;relative to a broad class of Lebesgue densities on the real line, using a&#10;different approach. In a data example, we use local and nonlocal proper scoring&#10;rules to assess statistically postprocessed ensemble weather forecasts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="437" source="Yutao Ma" target="Shi Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="438" source="Yutao Ma" target="Xinyu Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="439" source="Yutao Ma" target="Liming Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="440" source="Shi Shen" target="Xinyu Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="441" source="Shi Shen" target="Liming Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="442" source="Xinyu Wang" target="Liming Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2297v1" />
          <attvalue for="2" value="Transportation inequalities: From Poisson to Gibbs measures" />
          <attvalue for="3" value="We establish an optimal transportation inequality for the Poisson measure on&#10;the configuration space. Furthermore, under the Dobrushin uniqueness condition,&#10;we obtain a sharp transportation inequality for the Gibbs measure on&#10;$\mathbb{N}^{\Lambda}$ or the continuum Gibbs measure on the configuration&#10;space." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="443" source="Paul Kabaila" target="Matthew Vicendese">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.5461v2" />
          <attvalue for="2" value="The performance of a two-stage analysis of ABAB/BABA crossover trials" />
          <attvalue for="3" value="Freeman has considered the following two-stage procedure for finding a&#10;confidence interval for the treatment difference theta, using data from an&#10;AB/BA crossover trial. In the first stage, a preliminary test of the null&#10;hypothesis that the differential carryover is zero, is carried out. If this&#10;hypothesis is accepted then the confidence interval for theta is constructed&#10;assuming that the differential carryover is zero. If, on the other hand, this&#10;hypothesis is rejected then this confidence interval is constructed using only&#10;data from the first period. Freeman has shown that this confidence interval has&#10;minimum coverage probability far below nominal. He therefore concludes that&#10;this confidence interval should not be used. In the present paper, we analyse&#10;the performance of a similar two-stage procedure for an ABAB/BABA crossover&#10;trial. This trial differs in very significant ways from an AB/BA crossover&#10;trial, including the fact that for an ABAB/BABA crossover trial there is an&#10;unbiased estimator of the differential carryover that is unaffected by&#10;between-subject variation. Despite these great differences, we arrive at the&#10;same conclusion as Freeman. Namely, that the confidence interval resulting from&#10;the two-stage procedure should not be used." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="444" source="Karim Benhenni" target="David Degras">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.4058v1" />
          <attvalue for="2" value="Local Polynomial Regression Based on Functional Data" />
          <attvalue for="3" value="Suppose that $n$ statistical units are observed, each following the model&#10;$Y(x_j)=m(x_j)+ \epsilon(x_j),\, j=1,...,N,$ where $m$ is a regression&#10;function, $0 \leq x_1 &lt;...&lt;x_N \leq 1$ are observation times spaced according&#10;to a sampling density $f$, and $\epsilon$ is a continuous-time error process&#10;having mean zero and regular covariance function. Considering the local&#10;polynomial estimation of $m$ and its derivatives, we derive asymptotic&#10;expressions for the bias and variance as $n,N\to\infty$. Such results are&#10;particularly relevant in the context of functional data where essential&#10;information is contained in the derivatives. Based on these results, we deduce&#10;optimal sampling densities, optimal bandwidths and asymptotic normality of the&#10;estimator. Simulations are conducted in order to compare the performances of&#10;local polynomial estimators based on exact optimal bandwidths, asymptotic&#10;optimal bandwidths, and cross-validated bandwidths." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="445" source="Iryna Kyrychynska" target="Ostap Okhrin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1513v1" />
          <attvalue for="2" value="Asymptotic behaviour of the S-stopped branching processes with countable&#10;  state space" />
          <attvalue for="3" value="he starting process with countable number of types \mu(t) generates a stopped&#10;branching process \xi(t). The starting process stops, by falling into the&#10;nonempty set S. It is assumed, that the starting process is subcritical,&#10;indecomposable and noncyclic. It is proved, that the extinction probability&#10;converges to the cyclic function with period 1." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="446" source="Iryna Kyrychynska" target="Yaroslav Yeleyko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1513v1" />
          <attvalue for="2" value="Asymptotic behaviour of the S-stopped branching processes with countable&#10;  state space" />
          <attvalue for="3" value="he starting process with countable number of types \mu(t) generates a stopped&#10;branching process \xi(t). The starting process stops, by falling into the&#10;nonempty set S. It is assumed, that the starting process is subcritical,&#10;indecomposable and noncyclic. It is proved, that the extinction probability&#10;converges to the cyclic function with period 1." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="447" source="Ostap Okhrin" target="Yaroslav Yeleyko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1513v1" />
          <attvalue for="2" value="Asymptotic behaviour of the S-stopped branching processes with countable&#10;  state space" />
          <attvalue for="3" value="he starting process with countable number of types \mu(t) generates a stopped&#10;branching process \xi(t). The starting process stops, by falling into the&#10;nonempty set S. It is assumed, that the starting process is subcritical,&#10;indecomposable and noncyclic. It is proved, that the extinction probability&#10;converges to the cyclic function with period 1." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="448" source="David Azriel" target="Micha Mandel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4915v3" />
          <attvalue for="2" value="Are adaptive allocation designs beneficial for improving power in binary&#10;  response trials?" />
          <attvalue for="3" value="We consider the classical problem of selecting the best of two treatments in&#10;clinical trials with binary response. The target is to find the design that&#10;maximizes the power of the relevant test. Many papers use a normal&#10;approximation to the power function and claim that Neyman allocation that&#10;assigns subjects to treatment groups according to the ratio of the responses'&#10;standard deviations, should be used. As the standard deviations are unknown, an&#10;adaptive design is often recommended. The asymptotic justification of this&#10;approach is arguable, since it uses the normal approximation in tails where the&#10;error in the approximation is larger than the estimated quantity. We consider&#10;two different approaches for optimality of designs that are related to Pitman&#10;and Bahadur definitions of relative efficiency of tests. We prove that the&#10;optimal allocation according to the Pitman criterion is the balanced allocation&#10;and that the optimal allocation according to the Bahadur approach depends on&#10;the unknown parameters. Exact calculations reveal that the optimal allocation&#10;according to Bahadur is often close to the balanced design, and the powers of&#10;both are comparable to the Neyman allocation for small sample sizes and are&#10;generally better for large experiments. Our findings have important&#10;implications to the design of experiments, as the balanced design is proved to&#10;be optimal or close to optimal and the need for the complications involved in&#10;following an adaptive design for the purpose of increasing the power of tests&#10;is therefore questionable." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="449" source="David Azriel" target="Yosef Rinott">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4915v3" />
          <attvalue for="2" value="Are adaptive allocation designs beneficial for improving power in binary&#10;  response trials?" />
          <attvalue for="3" value="We consider the classical problem of selecting the best of two treatments in&#10;clinical trials with binary response. The target is to find the design that&#10;maximizes the power of the relevant test. Many papers use a normal&#10;approximation to the power function and claim that Neyman allocation that&#10;assigns subjects to treatment groups according to the ratio of the responses'&#10;standard deviations, should be used. As the standard deviations are unknown, an&#10;adaptive design is often recommended. The asymptotic justification of this&#10;approach is arguable, since it uses the normal approximation in tails where the&#10;error in the approximation is larger than the estimated quantity. We consider&#10;two different approaches for optimality of designs that are related to Pitman&#10;and Bahadur definitions of relative efficiency of tests. We prove that the&#10;optimal allocation according to the Pitman criterion is the balanced allocation&#10;and that the optimal allocation according to the Bahadur approach depends on&#10;the unknown parameters. Exact calculations reveal that the optimal allocation&#10;according to Bahadur is often close to the balanced design, and the powers of&#10;both are comparable to the Neyman allocation for small sample sizes and are&#10;generally better for large experiments. Our findings have important&#10;implications to the design of experiments, as the balanced design is proved to&#10;be optimal or close to optimal and the need for the complications involved in&#10;following an adaptive design for the purpose of increasing the power of tests&#10;is therefore questionable." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="450" source="Micha Mandel" target="Yosef Rinott">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4915v3" />
          <attvalue for="2" value="Are adaptive allocation designs beneficial for improving power in binary&#10;  response trials?" />
          <attvalue for="3" value="We consider the classical problem of selecting the best of two treatments in&#10;clinical trials with binary response. The target is to find the design that&#10;maximizes the power of the relevant test. Many papers use a normal&#10;approximation to the power function and claim that Neyman allocation that&#10;assigns subjects to treatment groups according to the ratio of the responses'&#10;standard deviations, should be used. As the standard deviations are unknown, an&#10;adaptive design is often recommended. The asymptotic justification of this&#10;approach is arguable, since it uses the normal approximation in tails where the&#10;error in the approximation is larger than the estimated quantity. We consider&#10;two different approaches for optimality of designs that are related to Pitman&#10;and Bahadur definitions of relative efficiency of tests. We prove that the&#10;optimal allocation according to the Pitman criterion is the balanced allocation&#10;and that the optimal allocation according to the Bahadur approach depends on&#10;the unknown parameters. Exact calculations reveal that the optimal allocation&#10;according to Bahadur is often close to the balanced design, and the powers of&#10;both are comparable to the Neyman allocation for small sample sizes and are&#10;generally better for large experiments. Our findings have important&#10;implications to the design of experiments, as the balanced design is proved to&#10;be optimal or close to optimal and the need for the complications involved in&#10;following an adaptive design for the purpose of increasing the power of tests&#10;is therefore questionable." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="451" source="Odalric-Ambrym Maillard" target="Rémi Munos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5820v1" />
          <attvalue for="2" value="A Finite-Time Analysis of Multi-armed Bandits Problems with&#10;  Kullback-Leibler Divergences" />
          <attvalue for="3" value="We consider a Kullback-Leibler-based algorithm for the stochastic multi-armed&#10;bandit problem in the case of distributions with finite supports (not&#10;necessarily known beforehand), whose asymptotic regret matches the lower bound&#10;of \cite{Burnetas96}. Our contribution is to provide a finite-time analysis of&#10;this algorithm; we get bounds whose main terms are smaller than the ones of&#10;previously known algorithms with finite-time analyses (like UCB-type&#10;algorithms)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="452" source="Daniel Bruynooghe" target="Henry P. Wynn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2118v1" />
          <attvalue for="2" value="Differential cumulants, hierachical models and monomial ideals" />
          <attvalue for="3" value="For a joint probability density function f(x) of a random vector X the mixed&#10;partial derivatives of log f(x) can be interpreted as limiting cumulants in an&#10;infinitesimally small open neighborhood around x. Moreover, setting them to&#10;zero everywhere gives independence and conditional independence conditions. The&#10;latter conditions can be mapped, using an algebraic differential duality, into&#10;monomial ideal conditions. This provides an isomorphism between hierarchical&#10;models and monomial ideals. It is thus shown that certain monomial ideals are&#10;associated with particular classes of hierarchical models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="453" source="G. Jogesh Babu" target="Zhidong Bai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="454" source="G. Jogesh Babu" target="Kwok Pui Choi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="455" source="G. Jogesh Babu" target="Vasudevan Mangalam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="456" source="Zhidong Bai" target="Kwok Pui Choi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="457" source="Zhidong Bai" target="Vasudevan Mangalam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="458" source="Kwok Pui Choi" target="Vasudevan Mangalam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4396v1" />
          <attvalue for="2" value="Limit theorems for functions of marginal quantiles" />
          <attvalue for="3" value="Multivariate distributions are explored using the joint distributions of&#10;marginal sample quantiles. Limit theory for the mean of a function of order&#10;statistics is presented. The results include a multivariate central limit&#10;theorem and a strong law of large numbers. A result similar to Bahadur's&#10;representation of quantiles is established for the mean of a function of the&#10;marginal quantiles. In particular, it is shown that&#10;\[\sqrt{n}\Biggl(\frac{1}{n}\sum_{i=1}^n\phi\bigl(X_{n:i}^{(1)},...,X_{n:i}^{(d)}\bigr)-\bar{\gamma}\Biggr)=\frac{1}{\sqrt{n}}\sum_{i=1}^nZ_{n,i}+\mathrm{o}_P(1)\]&#10;as $n\rightarrow\infty$, where $\bar{\gamma}$ is a constant and $Z_{n,i}$ are&#10;i.i.d. random variables for each $n$. This leads to the central limit theorem.&#10;Weak convergence to a Gaussian process using equicontinuity of functions is&#10;indicated. The results are established under very general conditions. These&#10;conditions are shown to be satisfied in many commonly occurring situations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="459" source="Theofanis Sapatinas" target="Irina A. Suslina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3442v3" />
          <attvalue for="2" value="Minimax nonparametric testing in a problem related to the Radon&#10;  transform" />
          <attvalue for="3" value="We consider the detection problem of a two-dimensional function from noisy&#10;observations of its integrals over lines. We study both rate and sharp&#10;asymptotics for the error probabilities in the minimax setup. By construction,&#10;the derived tests are non-adaptive. We also construct a minimax rate-optimal&#10;adaptive test of rather simple structure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="460" source="Theofanis Sapatinas" target="Marianna Pensky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2298v1" />
          <attvalue for="2" value="Multichannel Boxcar Deconvolution with Growing Number of Channels" />
          <attvalue for="3" value="We consider the problem of estimating the unknown response function in the&#10;multichannel deconvolution model with a boxcar-like kernel which is of&#10;particular interest in signal processing. It is known that, when the number of&#10;channels is finite, the precision of reconstruction of the response function&#10;increases as the number of channels $M$ grow (even when the total number of&#10;observations $n$ for all channels $M$ remains constant) and this requires that&#10;the parameter of the channels form a Badly Approximable $M$-tuple.&#10;  Recent advances in data collection and recording techniques made it of urgent&#10;interest to study the case when the number of channels $M=M_n$ grow with the&#10;total number of observations $n$. However, in real-life situations, the number&#10;of channels $M = M_n$ usually refers to the number of physical devices and,&#10;consequently, may grow to infinity only at a slow rate as $n \rightarrow&#10;\infty$. When $M=M_n$ grows slowly as $n$ increases, we develop a procedure for&#10;the construction of a Badly Approximable $M$-tuple on a specified interval, of&#10;a non-asymptotic length, together with a lower bound associated with this&#10;$M$-tuple, which explicitly shows its dependence on $M$ as $M$ is growing. This&#10;result is further used for the evaluation of the $L^2$-risk of the suggested&#10;adaptive wavelet thresholding estimator of the unknown response function and,&#10;furthermore, for the choice of the optimal number of channels $M$ which&#10;minimizes the $L^2$-risk." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="461" source="Theofanis Sapatinas" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="YES?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="462" source="Xiaolong Luo" target="Gongjun Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2790v2" />
          <attvalue for="2" value="Sequential Analysis of Cox Model under Response Dependent Allocation" />
          <attvalue for="3" value="Sellke and Siegmund (1983) developed the Brownian approximation to the Cox&#10;partial likelihood score as a process of calendar time, laying the foundation&#10;for group sequential analysis of survival studies. We extend their results to&#10;cover situations in which treatment allocations may depend on observed&#10;outcomes. The new development makes use of the entry time and calendar time&#10;along with the corresponding $\sigma$-filtrations to handle the natural&#10;information accumulation. Large sample properties are established under&#10;suitable regularity conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="463" source="Xiaolong Luo" target="Zhiliang Ying">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2790v2" />
          <attvalue for="2" value="Sequential Analysis of Cox Model under Response Dependent Allocation" />
          <attvalue for="3" value="Sellke and Siegmund (1983) developed the Brownian approximation to the Cox&#10;partial likelihood score as a process of calendar time, laying the foundation&#10;for group sequential analysis of survival studies. We extend their results to&#10;cover situations in which treatment allocations may depend on observed&#10;outcomes. The new development makes use of the entry time and calendar time&#10;along with the corresponding $\sigma$-filtrations to handle the natural&#10;information accumulation. Large sample properties are established under&#10;suitable regularity conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="464" source="Gongjun Xu" target="Zhiliang Ying">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2790v2" />
          <attvalue for="2" value="Sequential Analysis of Cox Model under Response Dependent Allocation" />
          <attvalue for="3" value="Sellke and Siegmund (1983) developed the Brownian approximation to the Cox&#10;partial likelihood score as a process of calendar time, laying the foundation&#10;for group sequential analysis of survival studies. We extend their results to&#10;cover situations in which treatment allocations may depend on observed&#10;outcomes. The new development makes use of the entry time and calendar time&#10;along with the corresponding $\sigma$-filtrations to handle the natural&#10;information accumulation. Large sample properties are established under&#10;suitable regularity conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="465" source="B. G. Manjunath" target="K. R. Parthasarathy">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1647v3" />
          <attvalue for="2" value="A note on gaussian distributions in R^n" />
          <attvalue for="3" value="Given any finite set F of (n - 1)-dimensional subspaces of R^n we give&#10;examples of nongaussian probability measures in R^n whose marginal distribution&#10;in each subspace from F is gaussian. However, if F is an infinite family of&#10;such (n - 1)-dimensional subspaces then such a nongaussian probability measure&#10;in R^n does not exist." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="466" source="Mingyuan Zhang" target="Marshall M. Joffe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1472v1" />
          <attvalue for="2" value="Causal inference for continuous-time processes when covariates are&#10;  observed only at discrete times" />
          <attvalue for="3" value="Most of the work on the structural nested model and g-estimation for causal&#10;inference in longitudinal data assumes a discrete-time underlying data&#10;generating process. However, in some observational studies, it is more&#10;reasonable to assume that the data are generated from a continuous-time process&#10;and are only observable at discrete time points. When these circumstances&#10;arise, the sequential randomization assumption in the observed discrete-time&#10;data, which is essential in justifying discrete-time g-estimation, may not be&#10;reasonable. Under a deterministic model, we discuss other useful assumptions&#10;that guarantee the consistency of discrete-time g-estimation. In more general&#10;cases, when those assumptions are violated, we propose a controlling-the-future&#10;method that performs at least as well as g-estimation in most scenarios and&#10;which provides consistent estimation in some cases where g-estimation is&#10;severely inconsistent. We apply the methods discussed in this paper to&#10;simulated data, as well as to a data set collected following a massive flood in&#10;Bangladesh, estimating the effect of diarrhea on children's height. Results&#10;from different methods are compared in both simulation and the real&#10;application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="467" source="Mingyuan Zhang" target="Dylan S. Small">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1472v1" />
          <attvalue for="2" value="Causal inference for continuous-time processes when covariates are&#10;  observed only at discrete times" />
          <attvalue for="3" value="Most of the work on the structural nested model and g-estimation for causal&#10;inference in longitudinal data assumes a discrete-time underlying data&#10;generating process. However, in some observational studies, it is more&#10;reasonable to assume that the data are generated from a continuous-time process&#10;and are only observable at discrete time points. When these circumstances&#10;arise, the sequential randomization assumption in the observed discrete-time&#10;data, which is essential in justifying discrete-time g-estimation, may not be&#10;reasonable. Under a deterministic model, we discuss other useful assumptions&#10;that guarantee the consistency of discrete-time g-estimation. In more general&#10;cases, when those assumptions are violated, we propose a controlling-the-future&#10;method that performs at least as well as g-estimation in most scenarios and&#10;which provides consistent estimation in some cases where g-estimation is&#10;severely inconsistent. We apply the methods discussed in this paper to&#10;simulated data, as well as to a data set collected following a massive flood in&#10;Bangladesh, estimating the effect of diarrhea on children's height. Results&#10;from different methods are compared in both simulation and the real&#10;application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="468" source="Marshall M. Joffe" target="Dylan S. Small">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1472v1" />
          <attvalue for="2" value="Causal inference for continuous-time processes when covariates are&#10;  observed only at discrete times" />
          <attvalue for="3" value="Most of the work on the structural nested model and g-estimation for causal&#10;inference in longitudinal data assumes a discrete-time underlying data&#10;generating process. However, in some observational studies, it is more&#10;reasonable to assume that the data are generated from a continuous-time process&#10;and are only observable at discrete time points. When these circumstances&#10;arise, the sequential randomization assumption in the observed discrete-time&#10;data, which is essential in justifying discrete-time g-estimation, may not be&#10;reasonable. Under a deterministic model, we discuss other useful assumptions&#10;that guarantee the consistency of discrete-time g-estimation. In more general&#10;cases, when those assumptions are violated, we propose a controlling-the-future&#10;method that performs at least as well as g-estimation in most scenarios and&#10;which provides consistent estimation in some cases where g-estimation is&#10;severely inconsistent. We apply the methods discussed in this paper to&#10;simulated data, as well as to a data set collected following a massive flood in&#10;Bangladesh, estimating the effect of diarrhea on children's height. Results&#10;from different methods are compared in both simulation and the real&#10;application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="469" source="Xinyu Zhang" target="Hua Liang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1480v1" />
          <attvalue for="2" value="Focused information criterion and model averaging for generalized&#10;  additive partial linear models" />
          <attvalue for="3" value="We study model selection and model averaging in generalized additive partial&#10;linear models (GAPLMs). Polynomial spline is used to approximate nonparametric&#10;functions. The corresponding estimators of the linear parameters are shown to&#10;be asymptotically normal. We then develop a focused information criterion (FIC)&#10;and a frequentist model average (FMA) estimator on the basis of the&#10;quasi-likelihood principle and examine theoretical properties of the FIC and&#10;FMA. The major advantages of the proposed procedures over the existing ones are&#10;their computational expediency and theoretical reliability. Simulation&#10;experiments have provided evidence of the superiority of the proposed&#10;procedures. The approach is further applied to a real-world data example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="470" source="Hua Liang" target="Li Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="471" source="Hua Liang" target="Xiang Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="472" source="Hua Liang" target="Raymond J. Carroll">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="473" source="Philipp Arbenz" target="Paul Embrechts">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2920v1" />
          <attvalue for="2" value="The AEP algorithm for the fast computation of the distribution of the&#10;  sum of dependent random variables" />
          <attvalue for="3" value="We propose a new algorithm to compute numerically the distribution function&#10;of the sum of $d$ dependent, non-negative random variables with given joint&#10;distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="474" source="Philipp Arbenz" target="Giovanni Puccetti">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2920v1" />
          <attvalue for="2" value="The AEP algorithm for the fast computation of the distribution of the&#10;  sum of dependent random variables" />
          <attvalue for="3" value="We propose a new algorithm to compute numerically the distribution function&#10;of the sum of $d$ dependent, non-negative random variables with given joint&#10;distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="475" source="Paul Embrechts" target="Giovanni Puccetti">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.2920v1" />
          <attvalue for="2" value="The AEP algorithm for the fast computation of the distribution of the&#10;  sum of dependent random variables" />
          <attvalue for="3" value="We propose a new algorithm to compute numerically the distribution function&#10;of the sum of $d$ dependent, non-negative random variables with given joint&#10;distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="476" source="Alexandre Tsybakov" target="Christiern Rose">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.2454v5" />
          <attvalue for="2" value="High-dimensional instrumental variables regression and confidence sets" />
          <attvalue for="3" value="This article considers inference in linear models with K regressors, some or&#10;many could be endogenous, and L instruments. L can range from less than K to&#10;any order smaller than an exponential in the sample size and K is arbitrary.&#10;For moderate K, identification robust confidence sets are obtained by solving a&#10;hierarchy of semidefinite programs. For larger K, we propose the STIV&#10;estimator. The analysis of its error uses sensitivity characteristics which are&#10;sharper than those in the literature on sparsity. Data-driven bounds on them&#10;and robust confidence sets are obtained by solving K linear programs. Results&#10;on rates of convergence, variable selection, and confidence sets which &quot;adapt&quot;&#10;to the sparsity are given. We generalize our approach to models with&#10;approximation errors, systems, endogenous instruments, and two-stage for&#10;confidence bands for vectors of linear functionals and functions. The&#10;application is to a demand system with many endogenous regressors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="477" source="Luai Al Labadi" target="Mahmoud Zarepour">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.5261v3" />
          <attvalue for="2" value="The Dirichlet Process with Large Concentration Parameter" />
          <attvalue for="3" value="Ferguson's Dirichlet process plays an important role in nonparametric&#10;Bayesian inference. Let $P_a$ be the Dirichlet process in $\mathbb{R}$ with a&#10;base probability measure $H$ and a concentration parameter $a&gt;0.$ In this&#10;paper, we show that $\sqrt {a} \big(P_a((-\infty,t]) -H((-\infty,t])\big)$&#10;converges to a certain Brownian bridge as $a \to \infty.$ We also derive a&#10;certain Glivenko-Cantelli theorem for the Dirichlet process. Using the&#10;functional delta method, the weak convergence of the quantile process is also&#10;obtained. A large concentration parameter occurs when a statistician puts too&#10;much emphasize on his/her prior guess. This scenario also happens when the&#10;sample size is large and the posterior is used to make inference." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="478" source="Abdelhakim Necir" target="Ričardas Zitikis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.6031v1" />
          <attvalue for="2" value="Coupled risk measures and their empirical estimation when losses follow&#10;  heavy-tailed distributions" />
          <attvalue for="3" value="Considerable literature has been devoted to developing statistical&#10;inferential results for risk measures, especially for those that are of the&#10;form of L-functionals. However, practical and theoretical considerations have&#10;highlighted quite a number of risk measures that are of the form of ratios, or&#10;even more complex combinations, of two L-functionals. In the present paper we&#10;call such combinations `coupled risk measures' and develop a statistical&#10;inferential theory for them when losses follow heavy-tailed distributions. Our&#10;theory implies -at a stroke- statistical inferential results for absolute and&#10;relative distortion risk measures, weighted premium calculation principles, as&#10;well as for many indices of economic inequality that have appeared in the&#10;econometric literature." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="479" source="Mehdi Fhima" target="Arnaud Guillin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.4029v1" />
          <attvalue for="2" value="Fast change point analysis on the Hurst index of piecewise fractional&#10;  Brownian motion" />
          <attvalue for="3" value="In this presentation, we introduce a new method for change point analysis on&#10;the Hurst index for a piecewise fractional Brownian motion. We first set the&#10;model and the statistical problem. The proposed method is a transposition of&#10;the FDpV (Filtered Derivative with p-value) method introduced for the detection&#10;of change points on the mean in Bertrand et al. (2011) to the case of changes&#10;on the Hurst index. The underlying statistics of the FDpV technology is a new&#10;statistic estimator for Hurst index, so-called Increment Bernoulli Statistic&#10;(IBS). Both FDpV and IBS are methods with linear time and memory complexity,&#10;with respect to the size of the series. Thus the resulting method for change&#10;point analysis on Hurst index reaches also a linear complexity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="480" source="Mehdi Fhima" target="Pierre R. Bertrand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.4029v1" />
          <attvalue for="2" value="Fast change point analysis on the Hurst index of piecewise fractional&#10;  Brownian motion" />
          <attvalue for="3" value="In this presentation, we introduce a new method for change point analysis on&#10;the Hurst index for a piecewise fractional Brownian motion. We first set the&#10;model and the statistical problem. The proposed method is a transposition of&#10;the FDpV (Filtered Derivative with p-value) method introduced for the detection&#10;of change points on the mean in Bertrand et al. (2011) to the case of changes&#10;on the Hurst index. The underlying statistics of the FDpV technology is a new&#10;statistic estimator for Hurst index, so-called Increment Bernoulli Statistic&#10;(IBS). Both FDpV and IBS are methods with linear time and memory complexity,&#10;with respect to the size of the series. Thus the resulting method for change&#10;point analysis on Hurst index reaches also a linear complexity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="481" source="Arnaud Guillin" target="Pierre R. Bertrand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.4029v1" />
          <attvalue for="2" value="Fast change point analysis on the Hurst index of piecewise fractional&#10;  Brownian motion" />
          <attvalue for="3" value="In this presentation, we introduce a new method for change point analysis on&#10;the Hurst index for a piecewise fractional Brownian motion. We first set the&#10;model and the statistical problem. The proposed method is a transposition of&#10;the FDpV (Filtered Derivative with p-value) method introduced for the detection&#10;of change points on the mean in Bertrand et al. (2011) to the case of changes&#10;on the Hurst index. The underlying statistics of the FDpV technology is a new&#10;statistic estimator for Hurst index, so-called Increment Bernoulli Statistic&#10;(IBS). Both FDpV and IBS are methods with linear time and memory complexity,&#10;with respect to the size of the series. Thus the resulting method for change&#10;point analysis on Hurst index reaches also a linear complexity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="482" source="Benoîte de Saporta" target="Anne Gégout-Petit">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3745v1" />
          <attvalue for="2" value="Asymmetry tests for Bifurcating Auto-Regressive Processes with missing&#10;  data" />
          <attvalue for="3" value="We present symmetry tests for bifurcating autoregressive processes (BAR) when&#10;some data are missing. BAR processes typically model cell division data. Each&#10;cell can be of one of two types \emph{odd} or \emph{even}. The goal of this&#10;paper is to study the possible asymmetry between odd and even cells in a single&#10;observed lineage. We first derive asymmetry tests for the lineage itself,&#10;modeled by a two-type Galton-Watson process, and then derive tests for the&#10;observed BAR process. We present applications on both simulated and real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="483" source="Benoîte de Saporta" target="Laurence Marsalle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3745v1" />
          <attvalue for="2" value="Asymmetry tests for Bifurcating Auto-Regressive Processes with missing&#10;  data" />
          <attvalue for="3" value="We present symmetry tests for bifurcating autoregressive processes (BAR) when&#10;some data are missing. BAR processes typically model cell division data. Each&#10;cell can be of one of two types \emph{odd} or \emph{even}. The goal of this&#10;paper is to study the possible asymmetry between odd and even cells in a single&#10;observed lineage. We first derive asymmetry tests for the lineage itself,&#10;modeled by a two-type Galton-Watson process, and then derive tests for the&#10;observed BAR process. We present applications on both simulated and real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="484" source="Anne Gégout-Petit" target="Laurence Marsalle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3745v1" />
          <attvalue for="2" value="Asymmetry tests for Bifurcating Auto-Regressive Processes with missing&#10;  data" />
          <attvalue for="3" value="We present symmetry tests for bifurcating autoregressive processes (BAR) when&#10;some data are missing. BAR processes typically model cell division data. Each&#10;cell can be of one of two types \emph{odd} or \emph{even}. The goal of this&#10;paper is to study the possible asymmetry between odd and even cells in a single&#10;observed lineage. We first derive asymmetry tests for the lineage itself,&#10;modeled by a two-type Galton-Watson process, and then derive tests for the&#10;observed BAR process. We present applications on both simulated and real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="485" source="Zuofeng Shang" target="Murray K. Clayton">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0826v2" />
          <attvalue for="2" value="Consistency of Bayesian Linear Model Selection With a Growing Number of&#10;  Parameters" />
          <attvalue for="3" value="Linear models with a growing number of parameters have been widely used in&#10;modern statistics. One important problem about this kind of model is the&#10;variable selection issue. Bayesian approaches, which provide a stochastic&#10;search of informative variables, have gained popularity. In this paper, we will&#10;study the asymptotic properties related to Bayesian model selection when the&#10;model dimension $p$ is growing with the sample size $n$. We consider $p\le n$&#10;and provide sufficient conditions under which: (1) with large probability, the&#10;posterior probability of the true model (from which samples are drawn)&#10;uniformly dominates the posterior probability of any incorrect models; and (2)&#10;with large probability, the posterior probability of the true model converges&#10;to one. Both (1) and (2) guarantee that the true model will be selected under a&#10;Bayesian framework. We also demonstrate several situations when (1) holds but&#10;(2) fails, which illustrates the difference between these two properties.&#10;Simulated examples are provided to illustrate the main results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="486" source="Mathieu Rosenbaum" target="Alexandre B. Tsybakov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4413v1" />
          <attvalue for="2" value="Improved Matrix Uncertainty Selector" />
          <attvalue for="3" value="We consider the regression model with observation error in the design:&#10;y=X\theta* + e, Z=X+N. Here the random vector y in R^n and the random n*p&#10;matrix Z are observed, the n*p matrix X is unknown, N is an n*p random noise&#10;matrix, e in R^n is a random noise vector, and \theta* is a vector of unknown&#10;parameters to be estimated. We consider the setting where the dimension p can&#10;be much larger than the sample size n and \theta* is sparse. Because of the&#10;presence of the noise matrix N, the commonly used Lasso and Dantzig selector&#10;are unstable. An alternative procedure called the Matrix Uncertainty (MU)&#10;selector has been proposed in Rosenbaum and Tsybakov (2010) in order to account&#10;for the noise. The properties of the MU selector have been studied in Rosenbaum&#10;and Tsybakov (2010) for sparse \theta* under the assumption that the noise&#10;matrix N is deterministic and its values are small. In this paper, we propose a&#10;modification of the MU selector when N is a random matrix with zero-mean&#10;entries having the variances that can be estimated. This is, for example, the&#10;case in the model where the entries of X are missing at random. We show both&#10;theoretically and numerically that, under these conditions, the new estimator&#10;called the Compensated MU selector achieves better accuracy of estimation than&#10;the original MU selector." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="487" source="A. Murillo-Salas" target="F. J. Rubio">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.5220v3" />
          <attvalue for="2" value="A note on the infinite divisibility of a class of transformations of&#10;  normal variables" />
          <attvalue for="3" value="This note examines the infinite divisibility of density-based transformations&#10;of normal random variables. We characterize a class of density-based&#10;transformations of normal variables which produces non-infinitely divisible&#10;distributions. We relate our result with some known skewing mechanisms." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="488" source="Antonio Galves" target="Aurélien Garivier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0673v3" />
          <attvalue for="2" value="Joint estimation of intersecting context tree models" />
          <attvalue for="3" value="We study a problem of model selection for data produced by two different&#10;context tree sources. Motivated by linguistic questions, we consider the case&#10;where the probabilistic context trees corresponding to the two sources are&#10;finite and share many of their contexts. In order to understand the differences&#10;between the two sources, it is important to identify which contexts and which&#10;transition probabilities are specific to each source.&#10;  We consider a class of probabilistic context tree models with three types of&#10;contexts: those which appear in one, the other, or both sources. We use a BIC&#10;penalized maximum likelihood procedure that jointly estimates the two sources.&#10;  We propose a new algorithm which efficiently computes the estimated context&#10;trees. We prove that the procedure is strongly consistent. We also present a&#10;simulation study showing the practical advantage of our procedure over a&#10;procedure that works separately on each dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="489" source="Antonio Galves" target="Elisabeth Gassiat">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0673v3" />
          <attvalue for="2" value="Joint estimation of intersecting context tree models" />
          <attvalue for="3" value="We study a problem of model selection for data produced by two different&#10;context tree sources. Motivated by linguistic questions, we consider the case&#10;where the probabilistic context trees corresponding to the two sources are&#10;finite and share many of their contexts. In order to understand the differences&#10;between the two sources, it is important to identify which contexts and which&#10;transition probabilities are specific to each source.&#10;  We consider a class of probabilistic context tree models with three types of&#10;contexts: those which appear in one, the other, or both sources. We use a BIC&#10;penalized maximum likelihood procedure that jointly estimates the two sources.&#10;  We propose a new algorithm which efficiently computes the estimated context&#10;trees. We prove that the procedure is strongly consistent. We also present a&#10;simulation study showing the practical advantage of our procedure over a&#10;procedure that works separately on each dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="490" source="Aurélien Garivier" target="Elisabeth Gassiat">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0673v3" />
          <attvalue for="2" value="Joint estimation of intersecting context tree models" />
          <attvalue for="3" value="We study a problem of model selection for data produced by two different&#10;context tree sources. Motivated by linguistic questions, we consider the case&#10;where the probabilistic context trees corresponding to the two sources are&#10;finite and share many of their contexts. In order to understand the differences&#10;between the two sources, it is important to identify which contexts and which&#10;transition probabilities are specific to each source.&#10;  We consider a class of probabilistic context tree models with three types of&#10;contexts: those which appear in one, the other, or both sources. We use a BIC&#10;penalized maximum likelihood procedure that jointly estimates the two sources.&#10;  We propose a new algorithm which efficiently computes the estimated context&#10;trees. We prove that the procedure is strongly consistent. We also present a&#10;simulation study showing the practical advantage of our procedure over a&#10;procedure that works separately on each dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="491" source="Xavier Brossat" target="Georges Oppenheim">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4351v1" />
          <attvalue for="2" value="Estimating and forecasting partially linear models with non stationary&#10;  exogeneous variables" />
          <attvalue for="3" value="This paper presents a backfitting-type method for estimating and forecasting&#10;a periodically correlated partially linear model with exogeneous variables and&#10;heteroskedastic input noise. A rate of convergence of the estimator is given.&#10;The results are valid even if the period is unknown." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="492" source="Xavier Brossat" target="Marie-Claude Viano">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4351v1" />
          <attvalue for="2" value="Estimating and forecasting partially linear models with non stationary&#10;  exogeneous variables" />
          <attvalue for="3" value="This paper presents a backfitting-type method for estimating and forecasting&#10;a periodically correlated partially linear model with exogeneous variables and&#10;heteroskedastic input noise. A rate of convergence of the estimator is given.&#10;The results are valid even if the period is unknown." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="493" source="Georges Oppenheim" target="Marie-Claude Viano">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4351v1" />
          <attvalue for="2" value="Estimating and forecasting partially linear models with non stationary&#10;  exogeneous variables" />
          <attvalue for="3" value="This paper presents a backfitting-type method for estimating and forecasting&#10;a periodically correlated partially linear model with exogeneous variables and&#10;heteroskedastic input noise. A rate of convergence of the estimator is given.&#10;The results are valid even if the period is unknown." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="494" source="Marianna Pensky" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.2766v3" />
          <attvalue for="2" value="Laplace deconvolution with noisy observations" />
          <attvalue for="3" value="In the present paper we consider Laplace deconvolution for discrete noisy&#10;data observed on the interval whose length may increase with a sample size.&#10;Although this problem arises in a variety of applications, to the best of our&#10;knowledge, it has been given very little attention by the statistical&#10;community. Our objective is to fill this gap and provide statistical treatment&#10;of Laplace deconvolution problem with noisy discrete data. The main&#10;contribution of the paper is explicit construction of an asymptotically&#10;rate-optimal (in the minimax sense) Laplace deconvolution estimator which is&#10;adaptive to the regularity of the unknown function. We show that the original&#10;Laplace deconvolution problem can be reduced to nonparametric estimation of a&#10;regression function and its derivatives on the interval of growing length T_n.&#10;Whereas the forms of the estimators remain standard, the choices of the&#10;parameters and the minimax convergence rates, which are expressed in terms of&#10;T_n^2/n in this case, are affected by the asymptotic growth of the length of&#10;the interval.&#10;  We derive an adaptive kernel estimator of the function of interest, and&#10;establish its asymptotic minimaxity over a range of Sobolev classes. We&#10;illustrate the theory by examples of construction of explicit expressions of&#10;Laplace deconvolution estimators. A simulation study shows that, in addition to&#10;providing asymptotic optimality as the number of observations turns to&#10;infinity, the proposed estimator demonstrates good performance in finite sample&#10;examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="495" source="A. Philip Dawid" target="Steffen Lauritzen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.5011v4" />
          <attvalue for="2" value="Proper local scoring rules" />
          <attvalue for="3" value="We investigate proper scoring rules for continuous distributions on the real&#10;line. It is known that the log score is the only such rule that depends on the&#10;quoted density only through its value at the outcome that materializes. Here we&#10;allow further dependence on a finite number $m$ of derivatives of the density&#10;at the outcome, and describe a large class of such $m$-local proper scoring&#10;rules: these exist for all even $m$ but no odd $m$. We further show that for&#10;$m\geq2$ all such $m$-local rules can be computed without knowledge of the&#10;normalizing constant of the distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="496" source="A. Philip Dawid" target="Matthew Parry">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.5011v4" />
          <attvalue for="2" value="Proper local scoring rules" />
          <attvalue for="3" value="We investigate proper scoring rules for continuous distributions on the real&#10;line. It is known that the log score is the only such rule that depends on the&#10;quoted density only through its value at the outcome that materializes. Here we&#10;allow further dependence on a finite number $m$ of derivatives of the density&#10;at the outcome, and describe a large class of such $m$-local proper scoring&#10;rules: these exist for all even $m$ but no odd $m$. We further show that for&#10;$m\geq2$ all such $m$-local rules can be computed without knowledge of the&#10;normalizing constant of the distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="497" source="Steffen Lauritzen" target="Matthew Parry">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.5011v4" />
          <attvalue for="2" value="Proper local scoring rules" />
          <attvalue for="3" value="We investigate proper scoring rules for continuous distributions on the real&#10;line. It is known that the log score is the only such rule that depends on the&#10;quoted density only through its value at the outcome that materializes. Here we&#10;allow further dependence on a finite number $m$ of derivatives of the density&#10;at the outcome, and describe a large class of such $m$-local proper scoring&#10;rules: these exist for all even $m$ but no odd $m$. We further show that for&#10;$m\geq2$ all such $m$-local rules can be computed without knowledge of the&#10;normalizing constant of the distribution." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="498" source="John H. J. Einmahl" target="Estáte V. Khmaladze">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4220v1" />
          <attvalue for="2" value="Central limit theorems for local empirical processes near boundaries of&#10;  sets" />
          <attvalue for="3" value="We define the local empirical process, based on $n$ i.i.d. random vectors in&#10;dimension $d$, in the neighborhood of the boundary of a fixed set. Under&#10;natural conditions on the shrinking neighborhood, we show that, for these local&#10;empirical processes, indexed by classes of sets that vary with $n$ and satisfy&#10;certain conditions, an appropriately defined uniform central limit theorem&#10;holds. The concept of differentiation of sets in measure is very convenient for&#10;developing the results. Some examples and statistical applications are also&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="499" source="John H. J. Einmahl" target="Andrea Krajina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.0905v3" />
          <attvalue for="2" value="An M-estimator for tail dependence in arbitrary dimensions" />
          <attvalue for="3" value="Consider a random sample in the max-domain of attraction of a multivariate&#10;extreme value distribution such that the dependence structure of the attractor&#10;belongs to a parametric model. A new estimator for the unknown parameter is&#10;defined as the value that minimizes the distance between a vector of weighted&#10;integrals of the tail dependence function and their empirical counterparts. The&#10;minimization problem has, with probability tending to one, a unique, global&#10;solution. The estimator is consistent and asymptotically normal. The spectral&#10;measures of the tail dependence models to which the method applies can be&#10;discrete or continuous. Examples demonstrate the applicability and the&#10;performance of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="500" source="John H. J. Einmahl" target="Johan Segers">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.0905v3" />
          <attvalue for="2" value="An M-estimator for tail dependence in arbitrary dimensions" />
          <attvalue for="3" value="Consider a random sample in the max-domain of attraction of a multivariate&#10;extreme value distribution such that the dependence structure of the attractor&#10;belongs to a parametric model. A new estimator for the unknown parameter is&#10;defined as the value that minimizes the distance between a vector of weighted&#10;integrals of the tail dependence function and their empirical counterparts. The&#10;minimization problem has, with probability tending to one, a unique, global&#10;solution. The estimator is consistent and asymptotically normal. The spectral&#10;measures of the tail dependence models to which the method applies can be&#10;discrete or continuous. Examples demonstrate the applicability and the&#10;performance of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="501" source="Francesco Bartolucci" target="Silvia Bacci">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1498v1" />
          <attvalue for="2" value="Mixture latent autoregressive models for longitudinal data" />
          <attvalue for="3" value="Many relevant statistical and econometric models for the analysis of&#10;longitudinal data include a latent process to account for the unobserved&#10;heterogeneity between subjects in a dynamic fashion. Such a process may be&#10;continuous (typically an AR(1)) or discrete (typically a Markov chain). In this&#10;paper, we propose a model for longitudinal data which is based on a mixture of&#10;AR(1) processes with different means and correlation coefficients, but with&#10;equal variances. This model belongs to the class of models based on a&#10;continuous latent process, and then it has a natural interpretation in many&#10;contexts of application, but it is more flexible than other models in this&#10;class, reaching a goodness-of-fit similar to that of a discrete latent process&#10;model, with a reduced number of parameters. We show how to perform maximum&#10;likelihood estimation of the proposed model by the joint use of an&#10;Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented&#10;by means of recursions developed in the hidden Markov literature. We also&#10;introduce a simple method to obtain standard errors for the parameter estimates&#10;and a criterion to choose the number of mixture components. The proposed&#10;approach is illustrated by an application to a longitudinal dataset, coming&#10;from the Health and Retirement Study, about self-evaluation of the health&#10;status by a sample of subjects. In this application, the response variable is&#10;ordinal and time-constant and time-varying individual covariates are available." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="502" source="Francesco Bartolucci" target="Fulvia Pennoni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1498v1" />
          <attvalue for="2" value="Mixture latent autoregressive models for longitudinal data" />
          <attvalue for="3" value="Many relevant statistical and econometric models for the analysis of&#10;longitudinal data include a latent process to account for the unobserved&#10;heterogeneity between subjects in a dynamic fashion. Such a process may be&#10;continuous (typically an AR(1)) or discrete (typically a Markov chain). In this&#10;paper, we propose a model for longitudinal data which is based on a mixture of&#10;AR(1) processes with different means and correlation coefficients, but with&#10;equal variances. This model belongs to the class of models based on a&#10;continuous latent process, and then it has a natural interpretation in many&#10;contexts of application, but it is more flexible than other models in this&#10;class, reaching a goodness-of-fit similar to that of a discrete latent process&#10;model, with a reduced number of parameters. We show how to perform maximum&#10;likelihood estimation of the proposed model by the joint use of an&#10;Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented&#10;by means of recursions developed in the hidden Markov literature. We also&#10;introduce a simple method to obtain standard errors for the parameter estimates&#10;and a criterion to choose the number of mixture components. The proposed&#10;approach is illustrated by an application to a longitudinal dataset, coming&#10;from the Health and Retirement Study, about self-evaluation of the health&#10;status by a sample of subjects. In this application, the response variable is&#10;ordinal and time-constant and time-varying individual covariates are available." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="503" source="Silvia Bacci" target="Fulvia Pennoni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.1498v1" />
          <attvalue for="2" value="Mixture latent autoregressive models for longitudinal data" />
          <attvalue for="3" value="Many relevant statistical and econometric models for the analysis of&#10;longitudinal data include a latent process to account for the unobserved&#10;heterogeneity between subjects in a dynamic fashion. Such a process may be&#10;continuous (typically an AR(1)) or discrete (typically a Markov chain). In this&#10;paper, we propose a model for longitudinal data which is based on a mixture of&#10;AR(1) processes with different means and correlation coefficients, but with&#10;equal variances. This model belongs to the class of models based on a&#10;continuous latent process, and then it has a natural interpretation in many&#10;contexts of application, but it is more flexible than other models in this&#10;class, reaching a goodness-of-fit similar to that of a discrete latent process&#10;model, with a reduced number of parameters. We show how to perform maximum&#10;likelihood estimation of the proposed model by the joint use of an&#10;Expectation-Maximisation algorithm and a Newton-Raphson algorithm, implemented&#10;by means of recursions developed in the hidden Markov literature. We also&#10;introduce a simple method to obtain standard errors for the parameter estimates&#10;and a criterion to choose the number of mixture components. The proposed&#10;approach is illustrated by an application to a longitudinal dataset, coming&#10;from the Health and Retirement Study, about self-evaluation of the health&#10;status by a sample of subjects. In this application, the response variable is&#10;ordinal and time-constant and time-varying individual covariates are available." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="504" source="Koshi Yamada" target="Sumio Watanabe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1832v2" />
          <attvalue for="2" value="Statistical Learning Theory of Quasi-Regular Cases" />
          <attvalue for="3" value="Many learning machines such as normal mixtures and layered neural networks&#10;are not regular but singular statistical models, because the map from a&#10;parameter to a probability distribution is not one-to-one. The conventional&#10;statistical asymptotic theory can not be applied to such learning machines&#10;because the likelihood function can not be approximated by any normal&#10;distribution. Recently, new statistical theory has been established based on&#10;algebraic geometry and it was clarified that the generalization and training&#10;errors are determined by two birational invariants, the real log canonical&#10;threshold and the singular fluctuation. However, their concrete values are left&#10;unknown. In the present paper, we propose a new concept, a quasi-regular case&#10;in statistical learning theory. A quasi-regular case is not a regular case but&#10;a singular case, however, it has the same property as a regular case. In fact,&#10;we prove that, in a quasi-regular case, two birational invariants are equal to&#10;each other, resulting that the symmetry of the generalization and training&#10;errors holds. Moreover, the concrete values of two birational invariants are&#10;explicitly obtained, the quasi-regular case is useful to study statistical&#10;learning theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="505" source="Hélène Lescornel" target="Claudie Chabriac">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.4735v1" />
          <attvalue for="2" value="Unbiased risk estimation method for covariance estimation" />
          <attvalue for="3" value="We consider a model selection estimator of the covariance of a random&#10;process. Using the Unbiased Risk Estimation (URE) method, we build an estimator&#10;of the risk which allows to select an estimator in a collection of model. Then,&#10;we present an oracle inequality which ensures that the risk of the selected&#10;estimator is close to the risk of the oracle. Simulations show the efficiency&#10;of this methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="506" source="Angelika Franke" target="Gerhard Osius">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.0852v2" />
          <attvalue for="2" value="The Asymptotic Covariance Matrix of the Odds Ratio Parameter Estimator&#10;  in Semiparametric Log-bilinear Odds Ratio Models" />
          <attvalue for="3" value="The association between two random variables is often of primary interest in&#10;statistical research. In this paper semiparametric models for the association&#10;between random vectors X and Y are considered which leave the marginal&#10;distributions arbitrary. Given that the odds ratio function comprises the whole&#10;information about the association the focus is on bilinear log-odds ratio&#10;models and in particular on the odds ratio parameter vector {\theta}. The&#10;covariance structure of the maximum likelihood estimator {\theta}^ of {\theta}&#10;is of major importance for asymptotic inference. To this end different&#10;representations of the estimated covariance matrix are derived for conditional&#10;and unconditional sampling schemes and different asymptotic approaches&#10;depending on whether X and/or Y has finite or arbitrary support. The main&#10;result is the invariance of the estimated asymptotic covariance matrix of&#10;{\theta}^ with respect to all above approaches. As applications we compute the&#10;asymptotic power for tests of linear hypotheses about {\theta} - with emphasis&#10;to logistic and linear regression models - which allows to determine the&#10;necessary sample size to achieve a wanted power." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="507" source="Jun Shao" target="Yazhen Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="508" source="Jun Shao" target="Xinwei Deng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="509" source="Jun Shao" target="Sijian Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="510" source="Yazhen Wang" target="Xinwei Deng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="511" source="Yazhen Wang" target="Sijian Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="512" source="Xinwei Deng" target="Sijian Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3561v1" />
          <attvalue for="2" value="Sparse linear discriminant analysis by thresholding for high dimensional&#10;  data" />
          <attvalue for="3" value="In many social, economical, biological and medical studies, one objective is&#10;to classify a subject into one of several classes based on a set of variables&#10;observed from the subject. Because the probability distribution of the&#10;variables is usually unknown, the rule of classification is constructed using a&#10;training sample. The well-known linear discriminant analysis (LDA) works well&#10;for the situation where the number of variables used for classification is much&#10;smaller than the training sample size. Because of the advance in technologies,&#10;modern statistical studies often face classification problems with the number&#10;of variables much larger than the sample size, and the LDA may perform poorly.&#10;We explore when and why the LDA has poor performance and propose a sparse LDA&#10;that is asymptotically optimal under some sparsity conditions on the unknown&#10;parameters. For illustration of application, we discuss an example of&#10;classifying human cancer into two classes of leukemia based on a set of 7,129&#10;genes and a training sample of size 72. A simulation is also conducted to check&#10;the performance of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="513" source="Henrik Ohlsson" target="Allen Y. Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="514" source="Henrik Ohlsson" target="Roy Dong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="515" source="Henrik Ohlsson" target="S. Shankar Sastry">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="516" source="Allen Y. Yang" target="Roy Dong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="517" source="Allen Y. Yang" target="S. Shankar Sastry">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="518" source="Roy Dong" target="S. Shankar Sastry">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.6323v3" />
          <attvalue for="2" value="Compressive Phase Retrieval From Squared Output Measurements Via&#10;  Semidefinite Programming" />
          <attvalue for="3" value="Given a linear system in a real or complex domain, linear regression aims to&#10;recover the model parameters from a set of observations. Recent studies in&#10;compressive sensing have successfully shown that under certain conditions, a&#10;linear program, namely, l1-minimization, guarantees recovery of sparse&#10;parameter signals even when the system is underdetermined. In this paper, we&#10;consider a more challenging problem: when the phase of the output measurements&#10;from a linear system is omitted. Using a lifting technique, we show that even&#10;though the phase information is missing, the sparse signal can be recovered&#10;exactly by solving a simple semidefinite program when the sampling rate is&#10;sufficiently high, albeit the exact solutions to both sparse signal recovery&#10;and phase retrieval are combinatorial. The results extend the type of&#10;applications that compressive sensing can be applied to those where only output&#10;magnitudes can be observed. We demonstrate the accuracy of the algorithms&#10;through theoretical analysis, extensive simulations and a practical experiment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="519" source="Yoav Benjamini" target="Marina Bogomolov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.3670v1" />
          <attvalue for="2" value="Adjusting for selection bias in testing multiple families of hypotheses" />
          <attvalue for="3" value="In many large multiple testing problems the hypotheses are divided into&#10;families. Given the data, families with evidence for true discoveries are&#10;selected, and hypotheses within them are tested. Neither controlling the&#10;error-rate in each family separately nor controlling the error-rate over all&#10;hypotheses together can assure that an error-rate is controlled in the selected&#10;families. We formulate this concern about selective inference in its&#10;generality, for a very wide class of error-rates and for any selection&#10;criterion, and present an adjustment of the testing level inside the selected&#10;families that retains the average error-rate over the selected families." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="520" source="Serge Cohen" target="Céline Lacaux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.1077v1" />
          <attvalue for="2" value="LAN property for some fractional type Brownian motion" />
          <attvalue for="3" value="We study asymptotic expansion of the likelihood of a certain class of&#10;Gaussian processes characterized by their spectral density $f_\theta$. We&#10;consider the case where $f_\theta\PAR{x} \sim_{x\to 0}&#10;\ABS{x}^{-\al(\theta)}L_\theta(x)$ with $L_\theta$ a slowly varying function&#10;and $\al\PAR{\theta}\in (-\infty,1)$. We prove LAN property for these models&#10;which include in particular fractional Brownian motion %$B^\alpha_t,\: \alpha&#10;\geq 1/2$ or ARFIMA processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="521" source="Anil Aswani" target="Peter Bickel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1457v1" />
          <attvalue for="2" value="Regression on manifolds: Estimation of the exterior derivative" />
          <attvalue for="3" value="Collinearity and near-collinearity of predictors cause difficulties when&#10;doing regression. In these cases, variable selection becomes untenable because&#10;of mathematical issues concerning the existence and numerical stability of the&#10;regression coefficients, and interpretation of the coefficients is ambiguous&#10;because gradients are not defined. Using a differential geometric&#10;interpretation, in which the regression coefficients are interpreted as&#10;estimates of the exterior derivative of a function, we develop a new method to&#10;do regression in the presence of collinearities. Our regularization scheme can&#10;improve estimation error, and it can be easily modified to include lasso-type&#10;regularization. These estimators also have simple extensions to the &quot;large $p$,&#10;small $n$&quot; context." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="522" source="Anil Aswani" target="Claire Tomlin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1457v1" />
          <attvalue for="2" value="Regression on manifolds: Estimation of the exterior derivative" />
          <attvalue for="3" value="Collinearity and near-collinearity of predictors cause difficulties when&#10;doing regression. In these cases, variable selection becomes untenable because&#10;of mathematical issues concerning the existence and numerical stability of the&#10;regression coefficients, and interpretation of the coefficients is ambiguous&#10;because gradients are not defined. Using a differential geometric&#10;interpretation, in which the regression coefficients are interpreted as&#10;estimates of the exterior derivative of a function, we develop a new method to&#10;do regression in the presence of collinearities. Our regularization scheme can&#10;improve estimation error, and it can be easily modified to include lasso-type&#10;regularization. These estimators also have simple extensions to the &quot;large $p$,&#10;small $n$&quot; context." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="523" source="Peter Bickel" target="Claire Tomlin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1457v1" />
          <attvalue for="2" value="Regression on manifolds: Estimation of the exterior derivative" />
          <attvalue for="3" value="Collinearity and near-collinearity of predictors cause difficulties when&#10;doing regression. In these cases, variable selection becomes untenable because&#10;of mathematical issues concerning the existence and numerical stability of the&#10;regression coefficients, and interpretation of the coefficients is ambiguous&#10;because gradients are not defined. Using a differential geometric&#10;interpretation, in which the regression coefficients are interpreted as&#10;estimates of the exterior derivative of a function, we develop a new method to&#10;do regression in the presence of collinearities. Our regularization scheme can&#10;improve estimation error, and it can be easily modified to include lasso-type&#10;regularization. These estimators also have simple extensions to the &quot;large $p$,&#10;small $n$&quot; context." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="524" source="Gerard Kerkyacharian" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2450v1" />
          <attvalue for="2" value="Concentration Inequalities and Confidence Bands for Needlet Density&#10;  Estimators on Compact Homogeneous Manifolds" />
          <attvalue for="3" value="Let $X_1,...,X_n$ be a random sample from some unknown probability density&#10;$f$ defined on a compact homogeneous manifold $\mathbf M$ of dimension $d \ge&#10;1$. Consider a 'needlet frame' $\{\phi_{j \eta}\}$ describing a localised&#10;projection onto the space of eigenfunctions of the Laplace operator on $\mathbf&#10;M$ with corresponding eigenvalues less than $2^{2j}$, as constructed in&#10;\cite{GP10}. We prove non-asymptotic concentration inequalities for the uniform&#10;deviations of the linear needlet density estimator $f_n(j)$ obtained from an&#10;empirical estimate of the needlet projection $\sum_\eta \phi_{j \eta} \int f&#10;\phi_{j \eta}$ of $f$. We apply these results to construct risk-adaptive&#10;estimators and nonasymptotic confidence bands for the unknown density $f$. The&#10;confidence bands are adaptive over classes of differentiable and&#10;H\&quot;{older}-continuous functions on $\mathbf M$ that attain their H\&quot;{o}lder&#10;exponents." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="525" source="Gerard Kerkyacharian" target="Mathilde Mougeot">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="526" source="Gerard Kerkyacharian" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="NO?" />
          <attvalue for="2" value="title" />
          <attvalue for="3" value="summary" />
          <attvalue for="4" value="6" />
        </attvalues>
      </edge>
      <edge id="527" source="Dominique Picard" target="Mathilde Mougeot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3967v1" />
          <attvalue for="2" value="A new selection method for high-dimensionial instrumental setting:&#10;  application to the Growth Rate convergence hypothesis" />
          <attvalue for="3" value="This paper investigates the problem of selecting variables in regression-type&#10;models for an &quot;instrumental&quot; setting. Our study is motivated by empirically&#10;verifying the conditional convergence hypothesis used in the economical&#10;literature concerning the growth rate. To avoid unnecessary discussion about&#10;the choice and the pertinence of instrumental variables, we embed the model in&#10;a very high dimensional setting. We propose a selection procedure with no&#10;optimization step called LOLA, for Learning Out of Leaders with Adaptation.&#10;LOLA is an auto-driven algorithm with two thresholding steps. The consistency&#10;of the procedure is proved under sparsity conditions and simulations are&#10;conducted to illustrate the practical good performances of LOLA. The behavior&#10;of the algorithm is studied when instrumental variables are artificially added&#10;without a priori significant connection to the model. Using our algorithm, we&#10;provide a solution for modeling the link between the growth rate and the&#10;initial level of the gross domestic product and empirically prove the&#10;convergence hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="528" source="Dominique Picard" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3967v1" />
          <attvalue for="2" value="A new selection method for high-dimensionial instrumental setting:&#10;  application to the Growth Rate convergence hypothesis" />
          <attvalue for="3" value="This paper investigates the problem of selecting variables in regression-type&#10;models for an &quot;instrumental&quot; setting. Our study is motivated by empirically&#10;verifying the conditional convergence hypothesis used in the economical&#10;literature concerning the growth rate. To avoid unnecessary discussion about&#10;the choice and the pertinence of instrumental variables, we embed the model in&#10;a very high dimensional setting. We propose a selection procedure with no&#10;optimization step called LOLA, for Learning Out of Leaders with Adaptation.&#10;LOLA is an auto-driven algorithm with two thresholding steps. The consistency&#10;of the procedure is proved under sparsity conditions and simulations are&#10;conducted to illustrate the practical good performances of LOLA. The behavior&#10;of the algorithm is studied when instrumental variables are artificially added&#10;without a priori significant connection to the model. Using our algorithm, we&#10;provide a solution for modeling the link between the growth rate and the&#10;initial level of the gross domestic product and empirically prove the&#10;convergence hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="529" source="Ci-Ren Jiang" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.1726v1" />
          <attvalue for="2" value="Functional single index models for longitudinal data" />
          <attvalue for="3" value="A new single-index model that reflects the time-dynamic effects of the single&#10;index is proposed for longitudinal and functional response data, possibly&#10;measured with errors, for both longitudinal and time-invariant covariates. With&#10;appropriate initial estimates of the parametric index, the proposed estimator&#10;is shown to be $\sqrt{n}$-consistent and asymptotically normally distributed.&#10;We also address the nonparametric estimation of regression functions and&#10;provide estimates with optimal convergence rates. One advantage of the new&#10;approach is that the same bandwidth is used to estimate both the nonparametric&#10;mean function and the parameter in the index. The finite-sample performance for&#10;the proposed procedure is studied numerically." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="530" source="Jane-Ling Wang" target="Guozhong He">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="531" source="Jane-Ling Wang" target="Wenjing Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="532" source="Guozhong He" target="Wenjing Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.5212v1" />
          <attvalue for="2" value="Functional linear regression via canonical analysis" />
          <attvalue for="3" value="We study regression models for the situation where both dependent and&#10;independent variables are square-integrable stochastic processes. Questions&#10;concerning the definition and existence of the corresponding functional linear&#10;regression models and some basic properties are explored for this situation. We&#10;derive a representation of the regression parameter function in terms of the&#10;canonical components of the processes involved. This representation establishes&#10;a connection between functional regression and functional canonical analysis&#10;and suggests alternative approaches for the implementation of functional linear&#10;regression analysis. A specific procedure for the estimation of the regression&#10;parameter function using canonical expansions is proposed and compared with an&#10;established functional principal component regression approach. As an example&#10;of an application, we present an analysis of mortality data for cohorts of&#10;medflies, obtained in experimental studies of aging and longevity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="533" source="Zheng-Yan Lin" target="Yu-Ping Song">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.5547v2" />
          <attvalue for="2" value="Nonparametric Estimation of Second-Order Jump-Diffusion Model" />
          <attvalue for="3" value="We study the nonparametric estimators of the infinitesimal coefficients of&#10;the second-order jump-diffusion models. Under the mild conditions, we obtain&#10;the weak consistency and the asymptotic normalities of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="534" source="Zheng-Yan Lin" target="Han-Chao Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.5547v2" />
          <attvalue for="2" value="Nonparametric Estimation of Second-Order Jump-Diffusion Model" />
          <attvalue for="3" value="We study the nonparametric estimators of the infinitesimal coefficients of&#10;the second-order jump-diffusion models. Under the mild conditions, we obtain&#10;the weak consistency and the asymptotic normalities of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="535" source="Yu-Ping Song" target="Han-Chao Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.5547v2" />
          <attvalue for="2" value="Nonparametric Estimation of Second-Order Jump-Diffusion Model" />
          <attvalue for="3" value="We study the nonparametric estimators of the infinitesimal coefficients of&#10;the second-order jump-diffusion models. Under the mild conditions, we obtain&#10;the weak consistency and the asymptotic normalities of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="536" source="Nadir Maaroufi" target="Camille Sabbah">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="537" source="Nadir Maaroufi" target="Yvik Swan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="538" source="Nadir Maaroufi" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="539" source="Camille Sabbah" target="Yvik Swan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="540" source="Camille Sabbah" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="541" source="Yvik Swan" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.4517v2" />
          <attvalue for="2" value="A remark on the ARE between Wilcoxon's and van~der~Waerden's scores" />
          <attvalue for="3" value="This paper is concerned with a comparison of van der Waerden's and Wilcoxon's&#10;scores." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="542" source="Lutz Duembgen" target="Perla Zerial">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.0417v3" />
          <attvalue for="2" value="On Low-Dimensional Projections of High-Dimensional Distributions" />
          <attvalue for="3" value="Let $P$ be a probability distribution on $q$-dimensional space. The so-called&#10;Diaconis-Freedman effect means that for a fixed dimension $d &lt;&lt; q$, most&#10;$d$-dimensional projections of $P$ look like a scale mixture of spherically&#10;symmetric Gaussian distributions. The present paper provides necessary and&#10;sufficient conditions for this phenomenon in a suitable asymptotic framework&#10;with increasing dimension $q$. It turns out, that the conditions formulated by&#10;Diaconis and Freedman (1984) are not only sufficient but necessary as well.&#10;Moreover, letting $\hat{P}$ be the empirical distribution of $n$ independent&#10;random vectors with distribution $P$, we investigate the behavior of the&#10;empirical process $\sqrt{n}(\hat{P} - P)$ under random projections, conditional&#10;on $\hat{P}$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="543" source="Michael Falk" target="Diana Tichy">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1108.0853v1" />
          <attvalue for="2" value="Asymptotic Conditional Distribution of Exceedance Counts: Fragility&#10;  Index with Different Margins" />
          <attvalue for="3" value="Let $\bm X=(X_1,...,X_d)$ be a random vector, whose components are not&#10;necessarily independent nor are they required to have identical distribution&#10;functions $F_1,...,F_d$. Denote by $N_s$ the number of exceedances among&#10;$X_1,...,X_d$ above a high threshold $s$. The fragility index, defined by&#10;$FI=\lim_{s\nearrow}E(N_s\mid N_s&gt;0)$ if this limit exists, measures the&#10;asymptotic stability of the stochastic system $\bm X$ as the threshold&#10;increases. The system is called stable if $FI=1$ and fragile otherwise. In this&#10;paper we show that the asymptotic conditional distribution of exceedance counts&#10;(ACDEC) $p_k=\lim_{s\nearrow}P(N_s=k\mid N_s&gt;0)$, $1\le k\le d$, exists, if the&#10;copula of $\bm X$ is in the domain of attraction of a multivariate extreme&#10;value distribution, and if&#10;$\lim_{s\nearrow}(1-F_i(s))/(1-F_\kappa(s))=\gamma_i\in[0,\infty)$ exists for&#10;$1\le i\le d$ and some $\kappa\in{1,...,d}$. This enables the computation of&#10;the FI corresponding to $\bm X$ and of the extended FI as well as of the&#10;asymptotic distribution of the exceedance cluster length also in that case,&#10;where the components of $\bm X$ are not identically distributed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="544" source="Christophe Chesneau" target="Jalal M. Fadili">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3994v2" />
          <attvalue for="2" value="Adaptive estimation of an additive regression function from weakly&#10;  dependent data" />
          <attvalue for="3" value="A $d$-dimensional nonparametric additive regression model with dependent&#10;observations is considered. Using the marginal integration technique and&#10;wavelets methodology, we develop a new adaptive estimator for a component of&#10;the additive regression function. Its asymptotic properties are investigated&#10;via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We&#10;prove that it attains a sharp rate of convergence which turns to be the one&#10;obtained in the $\iid$ case for the standard univariate regression estimation&#10;problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="545" source="Christophe Chesneau" target="Bertrand Maillot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3994v2" />
          <attvalue for="2" value="Adaptive estimation of an additive regression function from weakly&#10;  dependent data" />
          <attvalue for="3" value="A $d$-dimensional nonparametric additive regression model with dependent&#10;observations is considered. Using the marginal integration technique and&#10;wavelets methodology, we develop a new adaptive estimator for a component of&#10;the additive regression function. Its asymptotic properties are investigated&#10;via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We&#10;prove that it attains a sharp rate of convergence which turns to be the one&#10;obtained in the $\iid$ case for the standard univariate regression estimation&#10;problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="546" source="Jalal M. Fadili" target="Bertrand Maillot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.3994v2" />
          <attvalue for="2" value="Adaptive estimation of an additive regression function from weakly&#10;  dependent data" />
          <attvalue for="3" value="A $d$-dimensional nonparametric additive regression model with dependent&#10;observations is considered. Using the marginal integration technique and&#10;wavelets methodology, we develop a new adaptive estimator for a component of&#10;the additive regression function. Its asymptotic properties are investigated&#10;via the minimax approach under the $\mathbb{L}_2$ risk over Besov balls. We&#10;prove that it attains a sharp rate of convergence which turns to be the one&#10;obtained in the $\iid$ case for the standard univariate regression estimation&#10;problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="547" source="M. Lerasle" target="R. I. Oliveira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.3914v1" />
          <attvalue for="2" value="Robust empirical mean Estimators" />
          <attvalue for="3" value="We study robust estimators of the mean of a probability measure $P$, called&#10;robust empirical mean estimators. This elementary construction is then used to&#10;revisit a problem of aggregation and a problem of estimator selection,&#10;extending these methods to not necessarily bounded collections of previous&#10;estimators.&#10;  We consider then the problem of robust $M$-estimation. We propose a slightly&#10;more complicated construction to handle this problem and, as examples of&#10;applications, we apply our general approach to least-squares density&#10;estimation, to density estimation with K\&quot;ullback loss and to a non-Gaussian,&#10;unbounded, random design and heteroscedastic regression problem.&#10;  Finally, we show that our strategy can be used when the data are only assumed&#10;to be mixing." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="548" source="M. Lerasle" target="A. Garivier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2191v1" />
          <attvalue for="2" value="Oracle approach and slope heuristic in context tree estimation" />
          <attvalue for="3" value="We introduce a general approach to prove oracle properties in context tree&#10;selection. The results derive from a concentration condition that is verified,&#10;for example, by mixing processes. Moreover, we show the superiority of the&#10;oracle approach from a non-asymptotic point of view in simulations where the&#10;classical BIC estimator has nice oracle properties even when it does not&#10;recover the source.&#10;  Our second objective is to extend the slope algorithm of \cite{AM08} to&#10;context tree estimation. The algorithm gives a practical way to evaluate the&#10;leading constant in front of the penalties. We study the slope heuristic&#10;underlying this algorithm and obtain the first results on the slope phenomenon&#10;in a discrete, non i.i.d framework. We illustrate in simulations the&#10;improvement of the oracle properties of BIC estimators by the slope algorithm." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="549" source="Cécile Durot" target="Vladimir N. Kulikov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5934v3" />
          <attvalue for="2" value="The limit distribution of the $L_{\infty}$-error of Grenander-type&#10;  estimators" />
          <attvalue for="3" value="Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard&#10;regularity conditions, we derive the asymptotic distribution of the supremum&#10;norm of the difference between $f$ and its Grenander-type estimator on&#10;sub-intervals of $[0,1]$. The rate of convergence is found to be of order&#10;$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="550" source="Cécile Durot" target="Hendrik P. Lopuhaä">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5934v3" />
          <attvalue for="2" value="The limit distribution of the $L_{\infty}$-error of Grenander-type&#10;  estimators" />
          <attvalue for="3" value="Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard&#10;regularity conditions, we derive the asymptotic distribution of the supremum&#10;norm of the difference between $f$ and its Grenander-type estimator on&#10;sub-intervals of $[0,1]$. The rate of convergence is found to be of order&#10;$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="551" source="Vladimir N. Kulikov" target="Hendrik P. Lopuhaä">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5934v3" />
          <attvalue for="2" value="The limit distribution of the $L_{\infty}$-error of Grenander-type&#10;  estimators" />
          <attvalue for="3" value="Let $f$ be a nonincreasing function defined on $[0,1]$. Under standard&#10;regularity conditions, we derive the asymptotic distribution of the supremum&#10;norm of the difference between $f$ and its Grenander-type estimator on&#10;sub-intervals of $[0,1]$. The rate of convergence is found to be of order&#10;$(n/\log n)^{-1/3}$ and the limiting distribution to be Gumbel." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="552" source="Rajen D. Shah" target="Richard J. Samworth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.5578v2" />
          <attvalue for="2" value="Variable selection with error control: Another look at Stability&#10;  Selection" />
          <attvalue for="3" value="Stability Selection was recently introduced by Meinshausen and Buhlmann&#10;(2010) as a very general technique designed to improve the performance of a&#10;variable selection algorithm. It is based on aggregating the results of&#10;applying a selection procedure to subsamples of the data. We introduce a&#10;variant, called Complementary Pairs Stability Selection (CPSS), and derive&#10;bounds both on the expected number of variables included by CPSS that have low&#10;selection probability under the original procedure, and on the expected number&#10;of high selection probability variables that are excluded. These results&#10;require no (e.g. exchangeability) assumptions on the underlying model or on the&#10;quality of the original selection procedure. Under reasonable shape&#10;restrictions, the bounds can be further tightened, yielding improved error&#10;control, and therefore increasing the applicability of the methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="553" source="Aleksey S. Polunchenko" target="Alexander G. Tartakovsky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.2938v1" />
          <attvalue for="2" value="State-of-the-Art in Sequential Change-Point Detection" />
          <attvalue for="3" value="We provide an overview of the state-of-the-art in the area of sequential&#10;change-point detection assuming discrete time and known pre- and post-change&#10;distributions. The overview spans over all major formulations of the underlying&#10;optimization problem, namely, Bayesian, generalized Bayesian, and minimax. We&#10;pay particular attention to the latest advances in each. Also, we link together&#10;the generalized Bayesian problem with multi-cyclic disorder detection in a&#10;stationary regime when the change occurs at a distant time horizon. We conclude&#10;with two case studies to illustrate the cutting edge of the field at work." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="554" source="I. Bairamov" target="K. Bayramoglu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.4464v1" />
          <attvalue for="2" value="Baker- Lin-Huang type Bivariate distributions based on order statistics" />
          <attvalue for="3" value="Baker (2008) introduced a new class of bivariate distributions based on&#10;distributions of order statistics from two independent samples of size n.&#10;Lin-Huang (2010) discovered an important property of Baker's distribution and&#10;showed that the Pearson's correlation coefficient for this distribution&#10;converges to maximum attainable value, i.e. the correlation coefficient of the&#10;Frech\'et upper bound, as n increases to infinity. Bairamov and Bayramoglu&#10;(2011) investigated a new class of bivariate distributions constructed by using&#10;Baker's model and distributions of order statistics from dependent random&#10;variables, allowing high correlation than that of Baker's distribution. In this&#10;paper a new class of Baker's type bivariate distributions with high correlation&#10;are constructed on the base of distributions of order statistics by using an&#10;arbitrary continuous copula instead of the product copula.&#10;  Keywords: Bivariate distribution function, FGM distributions, copula,&#10;positive quadrant dependent, negative quadrant dependent, order statistics,&#10;Pearson's correlation coefficient." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="555" source="Yannick Baraud" target="Lucien Birgé">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.2818v2" />
          <attvalue for="2" value="Estimating composite functions by model selection" />
          <attvalue for="3" value="We consider the problem of estimating a function $s$ on $[-1,1]^{k}$ for&#10;large values of $k$ by looking for some best approximation by composite&#10;functions of the form $g\circ u$. Our solution is based on model selection and&#10;leads to a very general approach to solve this problem with respect to many&#10;different types of functions $g,u$ and statistical frameworks. In particular,&#10;we handle the problems of approximating $s$ by additive functions, single and&#10;multiple index models, neural networks, mixtures of Gaussian densities (when&#10;$s$ is a density) among other examples. We also investigate the situation where&#10;$s=g\circ u$ for functions $g$ and $u$ belonging to possibly anisotropic&#10;smoothness classes. In this case, our approach leads to a completely adaptive&#10;estimator with respect to the regularity of $s$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="556" source="Mireille Gettler Summa" target="Francesco Palumbo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.3830v3" />
          <attvalue for="2" value="Factor PD-Clustering" />
          <attvalue for="3" value="Factorial clustering methods have been developed in recent years thanks to&#10;the improving of computational power. These methods perform a linear&#10;transformation of data and a clustering on transformed data optimizing a common&#10;criterion. Factorial PD-clustering is based on Probabilistic Distance&#10;clustering (PD-clustering). PD-clustering is an iterative, distribution free,&#10;probabilistic, clustering method. Factor PD-clustering make a linear&#10;transformation of original variables into a reduced number of orthogonal ones&#10;using a common criterion with PD-Clustering. It is demonstrated that Tucker 3&#10;decomposition allows to obtain this transformation. Factor PD-clustering makes&#10;alternatively a Tucker 3 decomposition and a PD-clustering on transformed data&#10;until convergence. This method could significantly improve the algorithm&#10;performance and allows to work with large dataset, to improve the stability and&#10;the robustness of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="557" source="Mireille Gettler Summa" target="Cristina Tortora">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.3830v3" />
          <attvalue for="2" value="Factor PD-Clustering" />
          <attvalue for="3" value="Factorial clustering methods have been developed in recent years thanks to&#10;the improving of computational power. These methods perform a linear&#10;transformation of data and a clustering on transformed data optimizing a common&#10;criterion. Factorial PD-clustering is based on Probabilistic Distance&#10;clustering (PD-clustering). PD-clustering is an iterative, distribution free,&#10;probabilistic, clustering method. Factor PD-clustering make a linear&#10;transformation of original variables into a reduced number of orthogonal ones&#10;using a common criterion with PD-Clustering. It is demonstrated that Tucker 3&#10;decomposition allows to obtain this transformation. Factor PD-clustering makes&#10;alternatively a Tucker 3 decomposition and a PD-clustering on transformed data&#10;until convergence. This method could significantly improve the algorithm&#10;performance and allows to work with large dataset, to improve the stability and&#10;the robustness of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="558" source="Francesco Palumbo" target="Cristina Tortora">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1106.3830v3" />
          <attvalue for="2" value="Factor PD-Clustering" />
          <attvalue for="3" value="Factorial clustering methods have been developed in recent years thanks to&#10;the improving of computational power. These methods perform a linear&#10;transformation of data and a clustering on transformed data optimizing a common&#10;criterion. Factorial PD-clustering is based on Probabilistic Distance&#10;clustering (PD-clustering). PD-clustering is an iterative, distribution free,&#10;probabilistic, clustering method. Factor PD-clustering make a linear&#10;transformation of original variables into a reduced number of orthogonal ones&#10;using a common criterion with PD-Clustering. It is demonstrated that Tucker 3&#10;decomposition allows to obtain this transformation. Factor PD-clustering makes&#10;alternatively a Tucker 3 decomposition and a PD-clustering on transformed data&#10;until convergence. This method could significantly improve the algorithm&#10;performance and allows to work with large dataset, to improve the stability and&#10;the robustness of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="559" source="Guillaume Lecué" target="Shahar Mendelson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.4983v1" />
          <attvalue for="2" value="Sharper lower bounds on the performance of the empirical risk&#10;  minimization algorithm" />
          <attvalue for="3" value="We present an argument based on the multidimensional and the uniform central&#10;limit theorems, proving that, under some geometrical assumptions between the&#10;target function $T$ and the learning class $F$, the excess risk of the&#10;empirical risk minimization algorithm is lower bounded by&#10;\[\frac{\mathbb{E}\sup_{q\in Q}G_q}{\sqrt{n}}\delta,\] where $(G_q)_{q\in Q}$&#10;is a canonical Gaussian process associated with $Q$ (a well chosen subset of&#10;$F$) and $\delta$ is a parameter governing the oscillations of the empirical&#10;excess risk function over a small ball in $F$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="560" source="Maxime Lenormand" target="Floriana Gargiulo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6759v3" />
          <attvalue for="2" value="Generating French virtual commuting network at municipality level" />
          <attvalue for="3" value="We aim to generate virtual commuting networks in the French rural regions in&#10;order to study the dynamics of their municipalities. Since we have to model&#10;small commuting flows between municipalities with a few hundreds or thousands&#10;inhabitants, we opt for a stochastic model presented by Gargiulo et al. 2012.&#10;It reproduces the various possible complete networks using an iterative&#10;process, stochastically choosing a workplace in the region for each commuter&#10;living in the municipality of a region. The choice is made considering the job&#10;offers in each municipality of the region and the distance to all the possible&#10;destinations. This paper presents how to adapt and implement this model to&#10;generate French regions commuting networks between municipalities. We address&#10;three different questions: How to generate a reliable virtual commuting network&#10;for a region highly dependant of other regions for the satisfaction of its&#10;resident's demand for employment? What about a convenient deterrence function?&#10;How to calibrate the model when detailed data is not available? We answer&#10;proposing an extended job search geographical base for commuters living in the&#10;municipalities, we compare two different deterrence functions and we show that&#10;the parameter is a constant for network linking French municipalities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="561" source="Péter Kevei" target="David M. Mason">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1107.3365v1" />
          <attvalue for="2" value="A note on a maximal Bernstein inequality" />
          <attvalue for="3" value="We show somewhat unexpectedly that whenever a general Bernstein-type maximal&#10;inequality holds for partial sums of a sequence of random variables, a maximal&#10;form of the inequality is also valid." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="562" source="Oliver Linton" target="Jens Perch Nielsen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1857v1" />
          <attvalue for="2" value="Nonparametric regression with filtered data" />
          <attvalue for="3" value="We present a general principle for estimating a regression function&#10;nonparametrically, allowing for a wide variety of data filtering, for example,&#10;repeated left truncation and right censoring. Both the mean and the median&#10;regression cases are considered. The method works by first estimating the&#10;conditional hazard function or conditional survivor function and then&#10;integrating. We also investigate improved methods that take account of model&#10;structure such as independent errors and show that such methods can improve&#10;performance when the model structure is true. We establish the pointwise&#10;asymptotic normality of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="563" source="Ting Yan" target="Yuanzhang Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="564" source="Ting Yan" target="Jinfeng Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="565" source="Ting Yan" target="Yaning Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="566" source="Ting Yan" target="Ji Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="567" source="Yuanzhang Li" target="Jinfeng Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="568" source="Yuanzhang Li" target="Yaning Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="569" source="Yuanzhang Li" target="Ji Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="570" source="Jinfeng Xu" target="Yaning Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="571" source="Jinfeng Xu" target="Ji Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="572" source="Yaning Yang" target="Ji Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0058v4" />
          <attvalue for="2" value="High-dimensional Wilks phenomena in the Bradley-Terry model" />
          <attvalue for="3" value="In this paper, we show the Wilks type of results for the Bradley-Terry model.&#10;Specifically, for some simple and composite null hypotheses of interest, we&#10;show that the likelihood ratio test statistic $\Lambda$ enjoys a chi-square&#10;approximation in the sense that $(2p)^{-1/2}(-2\log \Lambda&#10;-p)\stackrel{L}{\rightarrow}N(0,1)$ as $p$ goes to infinity, where $p$ is the&#10;corresponding degrees of freedom. Simulation studies and an application to NBA&#10;data illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="573" source="Giorgio Celant" target="Marco Di Battista">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="574" source="Giorgio Celant" target="Samuela Leoni-Aubin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="575" source="Marco Di Battista" target="Samuela Leoni-Aubin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1101.4352v1" />
          <attvalue for="2" value="Upper bounds for the error in some interpolation and extrapolation&#10;  designs" />
          <attvalue for="3" value="This paper deals with probabilistic upper bounds for the error in functional&#10;estimation defined on some interpolation and extrapolation designs, when the&#10;function to estimate is supposed to be analytic. The error pertaining to the&#10;estimate may depend on various factors: the frequency of observations on the&#10;knots, the position and number of the knots, and also on the error committed&#10;when approximating the function through its Taylor expansion. When the number&#10;of observations is fixed, then all these parameters are determined by the&#10;choice of the design and by the choice estimator of the unknown function. The&#10;scope of the paper is therefore to determine a rule for the minimal number of&#10;observation required to achieve an upper bound of the error on the estimate&#10;with a given maximal probability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="576" source="Kanti V. Mardia" target="Jochen Voss">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.6042v2" />
          <attvalue for="2" value="Some Fundamental Properties of a Multivariate von Mises Distribution" />
          <attvalue for="3" value="In application areas like bioinformatics multivariate distributions on angles&#10;are encountered which show significant clustering. One approach to statistical&#10;modelling of such situations is to use mixtures of unimodal distributions. In&#10;the literature (Mardia et al., 2011), the multivariate von Mises distribution,&#10;also known as the multivariate sine distribution, has been suggested for&#10;components of such models, but work in the area has been hampered by the fact&#10;that no good criteria for the von Mises distribution to be unimodal were&#10;available. In this article we study the question about when a multivariate von&#10;Mises distribution is unimodal. We give sufficient criteria for this to be the&#10;case and show examples of distributions with multiple modes when these criteria&#10;are violated. In addition, we propose a method to generate samples from the von&#10;Mises distribution in the case of high concentration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="577" source="Gustavo Didier" target="Vladas Pipiras">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.1822v1" />
          <attvalue for="2" value="Integral representations and properties of operator fractional Brownian&#10;  motions" />
          <attvalue for="3" value="Operator fractional Brownian motions (OFBMs) are (i) Gaussian, (ii) operator&#10;self-similar and (iii) stationary increment processes. They are the natural&#10;multivariate generalizations of the well-studied fractional Brownian motions.&#10;Because of the possible lack of time-reversibility, the defining properties&#10;(i)--(iii) do not, in general, characterize the covariance structure of OFBMs.&#10;To circumvent this problem, the class of OFBMs is characterized here by means&#10;of their integral representations in the spectral and time domains. For the&#10;spectral domain representations, this involves showing how the operator&#10;self-similarity shapes the spectral density in the general representation of&#10;stationary increment processes. The time domain representations are derived by&#10;using primary matrix functions and taking the Fourier transforms of the&#10;deterministic spectral domain kernels. Necessary and sufficient conditions for&#10;OFBMs to be time-reversible are established in terms of their spectral and time&#10;domain representations. It is also shown that the spectral density of the&#10;stationary increments of an OFBM has a rigid structure, here called the&#10;dichotomy principle. The notion of operator Brownian motions is also explored." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="578" source="Andrea Krajina" target="Johan Segers">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.0905v3" />
          <attvalue for="2" value="An M-estimator for tail dependence in arbitrary dimensions" />
          <attvalue for="3" value="Consider a random sample in the max-domain of attraction of a multivariate&#10;extreme value distribution such that the dependence structure of the attractor&#10;belongs to a parametric model. A new estimator for the unknown parameter is&#10;defined as the value that minimizes the distance between a vector of weighted&#10;integrals of the tail dependence function and their empirical counterparts. The&#10;minimization problem has, with probability tending to one, a unique, global&#10;solution. The estimator is consistent and asymptotically normal. The spectral&#10;measures of the tail dependence models to which the method applies can be&#10;discrete or continuous. Examples demonstrate the applicability and the&#10;performance of the method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="579" source="Nathan Huntley" target="Matthias C. M. Troffaes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1109.3607v1" />
          <attvalue for="2" value="Subtree perfectness, backward induction, and normal-extensive form&#10;  equivalence for single agent sequential decision making under arbitrary&#10;  choice functions" />
          <attvalue for="3" value="We revisit and reinterpret Selten's concept of subgame perfectness in the&#10;context of single agent normal form sequential decision making, which leads us&#10;to the concept of subtree perfectness. Thereby, we extend Hammond's&#10;characterization of extensive form consequentialist consistent behaviour norms&#10;to the normal form and to arbitrary choice functions under very few&#10;assumptions. In particular, we do not need to assume probabilities on any event&#10;or utilities on any reward. We show that subtree perfectness is equivalent to&#10;normal-extensive form equivalence, and is sufficient, but, perhaps&#10;surprisingly, not necessary, for backward induction to work." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="580" source="Alessandro Felluga" target="Stefano Tiziani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1102.0625v3" />
          <attvalue for="2" value="Intensive natural distribution as Bernoulli success ratio extension to&#10;  continuous: enhanced Gaussian, continuous Poisson, and phenomena explanation" />
          <attvalue for="3" value="A new distribution named intensive natural distribution is introduced with&#10;the intent of consolidating statistics and empirical data. Based on the&#10;probability derived from the Bernoulli distribution, this method extended also&#10;Poisson distribution to continuous, preserving its skewness. Using this model,&#10;the Horwitz curve has been explained. The theoretical derivation of our method,&#10;which applies to every kind of measurements collected through sampling, is here&#10;supported by a mathematical demonstration and illustrated with several&#10;applications to real data collected from chemical and geotechnical fields. We&#10;compared the proposed intensive natural distribution to other widely used&#10;frequency functions to test the robustness of the proposed method in fitting&#10;the histograms and the probability charts obtained from various intensive&#10;variables." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="581" source="Rémi Gribonval" target="Gilles Chardon">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7248v1" />
          <attvalue for="2" value="Blind calibration for compressed sensing by convex optimization" />
          <attvalue for="3" value="We consider the problem of calibrating a compressed sensing measurement&#10;system under the assumption that the decalibration consists in unknown gains on&#10;each measure. We focus on {\em blind} calibration, using measures performed on&#10;a few unknown (but sparse) signals. A naive formulation of this blind&#10;calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind&#10;source separation and dictionary learning, which are known to be highly&#10;non-convex and riddled with local minima. In the considered context, we show&#10;that in fact this formulation can be exactly expressed as a convex optimization&#10;problem, and can be solved using off-the-shelf algorithms. Numerical&#10;simulations demonstrate the effectiveness of the approach even for highly&#10;uncalibrated measures, when a sufficient number of (unknown, but sparse)&#10;calibrating signals is provided. We observe that the success/failure of the&#10;approach seems to obey sharp phase transitions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="582" source="Rémi Gribonval" target="Laurent Daudet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7248v1" />
          <attvalue for="2" value="Blind calibration for compressed sensing by convex optimization" />
          <attvalue for="3" value="We consider the problem of calibrating a compressed sensing measurement&#10;system under the assumption that the decalibration consists in unknown gains on&#10;each measure. We focus on {\em blind} calibration, using measures performed on&#10;a few unknown (but sparse) signals. A naive formulation of this blind&#10;calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind&#10;source separation and dictionary learning, which are known to be highly&#10;non-convex and riddled with local minima. In the considered context, we show&#10;that in fact this formulation can be exactly expressed as a convex optimization&#10;problem, and can be solved using off-the-shelf algorithms. Numerical&#10;simulations demonstrate the effectiveness of the approach even for highly&#10;uncalibrated measures, when a sufficient number of (unknown, but sparse)&#10;calibrating signals is provided. We observe that the success/failure of the&#10;approach seems to obey sharp phase transitions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="583" source="Gilles Chardon" target="Laurent Daudet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.7248v1" />
          <attvalue for="2" value="Blind calibration for compressed sensing by convex optimization" />
          <attvalue for="3" value="We consider the problem of calibrating a compressed sensing measurement&#10;system under the assumption that the decalibration consists in unknown gains on&#10;each measure. We focus on {\em blind} calibration, using measures performed on&#10;a few unknown (but sparse) signals. A naive formulation of this blind&#10;calibration problem, using $\ell_{1}$ minimization, is reminiscent of blind&#10;source separation and dictionary learning, which are known to be highly&#10;non-convex and riddled with local minima. In the considered context, we show&#10;that in fact this formulation can be exactly expressed as a convex optimization&#10;problem, and can be solved using off-the-shelf algorithms. Numerical&#10;simulations demonstrate the effectiveness of the approach even for highly&#10;uncalibrated measures, when a sufficient number of (unknown, but sparse)&#10;calibrating signals is provided. We observe that the success/failure of the&#10;approach seems to obey sharp phase transitions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="584" source="Olivier Collier" target="Arnak S. Dalalyan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4210v6" />
          <attvalue for="2" value="Curve registration by nonparametric goodness-of-fit testing" />
          <attvalue for="3" value="The problem of curve registration appears in many different areas of&#10;applications ranging from neuroscience to road traffic modeling. In the present&#10;work, we propose a nonparametric testing framework in which we develop a&#10;generalized likelihood ratio test to perform curve registration. We first prove&#10;that, under the null hypothesis, the resulting test statistic is asymptotically&#10;distributed as a chi-squared random variable. This result, often referred to as&#10;Wilks' phenomenon, provides a natural threshold for the test of a prescribed&#10;asymptotic significance level and a natural measure of lack-of-fit in terms of&#10;the $p$-value of the $\chi^2$-test. We also prove that the proposed test is&#10;consistent, \textit{i.e.}, its power is asymptotically equal to $1$. Finite&#10;sample properties of the proposed methodology are demonstrated by numerical&#10;simulations. As an application, a new local descriptor for digital images is&#10;introduced and an experimental evaluation of its discriminative power is&#10;conducted." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="585" source="Rama Cont" target="Cecilia Mancini">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1104.4429v1" />
          <attvalue for="2" value="Nonparametric tests for pathwise properties of semimartingales" />
          <attvalue for="3" value="We propose two nonparametric tests for investigating the pathwise properties&#10;of a signal modeled as the sum of a L\'{e}vy process and a Brownian&#10;semimartingale. Using a nonparametric threshold estimator for the continuous&#10;component of the quadratic variation, we design a test for the presence of a&#10;continuous martingale component in the process and a test for establishing&#10;whether the jumps have finite or infinite variation, based on observations on a&#10;discrete-time grid. We evaluate the performance of our tests using simulations&#10;of various stochastic models and use the tests to investigate the fine&#10;structure of the DM/USD exchange rate fluctuations and SPX futures prices. In&#10;both cases, our tests reveal the presence of a non-zero Brownian component and&#10;a finite variation jump component." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="586" source="Li Wang" target="Xiang Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="587" source="Li Wang" target="Raymond J. Carroll">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="588" source="Xiang Liu" target="Raymond J. Carroll">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1112.2502v1" />
          <attvalue for="2" value="Estimation and variable selection for generalized additive partial&#10;  linear models" />
          <attvalue for="3" value="We study generalized additive partial linear models, proposing the use of&#10;polynomial spline smoothing for estimation of nonparametric functions, and&#10;deriving quasi-likelihood based estimators for the linear parameters. We&#10;establish asymptotic normality for the estimators of the parametric components.&#10;The procedure avoids solving large systems of equations as in kernel-based&#10;procedures and thus results in gains in computational simplicity. We further&#10;develop a class of variable selection procedures for the linear parameters by&#10;employing a nonconcave penalized quasi-likelihood, which is shown to have an&#10;asymptotic oracle property. Monte Carlo simulations and an empirical example&#10;are presented for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="589" source="Emmanuel Boissard" target="Thibaut Le Gouic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.5927v2" />
          <attvalue for="2" value="Distribution's template estimate with Wasserstein metrics" />
          <attvalue for="3" value="In this paper we tackle the problem of comparing distributions of random&#10;variables and defining a mean pattern between a sample of random events. Using&#10;barycenters of measures in the Wasserstein space, we propose an iterative&#10;version as an estimation of the mean distribution. Moreover, when the&#10;distributions are a common measure warped by a centered random operator, then&#10;the barycenter enables to recover this distribution template." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="590" source="Mathilde Mougeot" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1103.3967v1" />
          <attvalue for="2" value="A new selection method for high-dimensionial instrumental setting:&#10;  application to the Growth Rate convergence hypothesis" />
          <attvalue for="3" value="This paper investigates the problem of selecting variables in regression-type&#10;models for an &quot;instrumental&quot; setting. Our study is motivated by empirically&#10;verifying the conditional convergence hypothesis used in the economical&#10;literature concerning the growth rate. To avoid unnecessary discussion about&#10;the choice and the pertinence of instrumental variables, we embed the model in&#10;a very high dimensional setting. We propose a selection procedure with no&#10;optimization step called LOLA, for Learning Out of Leaders with Adaptation.&#10;LOLA is an auto-driven algorithm with two thresholding steps. The consistency&#10;of the procedure is proved under sparsity conditions and simulations are&#10;conducted to illustrate the practical good performances of LOLA. The behavior&#10;of the algorithm is studied when instrumental variables are artificially added&#10;without a priori significant connection to the model. Using our algorithm, we&#10;provide a solution for modeling the link between the growth rate and the&#10;initial level of the gross domestic product and empirically prove the&#10;convergence hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="591" source="Daniel Berend" target="Aryeh Kontorovich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1111.2328v1" />
          <attvalue for="2" value="The Missing Mass Problem" />
          <attvalue for="3" value="We give tight lower and upper bounds on the expected missing mass for&#10;distributions over finite and countably infinite spaces. An essential&#10;characterization of the extremal distributions is given. We also provide an&#10;extension to totally bounded metric spaces that may be of independent interest." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="592" source="T. Tony Cai" target="Mark G. Low">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1105.3039v1" />
          <attvalue for="2" value="Testing composite hypotheses, Hermite polynomials and optimal estimation&#10;  of a nonsmooth functional" />
          <attvalue for="3" value="A general lower bound is developed for the minimax risk when estimating an&#10;arbitrary functional. The bound is based on testing two composite hypotheses&#10;and is shown to be effective in estimating the nonsmooth functional&#10;${\frac{1}{n}}\sum|\theta_i|$ from an observation $Y\sim N(\theta,I_n)$. This&#10;problem exhibits some features that are significantly different from those that&#10;occur in estimating conventional smooth functionals. This is a setting where&#10;standard techniques fail to yield sharp results. A sharp minimax lower bound is&#10;established by applying the general lower bound technique based on testing two&#10;composite hypotheses. A key step is the construction of two special priors and&#10;bounding the chi-square distance between two normal mixtures. An estimator is&#10;constructed using approximation theory and Hermite polynomials and is shown to&#10;be asymptotically sharp minimax when the means are bounded by a given value&#10;$M$. It is shown that the minimax risk equals $\beta_*^2M^2({\frac{\log\log&#10;n}{\log n}})^2$ asymptotically, where $\beta_*$ is the Bernstein constant. The&#10;general techniques and results developed in the present paper can also be used&#10;to solve other related problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
    </edges>
  </graph>
</gexf>
