<?xml version='1.0' encoding='utf-8'?>
<gexf version="1.2" xmlns="http://www.gexf.net/1.2draft" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2001/XMLSchema-instance">
  <graph defaultedgetype="undirected" mode="static" name="">
    <attributes class="edge" mode="static">
      <attribute id="1" title="id_link" type="string" />
      <attribute id="2" title="title" type="string" />
      <attribute id="3" title="summary" type="string" />
      <attribute id="4" title="Weight" type="long" />
    </attributes>
    <attributes class="node" mode="static">
      <attribute id="0" title="summary_tokenize_stop_word" type="string" />
    </attributes>
    <meta>
      <creator>NetworkX 2.2</creator>
      <lastmodified>18/02/2019</lastmodified>
    </meta>
    <nodes>
      <node id="Lingzhou Xue" label="Lingzhou Xue">
        <attvalues />
      </node>
      <node id="Hui Zou" label="Hui Zou">
        <attvalues />
      </node>
      <node id="Tianxi Cai" label="Tianxi Cai">
        <attvalues />
      </node>
      <node id="Ryan Martin" label="Ryan Martin">
        <attvalues />
      </node>
      <node id="Liang Hong" label="Liang Hong">
        <attvalues />
      </node>
      <node id="Stephen G. Walker" label="Stephen G. Walker">
        <attvalues />
      </node>
      <node id="F. Bartolucci" label="F. Bartolucci">
        <attvalues />
      </node>
      <node id="M. Lupparelli" label="M. Lupparelli">
        <attvalues />
      </node>
      <node id="Jianqing Fan" label="Jianqing Fan">
        <attvalues />
      </node>
      <node id="Yingying Fan" label="Yingying Fan">
        <attvalues />
      </node>
      <node id="Emre Barut" label="Emre Barut">
        <attvalues />
      </node>
      <node id="Elisabeth Gassiat" label="Elisabeth Gassiat">
        <attvalues />
      </node>
      <node id="Ramon Van Handel" label="Ramon Van Handel">
        <attvalues />
      </node>
      <node id="Shota Gugushvili" label="Shota Gugushvili">
        <attvalues />
      </node>
      <node id="Peter Spreij" label="Peter Spreij">
        <attvalues />
      </node>
      <node id="Yunwei Cui" label="Yunwei Cui">
        <attvalues />
      </node>
      <node id="Rongning Wu" label="Rongning Wu">
        <attvalues />
      </node>
      <node id="Thomas J. Fisher" label="Thomas J. Fisher">
        <attvalues />
      </node>
      <node id="Marc Hallin" label="Marc Hallin">
        <attvalues />
      </node>
      <node id="Christophe Ley" label="Christophe Ley">
        <attvalues />
      </node>
      <node id="Richard A. Davis" label="Richard A. Davis">
        <attvalues />
      </node>
      <node id="Heng Liu" label="Heng Liu">
        <attvalues />
      </node>
      <node id="Peter J. Bickel" label="Peter J. Bickel">
        <attvalues />
      </node>
      <node id="Aiyou Chen" label="Aiyou Chen">
        <attvalues />
      </node>
      <node id="Elizaveta Levina" label="Elizaveta Levina">
        <attvalues />
      </node>
      <node id="Matieyendou Lamboni" label="Matieyendou Lamboni">
        <attvalues />
      </node>
      <node id="Bertrand Iooss" label="Bertrand Iooss">
        <attvalues />
      </node>
      <node id="Anne-Laure Popelin" label="Anne-Laure Popelin">
        <attvalues />
      </node>
      <node id="Fabrice Gamboa" label="Fabrice Gamboa">
        <attvalues />
      </node>
      <node id="Mohamed Hebiri" label="Mohamed Hebiri">
        <attvalues />
      </node>
      <node id="Johannes C. Lederer" label="Johannes C. Lederer">
        <attvalues />
      </node>
      <node id="Sidney I. Resnick" label="Sidney I. Resnick">
        <attvalues />
      </node>
      <node id="David Zeber" label="David Zeber">
        <attvalues />
      </node>
      <node id="L. Gardes" label="L. Gardes">
        <attvalues />
      </node>
      <node id="S. Girard" label="S. Girard">
        <attvalues />
      </node>
      <node id="Monica Billio" label="Monica Billio">
        <attvalues />
      </node>
      <node id="Roberto Casarin" label="Roberto Casarin">
        <attvalues />
      </node>
      <node id="Anthony Osuntuyi" label="Anthony Osuntuyi">
        <attvalues />
      </node>
      <node id="Rida Benhaddou" label="Rida Benhaddou">
        <attvalues />
      </node>
      <node id="Marianna Pensky" label="Marianna Pensky">
        <attvalues />
      </node>
      <node id="Dominique Picard" label="Dominique Picard">
        <attvalues />
      </node>
      <node id="Jan Beran" label="Jan Beran">
        <attvalues />
      </node>
      <node id="Yevgen Shumeyko" label="Yevgen Shumeyko">
        <attvalues />
      </node>
      <node id="Holger Dette" label="Holger Dette">
        <attvalues />
      </node>
      <node id="Viatcheslav B. Melas" label="Viatcheslav B. Melas">
        <attvalues />
      </node>
      <node id="Petr Shpilev" label="Petr Shpilev">
        <attvalues />
      </node>
      <node id="Hélène Boistard" label="Hélène Boistard">
        <attvalues />
      </node>
      <node id="Hendrik P. Lopuhaä" label="Hendrik P. Lopuhaä">
        <attvalues />
      </node>
      <node id="Anne Ruiz-Gazen" label="Anne Ruiz-Gazen">
        <attvalues />
      </node>
      <node id="Tristan Launay" label="Tristan Launay">
        <attvalues />
      </node>
      <node id="Anne Philippe" label="Anne Philippe">
        <attvalues />
      </node>
      <node id="Sophie Lamarche" label="Sophie Lamarche">
        <attvalues />
      </node>
      <node id="Takuma Yoshida" label="Takuma Yoshida">
        <attvalues />
      </node>
      <node id="Kanta Naito" label="Kanta Naito">
        <attvalues />
      </node>
      <node id="Kari Lock Morgan" label="Kari Lock Morgan">
        <attvalues />
      </node>
      <node id="Donald B. Rubin" label="Donald B. Rubin">
        <attvalues />
      </node>
      <node id="Mathias Vetter" label="Mathias Vetter">
        <attvalues />
      </node>
      <node id="Ying Ding" label="Ying Ding">
        <attvalues />
      </node>
      <node id="Bin Nan" label="Bin Nan">
        <attvalues />
      </node>
      <node id="Abram M. Kagan" label="Abram M. Kagan">
        <attvalues />
      </node>
      <node id="Yaakov Malinovsky" label="Yaakov Malinovsky">
        <attvalues />
      </node>
      <node id="Stéphanie Allassonniere" label="Stéphanie Allassonniere">
        <attvalues />
      </node>
      <node id="Estelle Kuhn" label="Estelle Kuhn">
        <attvalues />
      </node>
      <node id="Till Sabel" label="Till Sabel">
        <attvalues />
      </node>
      <node id="Johannes Schmidt-Hieber" label="Johannes Schmidt-Hieber">
        <attvalues />
      </node>
      <node id="Jean-Marc Azais" label="Jean-Marc Azais">
        <attvalues />
      </node>
      <node id="Jean-Claude Fort" label="Jean-Claude Fort">
        <attvalues />
      </node>
      <node id="Peter Hall" label="Peter Hall">
        <attvalues />
      </node>
      <node id="Tung Pham" label="Tung Pham">
        <attvalues />
      </node>
      <node id="M. P. Wand" label="M. P. Wand">
        <attvalues />
      </node>
      <node id="S. S. J. Wang" label="S. S. J. Wang">
        <attvalues />
      </node>
      <node id="B. B. Chen" label="B. B. Chen">
        <attvalues />
      </node>
      <node id="G. M. Pan" label="G. M. Pan">
        <attvalues />
      </node>
      <node id="Yehua Li" label="Yehua Li">
        <attvalues />
      </node>
      <node id="Tailen Hsing" label="Tailen Hsing">
        <attvalues />
      </node>
      <node id="Ngai Hang Chan" label="Ngai Hang Chan">
        <attvalues />
      </node>
      <node id="Ching-Kang Ing" label="Ching-Kang Ing">
        <attvalues />
      </node>
      <node id="Sara van de Geer" label="Sara van de Geer">
        <attvalues />
      </node>
      <node id="Peter Bühlmann" label="Peter Bühlmann">
        <attvalues />
      </node>
      <node id="Fadoua Balabdaoui" label="Fadoua Balabdaoui">
        <attvalues />
      </node>
      <node id="Jon A. Wellner" label="Jon A. Wellner">
        <attvalues />
      </node>
      <node id="Delphine Blanke" label="Delphine Blanke">
        <attvalues />
      </node>
      <node id="Denis Bosq" label="Denis Bosq">
        <attvalues />
      </node>
      <node id="Xin Chen" label="Xin Chen">
        <attvalues />
      </node>
      <node id="Changliang Zou" label="Changliang Zou">
        <attvalues />
      </node>
      <node id="R. Dennis Cook" label="R. Dennis Cook">
        <attvalues />
      </node>
      <node id="Sylvain Delattre" label="Sylvain Delattre">
        <attvalues />
      </node>
      <node id="Etienne Roquain" label="Etienne Roquain">
        <attvalues />
      </node>
      <node id="Sabyasachi Mukhopadhyay" label="Sabyasachi Mukhopadhyay">
        <attvalues />
      </node>
      <node id="Sourabh Bhattacharya" label="Sourabh Bhattacharya">
        <attvalues />
      </node>
      <node id="Ilia Negri" label="Ilia Negri">
        <attvalues />
      </node>
      <node id="Yoichi Nishiyama" label="Yoichi Nishiyama">
        <attvalues />
      </node>
      <node id="Teppei Ogihara" label="Teppei Ogihara">
        <attvalues />
      </node>
      <node id="Nakahiro Yoshida" label="Nakahiro Yoshida">
        <attvalues />
      </node>
      <node id="Boris Buchmann" label="Boris Buchmann">
        <attvalues />
      </node>
      <node id="Gernot Müller" label="Gernot Müller">
        <attvalues />
      </node>
      <node id="Dave Zachariah" label="Dave Zachariah">
        <attvalues />
      </node>
      <node id="Saikat Chatterjee" label="Saikat Chatterjee">
        <attvalues />
      </node>
      <node id="Magnus Jansson" label="Magnus Jansson">
        <attvalues />
      </node>
      <node id="Jean-Marc Bardet" label="Jean-Marc Bardet">
        <attvalues />
      </node>
      <node id="Béchir Dola" label="Béchir Dola">
        <attvalues />
      </node>
      <node id="Hannes Leeb" label="Hannes Leeb">
        <attvalues />
      </node>
      <node id="Benedikt M. Pötscher" label="Benedikt M. Pötscher">
        <attvalues />
      </node>
      <node id="A. E. Koudou" label="A. E. Koudou">
        <attvalues />
      </node>
      <node id="P. Vallois" label="P. Vallois">
        <attvalues />
      </node>
      <node id="Neil Bathia" label="Neil Bathia">
        <attvalues />
      </node>
      <node id="Qiwei Yao" label="Qiwei Yao">
        <attvalues />
      </node>
      <node id="Flavio Ziegelmann" label="Flavio Ziegelmann">
        <attvalues />
      </node>
      <node id="Peter S. Chami" label="Peter S. Chami">
        <attvalues />
      </node>
      <node id="Bernd Sing" label="Bernd Sing">
        <attvalues />
      </node>
      <node id="Doneal Thomas" label="Doneal Thomas">
        <attvalues />
      </node>
      <node id="Qiying Wang" label="Qiying Wang">
        <attvalues />
      </node>
      <node id="Peter C. B. Phillips" label="Peter C. B. Phillips">
        <attvalues />
      </node>
      <node id="Juan Lucas Bali" label="Juan Lucas Bali">
        <attvalues />
      </node>
      <node id="Graciela Boente" label="Graciela Boente">
        <attvalues />
      </node>
      <node id="David E. Tyler" label="David E. Tyler">
        <attvalues />
      </node>
      <node id="Jane-Ling Wang" label="Jane-Ling Wang">
        <attvalues />
      </node>
      <node id="Rolando Biscay" label="Rolando Biscay">
        <attvalues />
      </node>
      <node id="Hélène Lescornel" label="Hélène Lescornel">
        <attvalues />
      </node>
      <node id="Jean-Michel Loubes" label="Jean-Michel Loubes">
        <attvalues />
      </node>
      <node id="Alexander Aue" label="Alexander Aue">
        <attvalues />
      </node>
      <node id="Thomas C. M. Lee" label="Thomas C. M. Lee">
        <attvalues />
      </node>
      <node id="Jia Chen" label="Jia Chen">
        <attvalues />
      </node>
      <node id="Jiti Gao" label="Jiti Gao">
        <attvalues />
      </node>
      <node id="Degui Li" label="Degui Li">
        <attvalues />
      </node>
      <node id="Constantinos Georghiou" label="Constantinos Georghiou">
        <attvalues />
      </node>
      <node id="Andreas N. Philippou" label="Andreas N. Philippou">
        <attvalues />
      </node>
      <node id="Abolfazl Saghafi" label="Abolfazl Saghafi">
        <attvalues />
      </node>
      <node id="Song Xi Chen" label="Song Xi Chen">
        <attvalues />
      </node>
      <node id="Ping-Shou Zhong" label="Ping-Shou Zhong">
        <attvalues />
      </node>
      <node id="Dominique Bontemps" label="Dominique Bontemps">
        <attvalues />
      </node>
      <node id="Sébastien Gadat" label="Sébastien Gadat">
        <attvalues />
      </node>
      <node id="R. Colombi" label="R. Colombi">
        <attvalues />
      </node>
      <node id="A. Forcina" label="A. Forcina">
        <attvalues />
      </node>
      <node id="Gérard Biau" label="Gérard Biau">
        <attvalues />
      </node>
      <node id="Frédéric Cérou" label="Frédéric Cérou">
        <attvalues />
      </node>
      <node id="Arnaud Guyader" label="Arnaud Guyader">
        <attvalues />
      </node>
      <node id="Pengsheng Ji" label="Pengsheng Ji">
        <attvalues />
      </node>
      <node id="Michael Nussbaum" label="Michael Nussbaum">
        <attvalues />
      </node>
      <node id="Hervé Cardot" label="Hervé Cardot">
        <attvalues />
      </node>
      <node id="Peggy Cénac" label="Peggy Cénac">
        <attvalues />
      </node>
      <node id="Pierre-André Zitt" label="Pierre-André Zitt">
        <attvalues />
      </node>
      <node id="Liugen Xue" label="Liugen Xue">
        <attvalues />
      </node>
      <node id="Qihua Wang" label="Qihua Wang">
        <attvalues />
      </node>
      <node id="Mathias Drton" label="Mathias Drton">
        <attvalues />
      </node>
      <node id="Aldo Goia" label="Aldo Goia">
        <attvalues />
      </node>
      <node id="Vladimir Koltchinskii" label="Vladimir Koltchinskii">
        <attvalues />
      </node>
      <node id="Pedro Rangel" label="Pedro Rangel">
        <attvalues />
      </node>
      <node id="Claudia Kirch" label="Claudia Kirch">
        <attvalues />
      </node>
      <node id="Dimitris N. Politis" label="Dimitris N. Politis">
        <attvalues />
      </node>
      <node id="Laszlo Gyorfi" label="Laszlo Gyorfi">
        <attvalues />
      </node>
      <node id="Peter Harremoes" label="Peter Harremoes">
        <attvalues />
      </node>
      <node id="Gabor Tusnady" label="Gabor Tusnady">
        <attvalues />
      </node>
      <node id="José A. Díaz-García" label="José A. Díaz-García">
        <attvalues />
      </node>
      <node id="Francisco J. Caro-Lopera" label="Francisco J. Caro-Lopera">
        <attvalues />
      </node>
      <node id="Sergios Agapiou" label="Sergios Agapiou">
        <attvalues />
      </node>
      <node id="Andrew M. Stuart" label="Andrew M. Stuart">
        <attvalues />
      </node>
      <node id="Yuan-Xiang Zhang" label="Yuan-Xiang Zhang">
        <attvalues />
      </node>
      <node id="Yao Xie" label="Yao Xie">
        <attvalues />
      </node>
      <node id="David Siegmund" label="David Siegmund">
        <attvalues />
      </node>
      <node id="Xin Gao" label="Xin Gao">
        <attvalues />
      </node>
      <node id="Grace Y. Yi" label="Grace Y. Yi">
        <attvalues />
      </node>
      <node id="A. Farcomeni" label="A. Farcomeni">
        <attvalues />
      </node>
      <node id="F. Pennoni" label="F. Pennoni">
        <attvalues />
      </node>
      <node id="Camelia Goga" label="Camelia Goga">
        <attvalues />
      </node>
      <node id="Pauline Lardin" label="Pauline Lardin">
        <attvalues />
      </node>
      <node id="Nadia Morsli" label="Nadia Morsli">
        <attvalues />
      </node>
      <node id="Jean-François Coeurjolly" label="Jean-François Coeurjolly">
        <attvalues />
      </node>
      <node id="Christian Francq" label="Christian Francq">
        <attvalues />
      </node>
      <node id="Olivier Wintenberger" label="Olivier Wintenberger">
        <attvalues />
      </node>
      <node id="Jean-Michel Zakoïan" label="Jean-Michel Zakoïan">
        <attvalues />
      </node>
      <node id="Victor I. Ivanenko" label="Victor I. Ivanenko">
        <attvalues />
      </node>
      <node id="Valery A. Labkovsky" label="Valery A. Labkovsky">
        <attvalues />
      </node>
      <node id="Jinzhu Jia" label="Jinzhu Jia">
        <attvalues />
      </node>
      <node id="Karl Rohe" label="Karl Rohe">
        <attvalues />
      </node>
      <node id="Mahendra Mariadassou" label="Mahendra Mariadassou">
        <attvalues />
      </node>
      <node id="Catherine Matias" label="Catherine Matias">
        <attvalues />
      </node>
      <node id="Song Yu-Ping" label="Song Yu-Ping">
        <attvalues />
      </node>
      <node id="Lin Zheng-Yan" label="Lin Zheng-Yan">
        <attvalues />
      </node>
      <node id="Marc Hoffmann" label="Marc Hoffmann">
        <attvalues />
      </node>
      <node id="Richard Nickl" label="Richard Nickl">
        <attvalues />
      </node>
      <node id="Bing Li" label="Bing Li">
        <attvalues />
      </node>
      <node id="Andreas Artemiou" label="Andreas Artemiou">
        <attvalues />
      </node>
      <node id="Lexin Li" label="Lexin Li">
        <attvalues />
      </node>
      <node id="Romain Azaïs" label="Romain Azaïs">
        <attvalues />
      </node>
      <node id="François Dufour" label="François Dufour">
        <attvalues />
      </node>
      <node id="Anne Gégout-Petit" label="Anne Gégout-Petit">
        <attvalues />
      </node>
      <node id="Dan Shen" label="Dan Shen">
        <attvalues />
      </node>
      <node id="Haipeng Shen" label="Haipeng Shen">
        <attvalues />
      </node>
      <node id="J. S. Marron" label="J. S. Marron">
        <attvalues />
      </node>
      <node id="William Chakry Kengne" label="William Chakry Kengne">
        <attvalues />
      </node>
      <node id="Weining Shen" label="Weining Shen">
        <attvalues />
      </node>
      <node id="Subhashis Ghosal" label="Subhashis Ghosal">
        <attvalues />
      </node>
      <node id="Jean-Baptiste Durand" label="Jean-Baptiste Durand">
        <attvalues />
      </node>
      <node id="Y. Guédon" label="Y. Guédon">
        <attvalues />
      </node>
      <node id="Fatemeh Azizzadeh" label="Fatemeh Azizzadeh">
        <attvalues />
      </node>
      <node id="Saeid Rezakhah" label="Saeid Rezakhah">
        <attvalues />
      </node>
      <node id="Frédéric Lavancier" label="Frédéric Lavancier">
        <attvalues />
      </node>
      <node id="Jesper Møller" label="Jesper Møller">
        <attvalues />
      </node>
      <node id="Ege Rubak" label="Ege Rubak">
        <attvalues />
      </node>
      <node id="Jose A. Diaz-Garcia" label="Jose A. Diaz-Garcia">
        <attvalues />
      </node>
      <node id="Aharon Birnbaum" label="Aharon Birnbaum">
        <attvalues />
      </node>
      <node id="Iain M. Johnstone" label="Iain M. Johnstone">
        <attvalues />
      </node>
      <node id="Boaz Nadler" label="Boaz Nadler">
        <attvalues />
      </node>
      <node id="Debashis Paul" label="Debashis Paul">
        <attvalues />
      </node>
      <node id="Dong Li" label="Dong Li">
        <attvalues />
      </node>
      <node id="Shiqing Ling" label="Shiqing Ling">
        <attvalues />
      </node>
      <node id="Howell Tong" label="Howell Tong">
        <attvalues />
      </node>
      <node id="Runze Li" label="Runze Li">
        <attvalues />
      </node>
      <node id="Aurore Delaigle" label="Aurore Delaigle">
        <attvalues />
      </node>
      <node id="Juan-Juan Cai" label="Juan-Juan Cai">
        <attvalues />
      </node>
      <node id="John H. J. Einmahl" label="John H. J. Einmahl">
        <attvalues />
      </node>
      <node id="Laurens de Haan" label="Laurens de Haan">
        <attvalues />
      </node>
      <node id="Pierre Alquier" label="Pierre Alquier">
        <attvalues />
      </node>
      <node id="Cristina Butucea" label="Cristina Butucea">
        <attvalues />
      </node>
      <node id="Katia Meziani" label="Katia Meziani">
        <attvalues />
      </node>
      <node id="Morimae Tomoyuki" label="Morimae Tomoyuki">
        <attvalues />
      </node>
      <node id="Hua Liang" label="Hua Liang">
        <attvalues />
      </node>
      <node id="Xiang Liu" label="Xiang Liu">
        <attvalues />
      </node>
      <node id="Chih-Ling Tsai" label="Chih-Ling Tsai">
        <attvalues />
      </node>
      <node id="Valentin Konakov" label="Valentin Konakov">
        <attvalues />
      </node>
      <node id="Enno Mammen" label="Enno Mammen">
        <attvalues />
      </node>
      <node id="Jeannette Woerner" label="Jeannette Woerner">
        <attvalues />
      </node>
      <node id="Fabien Navarro" label="Fabien Navarro">
        <attvalues />
      </node>
      <node id="Christophe Chesneau" label="Christophe Chesneau">
        <attvalues />
      </node>
      <node id="Jalal Fadili" label="Jalal Fadili">
        <attvalues />
      </node>
      <node id="Taoufik Sassi" label="Taoufik Sassi">
        <attvalues />
      </node>
      <node id="Ming Yuan" label="Ming Yuan">
        <attvalues />
      </node>
      <node id="Sylvain Arlot" label="Sylvain Arlot">
        <attvalues />
      </node>
      <node id="Alain Celisse" label="Alain Celisse">
        <attvalues />
      </node>
      <node id="Zaid Harchaoui" label="Zaid Harchaoui">
        <attvalues />
      </node>
      <node id="Mohammad Jafari Jozani" label="Mohammad Jafari Jozani">
        <attvalues />
      </node>
      <node id="Eric Marchand" label="Eric Marchand">
        <attvalues />
      </node>
      <node id="William Strawderman" label="William Strawderman">
        <attvalues />
      </node>
      <node id="László Varga" label="László Varga">
        <attvalues />
      </node>
      <node id="András Zempléni" label="András Zempléni">
        <attvalues />
      </node>
      <node id="Yacine Aït-Sahalia" label="Yacine Aït-Sahalia">
        <attvalues />
      </node>
      <node id="Jean Jacod" label="Jean Jacod">
        <attvalues />
      </node>
      <node id="Filippo Palombi" label="Filippo Palombi">
        <attvalues />
      </node>
      <node id="Simona Toti" label="Simona Toti">
        <attvalues />
      </node>
      <node id="Romina Filippini" label="Romina Filippini">
        <attvalues />
      </node>
      <node id="Ery Arias-Castro" label="Ery Arias-Castro">
        <attvalues />
      </node>
      <node id="Karim Lounici" label="Karim Lounici">
        <attvalues />
      </node>
      <node id="Debdeep Pati" label="Debdeep Pati">
        <attvalues />
      </node>
      <node id="Anirban Bhattacharya" label="Anirban Bhattacharya">
        <attvalues />
      </node>
      <node id="Natesh S. Pillai" label="Natesh S. Pillai">
        <attvalues />
      </node>
      <node id="David Dunson" label="David Dunson">
        <attvalues />
      </node>
      <node id="Young K. Lee" label="Young K. Lee">
        <attvalues />
      </node>
      <node id="Byeong U. Park" label="Byeong U. Park">
        <attvalues />
      </node>
      <node id="John Kolassa" label="John Kolassa">
        <attvalues />
      </node>
      <node id="John Robinson" label="John Robinson">
        <attvalues />
      </node>
      <node id="Xiaoyin Li" label="Xiaoyin Li">
        <attvalues />
      </node>
      <node id="Christophe Giraud" label="Christophe Giraud">
        <attvalues />
      </node>
      <node id="Alexandre Tsybakov" label="Alexandre Tsybakov">
        <attvalues />
      </node>
      <node id="Daniel Commenges" label="Daniel Commenges">
        <attvalues />
      </node>
      <node id="Cécile Proust-Lima" label="Cécile Proust-Lima">
        <attvalues />
      </node>
      <node id="Cécilia Samieri" label="Cécilia Samieri">
        <attvalues />
      </node>
      <node id="Benoit Liquet" label="Benoit Liquet">
        <attvalues />
      </node>
      <node id="Lee Dicker" label="Lee Dicker">
        <attvalues />
      </node>
      <node id="Xihong Lin" label="Xihong Lin">
        <attvalues />
      </node>
      <node id="Joseph S. Koopmeiners" label="Joseph S. Koopmeiners">
        <attvalues />
      </node>
      <node id="Ziding Feng" label="Ziding Feng">
        <attvalues />
      </node>
      <node id="Yu-Ru Su" label="Yu-Ru Su">
        <attvalues />
      </node>
      <node id="Anita Behme" label="Anita Behme">
        <attvalues />
      </node>
      <node id="Makoto Maejima" label="Makoto Maejima">
        <attvalues />
      </node>
      <node id="Muneya Matsui" label="Muneya Matsui">
        <attvalues />
      </node>
      <node id="Noriyoshi Sakuma" label="Noriyoshi Sakuma">
        <attvalues />
      </node>
      <node id="Sébastien Da Veiga" label="Sébastien Da Veiga">
        <attvalues />
      </node>
      <node id="Fabienne Comte" label="Fabienne Comte">
        <attvalues />
      </node>
      <node id="Charles-André Cuénod" label="Charles-André Cuénod">
        <attvalues />
      </node>
      <node id="Yves Rozenholc" label="Yves Rozenholc">
        <attvalues />
      </node>
      <node id="Kshitij Khare" label="Kshitij Khare">
        <attvalues />
      </node>
      <node id="James P. Hobert" label="James P. Hobert">
        <attvalues />
      </node>
      <node id="Mark S. Kaiser" label="Mark S. Kaiser">
        <attvalues />
      </node>
      <node id="Soumendra N. Lahiri" label="Soumendra N. Lahiri">
        <attvalues />
      </node>
      <node id="Daniel J. Nordman" label="Daniel J. Nordman">
        <attvalues />
      </node>
      <node id="Yan Sun" label="Yan Sun">
        <attvalues />
      </node>
      <node id="Dan Ralescu" label="Dan Ralescu">
        <attvalues />
      </node>
      <node id="Nina Huber" label="Nina Huber">
        <attvalues />
      </node>
      <node id="Luai Al Labadi" label="Luai Al Labadi">
        <attvalues />
      </node>
      <node id="Mahmoud Zarepour" label="Mahmoud Zarepour">
        <attvalues />
      </node>
      <node id="Olivier Ledoit" label="Olivier Ledoit">
        <attvalues />
      </node>
      <node id="Michael Wolf" label="Michael Wolf">
        <attvalues />
      </node>
      <node id="Yuan Wu" label="Yuan Wu">
        <attvalues />
      </node>
      <node id="Ying Zhang" label="Ying Zhang">
        <attvalues />
      </node>
      <node id="Fuchang Gao" label="Fuchang Gao">
        <attvalues />
      </node>
      <node id="Christoph Rothe" label="Christoph Rothe">
        <attvalues />
      </node>
      <node id="Melanie Schienle" label="Melanie Schienle">
        <attvalues />
      </node>
      <node id="Othmane Kortbi" label="Othmane Kortbi">
        <attvalues />
      </node>
      <node id="Éric Marchand" label="Éric Marchand">
        <attvalues />
      </node>
      <node id="Miklos Csorgo" label="Miklos Csorgo">
        <attvalues />
      </node>
      <node id="Yuliya Martsynyuk" label="Yuliya Martsynyuk">
        <attvalues />
      </node>
      <node id="Masoud Nasari" label="Masoud Nasari">
        <attvalues />
      </node>
      <node id="Jan Draisma" label="Jan Draisma">
        <attvalues />
      </node>
      <node id="Sonja Kuhnt" label="Sonja Kuhnt">
        <attvalues />
      </node>
      <node id="Piotr Zwiernik" label="Piotr Zwiernik">
        <attvalues />
      </node>
      <node id="Sonia Petrone" label="Sonia Petrone">
        <attvalues />
      </node>
      <node id="Judith Rousseau" label="Judith Rousseau">
        <attvalues />
      </node>
      <node id="Catia Scricciolo" label="Catia Scricciolo">
        <attvalues />
      </node>
      <node id="Silvia L. P. Ferrari" label="Silvia L. P. Ferrari">
        <attvalues />
      </node>
      <node id="Eliane C. Pinheiro" label="Eliane C. Pinheiro">
        <attvalues />
      </node>
      <node id="Fasano María Victoria" label="Fasano María Victoria">
        <attvalues />
      </node>
      <node id="Ricardo A. Maronna" label="Ricardo A. Maronna">
        <attvalues />
      </node>
      <node id="Richard J. Samworth" label="Richard J. Samworth">
        <attvalues />
      </node>
      <node id="Jelena Bradic" label="Jelena Bradic">
        <attvalues />
      </node>
      <node id="Rui Song" label="Rui Song">
        <attvalues />
      </node>
      <node id="Zhao Ren" label="Zhao Ren">
        <attvalues />
      </node>
      <node id="Harrison H. Zhou" label="Harrison H. Zhou">
        <attvalues />
      </node>
      <node id="Jan Johannes" label="Jan Johannes">
        <attvalues />
      </node>
      <node id="Maik Schwarz" label="Maik Schwarz">
        <attvalues />
      </node>
      <node id="Sidney Resnick" label="Sidney Resnick">
        <attvalues />
      </node>
      <node id="Thibault Espinasse" label="Thibault Espinasse">
        <attvalues />
      </node>
      <node id="Paul Rochet" label="Paul Rochet">
        <attvalues />
      </node>
      <node id="Loic Le Gratiet" label="Loic Le Gratiet">
        <attvalues />
      </node>
      <node id="Josselin Garnier" label="Josselin Garnier">
        <attvalues />
      </node>
      <node id="Bas Kleijn" label="Bas Kleijn">
        <attvalues />
      </node>
      <node id="Bartek Knapik" label="Bartek Knapik">
        <attvalues />
      </node>
      <node id="Jens-Peter Kreiss" label="Jens-Peter Kreiss">
        <attvalues />
      </node>
      <node id="Efstathios Paparoditis" label="Efstathios Paparoditis">
        <attvalues />
      </node>
      <node id="Herold Dehling" label="Herold Dehling">
        <attvalues />
      </node>
      <node id="Daniel Vogel" label="Daniel Vogel">
        <attvalues />
      </node>
      <node id="Martin Wendler" label="Martin Wendler">
        <attvalues />
      </node>
      <node id="Dominik Wied" label="Dominik Wied">
        <attvalues />
      </node>
      <node id="Hans-Georg Müller" label="Hans-Georg Müller">
        <attvalues />
      </node>
      <node id="Fang Yao" label="Fang Yao">
        <attvalues />
      </node>
      <node id="Randal Douc" label="Randal Douc">
        <attvalues />
      </node>
      <node id="Paul Doukhan" label="Paul Doukhan">
        <attvalues />
      </node>
      <node id="Eric Moulines" label="Eric Moulines">
        <attvalues />
      </node>
      <node id="Mathilde Mougeot" label="Mathilde Mougeot">
        <attvalues />
      </node>
      <node id="Karine Tribouley" label="Karine Tribouley">
        <attvalues />
      </node>
      <node id="M. Doostparast" label="M. Doostparast">
        <attvalues />
      </node>
      <node id="N. Balakrishnan" label="N. Balakrishnan">
        <attvalues />
      </node>
      <node id="Nelson Antunes" label="Nelson Antunes">
        <attvalues />
      </node>
      <node id="Vladas Pipiras" label="Vladas Pipiras">
        <attvalues />
      </node>
      <node id="Subhajit Dutta" label="Subhajit Dutta">
        <attvalues />
      </node>
      <node id="Anil K. Ghosh" label="Anil K. Ghosh">
        <attvalues />
      </node>
      <node id="Probal Chaudhuri" label="Probal Chaudhuri">
        <attvalues />
      </node>
      <node id="Davy Paindaveine" label="Davy Paindaveine">
        <attvalues />
      </node>
      <node id="Germain Van Bever" label="Germain Van Bever">
        <attvalues />
      </node>
      <node id="Ismaël Castillo" label="Ismaël Castillo">
        <attvalues />
      </node>
      <node id="Brice Franke" label="Brice Franke">
        <attvalues />
      </node>
      <node id="Thomas Kott" label="Thomas Kott">
        <attvalues />
      </node>
      <node id="Reg Kulperger" label="Reg Kulperger">
        <attvalues />
      </node>
      <node id="Lothar Heinrich" label="Lothar Heinrich">
        <attvalues />
      </node>
      <node id="Sebastian Lück" label="Sebastian Lück">
        <attvalues />
      </node>
      <node id="Volker Schmidt" label="Volker Schmidt">
        <attvalues />
      </node>
      <node id="Viktor Todorov" label="Viktor Todorov">
        <attvalues />
      </node>
      <node id="George Tauchen" label="George Tauchen">
        <attvalues />
      </node>
      <node id="Marta Ferreira" label="Marta Ferreira">
        <attvalues />
      </node>
      <node id="Helena Ferreira" label="Helena Ferreira">
        <attvalues />
      </node>
      <node id="Pierre Del Moral" label="Pierre Del Moral">
        <attvalues />
      </node>
      <node id="Arnaud Doucet" label="Arnaud Doucet">
        <attvalues />
      </node>
      <node id="Ajay Jasra" label="Ajay Jasra">
        <attvalues />
      </node>
      <node id="Dong Chen" label="Dong Chen">
        <attvalues />
      </node>
      <node id="Ole E. Barndorff-Nielsen" label="Ole E. Barndorff-Nielsen">
        <attvalues />
      </node>
      <node id="José Manuel Corcuera" label="José Manuel Corcuera">
        <attvalues />
      </node>
      <node id="Mark Podolskij" label="Mark Podolskij">
        <attvalues />
      </node>
      <node id="Jérémie Bigot" label="Jérémie Bigot">
        <attvalues />
      </node>
      <node id="Thierry Klein" label="Thierry Klein">
        <attvalues />
      </node>
      <node id="Shaul K. Bar-Lev" label="Shaul K. Bar-Lev">
        <attvalues />
      </node>
      <node id="Andreas Löpker" label="Andreas Löpker">
        <attvalues />
      </node>
      <node id="Wolfgang Stadje" label="Wolfgang Stadje">
        <attvalues />
      </node>
      <node id="Amandine Schreck" label="Amandine Schreck">
        <attvalues />
      </node>
      <node id="Gersende Fort" label="Gersende Fort">
        <attvalues />
      </node>
      <node id="Anneleen Verhasselt" label="Anneleen Verhasselt">
        <attvalues />
      </node>
      <node id="Elena Di Bernadino" label="Elena Di Bernadino">
        <attvalues />
      </node>
      <node id="Thomas Laloë" label="Thomas Laloë">
        <attvalues />
      </node>
      <node id="Akram Kohansal" label="Akram Kohansal">
        <attvalues />
      </node>
      <node id="Yuan Liao" label="Yuan Liao">
        <attvalues />
      </node>
      <node id="Xianyang Zhang" label="Xianyang Zhang">
        <attvalues />
      </node>
      <node id="Xiaofeng Shao" label="Xiaofeng Shao">
        <attvalues />
      </node>
      <node id="Pongpol Ruankong" label="Pongpol Ruankong">
        <attvalues />
      </node>
      <node id="Tippawan Santiwipanont" label="Tippawan Santiwipanont">
        <attvalues />
      </node>
      <node id="Songkiat Sumetkijakan" label="Songkiat Sumetkijakan">
        <attvalues />
      </node>
      <node id="Ana Karina Fermin" label="Ana Karina Fermin">
        <attvalues />
      </node>
      <node id="Carenne Ludeña" label="Carenne Ludeña">
        <attvalues />
      </node>
      <node id="Bing-Yi Jing" label="Bing-Yi Jing">
        <attvalues />
      </node>
      <node id="Xin-Bing Kong" label="Xin-Bing Kong">
        <attvalues />
      </node>
      <node id="Zhi Liu" label="Zhi Liu">
        <attvalues />
      </node>
      <node id="Jana Jurečková" label="Jana Jurečková">
        <attvalues />
      </node>
      <node id="Jan Kalina" label="Jan Kalina">
        <attvalues />
      </node>
      <node id="Philippe Rigollet" label="Philippe Rigollet">
        <attvalues />
      </node>
      <node id="Anthony Brockwell" label="Anthony Brockwell">
        <attvalues />
      </node>
      <node id="Nicolas Jégou" label="Nicolas Jégou">
        <attvalues />
      </node>
      <node id="Alexander B. Németh" label="Alexander B. Németh">
        <attvalues />
      </node>
      <node id="Sándor Z. Németh" label="Sándor Z. Németh">
        <attvalues />
      </node>
      <node id="Xavier Gendre" label="Xavier Gendre">
        <attvalues />
      </node>
      <node id="Tatiane F. N. Melo" label="Tatiane F. N. Melo">
        <attvalues />
      </node>
      <node id="Alexandre G. Patriota" label="Alexandre G. Patriota">
        <attvalues />
      </node>
      <node id="William E. Strawderman" label="William E. Strawderman">
        <attvalues />
      </node>
      <node id="Alexander Jung" label="Alexander Jung">
        <attvalues />
      </node>
      <node id="Sebastian Schmutzhard" label="Sebastian Schmutzhard">
        <attvalues />
      </node>
      <node id="Franz Hlawatsch" label="Franz Hlawatsch">
        <attvalues />
      </node>
      <node id="Martin Sundin" label="Martin Sundin">
        <attvalues />
      </node>
      <node id="Claire Lacour" label="Claire Lacour">
        <attvalues />
      </node>
      <node id="Thanh Mai Pham Ngoc" label="Thanh Mai Pham Ngoc">
        <attvalues />
      </node>
      <node id="Wen Cao" label="Wen Cao">
        <attvalues />
      </node>
      <node id="Clifford Hurvich" label="Clifford Hurvich">
        <attvalues />
      </node>
      <node id="Philippe Soulier" label="Philippe Soulier">
        <attvalues />
      </node>
      <node id="Isaac Skog" label="Isaac Skog">
        <attvalues />
      </node>
      <node id="Peter Händel" label="Peter Händel">
        <attvalues />
      </node>
      <node id="Arun Chandrasekhar" label="Arun Chandrasekhar">
        <attvalues />
      </node>
      <node id="Victor Chernozhukov" label="Victor Chernozhukov">
        <attvalues />
      </node>
      <node id="Francesca Molinari" label="Francesca Molinari">
        <attvalues />
      </node>
      <node id="Paul Schrimpf" label="Paul Schrimpf">
        <attvalues />
      </node>
      <node id="Li Zhou" label="Li Zhou">
        <attvalues />
      </node>
      <node id="Nathan Huntley" label="Nathan Huntley">
        <attvalues />
      </node>
      <node id="Matthias C. M. Troffaes" label="Matthias C. M. Troffaes">
        <attvalues />
      </node>
      <node id="Evarist Giné" label="Evarist Giné">
        <attvalues />
      </node>
      <node id="Sándor Baran" label="Sándor Baran">
        <attvalues />
      </node>
      <node id="Gyula Pap" label="Gyula Pap">
        <attvalues />
      </node>
      <node id="Kinga Sikolya" label="Kinga Sikolya">
        <attvalues />
      </node>
      <node id="Runlong Tang" label="Runlong Tang">
        <attvalues />
      </node>
      <node id="Moulinath Banerjee" label="Moulinath Banerjee">
        <attvalues />
      </node>
      <node id="Michael R. Kosorok" label="Michael R. Kosorok">
        <attvalues />
      </node>
      <node id="J. Gao" label="J. Gao">
        <attvalues />
      </node>
      <node id="Y. Yang" label="Y. Yang">
        <attvalues />
      </node>
      <node id="M. Guo" label="M. Guo">
        <attvalues />
      </node>
      <node id="Yumou Qiu" label="Yumou Qiu">
        <attvalues />
      </node>
      <node id="Claire Cannamela" label="Claire Cannamela">
        <attvalues />
      </node>
      <node id="Ramón Gutierrez-Sanchez" label="Ramón Gutierrez-Sanchez">
        <attvalues />
      </node>
      <node id="Julyan Arbel" label="Julyan Arbel">
        <attvalues />
      </node>
      <node id="Ghislaine Gayraud" label="Ghislaine Gayraud">
        <attvalues />
      </node>
      <node id="Hiroki Hashiguchi" label="Hiroki Hashiguchi">
        <attvalues />
      </node>
      <node id="Yasuhide Numata" label="Yasuhide Numata">
        <attvalues />
      </node>
      <node id="Nobuki Takayama" label="Nobuki Takayama">
        <attvalues />
      </node>
      <node id="Akimichi Takemura" label="Akimichi Takemura">
        <attvalues />
      </node>
      <node id="Stig Larsson" label="Stig Larsson">
        <attvalues />
      </node>
      <node id="Y. Ritov" label="Y. Ritov">
        <attvalues />
      </node>
      <node id="P. J. Bickel" label="P. J. Bickel">
        <attvalues />
      </node>
      <node id="A. C. Gamst" label="A. C. Gamst">
        <attvalues />
      </node>
      <node id="B. J. K. Kleijn" label="B. J. K. Kleijn">
        <attvalues />
      </node>
      <node id="David Källberg" label="David Källberg">
        <attvalues />
      </node>
      <node id="Oleg Seleznjev" label="Oleg Seleznjev">
        <attvalues />
      </node>
      <node id="T. Tony Cai" label="T. Tony Cai">
        <attvalues />
      </node>
      <node id="Zongming Ma" label="Zongming Ma">
        <attvalues />
      </node>
      <node id="Yihong Wu" label="Yihong Wu">
        <attvalues />
      </node>
      <node id="Guillaume Lecué" label="Guillaume Lecué">
        <attvalues />
      </node>
      <node id="Shahar Mendelson" label="Shahar Mendelson">
        <attvalues />
      </node>
      <node id="Willem Kruijer" label="Willem Kruijer">
        <attvalues />
      </node>
      <node id="Chris Lloyd" label="Chris Lloyd">
        <attvalues />
      </node>
      <node id="Paul Kabaila" label="Paul Kabaila">
        <attvalues />
      </node>
      <node id="Abhishek Bhattacharya" label="Abhishek Bhattacharya">
        <attvalues />
      </node>
      <node id="Arup Bose" label="Arup Bose">
        <attvalues />
      </node>
      <node id="Davide Farchione" label="Davide Farchione">
        <attvalues />
      </node>
      <node id="G. Fort" label="G. Fort">
        <attvalues />
      </node>
      <node id="E. Moulines" label="E. Moulines">
        <attvalues />
      </node>
      <node id="P. Priouret" label="P. Priouret">
        <attvalues />
      </node>
      <node id="Aboubacar Amiri" label="Aboubacar Amiri">
        <attvalues />
      </node>
      <node id="Christophe Crambes" label="Christophe Crambes">
        <attvalues />
      </node>
      <node id="Baba Thiam" label="Baba Thiam">
        <attvalues />
      </node>
      <node id="Y. Maleki" label="Y. Maleki">
        <attvalues />
      </node>
      <node id="S. Rezakhah" label="S. Rezakhah">
        <attvalues />
      </node>
      <node id="Caroline Uhler" label="Caroline Uhler">
        <attvalues />
      </node>
      <node id="Garvesh Raskutti" label="Garvesh Raskutti">
        <attvalues />
      </node>
      <node id="Bin Yu" label="Bin Yu">
        <attvalues />
      </node>
      <node id="Xinjia Chen" label="Xinjia Chen">
        <attvalues />
      </node>
      <node id="Zhengjia Chen" label="Zhengjia Chen">
        <attvalues />
      </node>
      <node id="István Berkes" label="István Berkes">
        <attvalues />
      </node>
      <node id="Lajos Horváth" label="Lajos Horváth">
        <attvalues />
      </node>
      <node id="Johannes Schauer" label="Johannes Schauer">
        <attvalues />
      </node>
      <node id="Angela Blanco-Fernández" label="Angela Blanco-Fernández">
        <attvalues />
      </node>
      <node id="Marta García-Bárzana" label="Marta García-Bárzana">
        <attvalues />
      </node>
      <node id="Ana Colubi" label="Ana Colubi">
        <attvalues />
      </node>
      <node id="Erricos J. Kontoghiorghes" label="Erricos J. Kontoghiorghes">
        <attvalues />
      </node>
      <node id="Céline Vial" label="Céline Vial">
        <attvalues />
      </node>
      <node id="Valentin Patilea" label="Valentin Patilea">
        <attvalues />
      </node>
      <node id="Cesar Sanchez-Sellero" label="Cesar Sanchez-Sellero">
        <attvalues />
      </node>
      <node id="Matthieu Saumard" label="Matthieu Saumard">
        <attvalues />
      </node>
      <node id="Alois Kneip" label="Alois Kneip">
        <attvalues />
      </node>
      <node id="Pascal Sarda" label="Pascal Sarda">
        <attvalues />
      </node>
      <node id="Axel Bücher" label="Axel Bücher">
        <attvalues />
      </node>
      <node id="Liliana Forzani" label="Liliana Forzani">
        <attvalues />
      </node>
      <node id="Adam J. Rothman" label="Adam J. Rothman">
        <attvalues />
      </node>
      <node id="Pedro C. Álvarez-Esteban" label="Pedro C. Álvarez-Esteban">
        <attvalues />
      </node>
      <node id="Eustasio del Barrio" label="Eustasio del Barrio">
        <attvalues />
      </node>
      <node id="Juan A. Cuesta-Albertos" label="Juan A. Cuesta-Albertos">
        <attvalues />
      </node>
      <node id="Carlos Matrán" label="Carlos Matrán">
        <attvalues />
      </node>
      <node id="Davit Varron" label="Davit Varron">
        <attvalues />
      </node>
      <node id="Ingrid Van Keilegom" label="Ingrid Van Keilegom">
        <attvalues />
      </node>
      <node id="Magalie Fromont" label="Magalie Fromont">
        <attvalues />
      </node>
      <node id="Béatrice Laurent" label="Béatrice Laurent">
        <attvalues />
      </node>
      <node id="Patricia Reynaud-Bouret" label="Patricia Reynaud-Bouret">
        <attvalues />
      </node>
      <node id="Xiao Wang" label="Xiao Wang">
        <attvalues />
      </node>
      <node id="Jinglai Shen" label="Jinglai Shen">
        <attvalues />
      </node>
      <node id="Seunggeun Lee" label="Seunggeun Lee">
        <attvalues />
      </node>
      <node id="Fei Zou" label="Fei Zou">
        <attvalues />
      </node>
      <node id="Fred A. Wright" label="Fred A. Wright">
        <attvalues />
      </node>
      <node id="Jelle J. Goeman" label="Jelle J. Goeman">
        <attvalues />
      </node>
      <node id="Aldo Solari" label="Aldo Solari">
        <attvalues />
      </node>
      <node id="Masoud Asgharian" label="Masoud Asgharian">
        <attvalues />
      </node>
      <node id="Marco Carone" label="Marco Carone">
        <attvalues />
      </node>
      <node id="Vahid Fakoor" label="Vahid Fakoor">
        <attvalues />
      </node>
      <node id="Li Song" label="Li Song">
        <attvalues />
      </node>
      <node id="Florent Autin" label="Florent Autin">
        <attvalues />
      </node>
      <node id="Christophe Pouet" label="Christophe Pouet">
        <attvalues />
      </node>
      <node id="Armin Schwartzman" label="Armin Schwartzman">
        <attvalues />
      </node>
      <node id="Yulia Gavrilov" label="Yulia Gavrilov">
        <attvalues />
      </node>
      <node id="Robert J. Adler" label="Robert J. Adler">
        <attvalues />
      </node>
      <node id="Loïc Hervé" label="Loïc Hervé">
        <attvalues />
      </node>
      <node id="James Ledoux" label="James Ledoux">
        <attvalues />
      </node>
      <node id="Yuejie Chi" label="Yuejie Chi">
        <attvalues />
      </node>
      <node id="Robert Calderbank" label="Robert Calderbank">
        <attvalues />
      </node>
      <node id="Myriam Maumy" label="Myriam Maumy">
        <attvalues />
      </node>
      <node id="Ahmed Bensalma" label="Ahmed Bensalma">
        <attvalues />
      </node>
      <node id="Mohamed Bentarzi" label="Mohamed Bentarzi">
        <attvalues />
      </node>
      <node id="Tamio Koyama" label="Tamio Koyama">
        <attvalues />
      </node>
      <node id="Hiromasa Nakayama" label="Hiromasa Nakayama">
        <attvalues />
      </node>
      <node id="Kenta Nishiyama" label="Kenta Nishiyama">
        <attvalues />
      </node>
      <node id="John Ehrlinger" label="John Ehrlinger">
        <attvalues />
      </node>
      <node id="Hemant Ishwaran" label="Hemant Ishwaran">
        <attvalues />
      </node>
      <node id="Theofanis Sapatinas" label="Theofanis Sapatinas">
        <attvalues />
      </node>
      <node id="Yair Goldberg" label="Yair Goldberg">
        <attvalues />
      </node>
      <node id="Anthony J Webster" label="Anthony J Webster">
        <attvalues />
      </node>
      <node id="Richard Kemp" label="Richard Kemp">
        <attvalues />
      </node>
      <node id="Vladimir Vovk" label="Vladimir Vovk">
        <attvalues />
      </node>
      <node id="Ruodu Wang" label="Ruodu Wang">
        <attvalues />
      </node>
      <node id="A. Kohansal" label="A. Kohansal">
        <attvalues />
      </node>
      <node id="Piotr Pokarowski" label="Piotr Pokarowski">
        <attvalues />
      </node>
      <node id="Jan Mielniczuk" label="Jan Mielniczuk">
        <attvalues />
      </node>
      <node id="Paweł Teisseyre" label="Paweł Teisseyre">
        <attvalues />
      </node>
      <node id="Eric J. Tchetgen Tchetgen" label="Eric J. Tchetgen Tchetgen">
        <attvalues />
      </node>
      <node id="Ilya Shpitser" label="Ilya Shpitser">
        <attvalues />
      </node>
      <node id="Yanqing Hu" label="Yanqing Hu">
        <attvalues />
      </node>
      <node id="Feifang Hu" label="Feifang Hu">
        <attvalues />
      </node>
      <node id="Yvik Swan" label="Yvik Swan">
        <attvalues />
      </node>
      <node id="Thomas Verdebout" label="Thomas Verdebout">
        <attvalues />
      </node>
      <node id="Chun-Yang Wang" label="Chun-Yang Wang">
        <attvalues />
      </node>
      <node id="Wen-Xian Yang" label="Wen-Xian Yang">
        <attvalues />
      </node>
      <node id="Hong Du" label="Hong Du">
        <attvalues />
      </node>
      <node id="Pauliina Ilmonen" label="Pauliina Ilmonen">
        <attvalues />
      </node>
      <node id="Markus Reiss" label="Markus Reiss">
        <attvalues />
      </node>
      <node id="Ben Haaland" label="Ben Haaland">
        <attvalues />
      </node>
      <node id="Peter Z. G. Qian" label="Peter Z. G. Qian">
        <attvalues />
      </node>
      <node id="Natalia A. Bochkina" label="Natalia A. Bochkina">
        <attvalues />
      </node>
      <node id="Peter J. Green" label="Peter J. Green">
        <attvalues />
      </node>
      <node id="Yindeng Jiang" label="Yindeng Jiang">
        <attvalues />
      </node>
      <node id="Michael D. Perlman" label="Michael D. Perlman">
        <attvalues />
      </node>
      <node id="Clifford Lam" label="Clifford Lam">
        <attvalues />
      </node>
      <node id="Shuyuan He" label="Shuyuan He">
        <attvalues />
      </node>
      <node id="Wei Liang" label="Wei Liang">
        <attvalues />
      </node>
      <node id="Junshan Shen" label="Junshan Shen">
        <attvalues />
      </node>
      <node id="Grace Yang" label="Grace Yang">
        <attvalues />
      </node>
      <node id="Mathieu Rosenbaum" label="Mathieu Rosenbaum">
        <attvalues />
      </node>
      <node id="M. J. Bayarri" label="M. J. Bayarri">
        <attvalues />
      </node>
      <node id="J. O. Berger" label="J. O. Berger">
        <attvalues />
      </node>
      <node id="A. Forte" label="A. Forte">
        <attvalues />
      </node>
      <node id="G. García-Donato" label="G. García-Donato">
        <attvalues />
      </node>
      <node id="Stefano Favaro" label="Stefano Favaro">
        <attvalues />
      </node>
      <node id="Alessandra Guglielmi" label="Alessandra Guglielmi">
        <attvalues />
      </node>
      <node id="Masoud M. Nasari" label="Masoud M. Nasari">
        <attvalues />
      </node>
      <node id="Yonggang Hu" label="Yonggang Hu">
        <attvalues />
      </node>
      <node id="Yong Wang" label="Yong Wang">
        <attvalues />
      </node>
      <node id="Yi Wu" label="Yi Wu">
        <attvalues />
      </node>
      <node id="David B. Dunson" label="David B. Dunson">
        <attvalues />
      </node>
      <node id="A. Goldenshluger" label="A. Goldenshluger">
        <attvalues />
      </node>
      <node id="O. Lepski" label="O. Lepski">
        <attvalues />
      </node>
      <node id="Xia Cui" label="Xia Cui">
        <attvalues />
      </node>
      <node id="Wolfgang Karl Härdle" label="Wolfgang Karl Härdle">
        <attvalues />
      </node>
      <node id="Lixing Zhu" label="Lixing Zhu">
        <attvalues />
      </node>
      <node id="Stephane Girard" label="Stephane Girard">
        <attvalues />
      </node>
      <node id="Armelle Guillou" label="Armelle Guillou">
        <attvalues />
      </node>
      <node id="Gilles Stupfler" label="Gilles Stupfler">
        <attvalues />
      </node>
      <node id="Kung-Sik Chan" label="Kung-Sik Chan">
        <attvalues />
      </node>
      <node id="Henghsiu Tsai" label="Henghsiu Tsai">
        <attvalues />
      </node>
      <node id="João Renato Sebastião" label="João Renato Sebastião">
        <attvalues />
      </node>
      <node id="Ana Paula Martins" label="Ana Paula Martins">
        <attvalues />
      </node>
      <node id="Luísa Pereira" label="Luísa Pereira">
        <attvalues />
      </node>
      <node id="Aad van der Vaart" label="Aad van der Vaart">
        <attvalues />
      </node>
      <node id="Javier Hualde" label="Javier Hualde">
        <attvalues />
      </node>
      <node id="Peter M. Robinson" label="Peter M. Robinson">
        <attvalues />
      </node>
      <node id="L. R. Haff" label="L. R. Haff">
        <attvalues />
      </node>
      <node id="P. T. Kim" label="P. T. Kim">
        <attvalues />
      </node>
      <node id="J. -Y. Koo" label="J. -Y. Koo">
        <attvalues />
      </node>
      <node id="D. St. P. Richards" label="D. St. P. Richards">
        <attvalues />
      </node>
      <node id="Francis Comets" label="Francis Comets">
        <attvalues />
      </node>
      <node id="Mikael Falconnet" label="Mikael Falconnet">
        <attvalues />
      </node>
      <node id="Oleg Loukianov" label="Oleg Loukianov">
        <attvalues />
      </node>
      <node id="Dasha Loukianova" label="Dasha Loukianova">
        <attvalues />
      </node>
      <node id="Darren Homrighausen" label="Darren Homrighausen">
        <attvalues />
      </node>
      <node id="Daniel J. McDonald" label="Daniel J. McDonald">
        <attvalues />
      </node>
      <node id="Cécile Durot" label="Cécile Durot">
        <attvalues />
      </node>
      <node id="Piet Groeneboom" label="Piet Groeneboom">
        <attvalues />
      </node>
      <node id="Gourab Mukherjee" label="Gourab Mukherjee">
        <attvalues />
      </node>
      <node id="Lyudmila Grigoryeva" label="Lyudmila Grigoryeva">
        <attvalues />
      </node>
      <node id="Juan-Pablo Ortega" label="Juan-Pablo Ortega">
        <attvalues />
      </node>
      <node id="Bernard Bercu" label="Bernard Bercu">
        <attvalues />
      </node>
      <node id="Thi Mong Ngoc Nguyen" label="Thi Mong Ngoc Nguyen">
        <attvalues />
      </node>
      <node id="Jerome Saracco" label="Jerome Saracco">
        <attvalues />
      </node>
      <node id="Jimmy Olsson" label="Jimmy Olsson">
        <attvalues />
      </node>
      <node id="Yuichi Hirose" label="Yuichi Hirose">
        <attvalues />
      </node>
      <node id="Alan Lee" label="Alan Lee">
        <attvalues />
      </node>
      <node id="Paulo C. Marques F." label="Paulo C. Marques F.">
        <attvalues />
      </node>
      <node id="Carlos A. de B. Pereira" label="Carlos A. de B. Pereira">
        <attvalues />
      </node>
      <node id="Jinyuan Chang" label="Jinyuan Chang">
        <attvalues />
      </node>
      <node id="Arnak Dalalyan" label="Arnak Dalalyan">
        <attvalues />
      </node>
      <node id="Yuri Ingster" label="Yuri Ingster">
        <attvalues />
      </node>
      <node id="Min Yang" label="Min Yang">
        <attvalues />
      </node>
      <node id="John Stufken" label="John Stufken">
        <attvalues />
      </node>
      <node id="David Dereudre" label="David Dereudre">
        <attvalues />
      </node>
      <node id="Katerina Helisova Stankova" label="Katerina Helisova Stankova">
        <attvalues />
      </node>
      <node id="Tony Cai" label="Tony Cai">
        <attvalues />
      </node>
      <node id="Nate Strawn" label="Nate Strawn">
        <attvalues />
      </node>
      <node id="Artin Armagan" label="Artin Armagan">
        <attvalues />
      </node>
      <node id="Rayan Saab" label="Rayan Saab">
        <attvalues />
      </node>
      <node id="Lawrence Carin" label="Lawrence Carin">
        <attvalues />
      </node>
      <node id="Daniel Berend" label="Daniel Berend">
        <attvalues />
      </node>
      <node id="Aryeh Kontorovich" label="Aryeh Kontorovich">
        <attvalues />
      </node>
      <node id="Yu Tang" label="Yu Tang">
        <attvalues />
      </node>
      <node id="Hongquan Xu" label="Hongquan Xu">
        <attvalues />
      </node>
      <node id="Dennis K. J. Lin" label="Dennis K. J. Lin">
        <attvalues />
      </node>
      <node id="Samuel Vaiter" label="Samuel Vaiter">
        <attvalues />
      </node>
      <node id="Charles Deledalle" label="Charles Deledalle">
        <attvalues />
      </node>
      <node id="Gabriel Peyré" label="Gabriel Peyré">
        <attvalues />
      </node>
      <node id="Charles Dossal" label="Charles Dossal">
        <attvalues />
      </node>
      <node id="Eric Beutner" label="Eric Beutner">
        <attvalues />
      </node>
      <node id="Henryk Zähle" label="Henryk Zähle">
        <attvalues />
      </node>
      <node id="Ismael Castillo" label="Ismael Castillo">
        <attvalues />
      </node>
      <node id="Gerard Kerkyacharian" label="Gerard Kerkyacharian">
        <attvalues />
      </node>
      <node id="Ke Zhu" label="Ke Zhu">
        <attvalues />
      </node>
      <node id="Xiang Zhang" label="Xiang Zhang">
        <attvalues />
      </node>
      <node id="Yanbing Zheng" label="Yanbing Zheng">
        <attvalues />
      </node>
      <node id="Tze Leung Lai" label="Tze Leung Lai">
        <attvalues />
      </node>
      <node id="Shulamith T. Gross" label="Shulamith T. Gross">
        <attvalues />
      </node>
      <node id="David Bo Shen" label="David Bo Shen">
        <attvalues />
      </node>
      <node id="Niels Richard Hansen" label="Niels Richard Hansen">
        <attvalues />
      </node>
      <node id="Vincent Rivoirard" label="Vincent Rivoirard">
        <attvalues />
      </node>
      <node id="Jean-David Fermanian" label="Jean-David Fermanian">
        <attvalues />
      </node>
      <node id="Dragan Radulovic" label="Dragan Radulovic">
        <attvalues />
      </node>
      <node id="Marten Wegkamp" label="Marten Wegkamp">
        <attvalues />
      </node>
      <node id="Guangming Pan" label="Guangming Pan">
        <attvalues />
      </node>
      <node id="Qi-Man Shao" label="Qi-Man Shao">
        <attvalues />
      </node>
      <node id="Wang Zhou" label="Wang Zhou">
        <attvalues />
      </node>
      <node id="M. Z. Anis" label="M. Z. Anis">
        <attvalues />
      </node>
      <node id="Kinjal Basu" label="Kinjal Basu">
        <attvalues />
      </node>
      <node id="Sokbae Lee" label="Sokbae Lee">
        <attvalues />
      </node>
      <node id="Myung Hwan Seo" label="Myung Hwan Seo">
        <attvalues />
      </node>
      <node id="Youngki Shin" label="Youngki Shin">
        <attvalues />
      </node>
      <node id="Roberto Fontana" label="Roberto Fontana">
        <attvalues />
      </node>
      <node id="Fabio Rapallo" label="Fabio Rapallo">
        <attvalues />
      </node>
      <node id="Maria Piera Rogantin" label="Maria Piera Rogantin">
        <attvalues />
      </node>
      <node id="Mélina Bec" label="Mélina Bec">
        <attvalues />
      </node>
      <node id="Xiangrong Yin" label="Xiangrong Yin">
        <attvalues />
      </node>
      <node id="Alexandra Carpentier" label="Alexandra Carpentier">
        <attvalues />
      </node>
      <node id="Rémi Munos" label="Rémi Munos">
        <attvalues />
      </node>
      <node id="Olga Klopp" label="Olga Klopp">
        <attvalues />
      </node>
      <node id="Martin Ruppert" label="Martin Ruppert">
        <attvalues />
      </node>
      <node id="Yuqiang Li" label="Yuqiang Li">
        <attvalues />
      </node>
      <node id="Hongshuai Dai" label="Hongshuai Dai">
        <attvalues />
      </node>
      <node id="Eugenia Buta" label="Eugenia Buta">
        <attvalues />
      </node>
      <node id="Hani Doss" label="Hani Doss">
        <attvalues />
      </node>
      <node id="Álvaro Calvache" label="Álvaro Calvache">
        <attvalues />
      </node>
      <node id="Viswanathan Arunachalam" label="Viswanathan Arunachalam">
        <attvalues />
      </node>
      <node id="Dominik Janzing" label="Dominik Janzing">
        <attvalues />
      </node>
      <node id="David Balduzzi" label="David Balduzzi">
        <attvalues />
      </node>
      <node id="Moritz Grosse-Wentrup" label="Moritz Grosse-Wentrup">
        <attvalues />
      </node>
      <node id="Bernhard Schölkopf" label="Bernhard Schölkopf">
        <attvalues />
      </node>
      <node id="Romain Guy" label="Romain Guy">
        <attvalues />
      </node>
      <node id="Catherine Laredo" label="Catherine Laredo">
        <attvalues />
      </node>
      <node id="Elisabeta Vergu" label="Elisabeta Vergu">
        <attvalues />
      </node>
      <node id="Sébastien Loustau" label="Sébastien Loustau">
        <attvalues />
      </node>
      <node id="Clément Marteau" label="Clément Marteau">
        <attvalues />
      </node>
      <node id="S. Delattre" label="S. Delattre">
        <attvalues />
      </node>
      <node id="M. Hoffmann" label="M. Hoffmann">
        <attvalues />
      </node>
      <node id="D. Picard" label="D. Picard">
        <attvalues />
      </node>
      <node id="T. Vareschi" label="T. Vareschi">
        <attvalues />
      </node>
      <node id="B. T. Knapik" label="B. T. Knapik">
        <attvalues />
      </node>
      <node id="B. T. Szabó" label="B. T. Szabó">
        <attvalues />
      </node>
      <node id="A. W. van der Vaart" label="A. W. van der Vaart">
        <attvalues />
      </node>
      <node id="J. H. van Zanten" label="J. H. van Zanten">
        <attvalues />
      </node>
      <node id="Víctor Pérez-Abreu" label="Víctor Pérez-Abreu">
        <attvalues />
      </node>
      <node id="Ken-iti Sato" label="Ken-iti Sato">
        <attvalues />
      </node>
      <node id="Arnold Janssen" label="Arnold Janssen">
        <attvalues />
      </node>
      <node id="Martin Tietje" label="Martin Tietje">
        <attvalues />
      </node>
      <node id="Antar Bandyopadhyay" label="Antar Bandyopadhyay">
        <attvalues />
      </node>
      <node id="Sanjay Chaudhuri" label="Sanjay Chaudhuri">
        <attvalues />
      </node>
      <node id="Paul Lemaître" label="Paul Lemaître">
        <attvalues />
      </node>
      <node id="Ekatarina Sergienko" label="Ekatarina Sergienko">
        <attvalues />
      </node>
      <node id="Aurélie Arnaud" label="Aurélie Arnaud">
        <attvalues />
      </node>
      <node id="Nicolas Bousquet" label="Nicolas Bousquet">
        <attvalues />
      </node>
      <node id="Antonio Lijoi" label="Antonio Lijoi">
        <attvalues />
      </node>
      <node id="Igor Prünster" label="Igor Prünster">
        <attvalues />
      </node>
      <node id="Laëtitia Comminges" label="Laëtitia Comminges">
        <attvalues />
      </node>
      <node id="A. N. Varaksin" label="A. N. Varaksin">
        <attvalues />
      </node>
      <node id="V. G. Panov" label="V. G. Panov">
        <attvalues />
      </node>
      <node id="C. Fonseca" label="C. Fonseca">
        <attvalues />
      </node>
      <node id="H. Ferreira" label="H. Ferreira">
        <attvalues />
      </node>
      <node id="L. Pereira A. P." label="L. Pereira A. P.">
        <attvalues />
      </node>
      <node id="Martins" label="Martins">
        <attvalues />
      </node>
      <node id="Naftali Harris" label="Naftali Harris">
        <attvalues />
      </node>
      <node id="Stéphane Girard" label="Stéphane Girard">
        <attvalues />
      </node>
      <node id="Pierre Jacob" label="Pierre Jacob">
        <attvalues />
      </node>
      <node id="Alessandro Soranzo" label="Alessandro Soranzo">
        <attvalues />
      </node>
      <node id="Emanuela Epure" label="Emanuela Epure">
        <attvalues />
      </node>
      <node id="Elena Chernousova" label="Elena Chernousova">
        <attvalues />
      </node>
      <node id="Yuri Golubev" label="Yuri Golubev">
        <attvalues />
      </node>
      <node id="Katerina Krymova" label="Katerina Krymova">
        <attvalues />
      </node>
      <node id="Ting Yan" label="Ting Yan">
        <attvalues />
      </node>
      <node id="Xu Jinfeng" label="Xu Jinfeng">
        <attvalues />
      </node>
      <node id="Siegfried Hörmann" label="Siegfried Hörmann">
        <attvalues />
      </node>
      <node id="Łukasz Kidziński" label="Łukasz Kidziński">
        <attvalues />
      </node>
      <node id="E. Rufeil Fiori" label="E. Rufeil Fiori">
        <attvalues />
      </node>
      <node id="A. Plastino" label="A. Plastino">
        <attvalues />
      </node>
      <node id="Michaël Chichignoud" label="Michaël Chichignoud">
        <attvalues />
      </node>
      <node id="Johannes Lederer" label="Johannes Lederer">
        <attvalues />
      </node>
      <node id="Michal Demetrian" label="Michal Demetrian">
        <attvalues />
      </node>
      <node id="Martin Nehez" label="Martin Nehez">
        <attvalues />
      </node>
      <node id="Pierre-Yves Massé" label="Pierre-Yves Massé">
        <attvalues />
      </node>
      <node id="William Meiniel" label="William Meiniel">
        <attvalues />
      </node>
      <node id="Sébastien Bubeck" label="Sébastien Bubeck">
        <attvalues />
      </node>
      <node id="Gábor Lugosi" label="Gábor Lugosi">
        <attvalues />
      </node>
      <node id="Jose E. Rodriguez" label="Jose E. Rodriguez">
        <attvalues />
      </node>
      <node id="Rogelio Ramos-Quiroga" label="Rogelio Ramos-Quiroga">
        <attvalues />
      </node>
      <node id="Eckhard Schlemm" label="Eckhard Schlemm">
        <attvalues />
      </node>
      <node id="Robert Stelzer" label="Robert Stelzer">
        <attvalues />
      </node>
      <node id="Luo Xiao" label="Luo Xiao">
        <attvalues />
      </node>
      <node id="Yingxing Li" label="Yingxing Li">
        <attvalues />
      </node>
      <node id="Tatiyana V. Apanasovich" label="Tatiyana V. Apanasovich">
        <attvalues />
      </node>
      <node id="David Ruppert" label="David Ruppert">
        <attvalues />
      </node>
      <node id="Clément Dombry" label="Clément Dombry">
        <attvalues />
      </node>
      <node id="Mathieu Ribatet" label="Mathieu Ribatet">
        <attvalues />
      </node>
      <node id="Bodhisattva Sen" label="Bodhisattva Sen">
        <attvalues />
      </node>
      <node id="Michael Woodroofe" label="Michael Woodroofe">
        <attvalues />
      </node>
      <node id="Daniel Bonnéry" label="Daniel Bonnéry">
        <attvalues />
      </node>
      <node id="F. Jay Breidt" label="F. Jay Breidt">
        <attvalues />
      </node>
      <node id="François Coquet" label="François Coquet">
        <attvalues />
      </node>
      <node id="Evgueni Haroutunian" label="Evgueni Haroutunian">
        <attvalues />
      </node>
      <node id="Parandzem Hakobyan" label="Parandzem Hakobyan">
        <attvalues />
      </node>
      <node id="Ahmed A. Quadeer" label="Ahmed A. Quadeer">
        <attvalues />
      </node>
      <node id="Tareq Y. Al-Naffouri" label="Tareq Y. Al-Naffouri">
        <attvalues />
      </node>
      <node id="Luc Devroye" label="Luc Devroye">
        <attvalues />
      </node>
      <node id="Vida Dujmovic" label="Vida Dujmovic">
        <attvalues />
      </node>
      <node id="Adam Krzyzak" label="Adam Krzyzak">
        <attvalues />
      </node>
      <node id="Noureddine Berrahou" label="Noureddine Berrahou">
        <attvalues />
      </node>
      <node id="Lahcen Douge" label="Lahcen Douge">
        <attvalues />
      </node>
      <node id="Jinfeng Xu" label="Jinfeng Xu">
        <attvalues />
      </node>
      <node id="Ting Zhang" label="Ting Zhang">
        <attvalues />
      </node>
      <node id="Wei Biao Wu" label="Wei Biao Wu">
        <attvalues />
      </node>
      <node id="Felix Abramovich" label="Felix Abramovich">
        <attvalues />
      </node>
      <node id="Vadim Grinshtein" label="Vadim Grinshtein">
        <attvalues />
      </node>
      <node id="R. de Jonge" label="R. de Jonge">
        <attvalues />
      </node>
      <node id="Jianhua Hu" label="Jianhua Hu">
        <attvalues />
      </node>
      <node id="Guohua Yan" label="Guohua Yan">
        <attvalues />
      </node>
      <node id="Jinhong You" label="Jinhong You">
        <attvalues />
      </node>
      <node id="Ping Wu" label="Ping Wu">
        <attvalues />
      </node>
      <node id="Winfried Stute" label="Winfried Stute">
        <attvalues />
      </node>
      <node id="Li-Xing Zhu" label="Li-Xing Zhu">
        <attvalues />
      </node>
      <node id="Tiago M. Vargas" label="Tiago M. Vargas">
        <attvalues />
      </node>
      <node id="Artur J. Lemonte" label="Artur J. Lemonte">
        <attvalues />
      </node>
      <node id="Victoria Plamadeala" label="Victoria Plamadeala">
        <attvalues />
      </node>
      <node id="William F. Rosenberger" label="William F. Rosenberger">
        <attvalues />
      </node>
      <node id="Jun Li" label="Jun Li">
        <attvalues />
      </node>
      <node id="Stéphane Boucheron" label="Stéphane Boucheron">
        <attvalues />
      </node>
      <node id="Yunwen Yang" label="Yunwen Yang">
        <attvalues />
      </node>
      <node id="Xuming He" label="Xuming He">
        <attvalues />
      </node>
      <node id="Persi Diaconis" label="Persi Diaconis">
        <attvalues />
      </node>
      <node id="Susan Holmes" label="Susan Holmes">
        <attvalues />
      </node>
      <node id="Mehrdad Shahshahani" label="Mehrdad Shahshahani">
        <attvalues />
      </node>
      <node id="Francesco Bartolucci" label="Francesco Bartolucci">
        <attvalues />
      </node>
      <node id="Luisa Scaccia" label="Luisa Scaccia">
        <attvalues />
      </node>
      <node id="Alessio Farcomeni" label="Alessio Farcomeni">
        <attvalues />
      </node>
      <node id="Wei Liu" label="Wei Liu">
        <attvalues />
      </node>
      <node id="Yuhong Yang" label="Yuhong Yang">
        <attvalues />
      </node>
      <node id="M. Jafari Jozani" label="M. Jafari Jozani">
        <attvalues />
      </node>
      <node id="K. F. Davies" label="K. F. Davies">
        <attvalues />
      </node>
      <node id="Aixin Tan" label="Aixin Tan">
        <attvalues />
      </node>
      <node id="Galin L. Jones" label="Galin L. Jones">
        <attvalues />
      </node>
      <node id="Jun Shao" label="Jun Shao">
        <attvalues />
      </node>
      <node id="Xinwei Deng" label="Xinwei Deng">
        <attvalues />
      </node>
      <node id="Thierry Dumont" label="Thierry Dumont">
        <attvalues />
      </node>
      <node id="Sylvain Le Corff" label="Sylvain Le Corff">
        <attvalues />
      </node>
      <node id="Jushan Bai" label="Jushan Bai">
        <attvalues />
      </node>
      <node id="Kunpeng Li" label="Kunpeng Li">
        <attvalues />
      </node>
      <node id="Mor Ndongo" label="Mor Ndongo">
        <attvalues />
      </node>
      <node id="Abdou Kâ Diongue" label="Abdou Kâ Diongue">
        <attvalues />
      </node>
      <node id="Aliou Diop" label="Aliou Diop">
        <attvalues />
      </node>
      <node id="Simplice Dossou-Gbété" label="Simplice Dossou-Gbété">
        <attvalues />
      </node>
      <node id="Navideh Modarresi" label="Navideh Modarresi">
        <attvalues />
      </node>
      <node id="Vladimir Spokoiny" label="Vladimir Spokoiny">
        <attvalues />
      </node>
      <node id="Weining Wang" label="Weining Wang">
        <attvalues />
      </node>
    </nodes>
    <edges>
      <edge id="0" source="Lingzhou Xue" target="Hui Zou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3555v1" />
          <attvalue for="2" value="Nonconcave penalized composite conditional likelihood estimation of&#10;  sparse Ising models" />
          <attvalue for="3" value="The Ising model is a useful tool for studying complex interactions within a&#10;system. The estimation of such a model, however, is rather challenging,&#10;especially in the presence of high-dimensional parameters. In this work, we&#10;propose efficient procedures for learning a sparse Ising model based on a&#10;penalized composite conditional likelihood with nonconcave penalties.&#10;Nonconcave penalized likelihood estimation has received a lot of attention in&#10;recent years. However, such an approach is computationally prohibitive under&#10;high-dimensional Ising models. To overcome such difficulties, we extend the&#10;methodology and theory of nonconcave penalized likelihood to penalized&#10;composite conditional likelihood estimation. The proposed method can be&#10;efficiently implemented by taking advantage of coordinate-ascent and&#10;minorization--maximization principles. Asymptotic oracle properties of the&#10;proposed method are established with NP-dimensionality. Optimality of the&#10;computed local solution is discussed. We demonstrate its finite sample&#10;performance via simulation studies and further illustrate our proposal by&#10;studying the Human Immunodeficiency Virus type 1 protease structure based on&#10;data from the Stanford HIV drug resistance database. Our statistical learning&#10;results match the known biological findings very well, although no prior&#10;biological information is used in the data analysis procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="1" source="Lingzhou Xue" target="Tianxi Cai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3555v1" />
          <attvalue for="2" value="Nonconcave penalized composite conditional likelihood estimation of&#10;  sparse Ising models" />
          <attvalue for="3" value="The Ising model is a useful tool for studying complex interactions within a&#10;system. The estimation of such a model, however, is rather challenging,&#10;especially in the presence of high-dimensional parameters. In this work, we&#10;propose efficient procedures for learning a sparse Ising model based on a&#10;penalized composite conditional likelihood with nonconcave penalties.&#10;Nonconcave penalized likelihood estimation has received a lot of attention in&#10;recent years. However, such an approach is computationally prohibitive under&#10;high-dimensional Ising models. To overcome such difficulties, we extend the&#10;methodology and theory of nonconcave penalized likelihood to penalized&#10;composite conditional likelihood estimation. The proposed method can be&#10;efficiently implemented by taking advantage of coordinate-ascent and&#10;minorization--maximization principles. Asymptotic oracle properties of the&#10;proposed method are established with NP-dimensionality. Optimality of the&#10;computed local solution is discussed. We demonstrate its finite sample&#10;performance via simulation studies and further illustrate our proposal by&#10;studying the Human Immunodeficiency Virus type 1 protease structure based on&#10;data from the Stanford HIV drug resistance database. Our statistical learning&#10;results match the known biological findings very well, although no prior&#10;biological information is used in the data analysis procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="2" source="Hui Zou" target="Tianxi Cai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3555v1" />
          <attvalue for="2" value="Nonconcave penalized composite conditional likelihood estimation of&#10;  sparse Ising models" />
          <attvalue for="3" value="The Ising model is a useful tool for studying complex interactions within a&#10;system. The estimation of such a model, however, is rather challenging,&#10;especially in the presence of high-dimensional parameters. In this work, we&#10;propose efficient procedures for learning a sparse Ising model based on a&#10;penalized composite conditional likelihood with nonconcave penalties.&#10;Nonconcave penalized likelihood estimation has received a lot of attention in&#10;recent years. However, such an approach is computationally prohibitive under&#10;high-dimensional Ising models. To overcome such difficulties, we extend the&#10;methodology and theory of nonconcave penalized likelihood to penalized&#10;composite conditional likelihood estimation. The proposed method can be&#10;efficiently implemented by taking advantage of coordinate-ascent and&#10;minorization--maximization principles. Asymptotic oracle properties of the&#10;proposed method are established with NP-dimensionality. Optimality of the&#10;computed local solution is discussed. We demonstrate its finite sample&#10;performance via simulation studies and further illustrate our proposal by&#10;studying the Human Immunodeficiency Virus type 1 protease structure based on&#10;data from the Stanford HIV drug resistance database. Our statistical learning&#10;results match the known biological findings very well, although no prior&#10;biological information is used in the data analysis procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="3" source="Ryan Martin" target="Liang Hong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.0103v1" />
          <attvalue for="2" value="On convergence rates of Bayesian predictive densities and posterior&#10;  distributions" />
          <attvalue for="3" value="Frequentist-style large-sample properties of Bayesian posterior&#10;distributions, such as consistency and convergence rates, are important&#10;considerations in nonparametric problems. In this paper we give an analysis of&#10;Bayesian asymptotics based primarily on predictive densities. Our analysis is&#10;unified in the sense that essentially the same approach can be taken to develop&#10;convergence rate results in iid, mis-specified iid, independent non-iid, and&#10;dependent data cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="4" source="Ryan Martin" target="Stephen G. Walker">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3102v7" />
          <attvalue for="2" value="A note on Bayesian convergence rates under local prior support&#10;  conditions" />
          <attvalue for="3" value="Bounds on Bayesian posterior convergence rates, assuming the prior satisfies&#10;both local and global support conditions, are now readily available. In this&#10;paper we explore, in the context of density estimation, Bayesian convergence&#10;rates assuming only local prior support conditions. Our results give optimal&#10;rates under minimal conditions using very simple arguments." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="5" source="Liang Hong" target="Stephen G. Walker">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3102v7" />
          <attvalue for="2" value="A note on Bayesian convergence rates under local prior support&#10;  conditions" />
          <attvalue for="3" value="Bounds on Bayesian posterior convergence rates, assuming the prior satisfies&#10;both local and global support conditions, are now readily available. In this&#10;paper we explore, in the context of density estimation, Bayesian convergence&#10;rates assuming only local prior support conditions. Our results give optimal&#10;rates under minimal conditions using very simple arguments." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="6" source="Stephen G. Walker" target="Stefano Favaro">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6228v1" />
          <attvalue for="2" value="A class of measure-valued Markov chains and Bayesian nonparametrics" />
          <attvalue for="3" value="Measure-valued Markov chains have raised interest in Bayesian nonparametrics&#10;since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)&#10;579--585) where a Markov chain having the law of the Dirichlet process as&#10;unique invariant measure has been introduced. In the present paper, we propose&#10;and investigate a new class of measure-valued Markov chains defined via&#10;exchangeable sequences of random variables. Asymptotic properties for this new&#10;class are derived and applications related to Bayesian nonparametric mixture&#10;modeling, and to a generalization of the Markov chain proposed by (Math. Proc.&#10;Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and&#10;their applications highlight once again the interplay between Bayesian&#10;nonparametrics and the theory of measure-valued Markov chains." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="7" source="Stephen G. Walker" target="Alessandra Guglielmi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6228v1" />
          <attvalue for="2" value="A class of measure-valued Markov chains and Bayesian nonparametrics" />
          <attvalue for="3" value="Measure-valued Markov chains have raised interest in Bayesian nonparametrics&#10;since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)&#10;579--585) where a Markov chain having the law of the Dirichlet process as&#10;unique invariant measure has been introduced. In the present paper, we propose&#10;and investigate a new class of measure-valued Markov chains defined via&#10;exchangeable sequences of random variables. Asymptotic properties for this new&#10;class are derived and applications related to Bayesian nonparametric mixture&#10;modeling, and to a generalization of the Markov chain proposed by (Math. Proc.&#10;Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and&#10;their applications highlight once again the interplay between Bayesian&#10;nonparametrics and the theory of measure-valued Markov chains." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="8" source="F. Bartolucci" target="M. Lupparelli">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.1864v1" />
          <attvalue for="2" value="Nested hidden Markov chains for modeling dynamic unobserved&#10;  heterogeneity in multilevel longitudinal data" />
          <attvalue for="3" value="In the context of multilevel longitudinal data, where sample units are&#10;collected in clusters, an important aspect that should be accounted for is the&#10;unobserved heterogeneity between sample units and between clusters. For this&#10;aim we propose an approach based on nested hidden (latent) Markov chains, which&#10;are associated to every sample unit and to every cluster. The approach allows&#10;us to account for the mentioned forms of unobserved heterogeneity in a dynamic&#10;fashion; it also allows us to account for the correlation which may arise&#10;between the responses provided by the units belonging to the same cluster.&#10;Given the complexity in computing the manifest distribution of these response&#10;variables, we make inference on the proposed model through a composite&#10;likelihood function based on all the possible pairs of subjects within every&#10;cluster. The proposed approach is illustrated through an application to a&#10;dataset concerning a sample of Italian workers in which a binary response&#10;variable for the worker receiving an illness benefit was repeatedly observed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="9" source="F. Bartolucci" target="A. Farcomeni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6678v1" />
          <attvalue for="2" value="Causal inference in paired two-arm experimental studies under&#10;  non-compliance with application to prognosis of myocardial infarction" />
          <attvalue for="3" value="Motivated by a study about prompt coronary angiography in myocardial&#10;infarction, we propose a method to estimate the causal effect of a treatment in&#10;two-arm experimental studies with possible non-compliance in both treatment and&#10;control arms. The method is based on a causal model for repeated binary&#10;outcomes (before and after the treatment), which includes individual covariates&#10;and latent variables for the unobserved heterogeneity between subjects.&#10;Moreover, given the type of non-compliance, the model assumes the existence of&#10;three subpopulations of subjects: compliers, never-takers, and always-takers.&#10;The model is estimated by a two-step estimator: at the first step the&#10;probability that a subject belongs to one of the three subpopulations is&#10;estimated on the basis of the available covariates; at the second step the&#10;causal effects are estimated through a conditional logistic method, the&#10;implementation of which depends on the results from the first step. Standard&#10;errors for this estimator are computed on the basis of a sandwich formula. The&#10;application shows that prompt coronary angiography in patients with myocardial&#10;infarction may significantly decrease the risk of other events within the next&#10;two years, with a log-odds of about -2. Given that non-compliance is&#10;significant for patients being given the treatment because of high risk&#10;conditions, classical estimators fail to detect, or at least underestimate,&#10;this effect." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="10" source="F. Bartolucci" target="F. Pennoni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.5990v1" />
          <attvalue for="2" value="A note on the application of the Oakes' identity to obtain the observed&#10;  information matrix of hidden Markov models" />
          <attvalue for="3" value="We derive the observed information matrix of hidden Markov models by the&#10;application of the Oakes (1999)'s identity. The method only requires the first&#10;derivative of the forward-backward recursions of Baum and Welch (1970), instead&#10;of the second derivative of the forward recursion, which is required within the&#10;approach of Lystig and Hughes (2002). The method is illustrated by an example&#10;based on the analysis of a longitudinal dataset which is well known in&#10;sociology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="11" source="Jianqing Fan" target="Yingying Fan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4795v3" />
          <attvalue for="2" value="Adaptive robust variable selection" />
          <attvalue for="3" value="Heavy-tailed high-dimensional data are commonly encountered in various&#10;scientific fields and pose great challenges to modern statistical analysis. A&#10;natural procedure to address this problem is to use penalized quantile&#10;regression with weighted $L_1$-penalty, called weighted robust Lasso&#10;(WR-Lasso), in which weights are introduced to ameliorate the bias problem&#10;induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the&#10;dimensionality can grow exponentially with the sample size, we investigate the&#10;model selection oracle property and establish the asymptotic normality of the&#10;WR-Lasso. We show that only mild conditions on the model error distribution are&#10;needed. Our theoretical results also reveal that adaptive choice of the weight&#10;vector is essential for the WR-Lasso to enjoy these nice asymptotic properties.&#10;To make the WR-Lasso practically feasible, we propose a two-step procedure,&#10;called adaptive robust Lasso (AR-Lasso), in which the weight vector in the&#10;second step is constructed based on the $L_1$-penalized quantile regression&#10;estimate from the first step. This two-step procedure is justified&#10;theoretically to possess the oracle property and the asymptotic normality.&#10;Numerical studies demonstrate the favorable finite-sample performance of the&#10;AR-Lasso." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="12" source="Jianqing Fan" target="Emre Barut">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1024v2" />
          <attvalue for="2" value="Conditional Sure Independence Screening" />
          <attvalue for="3" value="Independence screening is a powerful method for variable selection for `Big&#10;Data' when the number of variables is massive. Commonly used independence&#10;screening methods are based on marginal correlations or variations of it. In&#10;many applications, researchers often have some prior knowledge that a certain&#10;set of variables is related to the response. In such a situation, a natural&#10;assessment on the relative importance of the other predictors is the&#10;conditional contributions of the individual predictors in presence of the known&#10;set of variables. This results in conditional sure independence screening&#10;(CSIS). Conditioning helps for reducing the false positive and the false&#10;negative rates in the variable selection process. In this paper, we propose and&#10;study CSIS in the context of generalized linear models. For&#10;ultrahigh-dimensional statistical problems, we give conditions under which sure&#10;screening is possible and derive an upper bound on the number of selected&#10;variables. We also spell out the situation under which CSIS yields model&#10;selection consistency. Moreover, we provide two data-driven methods to select&#10;the thresholding parameter of conditional screening. The utility of the&#10;procedure is illustrated by simulation studies and analysis of two real data&#10;sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="13" source="Jianqing Fan" target="Anneleen Verhasselt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1024v2" />
          <attvalue for="2" value="Conditional Sure Independence Screening" />
          <attvalue for="3" value="Independence screening is a powerful method for variable selection for `Big&#10;Data' when the number of variables is massive. Commonly used independence&#10;screening methods are based on marginal correlations or variations of it. In&#10;many applications, researchers often have some prior knowledge that a certain&#10;set of variables is related to the response. In such a situation, a natural&#10;assessment on the relative importance of the other predictors is the&#10;conditional contributions of the individual predictors in presence of the known&#10;set of variables. This results in conditional sure independence screening&#10;(CSIS). Conditioning helps for reducing the false positive and the false&#10;negative rates in the variable selection process. In this paper, we propose and&#10;study CSIS in the context of generalized linear models. For&#10;ultrahigh-dimensional statistical problems, we give conditions under which sure&#10;screening is possible and derive an upper bound on the number of selected&#10;variables. We also spell out the situation under which CSIS yields model&#10;selection consistency. Moreover, we provide two data-driven methods to select&#10;the thresholding parameter of conditional screening. The utility of the&#10;procedure is illustrated by simulation studies and analysis of two real data&#10;sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="14" source="Jianqing Fan" target="Yuan Liao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.5536v2" />
          <attvalue for="2" value="Endogeneity in high dimensions" />
          <attvalue for="3" value="Most papers on high-dimensional statistics are based on the assumption that&#10;none of the regressors are correlated with the regression error, namely, they&#10;are exogenous. Yet, endogeneity can arise incidentally from a large pool of&#10;regressors in a high-dimensional regression. This causes the inconsistency of&#10;the penalized least-squares method and possible false scientific discoveries. A&#10;necessary condition for model selection consistency of a general class of&#10;penalized regression methods is given, which allows us to prove formally the&#10;inconsistency claim. To cope with the incidental endogeneity, we construct a&#10;novel penalized focused generalized method of moments (FGMM) criterion&#10;function. The FGMM effectively achieves the dimension reduction and applies the&#10;instrumental variable methods. We show that it possesses the oracle property&#10;even in the presence of endogenous predictors, and that the solution is also&#10;near global minimum under the over-identification assumption. Finally, we also&#10;show how the semi-parametric efficiency of estimation can be achieved via a&#10;two-step approach." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="15" source="Yingying Fan" target="Emre Barut">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4795v3" />
          <attvalue for="2" value="Adaptive robust variable selection" />
          <attvalue for="3" value="Heavy-tailed high-dimensional data are commonly encountered in various&#10;scientific fields and pose great challenges to modern statistical analysis. A&#10;natural procedure to address this problem is to use penalized quantile&#10;regression with weighted $L_1$-penalty, called weighted robust Lasso&#10;(WR-Lasso), in which weights are introduced to ameliorate the bias problem&#10;induced by the $L_1$-penalty. In the ultra-high dimensional setting, where the&#10;dimensionality can grow exponentially with the sample size, we investigate the&#10;model selection oracle property and establish the asymptotic normality of the&#10;WR-Lasso. We show that only mild conditions on the model error distribution are&#10;needed. Our theoretical results also reveal that adaptive choice of the weight&#10;vector is essential for the WR-Lasso to enjoy these nice asymptotic properties.&#10;To make the WR-Lasso practically feasible, we propose a two-step procedure,&#10;called adaptive robust Lasso (AR-Lasso), in which the weight vector in the&#10;second step is constructed based on the $L_1$-penalized quantile regression&#10;estimate from the first step. This two-step procedure is justified&#10;theoretically to possess the oracle property and the asymptotic normality.&#10;Numerical studies demonstrate the favorable finite-sample performance of the&#10;AR-Lasso." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="16" source="Yingying Fan" target="Runze Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0457v1" />
          <attvalue for="2" value="Variable selection in linear mixed effects models" />
          <attvalue for="3" value="This paper is concerned with the selection and estimation of fixed and random&#10;effects in linear mixed effects models. We propose a class of nonconcave&#10;penalized profile likelihood methods for selecting and estimating important&#10;fixed effects. To overcome the difficulty of unknown covariance matrix of&#10;random effects, we propose to use a proxy matrix in the penalized profile&#10;likelihood. We establish conditions on the choice of the proxy matrix and show&#10;that the proposed procedure enjoys the model selection consistency where the&#10;number of fixed effects is allowed to grow exponentially with the sample size.&#10;We further propose a group variable selection strategy to simultaneously select&#10;and estimate important random effects, where the unknown covariance matrix of&#10;random effects is replaced with a proxy matrix. We prove that, with the proxy&#10;matrix appropriately chosen, the proposed procedure can identify all true&#10;random effects with asymptotic probability one, where the dimension of random&#10;effects vector is allowed to increase exponentially with the sample size. Monte&#10;Carlo simulation studies are conducted to examine the finite-sample performance&#10;of the proposed procedures. We further illustrate the proposed procedures via a&#10;real data example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="17" source="Emre Barut" target="Anneleen Verhasselt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1024v2" />
          <attvalue for="2" value="Conditional Sure Independence Screening" />
          <attvalue for="3" value="Independence screening is a powerful method for variable selection for `Big&#10;Data' when the number of variables is massive. Commonly used independence&#10;screening methods are based on marginal correlations or variations of it. In&#10;many applications, researchers often have some prior knowledge that a certain&#10;set of variables is related to the response. In such a situation, a natural&#10;assessment on the relative importance of the other predictors is the&#10;conditional contributions of the individual predictors in presence of the known&#10;set of variables. This results in conditional sure independence screening&#10;(CSIS). Conditioning helps for reducing the false positive and the false&#10;negative rates in the variable selection process. In this paper, we propose and&#10;study CSIS in the context of generalized linear models. For&#10;ultrahigh-dimensional statistical problems, we give conditions under which sure&#10;screening is possible and derive an upper bound on the number of selected&#10;variables. We also spell out the situation under which CSIS yields model&#10;selection consistency. Moreover, we provide two data-driven methods to select&#10;the thresholding parameter of conditional screening. The utility of the&#10;procedure is illustrated by simulation studies and analysis of two real data&#10;sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="18" source="Elisabeth Gassiat" target="Ramon Van Handel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3482v2" />
          <attvalue for="2" value="The local geometry of finite mixtures" />
          <attvalue for="3" value="We establish that for q&gt;=1, the class of convex combinations of q translates&#10;of a smooth probability density has local doubling dimension proportional to q.&#10;The key difficulty in the proof is to control the local geometric structure of&#10;mixture classes. Our local geometry theorem yields a bound on the (bracketing)&#10;metric entropy of a class of normalized densities, from which a local entropy&#10;bound is deduced by a general slicing procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="19" source="Elisabeth Gassiat" target="Judith Rousseau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2064v2" />
          <attvalue for="2" value="About the posterior distribution in hidden Markov models with unknown&#10;  number of states" />
          <attvalue for="3" value="We consider finite state space stationary hidden Markov models (HMMs) in the&#10;situation where the number of hidden states is unknown. We provide a&#10;frequentist asymptotic evaluation of Bayesian analysis methods. Our main result&#10;gives posterior concentration rates for the marginal densities, that is for the&#10;density of a fixed number of consecutive observations. Using conditions on the&#10;prior, we are then able to define a consistent Bayesian estimator of the number&#10;of hidden states. It is known that the likelihood ratio test statistic for&#10;overfitted HMMs has a nonstandard behaviour and is unbounded. Our conditions on&#10;the prior may be seen as a way to penalize parameters to avoid this phenomenon.&#10;Inference of parameters is a much more difficult task than inference of&#10;marginal densities, we still provide a precise description of the situation&#10;when the observations are i.i.d. and we allow for $2$ possible hidden states." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="20" source="Elisabeth Gassiat" target="Dominique Bontemps">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0258v4" />
          <attvalue for="2" value="About adaptive coding on countable alphabets" />
          <attvalue for="3" value="This paper sheds light on universal coding with respect to classes of&#10;memoryless sources over a countable alphabet defined by an envelope function&#10;with finite and non-decreasing hazard rate. We prove that the auto-censuring AC&#10;code introduced by Bontemps (2011) is adaptive with respect to the collection&#10;of such classes. The analysis builds on the tight characterization of universal&#10;redundancy rate in terms of metric entropy % of small source classes by Opper&#10;and Haussler (1997) and on a careful analysis of the performance of the&#10;AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of&#10;samples from discrete distributions with finite and non-decreasing hazard rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="21" source="Elisabeth Gassiat" target="Stéphane Boucheron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0258v4" />
          <attvalue for="2" value="About adaptive coding on countable alphabets" />
          <attvalue for="3" value="This paper sheds light on universal coding with respect to classes of&#10;memoryless sources over a countable alphabet defined by an envelope function&#10;with finite and non-decreasing hazard rate. We prove that the auto-censuring AC&#10;code introduced by Bontemps (2011) is adaptive with respect to the collection&#10;of such classes. The analysis builds on the tight characterization of universal&#10;redundancy rate in terms of metric entropy % of small source classes by Opper&#10;and Haussler (1997) and on a careful analysis of the performance of the&#10;AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of&#10;samples from discrete distributions with finite and non-decreasing hazard rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="22" source="Shota Gugushvili" target="Peter Spreij">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.4981v2" />
          <attvalue for="2" value="Non-parametric Bayesian drift estimation for stochastic differential&#10;  equations" />
          <attvalue for="3" value="We consider non-parametric Bayesian estimation of the drift coefficient of a&#10;one-dimensional stochastic differential equation from discrete-time&#10;observations on the solution of this equation. Under suitable regularity&#10;conditions that are weaker than those previosly suggested in the literature, we&#10;establish posterior consistency in this context. Furthermore, we show that&#10;posterior consistency extends to the multidimensional setting as well, which,&#10;to the best of our knowledge, is a new result in this setting." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="23" source="Yunwei Cui" target="Rongning Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4013v1" />
          <attvalue for="2" value="Diagnostic Tests for Non-causal Time Series with Infinite Variance" />
          <attvalue for="3" value="We study goodness-of-fit testing for non-causal autoregressive time series&#10;with non-Gaussian stable noise. To model time series exhibiting sharp spikes or&#10;occasional bursts of outlying observations, the exponent of the non-Gaussian&#10;stable variables is assumed to be less than two. Under such conditions, the&#10;innovation variables have no finite second moment. We proved that the sample&#10;autocorrelation functions of the trimmed residuals are asymptotically normal.&#10;Nonparametric tests are also investigated. The rank correlations of the&#10;residuals or the squared residuals are shown to be asymptotically normal. Thus,&#10;an assortment of portmanteau statistics are available for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="24" source="Yunwei Cui" target="Thomas J. Fisher">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4013v1" />
          <attvalue for="2" value="Diagnostic Tests for Non-causal Time Series with Infinite Variance" />
          <attvalue for="3" value="We study goodness-of-fit testing for non-causal autoregressive time series&#10;with non-Gaussian stable noise. To model time series exhibiting sharp spikes or&#10;occasional bursts of outlying observations, the exponent of the non-Gaussian&#10;stable variables is assumed to be less than two. Under such conditions, the&#10;innovation variables have no finite second moment. We proved that the sample&#10;autocorrelation functions of the trimmed residuals are asymptotically normal.&#10;Nonparametric tests are also investigated. The rank correlations of the&#10;residuals or the squared residuals are shown to be asymptotically normal. Thus,&#10;an assortment of portmanteau statistics are available for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="25" source="Rongning Wu" target="Thomas J. Fisher">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4013v1" />
          <attvalue for="2" value="Diagnostic Tests for Non-causal Time Series with Infinite Variance" />
          <attvalue for="3" value="We study goodness-of-fit testing for non-causal autoregressive time series&#10;with non-Gaussian stable noise. To model time series exhibiting sharp spikes or&#10;occasional bursts of outlying observations, the exponent of the non-Gaussian&#10;stable variables is assumed to be less than two. Under such conditions, the&#10;innovation variables have no finite second moment. We proved that the sample&#10;autocorrelation functions of the trimmed residuals are asymptotically normal.&#10;Nonparametric tests are also investigated. The rank correlations of the&#10;residuals or the squared residuals are shown to be asymptotically normal. Thus,&#10;an assortment of portmanteau statistics are available for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="26" source="Marc Hallin" target="Christophe Ley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0282v1" />
          <attvalue for="2" value="Skew-symmetric distributions and Fisher information -- a tale of two&#10;  densities" />
          <attvalue for="3" value="Skew-symmetric densities recently received much attention in the literature,&#10;giving rise to increasingly general families of univariate and multivariate&#10;skewed densities. Most of those families, however, suffer from the inferential&#10;drawback of a potentially singular Fisher information in the vicinity of&#10;symmetry. All existing results indicate that Gaussian densities (possibly after&#10;restriction to some linear subspace) play a special and somewhat intriguing&#10;role in that context. We dispel that widespread opinion by providing a full&#10;characterization, in a general multivariate context, of the information&#10;singularity phenomenon, highlighting its relation to a possible link between&#10;symmetric kernels and skewing functions -- a link that can be interpreted as&#10;the mismatch of two densities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="27" source="Marc Hallin" target="Davy Paindaveine">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2117v1" />
          <attvalue for="2" value="Optimal rank-based testing for principal components" />
          <attvalue for="3" value="This paper provides parametric and rank-based optimal tests for eigenvectors&#10;and eigenvalues of covariance or scatter matrices in elliptical families. The&#10;parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)&#10;and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,&#10;1983). The rank-based tests address a much broader class of problems, where&#10;covariance matrices need not exist and principal components are associated with&#10;more general scatter matrices. The proposed tests are shown to outperform daily&#10;practice both from the point of view of validity as from the point of view of&#10;efficiency. This is achieved by utilizing the Le Cam theory of locally&#10;asymptotically normal experiments, in the nonstandard context, however, of a&#10;curved parametrization. The results we derive for curved experiments are of&#10;independent interest, and likely to apply in other contexts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="28" source="Marc Hallin" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2117v1" />
          <attvalue for="2" value="Optimal rank-based testing for principal components" />
          <attvalue for="3" value="This paper provides parametric and rank-based optimal tests for eigenvectors&#10;and eigenvalues of covariance or scatter matrices in elliptical families. The&#10;parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)&#10;and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,&#10;1983). The rank-based tests address a much broader class of problems, where&#10;covariance matrices need not exist and principal components are associated with&#10;more general scatter matrices. The proposed tests are shown to outperform daily&#10;practice both from the point of view of validity as from the point of view of&#10;efficiency. This is achieved by utilizing the Le Cam theory of locally&#10;asymptotically normal experiments, in the nonstandard context, however, of a&#10;curved parametrization. The results we derive for curved experiments are of&#10;independent interest, and likely to apply in other contexts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="29" source="Christophe Ley" target="Yvik Swan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4259v2" />
          <attvalue for="2" value="Efficient ANOVA for directional data" />
          <attvalue for="3" value="In this paper we tackle the ANOVA problem for directional data (with&#10;particular emphasis on geological data) by having recourse to the Le Cam&#10;methodology usually reserved for linear multivariate analysis. We construct&#10;locally and asymptotically most stringent parametric tests for ANOVA for&#10;directional data within the class of rotationally symmetric distributions. We&#10;turn these parametric tests into semi-parametric ones by (i) using a&#10;studentization argument (which leads to what we call pseudo-FvML tests) and by&#10;(ii) resorting to the invariance principle (which leads to efficient rank-based&#10;tests). Within each construction the semi-parametric tests inherit optimality&#10;under a given distribution (the FvML distribution in the first case, any&#10;rotationally symmetric distribution in the second) from their parametric&#10;antecedents and also improve on the latter by being valid under the whole class&#10;of rotationally symmetric distributions. Asymptotic relative efficiencies are&#10;calculated and the finite-sample behavior of the proposed tests is investigated&#10;by means of a Monte Carlo simulation. We conclude by applying our findings on a&#10;real-data example involving geological data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="30" source="Christophe Ley" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4259v2" />
          <attvalue for="2" value="Efficient ANOVA for directional data" />
          <attvalue for="3" value="In this paper we tackle the ANOVA problem for directional data (with&#10;particular emphasis on geological data) by having recourse to the Le Cam&#10;methodology usually reserved for linear multivariate analysis. We construct&#10;locally and asymptotically most stringent parametric tests for ANOVA for&#10;directional data within the class of rotationally symmetric distributions. We&#10;turn these parametric tests into semi-parametric ones by (i) using a&#10;studentization argument (which leads to what we call pseudo-FvML tests) and by&#10;(ii) resorting to the invariance principle (which leads to efficient rank-based&#10;tests). Within each construction the semi-parametric tests inherit optimality&#10;under a given distribution (the FvML distribution in the first case, any&#10;rotationally symmetric distribution in the second) from their parametric&#10;antecedents and also improve on the latter by being valid under the whole class&#10;of rotationally symmetric distributions. Asymptotic relative efficiencies are&#10;calculated and the finite-sample behavior of the proposed tests is investigated&#10;by means of a Monte Carlo simulation. We conclude by applying our findings on a&#10;real-data example involving geological data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="31" source="Richard A. Davis" target="Heng Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3915v1" />
          <attvalue for="2" value="Theory and Inference for a Class of Observation-driven Models with&#10;  Application to Time Series of Counts" />
          <attvalue for="3" value="This paper studies theory and inference related to a class of time series&#10;models that incorporates nonlinear dynamics. It is assumed that the&#10;observations follow a one-parameter exponential family of distributions given&#10;an accompanying process that evolves as a function of lagged observations. We&#10;employ an iterated random function approach and a special coupling technique to&#10;show that, under suitable conditions on the parameter space, the conditional&#10;mean process is a geometric moment contracting Markov chain and that the&#10;observation process is absolutely regular with geometrically decaying&#10;coefficients. Moreover the asymptotic theory of the maximum likelihood&#10;estimates of the parameters is established under some mild assumptions. These&#10;models are applied to two examples; the first is the number of transactions per&#10;minute of Ericsson stock and the second is related to return times of extreme&#10;events of Goldman Sachs Group stock." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="32" source="Richard A. Davis" target="Li Song">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2496v1" />
          <attvalue for="2" value="Unit roots in moving averages beyond first order" />
          <attvalue for="3" value="The asymptotic theory of various estimators based on Gaussian likelihood has&#10;been developed for the unit root and near unit root cases of a first-order&#10;moving average model. Previous studies of the MA(1) unit root problem rely on&#10;the special autocovariance structure of the MA(1) process, in which case, the&#10;eigenvalues and eigenvectors of the covariance matrix of the data vector have&#10;known analytical forms. In this paper, we take a different approach to first&#10;consider the joint likelihood by including an augmented initial value as a&#10;parameter and then recover the exact likelihood by integrating out the initial&#10;value. This approach by-passes the difficulty of computing an explicit&#10;decomposition of the covariance matrix and can be used to study unit root&#10;behavior in moving averages beyond first order. The asymptotics of the&#10;generalized likelihood ratio (GLR) statistic for testing unit roots are also&#10;studied. The GLR test has operating characteristics that are competitive with&#10;the locally best invariant unbiased (LBIU) test of Tanaka for some local&#10;alternatives and dominates for all other alternatives." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="33" source="Peter J. Bickel" target="Aiyou Chen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5101v1" />
          <attvalue for="2" value="The method of moments and degree distributions for network models" />
          <attvalue for="3" value="Probability models on graphs are becoming increasingly important in many&#10;applications, but statistical tools for fitting such models are not yet well&#10;developed. Here we propose a general method of moments approach that can be&#10;used to fit a large class of probability models through empirical counts of&#10;certain patterns in a graph. We establish some general asymptotic properties of&#10;empirical graph moments and prove consistency of the estimates as the graph&#10;size grows for all ranges of the average degree including $\Omega(1)$.&#10;Additional results are obtained for the important special case of degree&#10;distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="34" source="Peter J. Bickel" target="Elizaveta Levina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5101v1" />
          <attvalue for="2" value="The method of moments and degree distributions for network models" />
          <attvalue for="3" value="Probability models on graphs are becoming increasingly important in many&#10;applications, but statistical tools for fitting such models are not yet well&#10;developed. Here we propose a general method of moments approach that can be&#10;used to fit a large class of probability models through empirical counts of&#10;certain patterns in a graph. We establish some general asymptotic properties of&#10;empirical graph moments and prove consistency of the estimates as the graph&#10;size grows for all ranges of the average degree including $\Omega(1)$.&#10;Additional results are obtained for the important special case of degree&#10;distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="35" source="Aiyou Chen" target="Elizaveta Levina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5101v1" />
          <attvalue for="2" value="The method of moments and degree distributions for network models" />
          <attvalue for="3" value="Probability models on graphs are becoming increasingly important in many&#10;applications, but statistical tools for fitting such models are not yet well&#10;developed. Here we propose a general method of moments approach that can be&#10;used to fit a large class of probability models through empirical counts of&#10;certain patterns in a graph. We establish some general asymptotic properties of&#10;empirical graph moments and prove consistency of the estimates as the graph&#10;size grows for all ranges of the average degree including $\Omega(1)$.&#10;Additional results are obtained for the important special case of degree&#10;distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="36" source="Matieyendou Lamboni" target="Bertrand Iooss">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0943v2" />
          <attvalue for="2" value="Derivative-based global sensitivity measures: general links with Sobol'&#10;  indices and numerical tests" />
          <attvalue for="3" value="The estimation of variance-based importance measures (called Sobol' indices)&#10;of the input variables of a numerical model can require a large number of model&#10;evaluations. It turns to be unacceptable for high-dimensional model involving a&#10;large number of input variables (typically more than ten). Recently, Sobol and&#10;Kucherenko have proposed the Derivative-based Global Sensitivity Measures&#10;(DGSM), defined as the integral of the squared derivatives of the model output,&#10;showing that it can help to solve the problem of dimensionality in some cases.&#10;We provide a general inequality link between DGSM and total Sobol' indices for&#10;input variables belonging to the class of Boltzmann probability measures, thus&#10;extending the previous results of Sobol and Kucherenko for uniform and normal&#10;measures. The special case of log-concave measures is also described. This link&#10;provides a DGSM-based maximal bound for the total Sobol indices. Numerical&#10;tests show the performance of the bound and its usefulness in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="37" source="Matieyendou Lamboni" target="Anne-Laure Popelin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0943v2" />
          <attvalue for="2" value="Derivative-based global sensitivity measures: general links with Sobol'&#10;  indices and numerical tests" />
          <attvalue for="3" value="The estimation of variance-based importance measures (called Sobol' indices)&#10;of the input variables of a numerical model can require a large number of model&#10;evaluations. It turns to be unacceptable for high-dimensional model involving a&#10;large number of input variables (typically more than ten). Recently, Sobol and&#10;Kucherenko have proposed the Derivative-based Global Sensitivity Measures&#10;(DGSM), defined as the integral of the squared derivatives of the model output,&#10;showing that it can help to solve the problem of dimensionality in some cases.&#10;We provide a general inequality link between DGSM and total Sobol' indices for&#10;input variables belonging to the class of Boltzmann probability measures, thus&#10;extending the previous results of Sobol and Kucherenko for uniform and normal&#10;measures. The special case of log-concave measures is also described. This link&#10;provides a DGSM-based maximal bound for the total Sobol indices. Numerical&#10;tests show the performance of the bound and its usefulness in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="38" source="Matieyendou Lamboni" target="Fabrice Gamboa">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0943v2" />
          <attvalue for="2" value="Derivative-based global sensitivity measures: general links with Sobol'&#10;  indices and numerical tests" />
          <attvalue for="3" value="The estimation of variance-based importance measures (called Sobol' indices)&#10;of the input variables of a numerical model can require a large number of model&#10;evaluations. It turns to be unacceptable for high-dimensional model involving a&#10;large number of input variables (typically more than ten). Recently, Sobol and&#10;Kucherenko have proposed the Derivative-based Global Sensitivity Measures&#10;(DGSM), defined as the integral of the squared derivatives of the model output,&#10;showing that it can help to solve the problem of dimensionality in some cases.&#10;We provide a general inequality link between DGSM and total Sobol' indices for&#10;input variables belonging to the class of Boltzmann probability measures, thus&#10;extending the previous results of Sobol and Kucherenko for uniform and normal&#10;measures. The special case of log-concave measures is also described. This link&#10;provides a DGSM-based maximal bound for the total Sobol indices. Numerical&#10;tests show the performance of the bound and its usefulness in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="39" source="Bertrand Iooss" target="Anne-Laure Popelin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0943v2" />
          <attvalue for="2" value="Derivative-based global sensitivity measures: general links with Sobol'&#10;  indices and numerical tests" />
          <attvalue for="3" value="The estimation of variance-based importance measures (called Sobol' indices)&#10;of the input variables of a numerical model can require a large number of model&#10;evaluations. It turns to be unacceptable for high-dimensional model involving a&#10;large number of input variables (typically more than ten). Recently, Sobol and&#10;Kucherenko have proposed the Derivative-based Global Sensitivity Measures&#10;(DGSM), defined as the integral of the squared derivatives of the model output,&#10;showing that it can help to solve the problem of dimensionality in some cases.&#10;We provide a general inequality link between DGSM and total Sobol' indices for&#10;input variables belonging to the class of Boltzmann probability measures, thus&#10;extending the previous results of Sobol and Kucherenko for uniform and normal&#10;measures. The special case of log-concave measures is also described. This link&#10;provides a DGSM-based maximal bound for the total Sobol indices. Numerical&#10;tests show the performance of the bound and its usefulness in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="40" source="Bertrand Iooss" target="Fabrice Gamboa">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="41" source="Bertrand Iooss" target="Paul Lemaître">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="42" source="Bertrand Iooss" target="Ekatarina Sergienko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="43" source="Bertrand Iooss" target="Aurélie Arnaud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="44" source="Bertrand Iooss" target="Nicolas Bousquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="45" source="Anne-Laure Popelin" target="Fabrice Gamboa">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0943v2" />
          <attvalue for="2" value="Derivative-based global sensitivity measures: general links with Sobol'&#10;  indices and numerical tests" />
          <attvalue for="3" value="The estimation of variance-based importance measures (called Sobol' indices)&#10;of the input variables of a numerical model can require a large number of model&#10;evaluations. It turns to be unacceptable for high-dimensional model involving a&#10;large number of input variables (typically more than ten). Recently, Sobol and&#10;Kucherenko have proposed the Derivative-based Global Sensitivity Measures&#10;(DGSM), defined as the integral of the squared derivatives of the model output,&#10;showing that it can help to solve the problem of dimensionality in some cases.&#10;We provide a general inequality link between DGSM and total Sobol' indices for&#10;input variables belonging to the class of Boltzmann probability measures, thus&#10;extending the previous results of Sobol and Kucherenko for uniform and normal&#10;measures. The special case of log-concave measures is also described. This link&#10;provides a DGSM-based maximal bound for the total Sobol indices. Numerical&#10;tests show the performance of the bound and its usefulness in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="46" source="Fabrice Gamboa" target="Sébastien Da Veiga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2899v1" />
          <attvalue for="2" value="Efficient Estimation of Sensitivity Indices" />
          <attvalue for="3" value="In this paper we address the problem of efficient estimation of Sobol&#10;sensitivy indices. First, we focus on general functional integrals of&#10;conditional moments of the form $\E(\psi(\E(\varphi(Y)|X)))$ where $(X,Y)$ is a&#10;random vector with joint density $f$ and $\psi$ and $\varphi$ are functions&#10;that are differentiable enough. In particular, we show that asymptotical&#10;efficient estimation of this functional boils down to the estimation of crossed&#10;quadratic functionals. An efficient estimate of first-order sensitivity indices&#10;is then derived as a special case. We investigate its properties on several&#10;analytical functions and illustrate its interest on a reservoir engineering&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="47" source="Fabrice Gamboa" target="Paul Lemaître">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="48" source="Fabrice Gamboa" target="Ekatarina Sergienko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="49" source="Fabrice Gamboa" target="Aurélie Arnaud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="50" source="Fabrice Gamboa" target="Nicolas Bousquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="51" source="Mohamed Hebiri" target="Johannes C. Lederer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1605v2" />
          <attvalue for="2" value="How Correlations Influence Lasso Prediction" />
          <attvalue for="3" value="We study how correlations in the design matrix influence Lasso prediction.&#10;First, we argue that the higher the correlations are, the smaller the optimal&#10;tuning parameter is. This implies in particular that the standard tuning&#10;parameters, that do not depend on the design matrix, are not favorable.&#10;Furthermore, we argue that Lasso prediction works well for any degree of&#10;correlations if suitable tuning parameters are chosen. We study these two&#10;subjects theoretically as well as with simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="52" source="Mohamed Hebiri" target="Pierre Alquier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="53" source="Mohamed Hebiri" target="Cristina Butucea">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="54" source="Mohamed Hebiri" target="Katia Meziani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="55" source="Mohamed Hebiri" target="Morimae Tomoyuki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="56" source="Sidney I. Resnick" target="David Zeber">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.2314v2" />
          <attvalue for="2" value="Clustering of Markov chain exceedances" />
          <attvalue for="3" value="The tail chain of a Markov chain can be used to model the dependence between&#10;extreme observations. For a positive recurrent Markov chain, the tail chain&#10;aids in describing the limit of a sequence of point processes $\{N_n,n\geq1\}$,&#10;consisting of normalized observations plotted against scaled time points. Under&#10;fairly general conditions on extremal behaviour, $\{N_n\}$ converges to a&#10;cluster Poisson process. Our technique decomposes the sample path of the chain&#10;into i.i.d. regenerative cycles rather than using blocking argument typically&#10;employed in the context of stationarity with mixing." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="57" source="David Zeber" target="Sidney Resnick">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3060v1" />
          <attvalue for="2" value="Markov Kernels and the Conditional Extreme Value Model" />
          <attvalue for="3" value="The classical approach to multivariate extreme value modelling assumes that&#10;the joint distribution belongs to a multivariate domain of attraction. This&#10;requires each marginal distribution be individually attracted to a univariate&#10;extreme value distribution. An apparently more flexible extremal model for&#10;multivariate data was proposed by Heffernan and Tawn under which not all the&#10;components are required to belong to an extremal domain of attraction but&#10;assumes instead the existence of an asymptotic approximation to the conditional&#10;distribution of the random vector given one of the components is extreme.&#10;Combined with the knowledge that the conditioning component belongs to a&#10;univariate domain of attraction, this leads to an approximation of the&#10;probability of certain risk regions. The original focus on conditional&#10;distributions had technical drawbacks but is natural in several contexts. We&#10;place this approach in the context of the more general approach using&#10;convergence of measures and multivariate regular variation on cones." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="58" source="L. Gardes" target="S. Girard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.1076v1" />
          <attvalue for="2" value="Functional kernel estimators of conditional extreme quantiles" />
          <attvalue for="3" value="We address the estimation of &quot;extreme&quot; conditional quantiles i.e. when their&#10;order converges to one as the sample size increases. Conditions on the rate of&#10;convergence of their order to one are provided to obtain asymptotically&#10;Gaussian distributed kernel estimators. A Weissman-type estimator and kernel&#10;estimators of the conditional tail-index are derived, permitting to estimate&#10;extreme conditional quantiles of arbitrary order." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="59" source="Monica Billio" target="Roberto Casarin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5397v1" />
          <attvalue for="2" value="Efficient Gibbs Sampling for Markov Switching GARCH Models" />
          <attvalue for="3" value="We develop efficient simulation techniques for Bayesian inference on&#10;switching GARCH models. Our contribution to existing literature is manifold.&#10;First, we discuss different multi-move sampling techniques for Markov Switching&#10;(MS) state space models with particular attention to MS-GARCH models. Our&#10;multi-move sampling strategy is based on the Forward Filtering Backward&#10;Sampling (FFBS) applied to an approximation of MS-GARCH. Another important&#10;contribution is the use of multi-point samplers, such as the Multiple-Try&#10;Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in&#10;combination with FFBS for the MS-GARCH process. In this sense we ex- tend to&#10;the MS state space models the work of So [2006] on efficient MTM sampler for&#10;continuous state space models. Finally, we suggest to further improve the&#10;sampler efficiency by introducing the antithetic sampling of Craiu and Meng&#10;[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments&#10;on MS-GARCH model show that our multi-point and multi-move strategies allow the&#10;sampler to gain efficiency when compared with single-move Gibbs sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="60" source="Monica Billio" target="Anthony Osuntuyi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5397v1" />
          <attvalue for="2" value="Efficient Gibbs Sampling for Markov Switching GARCH Models" />
          <attvalue for="3" value="We develop efficient simulation techniques for Bayesian inference on&#10;switching GARCH models. Our contribution to existing literature is manifold.&#10;First, we discuss different multi-move sampling techniques for Markov Switching&#10;(MS) state space models with particular attention to MS-GARCH models. Our&#10;multi-move sampling strategy is based on the Forward Filtering Backward&#10;Sampling (FFBS) applied to an approximation of MS-GARCH. Another important&#10;contribution is the use of multi-point samplers, such as the Multiple-Try&#10;Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in&#10;combination with FFBS for the MS-GARCH process. In this sense we ex- tend to&#10;the MS state space models the work of So [2006] on efficient MTM sampler for&#10;continuous state space models. Finally, we suggest to further improve the&#10;sampler efficiency by introducing the antithetic sampling of Craiu and Meng&#10;[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments&#10;on MS-GARCH model show that our multi-point and multi-move strategies allow the&#10;sampler to gain efficiency when compared with single-move Gibbs sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="61" source="Roberto Casarin" target="Anthony Osuntuyi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5397v1" />
          <attvalue for="2" value="Efficient Gibbs Sampling for Markov Switching GARCH Models" />
          <attvalue for="3" value="We develop efficient simulation techniques for Bayesian inference on&#10;switching GARCH models. Our contribution to existing literature is manifold.&#10;First, we discuss different multi-move sampling techniques for Markov Switching&#10;(MS) state space models with particular attention to MS-GARCH models. Our&#10;multi-move sampling strategy is based on the Forward Filtering Backward&#10;Sampling (FFBS) applied to an approximation of MS-GARCH. Another important&#10;contribution is the use of multi-point samplers, such as the Multiple-Try&#10;Metropolis (MTM) and the Multiple trial Metropolize Independent Sampler, in&#10;combination with FFBS for the MS-GARCH process. In this sense we ex- tend to&#10;the MS state space models the work of So [2006] on efficient MTM sampler for&#10;continuous state space models. Finally, we suggest to further improve the&#10;sampler efficiency by introducing the antithetic sampling of Craiu and Meng&#10;[2005] and Craiu and Lemieux [2007] within the FFBS. Our simulation experiments&#10;on MS-GARCH model show that our multi-point and multi-move strategies allow the&#10;sampler to gain efficiency when compared with single-move Gibbs sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="62" source="Rida Benhaddou" target="Marianna Pensky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.0990v2" />
          <attvalue for="2" value="Adaptive Nonparametric Empirical Bayes Estimation Via Wavelet Series:&#10;  the Minimax Study" />
          <attvalue for="3" value="In the present paper, we derive lower bounds for the risk of the&#10;nonparametric empirical Bayes estimators. In order to attain the optimal&#10;convergence rate, we propose generalization of the linear empirical Bayes&#10;estimation method which takes advantage of the flexibility of the wavelet&#10;techniques. We present an empirical Bayes estimator as a wavelet series&#10;expansion and estimate coefficients by minimizing the prior risk of the&#10;estimator. As a result, estimation of wavelet coefficients requires solution of&#10;a well-posed low-dimensional sparse system of linear equations. The dimension&#10;of the system depends on the size of wavelet support and smoothness of the&#10;Bayes estimator. An adaptive choice of the resolution level is carried out&#10;using Lepski (1997) method. The method is computationally efficient and&#10;provides asymptotically optimal adaptive EB estimators. The theory is&#10;supplemented by numerous examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="63" source="Rida Benhaddou" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7114v2" />
          <attvalue for="2" value="Anisotropic Denoising in Functional Deconvolution Model with&#10;  Dimension-free Convergence Rates" />
          <attvalue for="3" value="In the present paper we consider the problem of estimating a periodic&#10;$(r+1)$-dimensional function $f$ based on observations from its noisy&#10;convolution. We construct a wavelet estimator of $f$, derive minimax lower&#10;bounds for the $L^2$-risk when $f$ belongs to a Besov ball of mixed smoothness&#10;and demonstrate that the wavelet estimator is adaptive and asymptotically&#10;near-optimal within a logarithmic factor, in a wide range of Besov balls. We&#10;prove in particular that choosing this type of mixed smoothness leads to rates&#10;of convergence which are free of the &quot;curse of dimensionality&quot; and, hence, are&#10;higher than usual convergence rates when $r$ is large. The problem studied in&#10;the paper is motivated by seismic inversion which can be reduced to solution of&#10;noisy two-dimensional convolution equations that allow to draw inference on&#10;underground layer structures along the chosen profiles. The common practice in&#10;seismology is to recover layer structures separately for each profile and then&#10;to combine the derived estimates into a two-dimensional function. By studying&#10;the two-dimensional version of the model, we demonstrate that this strategy&#10;usually leads to estimators which are less accurate than the ones obtained as&#10;two-dimensional functional deconvolutions. Indeed, we show that unless the&#10;function $f$ is very smooth in the direction of the profiles, very spatially&#10;inhomogeneous along the other direction and the number of profiles is very&#10;limited, the functional deconvolution solution has a much better precision&#10;compared to a combination of $M$ solutions of separate convolution equations. A&#10;limited simulation study in the case of $r=1$ confirms theoretical claims of&#10;the paper." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="64" source="Marianna Pensky" target="Dominique Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7114v2" />
          <attvalue for="2" value="Anisotropic Denoising in Functional Deconvolution Model with&#10;  Dimension-free Convergence Rates" />
          <attvalue for="3" value="In the present paper we consider the problem of estimating a periodic&#10;$(r+1)$-dimensional function $f$ based on observations from its noisy&#10;convolution. We construct a wavelet estimator of $f$, derive minimax lower&#10;bounds for the $L^2$-risk when $f$ belongs to a Besov ball of mixed smoothness&#10;and demonstrate that the wavelet estimator is adaptive and asymptotically&#10;near-optimal within a logarithmic factor, in a wide range of Besov balls. We&#10;prove in particular that choosing this type of mixed smoothness leads to rates&#10;of convergence which are free of the &quot;curse of dimensionality&quot; and, hence, are&#10;higher than usual convergence rates when $r$ is large. The problem studied in&#10;the paper is motivated by seismic inversion which can be reduced to solution of&#10;noisy two-dimensional convolution equations that allow to draw inference on&#10;underground layer structures along the chosen profiles. The common practice in&#10;seismology is to recover layer structures separately for each profile and then&#10;to combine the derived estimates into a two-dimensional function. By studying&#10;the two-dimensional version of the model, we demonstrate that this strategy&#10;usually leads to estimators which are less accurate than the ones obtained as&#10;two-dimensional functional deconvolutions. Indeed, we show that unless the&#10;function $f$ is very smooth in the direction of the profiles, very spatially&#10;inhomogeneous along the other direction and the number of profiles is very&#10;limited, the functional deconvolution solution has a much better precision&#10;compared to a combination of $M$ solutions of separate convolution equations. A&#10;limited simulation study in the case of $r=1$ confirms theoretical claims of&#10;the paper." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="65" source="Marianna Pensky" target="Fabienne Comte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="66" source="Marianna Pensky" target="Charles-André Cuénod">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="67" source="Marianna Pensky" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="68" source="Marianna Pensky" target="Olga Klopp">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3394v2" />
          <attvalue for="2" value="Non-asymptotic approach to varying coefficient model" />
          <attvalue for="3" value="In the present paper we consider the varying coefficient model which&#10;represents a useful tool for exploring dynamic patterns in many applications.&#10;Existing methods typically provide asymptotic evaluation of precision of&#10;estimation procedures under the assumption that the number of observations&#10;tends to infinity. In practical applications, however, only a finite number of&#10;measurements are available. In the present paper we focus on a non-asymptotic&#10;approach to the problem. We propose a novel estimation procedure which is based&#10;on recent developments in matrix estimation. In particular, for our estimator,&#10;we obtain upper bounds for the mean squared and the pointwise estimation&#10;errors. The obtained oracle inequalities are non-asymptotic and hold for finite&#10;sample size." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="69" source="Dominique Picard" target="Mathilde Mougeot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2067v1" />
          <attvalue for="2" value="Grouping Strategies and Thresholding for High Dimensional Linear Models" />
          <attvalue for="3" value="The estimation problem in a high regression model with structured sparsity is&#10;investigated. An algorithm using a two steps block thresholding procedure&#10;called GR-LOL is provided. Convergence rates are produced: they depend on&#10;simple coherence-type indices of the Gram matrix -easily checkable on the data-&#10;as well as sparsity assumptions of the model parameters measured by a&#10;combination of $l_1$ within-blocks with $l_q,q&lt;1$ between-blocks norms. The&#10;simplicity of the coherence indicator suggests ways to optimize the rates of&#10;convergence when the group structure is not naturally given by the problem and&#10;is unknown. In such a case, an auto-driven procedure is provided to determine&#10;the regressors groups (number and contents). An intensive practical study&#10;compares our grouping methods with the standard LOL algorithm. We prove that&#10;the grouping rarely deteriorates the results but can improve them very&#10;significantly. GR-LOL is also compared with group-Lasso procedures and exhibits&#10;a very encouraging behavior. The results are quite impressive, especially when&#10;GR-LOL algorithm is combined with a grouping pre-processing." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="70" source="Dominique Picard" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2067v1" />
          <attvalue for="2" value="Grouping Strategies and Thresholding for High Dimensional Linear Models" />
          <attvalue for="3" value="The estimation problem in a high regression model with structured sparsity is&#10;investigated. An algorithm using a two steps block thresholding procedure&#10;called GR-LOL is provided. Convergence rates are produced: they depend on&#10;simple coherence-type indices of the Gram matrix -easily checkable on the data-&#10;as well as sparsity assumptions of the model parameters measured by a&#10;combination of $l_1$ within-blocks with $l_q,q&lt;1$ between-blocks norms. The&#10;simplicity of the coherence indicator suggests ways to optimize the rates of&#10;convergence when the group structure is not naturally given by the problem and&#10;is unknown. In such a case, an auto-driven procedure is provided to determine&#10;the regressors groups (number and contents). An intensive practical study&#10;compares our grouping methods with the standard LOL algorithm. We prove that&#10;the grouping rarely deteriorates the results but can improve them very&#10;significantly. GR-LOL is also compared with group-Lasso procedures and exhibits&#10;a very encouraging behavior. The results are quite impressive, especially when&#10;GR-LOL algorithm is combined with a grouping pre-processing." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="71" source="Dominique Picard" target="Ismael Castillo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0459v1" />
          <attvalue for="2" value="Thomas Bayes' walk on manifolds" />
          <attvalue for="3" value="Convergence of the Bayes posterior measure is considered in canonical&#10;statistical settings where observations sit on a geometrical object such as a&#10;compact manifold, or more generally on a compact metric space verifying some&#10;conditions. A natural geometric prior based on randomly rescaled solutions of&#10;the heat equation is considered. Upper and lower bound posterior contraction&#10;rates are derived." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="72" source="Dominique Picard" target="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0459v1" />
          <attvalue for="2" value="Thomas Bayes' walk on manifolds" />
          <attvalue for="3" value="Convergence of the Bayes posterior measure is considered in canonical&#10;statistical settings where observations sit on a geometrical object such as a&#10;compact manifold, or more generally on a compact metric space verifying some&#10;conditions. A natural geometric prior based on randomly rescaled solutions of&#10;the heat equation is considered. Upper and lower bound posterior contraction&#10;rates are derived." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="73" source="Jan Beran" target="Yevgen Shumeyko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0392v1" />
          <attvalue for="2" value="On asymptotically optimal wavelet estimation of trend functions under&#10;  long-range dependence" />
          <attvalue for="3" value="We consider data-adaptive wavelet estimation of a trend function in a time&#10;series model with strongly dependent Gaussian residuals. Asymptotic expressions&#10;for the optimal mean integrated squared error and corresponding optimal&#10;smoothing and resolution parameters are derived. Due to adaptation to the&#10;properties of the underlying trend function, the approach shows very good&#10;performance for smooth trend functions while remaining competitive with minimax&#10;wavelet estimation for functions with discontinuities. Simulations illustrate&#10;the asymptotic results and finite-sample behavior." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="74" source="Holger Dette" target="Viatcheslav B. Melas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6283v1" />
          <attvalue for="2" value="$T$-optimal designs for discrimination between two polynomial models" />
          <attvalue for="3" value="This paper is devoted to the explicit construction of optimal designs for&#10;discrimination between two polynomial regression models of degree $n-2$ and&#10;$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70] proposed the $T$-optimality criterion for this purpose. Recently,&#10;Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]&#10;determined $T$-optimal designs for polynomials up to degree 6 numerically and&#10;based on these results he conjectured that the support points of the optimal&#10;design are cosines of the angles that divide half of the circle into equal&#10;parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree&#10;vanishes. In the present paper we give a strong justification of the conjecture&#10;and determine all $T$-optimal designs explicitly for any degree&#10;$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional&#10;class of $T$-optimal designs. Moreover, we also present a generalization to the&#10;case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller&#10;than a certain critical value. Because of the complexity of the optimization&#10;problem, $T$-optimal designs have only been determined numerically so far, and&#10;this paper provides the first explicit solution of the $T$-optimal design&#10;problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70]. Finally, for the remaining cases (where the ratio of coefficients is&#10;larger than the critical value), we propose a numerical procedure to calculate&#10;the $T$-optimal designs. The results are also illustrated in an example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="75" source="Holger Dette" target="Petr Shpilev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6283v1" />
          <attvalue for="2" value="$T$-optimal designs for discrimination between two polynomial models" />
          <attvalue for="3" value="This paper is devoted to the explicit construction of optimal designs for&#10;discrimination between two polynomial regression models of degree $n-2$ and&#10;$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70] proposed the $T$-optimality criterion for this purpose. Recently,&#10;Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]&#10;determined $T$-optimal designs for polynomials up to degree 6 numerically and&#10;based on these results he conjectured that the support points of the optimal&#10;design are cosines of the angles that divide half of the circle into equal&#10;parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree&#10;vanishes. In the present paper we give a strong justification of the conjecture&#10;and determine all $T$-optimal designs explicitly for any degree&#10;$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional&#10;class of $T$-optimal designs. Moreover, we also present a generalization to the&#10;case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller&#10;than a certain critical value. Because of the complexity of the optimization&#10;problem, $T$-optimal designs have only been determined numerically so far, and&#10;this paper provides the first explicit solution of the $T$-optimal design&#10;problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70]. Finally, for the remaining cases (where the ratio of coefficients is&#10;larger than the critical value), we propose a numerical procedure to calculate&#10;the $T$-optimal designs. The results are also illustrated in an example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="76" source="Holger Dette" target="Mathias Vetter">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5507v1" />
          <attvalue for="2" value="Model checks for the volatility under microstructure noise" />
          <attvalue for="3" value="We consider the problem of testing the parametric form of the volatility for&#10;high frequency data. It is demonstrated that in the presence of microstructure&#10;noise commonly used tests do not keep the preassigned level and are&#10;inconsistent. The concept of preaveraging is used to construct new tests, which&#10;do not suffer from these drawbacks. These tests are based on a&#10;Kolmogorov-Smirnov or Cramer-von-Mises functional of an integrated stochastic&#10;process, for which weak convergence to a (conditional) Gaussian process is&#10;established. The finite sample properties of a bootstrap version of the test&#10;are illustrated by means of a simulation study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="77" source="Viatcheslav B. Melas" target="Petr Shpilev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6283v1" />
          <attvalue for="2" value="$T$-optimal designs for discrimination between two polynomial models" />
          <attvalue for="3" value="This paper is devoted to the explicit construction of optimal designs for&#10;discrimination between two polynomial regression models of degree $n-2$ and&#10;$n$. In a fundamental paper, Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70] proposed the $T$-optimality criterion for this purpose. Recently,&#10;Atkinson [MODA 9, Advances in Model-Oriented Design and Analysis (2010) 9--16]&#10;determined $T$-optimal designs for polynomials up to degree 6 numerically and&#10;based on these results he conjectured that the support points of the optimal&#10;design are cosines of the angles that divide half of the circle into equal&#10;parts if the coefficient of $x^{n-1}$ in the polynomial of larger degree&#10;vanishes. In the present paper we give a strong justification of the conjecture&#10;and determine all $T$-optimal designs explicitly for any degree&#10;$n\in\mathbb{N}$. In particular, we show that there exists a one-dimensional&#10;class of $T$-optimal designs. Moreover, we also present a generalization to the&#10;case when the ratio between the coefficients of $x^{n-1}$ and $x^n$ is smaller&#10;than a certain critical value. Because of the complexity of the optimization&#10;problem, $T$-optimal designs have only been determined numerically so far, and&#10;this paper provides the first explicit solution of the $T$-optimal design&#10;problem since its introduction by Atkinson and Fedorov [Biometrika 62 (1975a)&#10;57--70]. Finally, for the remaining cases (where the ratio of coefficients is&#10;larger than the critical value), we propose a numerical procedure to calculate&#10;the $T$-optimal designs. The results are also illustrated in an example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="78" source="Hélène Boistard" target="Hendrik P. Lopuhaä">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5654v1" />
          <attvalue for="2" value="Approximation of rejective sampling inclusion probabilities and&#10;  application to high order correlations" />
          <attvalue for="3" value="This paper is devoted to rejective sampling. We provide an expansion of joint&#10;inclusion probabilities of any order in terms of the inclusion probabilities of&#10;order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and&#10;making the remainder term more precise. Following H\'ajek (1981), the proof is&#10;based on Edgeworth expansions. The main result is applied to derive bounds on&#10;higher order correlations, which are needed for the consistency and asymptotic&#10;normality of several complex estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="79" source="Hélène Boistard" target="Anne Ruiz-Gazen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5654v1" />
          <attvalue for="2" value="Approximation of rejective sampling inclusion probabilities and&#10;  application to high order correlations" />
          <attvalue for="3" value="This paper is devoted to rejective sampling. We provide an expansion of joint&#10;inclusion probabilities of any order in terms of the inclusion probabilities of&#10;order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and&#10;making the remainder term more precise. Following H\'ajek (1981), the proof is&#10;based on Edgeworth expansions. The main result is applied to derive bounds on&#10;higher order correlations, which are needed for the consistency and asymptotic&#10;normality of several complex estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="80" source="Hendrik P. Lopuhaä" target="Anne Ruiz-Gazen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5654v1" />
          <attvalue for="2" value="Approximation of rejective sampling inclusion probabilities and&#10;  application to high order correlations" />
          <attvalue for="3" value="This paper is devoted to rejective sampling. We provide an expansion of joint&#10;inclusion probabilities of any order in terms of the inclusion probabilities of&#10;order one, extending previous results by H\'ajek (1964) and H\'ajek (1981) and&#10;making the remainder term more precise. Following H\'ajek (1981), the proof is&#10;based on Edgeworth expansions. The main result is applied to derive bounds on&#10;higher order correlations, which are needed for the consistency and asymptotic&#10;normality of several complex estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="81" source="Hendrik P. Lopuhaä" target="Cécile Durot">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2118v2" />
          <attvalue for="2" value="Testing equality of functions under monotonicity constraints" />
          <attvalue for="3" value="We consider the problem of testing equality of functions $f_j:[0,1]\to&#10;\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from&#10;possibly different distributions under the assumption that the functions are&#10;monotone. We provide a uniform approach that covers testing equality of&#10;monotone regression curves, equality of monotone densities and equality of&#10;monotone hazards in the random censorship model. Two test statistics are&#10;proposed based on $L_1$-distances. We show that both statistics are&#10;asymptotically normal and we provide bootstrap implementations, which are shown&#10;to have critical regions with asymptotic level $\alpha$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="82" source="Hendrik P. Lopuhaä" target="Piet Groeneboom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2118v2" />
          <attvalue for="2" value="Testing equality of functions under monotonicity constraints" />
          <attvalue for="3" value="We consider the problem of testing equality of functions $f_j:[0,1]\to&#10;\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from&#10;possibly different distributions under the assumption that the functions are&#10;monotone. We provide a uniform approach that covers testing equality of&#10;monotone regression curves, equality of monotone densities and equality of&#10;monotone hazards in the random censorship model. Two test statistics are&#10;proposed based on $L_1$-distances. We show that both statistics are&#10;asymptotically normal and we provide bootstrap implementations, which are shown&#10;to have critical regions with asymptotic level $\alpha$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="83" source="Tristan Launay" target="Anne Philippe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4753v2" />
          <attvalue for="2" value="Consistency of the posterior distribution and MLE for piecewise linear&#10;  regression" />
          <attvalue for="3" value="We prove the weak consistency of the posterior distribution and that of the&#10;Bayes estimator for a two-phase piecewise linear regression mdoel where the&#10;break-point is unknown. The non-differentiability of the likelihood of the&#10;model with regard to the break- point parameter induces technical difficulties&#10;that we overcome by creating a regularised version of the problem at hand. We&#10;first recover the strong consistency of the quantities of interest for the&#10;regularised version, using results about the MLE, and we then prove that the&#10;regularised version and the original version of the problem share the same&#10;asymptotic properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="84" source="Tristan Launay" target="Sophie Lamarche">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4753v2" />
          <attvalue for="2" value="Consistency of the posterior distribution and MLE for piecewise linear&#10;  regression" />
          <attvalue for="3" value="We prove the weak consistency of the posterior distribution and that of the&#10;Bayes estimator for a two-phase piecewise linear regression mdoel where the&#10;break-point is unknown. The non-differentiability of the likelihood of the&#10;model with regard to the break- point parameter induces technical difficulties&#10;that we overcome by creating a regularised version of the problem at hand. We&#10;first recover the strong consistency of the quantities of interest for the&#10;regularised version, using results about the MLE, and we then prove that the&#10;regularised version and the original version of the problem share the same&#10;asymptotic properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="85" source="Anne Philippe" target="Sophie Lamarche">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4753v2" />
          <attvalue for="2" value="Consistency of the posterior distribution and MLE for piecewise linear&#10;  regression" />
          <attvalue for="3" value="We prove the weak consistency of the posterior distribution and that of the&#10;Bayes estimator for a two-phase piecewise linear regression mdoel where the&#10;break-point is unknown. The non-differentiability of the likelihood of the&#10;model with regard to the break- point parameter induces technical difficulties&#10;that we overcome by creating a regularised version of the problem at hand. We&#10;first recover the strong consistency of the quantities of interest for the&#10;regularised version, using results about the MLE, and we then prove that the&#10;regularised version and the original version of the problem share the same&#10;asymptotic properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="86" source="Anne Philippe" target="Saeid Rezakhah">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2450v1" />
          <attvalue for="2" value="Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes" />
          <attvalue for="3" value="The characteristic feature of semi-selfsimilar process is the invariance of&#10;its finite dimensional distributions by certain dilation for specific scaling&#10;factor. Estimating the scale parameter $\lambda$ and the Hurst index of such&#10;processes is one of the fundamental problem in the literature. We present some&#10;iterative method for estimation of the scale and Hurst parameters which is&#10;addressed for semi-selfsimilar processes with stationary increments. This&#10;method is based on some flexible sampling scheme and evaluating sample variance&#10;of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in&#10;\mathbb{N}$. For such iterative method we find the initial estimation for the&#10;scale parameter by evaluating cumulative sum of moving sample variances and&#10;also by evaluating sample variance of preceding and succeeding moving sample&#10;variance of increments. We also present a new efficient method for estimation&#10;of Hurst parameter of selfsimilar processes. As an example we introduce simple&#10;fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary&#10;increments. We present some simulations and numerical evaluation to illustrate&#10;the results and to estimate the scale for sfBm as a semi-selfsimilar process.&#10;We also present another simulation and show the efficiency of our method in&#10;estimation of Hurst parameter by comparing its performance with some previous&#10;methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="87" source="Anne Philippe" target="Navideh Modarresi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2450v1" />
          <attvalue for="2" value="Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes" />
          <attvalue for="3" value="The characteristic feature of semi-selfsimilar process is the invariance of&#10;its finite dimensional distributions by certain dilation for specific scaling&#10;factor. Estimating the scale parameter $\lambda$ and the Hurst index of such&#10;processes is one of the fundamental problem in the literature. We present some&#10;iterative method for estimation of the scale and Hurst parameters which is&#10;addressed for semi-selfsimilar processes with stationary increments. This&#10;method is based on some flexible sampling scheme and evaluating sample variance&#10;of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in&#10;\mathbb{N}$. For such iterative method we find the initial estimation for the&#10;scale parameter by evaluating cumulative sum of moving sample variances and&#10;also by evaluating sample variance of preceding and succeeding moving sample&#10;variance of increments. We also present a new efficient method for estimation&#10;of Hurst parameter of selfsimilar processes. As an example we introduce simple&#10;fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary&#10;increments. We present some simulations and numerical evaluation to illustrate&#10;the results and to estimate the scale for sfBm as a semi-selfsimilar process.&#10;We also present another simulation and show the efficiency of our method in&#10;estimation of Hurst parameter by comparing its performance with some previous&#10;methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="88" source="Takuma Yoshida" target="Kanta Naito">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3483v1" />
          <attvalue for="2" value="Semiparametric Penalized Spline Regression" />
          <attvalue for="3" value="In this paper, we propose a new semiparametric regression estimator by using&#10;a hybrid technique of a parametric approach and a nonparametric penalized&#10;spline method. The overall shape of the true regression function is captured by&#10;the parametric part, while its residual is consistently estimated by the&#10;nonparametric part. Asymptotic theory for the proposed semiparametric estimator&#10;is developed, showing that its behavior is dependent on the asymptotics for the&#10;nonparametric penalized spline estimator as well as on the discrepancy between&#10;the true regression function and the parametric part. As a naturally associated&#10;application of asymptotics, some criteria for the selection of parametric&#10;models are addressed. Numerical experiments show that the proposed estimator&#10;performs better than the existing kernel-based semiparametric estimator and the&#10;fully nonparametric estimator, and that the proposed criteria work well for&#10;choosing a reasonable parametric model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="89" source="Kari Lock Morgan" target="Donald B. Rubin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5625v1" />
          <attvalue for="2" value="Rerandomization to improve covariate balance in experiments" />
          <attvalue for="3" value="Randomized experiments are the &quot;gold standard&quot; for estimating causal effects,&#10;yet often in practice, chance imbalances exist in covariate distributions&#10;between treatment groups. If covariate data are available before units are&#10;exposed to treatments, these chance imbalances can be mitigated by first&#10;checking covariate balance before the physical experiment takes place. Provided&#10;a precise definition of imbalance has been specified in advance, unbalanced&#10;randomizations can be discarded, followed by a rerandomization, and this&#10;process can continue until a randomization yielding balance according to the&#10;definition is achieved. By improving covariate balance, rerandomization&#10;provides more precise and trustworthy estimates of treatment effects." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="90" source="Mathias Vetter" target="Axel Bücher">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.0417v2" />
          <attvalue for="2" value="Nonparametric inference on Lévy measures and copulas" />
          <attvalue for="3" value="In this paper nonparametric methods to assess the multivariate L\'{e}vy&#10;measure are introduced. Starting from high-frequency observations of a L\'{e}vy&#10;process $\mathbf{X}$, we construct estimators for its tail integrals and the&#10;Pareto-L\'{e}vy copula and prove weak convergence of these estimators in&#10;certain function spaces. Given n observations of increments over intervals of&#10;length $\Delta_n$, the rate of convergence is $k_n^{-1/2}$ for $k_n=n\Delta_n$&#10;which is natural concerning inference on the L\'{e}vy measure. Besides&#10;extensions to nonequidistant sampling schemes analytic properties of the&#10;Pareto-L\'{e}vy copula which, to the best of our knowledge, have not been&#10;mentioned before in the literature are provided as well. We conclude with a&#10;short simulation study on the performance of our estimators and apply them to&#10;real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="91" source="Ying Ding" target="Bin Nan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2470v1" />
          <attvalue for="2" value="A sieve M-theorem for bundled parameters in semiparametric models, with&#10;  application to the efficient estimation in a linear model for censored data" />
          <attvalue for="3" value="In many semiparametric models that are parameterized by two types of&#10;parameters---a Euclidean parameter of interest and an infinite-dimensional&#10;nuisance parameter---the two parameters are bundled together, that is, the&#10;nuisance parameter is an unknown function that contains the parameter of&#10;interest as part of its argument. For example, in a linear regression model for&#10;censored survival data, the unspecified error distribution function involves&#10;the regression coefficients. Motivated by developing an efficient estimating&#10;method for the regression parameters, we propose a general sieve M-theorem for&#10;bundled parameters and apply the theorem to deriving the asymptotic theory for&#10;the sieve maximum likelihood estimation in the linear regression model for&#10;censored survival data. The numerical implementation of the proposed estimating&#10;method can be achieved through the conventional gradient-based search&#10;algorithms such as the Newton--Raphson algorithm. We show that the proposed&#10;estimator is consistent and asymptotically normal and achieves the&#10;semiparametric efficiency bound. Simulation studies demonstrate that the&#10;proposed method performs well in practical settings and yields more efficient&#10;estimates than existing estimating equation based methods. Illustration with a&#10;real data example is also provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="92" source="Bin Nan" target="Jon A. Wellner">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2579v1" />
          <attvalue for="2" value="A general semiparametric Z-estimation approach for case-cohort studies" />
          <attvalue for="3" value="Case-cohort design, an outcome-dependent sampling design for censored&#10;survival data, is increasingly used in biomedical research. The development of&#10;asymptotic theory for a case-cohort design in the current literature primarily&#10;relies on counting process stochastic integrals. Such an approach, however, is&#10;rather limited and lacks theoretical justification for outcome-dependent&#10;weighted methods due to non-predictability. Instead of stochastic integrals, we&#10;derive asymptotic properties for case-cohort studies based on a general&#10;Z-estimation theory for semiparametric models with bundled parameters using&#10;modern empirical processes. Both the Cox model and the additive hazards model&#10;with time-dependent covariates are considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="93" source="Abram M. Kagan" target="Yaakov Malinovsky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.6427v3" />
          <attvalue for="2" value="Monotonicity in the Sample Size of the Length of Classical Confidence&#10;  Intervals" />
          <attvalue for="3" value="It is proved that the average length of standard confidence intervals for&#10;parameters of gamma and normal distributions monotonically decrease with the&#10;sample size. The proofs are based on fine properties of the classical gamma&#10;function." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="94" source="Stéphanie Allassonniere" target="Estelle Kuhn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5938v4" />
          <attvalue for="2" value="Convergent Stochastic Expectation Maximization algorithm with efficient&#10;  sampling in high dimension. Application to deformable template model&#10;  estimation" />
          <attvalue for="3" value="Estimation in the deformable template model is a big challenge in image&#10;analysis. The issue is to estimate an atlas of a population. This atlas&#10;contains a template and the corresponding geometrical variability of the&#10;observed shapes. The goal is to propose an accurate algorithm with low&#10;computational cost and with theoretical guaranties of relevance. This becomes&#10;very demanding when dealing with high dimensional data which is particularly&#10;the case of medical images. We propose to use an optimized Monte Carlo Markov&#10;Chain method into a stochastic Expectation Maximization algorithm in order to&#10;estimate the model parameters by maximizing the likelihood. In this paper, we&#10;present a new Anisotropic Metropolis Adjusted Langevin Algorithm which we use&#10;as transition in the MCMC method. We first prove that this new sampler leads to&#10;a geometrically uniformly ergodic Markov chain. We prove also that under mild&#10;conditions, the estimated parameters converge almost surely and are&#10;asymptotically Gaussian distributed. The methodology developed is then tested&#10;on handwritten digits and some 2D and 3D medical images for the deformable&#10;model estimation. More widely, the proposed algorithm can be used for a large&#10;range of models in many fields of applications such as pharmacology or genetic." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="95" source="Till Sabel" target="Johannes Schmidt-Hieber">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.5501v3" />
          <attvalue for="2" value="Asymptotically efficient estimation of a scale parameter in Gaussian&#10;  time series and closed-form expressions for the Fisher information" />
          <attvalue for="3" value="Mimicking the maximum likelihood estimator, we construct first order&#10;Cramer-Rao efficient and explicitly computable estimators for the scale&#10;parameter $\sigma^2$ in the model $Z_{i,n}=\sigma&#10;n^{-\beta}X_i+Y_i,i=1,\ldots,n,\beta&gt;0$ with independent, stationary Gaussian&#10;processes $(X_i)_{i\in\mathbb{N}}$, $(Y_i)_{i\in\mathbb{N}}$, and&#10;$(X_i)_{i\in\mathbb{N}}$ exhibits possibly long-range dependence. In a second&#10;part, closed-form expressions for the asymptotic behavior of the corresponding&#10;Fisher information are derived. Our main finding is that depending on the&#10;behavior of the spectral densities at zero, the Fisher information has&#10;asymptotically two different scaling regimes, which are separated by a sharp&#10;phase transition. The most prominent example included in our analysis is the&#10;Fisher information for the scaling factor of a high-frequency sample of&#10;fractional Brownian motion under additive noise." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="96" source="Jean-Marc Azais" target="Jean-Claude Fort">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.0757v1" />
          <attvalue for="2" value="Remark on the finite-dimensional character of certain results of&#10;  functional statistics" />
          <attvalue for="3" value="This note shows that some assumption on small balls probability, frequently&#10;used in the domain of functional statistics, implies that the considered&#10;functional space is of finite dimension. To complete this result an example of&#10;L2 process is given that does not fulfill this assumption." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="97" source="Peter Hall" target="Tung Pham">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="98" source="Peter Hall" target="M. P. Wand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="99" source="Peter Hall" target="S. S. J. Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="100" source="Peter Hall" target="Aurore Delaigle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6102v1" />
          <attvalue for="2" value="Nonparametric regression with homogeneous group testing data" />
          <attvalue for="3" value="We introduce new nonparametric predictors for homogeneous pooled data in the&#10;context of group testing for rare abnormalities and show that they achieve&#10;optimal rates of convergence. In particular, when the level of pooling is&#10;moderate, then despite the cost savings, the method enjoys the same convergence&#10;rate as in the case of no pooling. In the setting of &quot;over-pooling&quot; the&#10;convergence rate differs from that of an optimal estimator by no more than a&#10;logarithmic factor. Our approach improves on the random-pooling nonparametric&#10;predictor, which is currently the only nonparametric method available, unless&#10;there is no pooling, in which case the two approaches are identical." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="101" source="Peter Hall" target="Dong Chen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5018v1" />
          <attvalue for="2" value="Single and multiple index functional regression models with&#10;  nonparametric link" />
          <attvalue for="3" value="Fully nonparametric methods for regression from functional data have poor&#10;accuracy from a statistical viewpoint, reflecting the fact that their&#10;convergence rates are slower than nonparametric rates for the estimation of&#10;high-dimensional functions. This difficulty has led to an emphasis on the&#10;so-called functional linear model, which is much more flexible than common&#10;linear models in finite dimension, but nevertheless imposes structural&#10;constraints on the relationship between predictors and responses. Recent&#10;advances have extended the linear approach by using it in conjunction with link&#10;functions, and by considering multiple indices, but the flexibility of this&#10;technique is still limited. For example, the link may be modeled parametrically&#10;or on a grid only, or may be constrained by an assumption such as monotonicity;&#10;multiple indices have been modeled by making finite-dimensional assumptions. In&#10;this paper we introduce a new technique for estimating the link function&#10;nonparametrically, and we suggest an approach to multi-index modeling using&#10;adaptively defined linear projections of functional data. We show that our&#10;methods enable prediction with polynomial convergence rates. The finite sample&#10;performance of our methods is studied in simulations, and is illustrated by an&#10;application to a functional regression problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="102" source="Peter Hall" target="Hans-Georg Müller">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5018v1" />
          <attvalue for="2" value="Single and multiple index functional regression models with&#10;  nonparametric link" />
          <attvalue for="3" value="Fully nonparametric methods for regression from functional data have poor&#10;accuracy from a statistical viewpoint, reflecting the fact that their&#10;convergence rates are slower than nonparametric rates for the estimation of&#10;high-dimensional functions. This difficulty has led to an emphasis on the&#10;so-called functional linear model, which is much more flexible than common&#10;linear models in finite dimension, but nevertheless imposes structural&#10;constraints on the relationship between predictors and responses. Recent&#10;advances have extended the linear approach by using it in conjunction with link&#10;functions, and by considering multiple indices, but the flexibility of this&#10;technique is still limited. For example, the link may be modeled parametrically&#10;or on a grid only, or may be constrained by an assumption such as monotonicity;&#10;multiple indices have been modeled by making finite-dimensional assumptions. In&#10;this paper we introduce a new technique for estimating the link function&#10;nonparametrically, and we suggest an approach to multi-index modeling using&#10;adaptively defined linear projections of functional data. We show that our&#10;methods enable prediction with polynomial convergence rates. The finite sample&#10;performance of our methods is studied in simulations, and is illustrated by an&#10;application to a functional regression problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="103" source="Tung Pham" target="M. P. Wand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="104" source="Tung Pham" target="S. S. J. Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="105" source="M. P. Wand" target="S. S. J. Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5183v1" />
          <attvalue for="2" value="Asymptotic normality and valid inference for Gaussian variational&#10;  approximation" />
          <attvalue for="3" value="We derive the precise asymptotic distributional behavior of Gaussian&#10;variational approximate estimators of the parameters in a single-predictor&#10;Poisson mixed model. These results are the deepest yet obtained concerning the&#10;statistical properties of a variational approximation method. Moreover, they&#10;give rise to asymptotically valid statistical inference. A simulation study&#10;demonstrates that Gaussian variational approximate confidence intervals possess&#10;good to excellent coverage properties, and have a similar precision to their&#10;exact likelihood counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="106" source="B. B. Chen" target="G. M. Pan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5479v1" />
          <attvalue for="2" value="Convergence of the largest eigenvalue of normalized sample covariance&#10;  matrices when p and n both tend to infinity with their ratio converging to&#10;  zero" />
          <attvalue for="3" value="Let $\mathbf{X}_p=(\mathbf{s}_1,...,\mathbf{s}_n)=(X_{ij})_{p \times n}$&#10;where $X_{ij}$'s are independent and identically distributed (i.i.d.) random&#10;variables with $EX_{11}=0,EX_{11}^2=1$ and $EX_{11}^4&lt;\infty$. It is showed&#10;that the largest eigenvalue of the random matrix&#10;$\mathbf{A}_p=\frac{1}{2\sqrt{np}}(\mathbf{X}_p\mathbf{X}_p^{\prime}-n\mathbf{I}_p)$&#10;tends to 1 almost surely as $p\rightarrow\infty,n\rightarrow\infty$ with&#10;$p/n\rightarrow0$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="107" source="G. M. Pan" target="J. Gao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="108" source="G. M. Pan" target="Y. Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="109" source="G. M. Pan" target="M. Guo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="110" source="Yehua Li" target="Tailen Hsing">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2137v1" />
          <attvalue for="2" value="Uniform convergence rates for nonparametric regression and principal&#10;  component analysis in functional/longitudinal data" />
          <attvalue for="3" value="We consider nonparametric estimation of the mean and covariance functions for&#10;functional/longitudinal data. Strong uniform convergence rates are developed&#10;for estimators that are local-linear smoothers. Our results are obtained in a&#10;unified framework in which the number of observations within each curve/cluster&#10;can be of any rate relative to the sample size. We show that the convergence&#10;rates for the procedures depend on both the number of sample curves and the&#10;number of observations on each curve. For sparse functional data, these rates&#10;are equivalent to the optimal rates in nonparametric regression. For dense&#10;functional data, root-n rates of convergence can be achieved with proper&#10;choices of bandwidths. We further derive almost sure rates of convergence for&#10;principal component analysis using the estimated covariance function. The&#10;results are illustrated with simulation studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="111" source="Ngai Hang Chan" target="Ching-Kang Ing">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.4962v1" />
          <attvalue for="2" value="Uniform moment bounds of Fisher's information with applications to time&#10;  series" />
          <attvalue for="3" value="In this paper, a uniform (over some parameter space) moment bound for the&#10;inverse of Fisher's information matrix is established. This result is then&#10;applied to develop moment bounds for the normalized least squares estimate in&#10;(nonlinear) stochastic regression models. The usefulness of these results is&#10;illustrated using time series models. In particular, an asymptotic expression&#10;for the mean squared prediction error of the least squares predictor in&#10;autoregressive moving average models is obtained. This asymptotic expression&#10;provides a solid theoretical foundation for some model selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="112" source="Sara van de Geer" target="Peter Bühlmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5473v2" />
          <attvalue for="2" value="$\ell_0$-penalized maximum likelihood for sparse directed acyclic graphs" />
          <attvalue for="3" value="We consider the problem of regularized maximum likelihood estimation for the&#10;structure and parameters of a high-dimensional, sparse directed acyclic&#10;graphical (DAG) model with Gaussian distribution, or equivalently, of a&#10;Gaussian structural equation model. We show that the $\ell_0$-penalized maximum&#10;likelihood estimator of a DAG has about the same number of edges as the&#10;minimal-edge I-MAP (a DAG with minimal number of edges representing the&#10;distribution), and that it converges in Frobenius norm. We allow the number of&#10;nodes p to be much larger than sample size n but assume a sparsity condition&#10;and that any representation of the true DAG has at least a fixed proportion of&#10;its nonzero edge weights above the noise level. Our results do not rely on the&#10;faithfulness assumption nor on the restrictive strong faithfulness condition&#10;which are required for methods based on conditional independence testing such&#10;as the PC-algorithm." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="113" source="Sara van de Geer" target="Richard Nickl">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.1508v4" />
          <attvalue for="2" value="Confidence sets in sparse regression" />
          <attvalue for="3" value="The problem of constructing confidence sets in the high-dimensional linear&#10;model with $n$ response variables and $p$ parameters, possibly $p\ge n$, is&#10;considered. Full honest adaptive inference is possible if the rate of sparse&#10;estimation does not exceed $n^{-1/4}$, otherwise sparse adaptive confidence&#10;sets exist only over strict subsets of the parameter spaces for which sparse&#10;estimators exist. Necessary and sufficient conditions for the existence of&#10;confidence sets that adapt to a fixed sparsity level of the parameter vector&#10;are given in terms of minimal $\ell^2$-separation conditions on the parameter&#10;space. The design conditions cover common coherence assumptions used in models&#10;for sparsity, including (possibly correlated) sub-Gaussian designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="114" source="Peter Bühlmann" target="Caroline Uhler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="115" source="Peter Bühlmann" target="Garvesh Raskutti">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="116" source="Peter Bühlmann" target="Bin Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="117" source="Peter Bühlmann" target="Tony Cai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5118v1" />
          <attvalue for="2" value="Introduction to the Lehmann special section" />
          <attvalue for="3" value="The current Special Issue of The Annals of Statistics contains three invited&#10;articles. Javier Rojo discusses Erich's scientific achievements and provides&#10;complete lists of his scientific writings and his former Ph.D. students. Willem&#10;van Zwet describes aspects of Erich's life and work, enriched with personal and&#10;interesting anecdotes of Erich's long and productive scientific journey.&#10;Finally, Peter Bickel, Aiyou Chen and Elizaveta Levina present a research paper&#10;on network models: they dedicate their contribution to Erich, emphasizing that&#10;their new nonparametric method and issues about optimality have been very much&#10;influenced by Erich's thinking." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="118" source="Fadoua Balabdaoui" target="Jon A. Wellner">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0828v2" />
          <attvalue for="2" value="Chernoff's density is log-concave" />
          <attvalue for="3" value="We show that the density of $Z=\mathop {\operatorname {argmax}}\{W(t)-t^2\}$,&#10;sometimes known as Chernoff's density, is log-concave. We conjecture that&#10;Chernoff's density is strongly log-concave or &quot;super-Gaussian&quot;, and provide&#10;evidence in support of the conjecture." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="119" source="Jon A. Wellner" target="Fuchang Gao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.0807v2" />
          <attvalue for="2" value="Global Rates of Convergence of the MLE for Multivariate Interval&#10;  Censoring" />
          <attvalue for="3" value="We establish global rates of convergence of the Maximum Likelihood Estimator&#10;(MLE) of a multivariate distribution function in the case of (one type of)&#10;&quot;interval censored&quot; data. The main finding is that the rate of convergence of&#10;the MLE in the Hellinger metric is no worse than $n^{-1/3} (\log n)^{\gamma}$&#10;for $\gamma = (5d - 4)/6$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="120" source="Delphine Blanke" target="Denis Bosq">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2300v2" />
          <attvalue for="2" value="Bayesian prediction for stochastic processes. Theory and applications" />
          <attvalue for="3" value="In this paper, we adopt a Bayesian point of view for predicting real&#10;continuous-time processes. We give two equivalent definitions of a Bayesian&#10;predictor and study some properties: admissibility, prediction sufficiency,&#10;non-unbiasedness, comparison with efficient predictors. Prediction of Poisson&#10;process and prediction of Ornstein-Uhlenbeck process in the continuous and&#10;sampled situations are considered. Various simulations illustrate comparison&#10;with non-Bayesian predictors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="121" source="Delphine Blanke" target="Céline Vial">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2763v2" />
          <attvalue for="2" value="Global smoothness estimation of a Gaussian process from regular sequence&#10;  designs" />
          <attvalue for="3" value="We consider a real Gaussian process $X$ having a global unknown smoothness&#10;$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$,&#10;$r_{\scriptscriptstyle 0}\in \mathds{N}_0$ and $\beta_{\scriptscriptstyle 0}&#10;\in]0,1[$, with $X^{(r_{\scriptscriptstyle 0})}$ (the mean-square derivative of&#10;$X$ if $r_{\scriptscriptstyle 0}\ge 1$) supposed to be locally stationary with&#10;index $\beta_{\scriptscriptstyle 0}$. From the behavior of quadratic variations&#10;built on divided differences of $X$, we derive an estimator of&#10;$(r_{\scriptscriptstyle 0},\beta_{\scriptscriptstyle 0})$ based on - not&#10;necessarily equally spaced - observations of $X$. Various numerical studies of&#10;these estimators exhibit their properties for finite sample size and different&#10;types of processes, and are also completed by two examples of application to&#10;real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="122" source="Xin Chen" target="Changliang Zou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3215v1" />
          <attvalue for="2" value="Coordinate-independent sparse sufficient dimension reduction and&#10;  variable selection" />
          <attvalue for="3" value="Sufficient dimension reduction (SDR) in regression, which reduces the&#10;dimension by replacing original predictors with a minimal set of their linear&#10;combinations without loss of information, is very helpful when the number of&#10;predictors is large. The standard SDR methods suffer because the estimated&#10;linear combinations usually consist of all original predictors, making it&#10;difficult to interpret. In this paper, we propose a unified method -&#10;coordinate-independent sparse estimation (CISE) - that can simultaneously&#10;achieve sparse sufficient dimension reduction and screen out irrelevant and&#10;redundant variables efficiently. CISE is subspace oriented in the sense that it&#10;incorporates a coordinate-independent penalty term with a broad series of&#10;model-based and model-free SDR approaches. This results in a Grassmann manifold&#10;optimization problem and a fast algorithm is suggested. Under mild conditions,&#10;based on manifold theories and techniques, it can be shown that CISE would&#10;perform asymptotically as well as if the true irrelevant predictors were known,&#10;which is referred to as the oracle property. Simulation studies and a real-data&#10;example demonstrate the effectiveness and efficiency of the proposed approach." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="123" source="Xin Chen" target="R. Dennis Cook">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3215v1" />
          <attvalue for="2" value="Coordinate-independent sparse sufficient dimension reduction and&#10;  variable selection" />
          <attvalue for="3" value="Sufficient dimension reduction (SDR) in regression, which reduces the&#10;dimension by replacing original predictors with a minimal set of their linear&#10;combinations without loss of information, is very helpful when the number of&#10;predictors is large. The standard SDR methods suffer because the estimated&#10;linear combinations usually consist of all original predictors, making it&#10;difficult to interpret. In this paper, we propose a unified method -&#10;coordinate-independent sparse estimation (CISE) - that can simultaneously&#10;achieve sparse sufficient dimension reduction and screen out irrelevant and&#10;redundant variables efficiently. CISE is subspace oriented in the sense that it&#10;incorporates a coordinate-independent penalty term with a broad series of&#10;model-based and model-free SDR approaches. This results in a Grassmann manifold&#10;optimization problem and a fast algorithm is suggested. Under mild conditions,&#10;based on manifold theories and techniques, it can be shown that CISE would&#10;perform asymptotically as well as if the true irrelevant predictors were known,&#10;which is referred to as the oracle property. Simulation studies and a real-data&#10;example demonstrate the effectiveness and efficiency of the proposed approach." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="124" source="Changliang Zou" target="R. Dennis Cook">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3215v1" />
          <attvalue for="2" value="Coordinate-independent sparse sufficient dimension reduction and&#10;  variable selection" />
          <attvalue for="3" value="Sufficient dimension reduction (SDR) in regression, which reduces the&#10;dimension by replacing original predictors with a minimal set of their linear&#10;combinations without loss of information, is very helpful when the number of&#10;predictors is large. The standard SDR methods suffer because the estimated&#10;linear combinations usually consist of all original predictors, making it&#10;difficult to interpret. In this paper, we propose a unified method -&#10;coordinate-independent sparse estimation (CISE) - that can simultaneously&#10;achieve sparse sufficient dimension reduction and screen out irrelevant and&#10;redundant variables efficiently. CISE is subspace oriented in the sense that it&#10;incorporates a coordinate-independent penalty term with a broad series of&#10;model-based and model-free SDR approaches. This results in a Grassmann manifold&#10;optimization problem and a fast algorithm is suggested. Under mild conditions,&#10;based on manifold theories and techniques, it can be shown that CISE would&#10;perform asymptotically as well as if the true irrelevant predictors were known,&#10;which is referred to as the oracle property. Simulation studies and a real-data&#10;example demonstrate the effectiveness and efficiency of the proposed approach." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="125" source="R. Dennis Cook" target="Liliana Forzani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6556v1" />
          <attvalue for="2" value="Estimating sufficient reductions of the predictors in abundant&#10;  high-dimensional regressions" />
          <attvalue for="3" value="We study the asymptotic behavior of a class of methods for sufficient&#10;dimension reduction in high-dimension regressions, as the sample size and&#10;number of predictors grow in various alignments. It is demonstrated that these&#10;methods are consistent in a variety of settings, particularly in abundant&#10;regressions where most predictors contribute some information on the response,&#10;and oracle rates are possible. Simulation results are presented to support the&#10;theoretical conclusion." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="126" source="R. Dennis Cook" target="Adam J. Rothman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6556v1" />
          <attvalue for="2" value="Estimating sufficient reductions of the predictors in abundant&#10;  high-dimensional regressions" />
          <attvalue for="3" value="We study the asymptotic behavior of a class of methods for sufficient&#10;dimension reduction in high-dimension regressions, as the sample size and&#10;number of predictors grow in various alignments. It is demonstrated that these&#10;methods are consistent in a variety of settings, particularly in abundant&#10;regressions where most predictors contribute some information on the response,&#10;and oracle rates are possible. Simulation results are presented to support the&#10;theoretical conclusion." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="127" source="Sylvain Delattre" target="Etienne Roquain">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.2489v2" />
          <attvalue for="2" value="On empirical distribution function of high-dimensional Gaussian vector&#10;  components with an application to multiple testing" />
          <attvalue for="3" value="This paper introduces a new framework to study the asymptotical behavior of&#10;the empirical distribution function (e.d.f.) of Gaussian vector components,&#10;whose correlation matrix $\Gamma^{(m)}$ is dimension-dependent. Hence, by&#10;contrast with the existing literature, the vector is not assumed to be&#10;stationary. Rather, we make a &quot;vanishing second order&quot; assumption ensuring that&#10;the covariance matrix $\Gamma^{(m)}$ is not too far from the identity matrix,&#10;while the behavior of the e.d.f. is affected by $\Gamma^{(m)}$ only through the&#10;sequence $\gamma_m=m^{-2} \sum_{i\neq j} \Gamma_{i,j}^{(m)}$, as $m$ grows to&#10;infinity. This result recovers some of the previous results for stationary&#10;long-range dependencies while it also applies to various, high-dimensional,&#10;non-stationary frameworks, for which the most correlated variables are not&#10;necessarily next to each other. Finally, we present an application of this work&#10;to the multiple testing problem, which was the initial statistical motivation&#10;for developing such a methodology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="128" source="Sabyasachi Mukhopadhyay" target="Sourabh Bhattacharya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5508v5" />
          <attvalue for="2" value="Bayesian MISE convergence rates of Polya urn based density estimators:&#10;  asymptotic comparisons and choice of prior parameters" />
          <attvalue for="3" value="Mixture models are well-known for their versatility, and the Bayesian&#10;paradigm is a suitable platform for mixture analysis, particularly when the&#10;number of components is unknown. Bhattacharya (2008) introduced a mixture model&#10;based on the Dirichlet process, where an upper bound on the unknown number of&#10;components is to be specified. Here we consider a Bayesian asymptotic framework&#10;for objectively specifying the upper bound, which we assume to depend on the&#10;sample size. In particular, we define a Bayesian analogue of the mean&#10;integrated squared error (Bayesian MISE), and select that form of the upper&#10;bound, and also that form of the precision parameter of the underlying&#10;Dirichlet process, for which Bayesian MISE of a specific density estimator,&#10;which is a suitable modification of the Polya-urn based prior predictive model,&#10;converges at a desired rate. As a byproduct of our approach, we investigate&#10;asymptotic choice of the precision parameter of the traditional Dirichlet&#10;process mixture model; the density estimator we consider here is a modification&#10;of the prior predictive distribution of Escobar &amp; West (1995) associated with&#10;the Polya urn model. Various asymptotic issues related to the two&#10;aforementioned mixtures, including comparative performances, are also&#10;investigated." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="129" source="Ilia Negri" target="Yoichi Nishiyama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6961v1" />
          <attvalue for="2" value="Moment convergence of $Z$-estimators and $Z$-process method for change&#10;  point problems" />
          <attvalue for="3" value="The problem to establish not only the asymptotic distribution results for&#10;statistical estimators but also the moment convergence of the estimators has&#10;been recognized as an important issue in advanced theories of statistics. One&#10;of the main goals of this paper is to present a metod to derive the moment&#10;convergence of $Z$-estimators as it has been done for $M$-estimators. Another&#10;goal of this paper is to develop a general, unified approach, based on some&#10;partial estimation functions which we call &quot;$Z$-process&quot;, to the change point&#10;problems for ergodic models as well as some models where the Fisher information&#10;matrix is random and inhomogeneous in time. Applications to some diffusion&#10;process models and Cox's regression model are also discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="130" source="Ilia Negri" target="Li Zhou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6547v1" />
          <attvalue for="2" value="On Goodness-of-fit Testing for Ergodic Diffusion Process with Shift&#10;  Parameter" />
          <attvalue for="3" value="A problem of goodness-of-fit test for ergodic diffusion processes is&#10;presented. In the null hypothesis the drift of the diffusion is supposed to be&#10;in a parametric form with unknown shift parameter. Two Cramer-Von Mises type&#10;test statistics are studied. The first one is based on local time estimator of&#10;the invariant density, the second one is based on the empirical distribution&#10;function. The unknown parameter is estimated via the maximum likelihood&#10;estimator. It is shown that both the limit distributions of the two test&#10;statistics do not depend on the unknown parameter, so the distributions of the&#10;tests are asymptotically parameter free. Some considerations on the consistency&#10;of the proposed tests and some simulation studies are also given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="131" source="Teppei Ogihara" target="Nakahiro Yoshida">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.4911v1" />
          <attvalue for="2" value="Quasi-Likelihood Analysis for Stochastic Regression Models with&#10;  Nonsynchronous Observations" />
          <attvalue for="3" value="We consider nonsynchronous sampling of parameterized stochastic regression&#10;models, which contain stochastic differential equations. Constructing a&#10;quasi-likelihood function, we prove that the quasi-maximum likelihood estimator&#10;and the Bayes type estimator are consistent and asymptotically mixed normal&#10;when the sampling frequency of the nonsynchronous data becomes large." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="132" source="Boris Buchmann" target="Gernot Müller">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0186v1" />
          <attvalue for="2" value="Limit experiments of GARCH" />
          <attvalue for="3" value="GARCH is one of the most prominent nonlinear time series models, both widely&#10;applied and thoroughly studied. Recently, it has been shown that the COGARCH&#10;model (which was introduced a few years ago by Kl\&quot;{u}ppelberg, Lindner and&#10;Maller) and Nelson's diffusion limit are the only functional continuous-time&#10;limits of GARCH in distribution. In contrast to Nelson's diffusion limit,&#10;COGARCH reproduces most of the stylized facts of financial time series. Since&#10;it has been proven that Nelson's diffusion is not asymptotically equivalent to&#10;GARCH in deficiency, in the present paper, we investigate the relation between&#10;GARCH and COGARCH in Le Cam's framework of statistical equivalence. We show&#10;that GARCH converges generically to COGARCH, even in deficiency, provided that&#10;the volatility processes are observed. Hence, from a theoretical point of view,&#10;COGARCH can indeed be considered as a continuous-time equivalent to GARCH.&#10;Otherwise, when the observations are incomplete, GARCH still has a limiting&#10;experiment, which we call MCOGARCH, which is not equivalent, but nevertheless&#10;quite similar, to COGARCH. In the COGARCH model, the jump times can be more&#10;random than for the MCOGARCH, a fact practitioners may see as an advantage of&#10;COGARCH." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="133" source="Dave Zachariah" target="Saikat Chatterjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2493v1" />
          <attvalue for="2" value="Alternating Least-Squares for Low-Rank Matrix Reconstruction" />
          <attvalue for="3" value="For reconstruction of low-rank matrices from undersampled measurements, we&#10;develop an iterative algorithm based on least-squares estimation. While the&#10;algorithm can be used for any low-rank matrix, it is also capable of exploiting&#10;a-priori knowledge of matrix structure. In particular, we consider linearly&#10;structured matrices, such as Hankel and Toeplitz, as well as positive&#10;semidefinite matrices. The performance of the algorithm, referred to as&#10;alternating least-squares (ALS), is evaluated by simulations and compared to&#10;the Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="134" source="Dave Zachariah" target="Magnus Jansson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="135" source="Dave Zachariah" target="Martin Sundin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2493v1" />
          <attvalue for="2" value="Alternating Least-Squares for Low-Rank Matrix Reconstruction" />
          <attvalue for="3" value="For reconstruction of low-rank matrices from undersampled measurements, we&#10;develop an iterative algorithm based on least-squares estimation. While the&#10;algorithm can be used for any low-rank matrix, it is also capable of exploiting&#10;a-priori knowledge of matrix structure. In particular, we consider linearly&#10;structured matrices, such as Hankel and Toeplitz, as well as positive&#10;semidefinite matrices. The performance of the algorithm, referred to as&#10;alternating least-squares (ALS), is evaluated by simulations and compared to&#10;the Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="136" source="Dave Zachariah" target="Isaac Skog">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="137" source="Dave Zachariah" target="Peter Händel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="138" source="Saikat Chatterjee" target="Magnus Jansson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2493v1" />
          <attvalue for="2" value="Alternating Least-Squares for Low-Rank Matrix Reconstruction" />
          <attvalue for="3" value="For reconstruction of low-rank matrices from undersampled measurements, we&#10;develop an iterative algorithm based on least-squares estimation. While the&#10;algorithm can be used for any low-rank matrix, it is also capable of exploiting&#10;a-priori knowledge of matrix structure. In particular, we consider linearly&#10;structured matrices, such as Hankel and Toeplitz, as well as positive&#10;semidefinite matrices. The performance of the algorithm, referred to as&#10;alternating least-squares (ALS), is evaluated by simulations and compared to&#10;the Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="139" source="Saikat Chatterjee" target="Martin Sundin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2493v1" />
          <attvalue for="2" value="Alternating Least-Squares for Low-Rank Matrix Reconstruction" />
          <attvalue for="3" value="For reconstruction of low-rank matrices from undersampled measurements, we&#10;develop an iterative algorithm based on least-squares estimation. While the&#10;algorithm can be used for any low-rank matrix, it is also capable of exploiting&#10;a-priori knowledge of matrix structure. In particular, we consider linearly&#10;structured matrices, such as Hankel and Toeplitz, as well as positive&#10;semidefinite matrices. The performance of the algorithm, referred to as&#10;alternating least-squares (ALS), is evaluated by simulations and compared to&#10;the Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="140" source="Magnus Jansson" target="Martin Sundin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2493v1" />
          <attvalue for="2" value="Alternating Least-Squares for Low-Rank Matrix Reconstruction" />
          <attvalue for="3" value="For reconstruction of low-rank matrices from undersampled measurements, we&#10;develop an iterative algorithm based on least-squares estimation. While the&#10;algorithm can be used for any low-rank matrix, it is also capable of exploiting&#10;a-priori knowledge of matrix structure. In particular, we consider linearly&#10;structured matrices, such as Hankel and Toeplitz, as well as positive&#10;semidefinite matrices. The performance of the algorithm, referred to as&#10;alternating least-squares (ALS), is evaluated by simulations and compared to&#10;the Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="141" source="Magnus Jansson" target="Isaac Skog">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="142" source="Magnus Jansson" target="Peter Händel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="143" source="Jean-Marc Bardet" target="Béchir Dola">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2453v2" />
          <attvalue for="2" value="Semiparametric stationarity tests based on adaptive multidimensional&#10;  increment ratio statistics" />
          <attvalue for="3" value="In this paper, we show that the adaptive multidimensional increment ratio&#10;estimator of the long range memory parameter defined in Bardet and Dola (2012)&#10;satisfies a central limit theorem (CLT in the sequel) for a large&#10;semiparametric class of Gaussian fractionally integrated processes with memory&#10;parameter $d \in (-0.5,1.25)$. Since the asymptotic variance of this CLT can be&#10;computed, tests of stationarity or nonstationarity distinguishing the&#10;assumptions $d&lt;0.5$ and $d \geq 0.5$ are constructed. These tests are also&#10;consistent tests of unit root. Simulations done on a large benchmark of short&#10;memory, long memory and non stationary processes show the accuracy of the tests&#10;with respect to other usual stationarity or nonstationarity tests (LMC, V/S,&#10;ADF and PP tests). Finally, the estimator and tests are applied to log-returns&#10;of famous economic data and to their absolute value power laws." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="144" source="Jean-Marc Bardet" target="William Chakry Kengne">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4746v2" />
          <attvalue for="2" value="Monitoring procedure for parameter change in causal time series" />
          <attvalue for="3" value="We propose a new sequential procedure to detect change in the parameters of a&#10;process $ X= (X_t)_{t\in \Z}$ belonging to a large class of causal models (such&#10;as AR($\infty$), ARCH($\infty$), TARCH($\infty$), ARMA-GARCH processes). The&#10;procedure is based on a difference between the historical parameter estimator&#10;and the updated parameter estimator, where both these estimators are based on a&#10;quasi-likelihood of the model. Unlike classical recursive fluctuation test, the&#10;updated estimator is computed without the historical observations. The&#10;asymptotic behavior of the test is studied and the consistency in power as well&#10;as an upper bound of the detection delay are obtained. Some simulation results&#10;are reported with comparisons to some other existing procedures exhibiting the&#10;accuracy of our new procedure. The procedure is also applied to the daily&#10;closing values of the Nikkei 225, S$&amp;$P 500 and FTSE 100 stock index. We show&#10;in this real-data applications how the procedure can be used to solve off-line&#10;multiple breaks detection." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="145" source="Hannes Leeb" target="Benedikt M. Pötscher">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4543v3" />
          <attvalue for="2" value="Testing in the Presence of Nuisance Parameters: Some Comments on Tests&#10;  Post-Model-Selection and Random Critical Values" />
          <attvalue for="3" value="We point out that the ideas underlying some test procedures recently proposed&#10;for testing post-model-selection (and for some other test problems) in the&#10;econometrics literature have been around for quite some time in the statistics&#10;literature. We also sharpen some of these results in the statistics literature.&#10;Furthermore, we show that some intuitively appealing testing procedures, that&#10;have found their way into the econometrics literature, lead to tests that do&#10;not have desirable size properties, not even asymptotically." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="146" source="Hannes Leeb" target="Nina Huber">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.0899v2" />
          <attvalue for="2" value="Shrinkage estimators for prediction out-of-sample: Conditional&#10;  performance" />
          <attvalue for="3" value="We find that, in a linear model, the James-Stein estimator, which dominates&#10;the maximum-likelihood estimator in terms of its in-sample prediction error,&#10;can perform poorly compared to the maximum-likelihood estimator in&#10;out-of-sample prediction. We give a detailed analysis of this phenomenon and&#10;discuss its implications. When evaluating the predictive performance of&#10;estimators, we treat the regressor matrix in the training data as fixed, i.e.,&#10;we condition on the design variables. Our findings contrast those obtained by&#10;Baranchik (1973, Ann. Stat. 1:312-321) and, more recently, by Dicker (2012,&#10;arXiv:1102.2952) in an unconditional performance evaluation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="147" source="A. E. Koudou" target="P. Vallois">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0381v1" />
          <attvalue for="2" value="Independence properties of the Matsumoto--Yor type" />
          <attvalue for="3" value="We define Letac-Wesolowski-Matsumoto-Yor (LWMY) functions as decreasing&#10;functions from $(0,\infty)$ onto $(0,\infty)$ with the following property:&#10;there exist independent, positive random variables $X$ and $Y$ such that the&#10;variables $f(X+Y)$ and $f(X)-f(X+Y)$ are independent. We prove that, under&#10;additional assumptions, there are essentially four such functions. The first&#10;one is $f(x)=1/x$. In this case, referred to in the literature as the&#10;Matsumoto-Yor property, the law of $X$ is generalized inverse Gaussian while&#10;$Y$ is gamma distributed. In the three other cases, the associated densities&#10;are provided. As a consequence, we obtain a new relation of convolution&#10;involving gamma distributions and Kummer distributions of type 2." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="148" source="Neil Bathia" target="Qiwei Yao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2522v1" />
          <attvalue for="2" value="Identifying the finite dimensionality of curve time series" />
          <attvalue for="3" value="The curve time series framework provides a convenient vehicle to accommodate&#10;some nonstationary features into a stationary setup. We propose a new method to&#10;identify the dimensionality of curve time series based on the dynamical&#10;dependence across different curves. The practical implementation of our method&#10;boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the&#10;determination of the dimensionality is equivalent to the identification of the&#10;nonzero eigenvalues of the matrix, which we carry out in terms of some&#10;bootstrap tests. Asymptotic properties of the proposed method are investigated.&#10;In particular, our estimators for zero-eigenvalues enjoy the fast convergence&#10;rate n while the estimators for nonzero eigenvalues converge at the standard&#10;$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated&#10;and real data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="149" source="Neil Bathia" target="Flavio Ziegelmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2522v1" />
          <attvalue for="2" value="Identifying the finite dimensionality of curve time series" />
          <attvalue for="3" value="The curve time series framework provides a convenient vehicle to accommodate&#10;some nonstationary features into a stationary setup. We propose a new method to&#10;identify the dimensionality of curve time series based on the dynamical&#10;dependence across different curves. The practical implementation of our method&#10;boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the&#10;determination of the dimensionality is equivalent to the identification of the&#10;nonzero eigenvalues of the matrix, which we carry out in terms of some&#10;bootstrap tests. Asymptotic properties of the proposed method are investigated.&#10;In particular, our estimators for zero-eigenvalues enjoy the fast convergence&#10;rate n while the estimators for nonzero eigenvalues converge at the standard&#10;$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated&#10;and real data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="150" source="Qiwei Yao" target="Flavio Ziegelmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2522v1" />
          <attvalue for="2" value="Identifying the finite dimensionality of curve time series" />
          <attvalue for="3" value="The curve time series framework provides a convenient vehicle to accommodate&#10;some nonstationary features into a stationary setup. We propose a new method to&#10;identify the dimensionality of curve time series based on the dynamical&#10;dependence across different curves. The practical implementation of our method&#10;boils down to an eigenanalysis of a finite-dimensional matrix. Furthermore, the&#10;determination of the dimensionality is equivalent to the identification of the&#10;nonzero eigenvalues of the matrix, which we carry out in terms of some&#10;bootstrap tests. Asymptotic properties of the proposed method are investigated.&#10;In particular, our estimators for zero-eigenvalues enjoy the fast convergence&#10;rate n while the estimators for nonzero eigenvalues converge at the standard&#10;$\sqrt{n}$-rate. The proposed methodology is illustrated with both simulated&#10;and real data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="151" source="Qiwei Yao" target="Clifford Lam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0613v1" />
          <attvalue for="2" value="Factor modeling for high-dimensional time series: Inference for the&#10;  number of factors" />
          <attvalue for="3" value="This paper deals with the factor modeling for high-dimensional time series&#10;based on a dimension-reduction viewpoint. Under stationary settings, the&#10;inference is simple in the sense that both the number of factors and the factor&#10;loadings are estimated in terms of an eigenanalysis for a nonnegative definite&#10;matrix, and is therefore applicable when the dimension of time series is on the&#10;order of a few thousands. Asymptotic properties of the proposed method are&#10;investigated under two settings: (i) the sample size goes to infinity while the&#10;dimension of time series is fixed; and (ii) both the sample size and the&#10;dimension of time series go to infinity together. In particular, our estimators&#10;for zero-eigenvalues enjoy faster convergence (or slower divergence) rates,&#10;hence making the estimation for the number of factors easier. In particular,&#10;when the sample size and the dimension of time series go to infinity together,&#10;the estimators for the eigenvalues are no longer consistent. However, our&#10;estimator for the number of the factors, which is based on the ratios of the&#10;estimated eigenvalues, still works fine. Furthermore, this estimation shows the&#10;so-called &quot;blessing of dimensionality&quot; property in the sense that the&#10;performance of the estimation may improve when the dimension of time series&#10;increases. A two-step procedure is investigated when the factors are of&#10;different degrees of strength. Numerical illustration with both simulated and&#10;real data is also reported." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="152" source="Peter S. Chami" target="Bernd Sing">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2395v1" />
          <attvalue for="2" value="A two parameter ratio-product-ratio estimator using auxiliary&#10;  information" />
          <attvalue for="3" value="We propose a two parameter ratio-product-ratio estimator for a finite&#10;population mean in a simple random sample without replacement following the&#10;methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai&#10;(1985) and Singh and Ruiz Espejo (2003).&#10;  The bias and mean square error of our proposed estimator are obtained to the&#10;first degree of approximation. We derive conditions for the parameters under&#10;which the proposed estimator has smaller mean square error than the sample&#10;mean, ratio and product estimators.&#10;  We carry out an application showing that the proposed estimator outperforms&#10;the traditional estimators using groundwater data taken from a geological site&#10;in the state of Florida." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="153" source="Peter S. Chami" target="Doneal Thomas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2395v1" />
          <attvalue for="2" value="A two parameter ratio-product-ratio estimator using auxiliary&#10;  information" />
          <attvalue for="3" value="We propose a two parameter ratio-product-ratio estimator for a finite&#10;population mean in a simple random sample without replacement following the&#10;methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai&#10;(1985) and Singh and Ruiz Espejo (2003).&#10;  The bias and mean square error of our proposed estimator are obtained to the&#10;first degree of approximation. We derive conditions for the parameters under&#10;which the proposed estimator has smaller mean square error than the sample&#10;mean, ratio and product estimators.&#10;  We carry out an application showing that the proposed estimator outperforms&#10;the traditional estimators using groundwater data taken from a geological site&#10;in the state of Florida." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="154" source="Bernd Sing" target="Doneal Thomas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2395v1" />
          <attvalue for="2" value="A two parameter ratio-product-ratio estimator using auxiliary&#10;  information" />
          <attvalue for="3" value="We propose a two parameter ratio-product-ratio estimator for a finite&#10;population mean in a simple random sample without replacement following the&#10;methodology in Ray and Sahai (1980), Sahai and Ray (1980), Sahai and Sahai&#10;(1985) and Singh and Ruiz Espejo (2003).&#10;  The bias and mean square error of our proposed estimator are obtained to the&#10;first degree of approximation. We derive conditions for the parameters under&#10;which the proposed estimator has smaller mean square error than the sample&#10;mean, ratio and product estimators.&#10;  We carry out an application showing that the proposed estimator outperforms&#10;the traditional estimators using groundwater data taken from a geological site&#10;in the state of Florida." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="155" source="Qiying Wang" target="Peter C. B. Phillips">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0825v1" />
          <attvalue for="2" value="A specification test for nonlinear nonstationary models" />
          <attvalue for="3" value="We provide a limit theory for a general class of kernel smoothed U-statistics&#10;that may be used for specification testing in time series regression with&#10;nonstationary data. The test framework allows for linear and nonlinear models&#10;with endogenous regressors that have autoregressive unit roots or near unit&#10;roots. The limit theory for the specification test depends on the&#10;self-intersection local time of a Gaussian process. A new weak convergence&#10;result is developed for certain partial sums of functions involving&#10;nonstationary time series that converges to the intersection local time&#10;process. This result is of independent interest and is useful in other&#10;applications. Simulations examine the finite sample performance of the test." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="156" source="Juan Lucas Bali" target="Graciela Boente">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="157" source="Juan Lucas Bali" target="David E. Tyler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="158" source="Juan Lucas Bali" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="159" source="Graciela Boente" target="David E. Tyler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="160" source="Graciela Boente" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="161" source="David E. Tyler" target="Jane-Ling Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2027v1" />
          <attvalue for="2" value="Robust functional principal components: A projection-pursuit approach" />
          <attvalue for="3" value="In many situations, data are recorded over a period of time and may be&#10;regarded as realizations of a stochastic process. In this paper, robust&#10;estimators for the principal components are considered by adapting the&#10;projection pursuit approach to the functional data setting. Our approach&#10;combines robust projection-pursuit with different smoothing methods.&#10;Consistency of the estimators are shown under mild assumptions. The performance&#10;of the classical and robust procedures are compared in a simulation study under&#10;different contamination schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="162" source="Jane-Ling Wang" target="Yu-Ru Su">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5183v1" />
          <attvalue for="2" value="Modeling left-truncated and right-censored survival data with&#10;  longitudinal covariates" />
          <attvalue for="3" value="There is a surge in medical follow-up studies that include longitudinal&#10;covariates in the modeling of survival data. So far, the focus has been largely&#10;on right-censored survival data. We consider survival data that are subject to&#10;both left truncation and right censoring. Left truncation is well known to&#10;produce biased sample. The sampling bias issue has been resolved in the&#10;literature for the case which involves baseline or time-varying covariates that&#10;are observable. The problem remains open, however, for the important case where&#10;longitudinal covariates are present in survival models. A joint likelihood&#10;approach has been shown in the literature to provide an effective way to&#10;overcome those difficulties for right-censored data, but this approach faces&#10;substantial additional challenges in the presence of left truncation. Here we&#10;thus propose an alternative likelihood to overcome these difficulties and show&#10;that the regression coefficient in the survival component can be estimated&#10;unbiasedly and efficiently. Issues about the bias for the longitudinal&#10;component are discussed. The new approach is illustrated numerically through&#10;simulations and data from a multi-center AIDS cohort study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="163" source="Rolando Biscay" target="Hélène Lescornel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0107v1" />
          <attvalue for="2" value="Adaptive Covariance Estimation with model selection" />
          <attvalue for="3" value="We provide in this paper a fully adaptive penalized procedure to select a&#10;covariance among a collection of models observing i.i.d replications of the&#10;process at fixed observation points. For this we generalize previous results of&#10;Bigot and al. and propose to use a data driven penalty to obtain an oracle&#10;inequality for the estimator. We prove that this method is an extension to the&#10;matricial regression model of the work by Baraud." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="164" source="Rolando Biscay" target="Jean-Michel Loubes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0107v1" />
          <attvalue for="2" value="Adaptive Covariance Estimation with model selection" />
          <attvalue for="3" value="We provide in this paper a fully adaptive penalized procedure to select a&#10;covariance among a collection of models observing i.i.d replications of the&#10;process at fixed observation points. For this we generalize previous results of&#10;Bigot and al. and propose to use a data driven penalty to obtain an oracle&#10;inequality for the estimator. We prove that this method is an extension to the&#10;matricial regression model of the work by Baraud." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="165" source="Hélène Lescornel" target="Jean-Michel Loubes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0107v1" />
          <attvalue for="2" value="Adaptive Covariance Estimation with model selection" />
          <attvalue for="3" value="We provide in this paper a fully adaptive penalized procedure to select a&#10;covariance among a collection of models observing i.i.d replications of the&#10;process at fixed observation points. For this we generalize previous results of&#10;Bigot and al. and propose to use a data driven penalty to obtain an oracle&#10;inequality for the estimator. We prove that this method is an extension to the&#10;matricial regression model of the work by Baraud." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="166" source="Alexander Aue" target="Thomas C. M. Lee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2087v1" />
          <attvalue for="2" value="On image segmentation using information theoretic criteria" />
          <attvalue for="3" value="Image segmentation is a long-studied and important problem in image&#10;processing. Different solutions have been proposed, many of which follow the&#10;information theoretic paradigm. While these information theoretic segmentation&#10;methods often produce excellent empirical results, their theoretical properties&#10;are still largely unknown. The main goal of this paper is to conduct a rigorous&#10;theoretical study into the statistical consistency properties of such methods.&#10;To be more specific, this paper investigates if these methods can accurately&#10;recover the true number of segments together with their true boundaries in the&#10;image as the number of pixels tends to infinity. Our theoretical results show&#10;that both the Bayesian information criterion (BIC) and the minimum description&#10;length (MDL) principle can be applied to derive statistically consistent&#10;segmentation methods, while the same is not true for the Akaike information&#10;criterion (AIC). Numerical experiments were conducted to illustrate and support&#10;our theoretical findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="167" source="Jia Chen" target="Jiti Gao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.3324v1" />
          <attvalue for="2" value="Estimation in semi-parametric regression with non-stationary regressors" />
          <attvalue for="3" value="In this paper, we consider a partially linear model of the form&#10;$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a&#10;$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly&#10;stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary&#10;sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a&#10;semi-parametric least-squares (SLS) estimation method. Under certain&#10;conditions, we then show that the proposed SLS estimator of $\theta_0$ is still&#10;asymptotically normal with the same rate as for the case of stationary time&#10;series. In addition, we also establish an asymptotic distribution for the&#10;nonparametric estimator of the function $g(\cdot)$. Some numerical examples are&#10;provided to show that our theory and estimation method work well in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="168" source="Jia Chen" target="Degui Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.3324v1" />
          <attvalue for="2" value="Estimation in semi-parametric regression with non-stationary regressors" />
          <attvalue for="3" value="In this paper, we consider a partially linear model of the form&#10;$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a&#10;$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly&#10;stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary&#10;sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a&#10;semi-parametric least-squares (SLS) estimation method. Under certain&#10;conditions, we then show that the proposed SLS estimator of $\theta_0$ is still&#10;asymptotically normal with the same rate as for the case of stationary time&#10;series. In addition, we also establish an asymptotic distribution for the&#10;nonparametric estimator of the function $g(\cdot)$. Some numerical examples are&#10;provided to show that our theory and estimation method work well in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="169" source="Jiti Gao" target="Degui Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.3324v1" />
          <attvalue for="2" value="Estimation in semi-parametric regression with non-stationary regressors" />
          <attvalue for="3" value="In this paper, we consider a partially linear model of the form&#10;$Y_t=X_t^{\tau}\theta_0+g(V_t)+\epsilon_t$, $t=1,...,n$, where $\{V_t\}$ is a&#10;$\beta$ null recurrent Markov chain, $\{X_t\}$ is a sequence of either strictly&#10;stationary or non-stationary regressors and $\{\epsilon_t\}$ is a stationary&#10;sequence. We propose to estimate both $\theta_0$ and $g(\cdot)$ by a&#10;semi-parametric least-squares (SLS) estimation method. Under certain&#10;conditions, we then show that the proposed SLS estimator of $\theta_0$ is still&#10;asymptotically normal with the same rate as for the case of stationary time&#10;series. In addition, we also establish an asymptotic distribution for the&#10;nonparametric estimator of the function $g(\cdot)$. Some numerical examples are&#10;provided to show that our theory and estimation method work well in practice." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="170" source="Constantinos Georghiou" target="Andreas N. Philippou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3186v4" />
          <attvalue for="2" value="On the modes of the Poisson distribution of order k" />
          <attvalue for="3" value="Sharp upper and lower bounds are established for the modes of the Poisson&#10;distribution of order k. The lower bound established in this paper is better&#10;than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a&#10;recent conjecture is presently proved solving partially an open problem since&#10;1983." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="171" source="Constantinos Georghiou" target="Abolfazl Saghafi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3186v4" />
          <attvalue for="2" value="On the modes of the Poisson distribution of order k" />
          <attvalue for="3" value="Sharp upper and lower bounds are established for the modes of the Poisson&#10;distribution of order k. The lower bound established in this paper is better&#10;than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a&#10;recent conjecture is presently proved solving partially an open problem since&#10;1983." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="172" source="Andreas N. Philippou" target="Abolfazl Saghafi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3186v4" />
          <attvalue for="2" value="On the modes of the Poisson distribution of order k" />
          <attvalue for="3" value="Sharp upper and lower bounds are established for the modes of the Poisson&#10;distribution of order k. The lower bound established in this paper is better&#10;than the previously established lower bound. In addition, for k = 2, 3, 4, 5, a&#10;recent conjecture is presently proved solving partially an open problem since&#10;1983." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="173" source="Song Xi Chen" target="Ping-Shou Zhong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2979v1" />
          <attvalue for="2" value="ANOVA for longitudinal data with missing values" />
          <attvalue for="3" value="We carry out ANOVA comparisons of multiple treatments for longitudinal&#10;studies with missing values. The treatment effects are modeled&#10;semiparametrically via a partially linear regression which is flexible in&#10;quantifying the time effects of treatments. The empirical likelihood is&#10;employed to formulate model-robust nonparametric ANOVA tests for treatment&#10;effects with respect to covariates, the nonparametric time-effect functions and&#10;interactions between covariates and time. The proposed tests can be readily&#10;modified for a variety of data and model combinations, that encompasses&#10;parametric, semiparametric and nonparametric regression models; cross-sectional&#10;and longitudinal data, and with or without missing values." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="174" source="Song Xi Chen" target="Yumou Qiu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3321v1" />
          <attvalue for="2" value="Test for bandedness of high-dimensional covariance matrices and&#10;  bandwidth estimation" />
          <attvalue for="3" value="Motivated by the latest effort to employ banded matrices to estimate a&#10;high-dimensional covariance $\Sigma$, we propose a test for $\Sigma$ being&#10;banded with possible diverging bandwidth. The test is adaptive to the &quot;large&#10;$p$, small $n$&quot; situations without assuming a specific parametric distribution&#10;for the data. We also formulate a consistent estimator for the bandwidth of a&#10;banded high-dimensional covariance matrix. The properties of the test and the&#10;bandwidth estimator are investigated by theoretical evaluations and simulation&#10;studies, as well as an empirical analysis on a protein mass spectroscopy data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="175" source="Song Xi Chen" target="Jinyuan Chang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2004v1" />
          <attvalue for="2" value="On the approximate maximum likelihood estimation for diffusion processes" />
          <attvalue for="3" value="The transition density of a diffusion process does not admit an explicit&#10;expression in general, which prevents the full maximum likelihood estimation&#10;(MLE) based on discretely observed sample paths. A\&quot;{\i}t-Sahalia [J. Finance&#10;54 (1999) 1361--1395; Econometrica 70 (2002) 223--262] proposed asymptotic&#10;expansions to the transition densities of diffusion processes, which lead to an&#10;approximate maximum likelihood estimation (AMLE) for parameters. Built on&#10;A\&quot;{\i}t-Sahalia's [Econometrica 70 (2002) 223--262; Ann. Statist. 36 (2008)&#10;906--937] proposal and analysis on the AMLE, we establish the consistency and&#10;convergence rate of the AMLE, which reveal the roles played by the number of&#10;terms used in the asymptotic density expansions and the sampling interval&#10;between successive observations. We find conditions under which the AMLE has&#10;the same asymptotic distribution as that of the full MLE. A first order&#10;approximation to the Fisher information matrix is proposed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="176" source="Song Xi Chen" target="Jun Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0917v1" />
          <attvalue for="2" value="Two sample tests for high-dimensional covariance matrices" />
          <attvalue for="3" value="We propose two tests for the equality of covariance matrices between two&#10;high-dimensional populations. One test is on the whole variance--covariance&#10;matrices, and the other is on off-diagonal sub-matrices, which define the&#10;covariance between two nonoverlapping segments of the high-dimensional random&#10;vectors. The tests are applicable (i) when the data dimension is much larger&#10;than the sample sizes, namely the &quot;large $p$, small $n$&quot; situations and (ii)&#10;without assuming parametric distributions for the two populations. These two&#10;aspects surpass the capability of the conventional likelihood ratio test. The&#10;proposed tests can be used to test on covariances associated with gene ontology&#10;terms." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="177" source="Dominique Bontemps" target="Sébastien Gadat">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5429v2" />
          <attvalue for="2" value="Bayesian posterior consistency in the functional randomly shifted curves&#10;  model" />
          <attvalue for="3" value="In this paper, we consider the so-called Shape Invariant Model which stands&#10;for the estimation of a function $f^0$ submitted to a random translation of law&#10;$g^0$ in a white noise model. We are interested in such a model when the law of&#10;the deformations is unknown. We aim to recover the law of the process&#10;$\PP_{f^0,g^0}$ as well as $f^0$ and $g^0$. In this perspective, we adopt a&#10;Bayesian point of view and find prior on $f$ and $g$ such that the posterior&#10;distribution concentrates around $\PP_{f^0,g^0}$ at a polynomial rate when $n$&#10;goes to $+\infty$. We obtain a logarithmic posterior contraction rate for the&#10;shape $f^0$ and the distribution $g^0$. We also derive logarithmic lower bounds&#10;for the estimation of $f^0$ and $g^0$ in a frequentist paradigm." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="178" source="Dominique Bontemps" target="Stéphane Boucheron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0258v4" />
          <attvalue for="2" value="About adaptive coding on countable alphabets" />
          <attvalue for="3" value="This paper sheds light on universal coding with respect to classes of&#10;memoryless sources over a countable alphabet defined by an envelope function&#10;with finite and non-decreasing hazard rate. We prove that the auto-censuring AC&#10;code introduced by Bontemps (2011) is adaptive with respect to the collection&#10;of such classes. The analysis builds on the tight characterization of universal&#10;redundancy rate in terms of metric entropy % of small source classes by Opper&#10;and Haussler (1997) and on a careful analysis of the performance of the&#10;AC-coding algorithm. The latter relies on non-asymptotic bounds for maxima of&#10;samples from discrete distributions with finite and non-decreasing hazard rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="179" source="R. Colombi" target="A. Forcina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.8050v1" />
          <attvalue for="2" value="A class of smooth models satisfying marginal and context specific&#10;  conditional independencies" />
          <attvalue for="3" value="We study a class of conditional independence models for discrete data with&#10;the property that one or more log-linear interactions are defined within two&#10;different marginal distributions and then constrained to 0; all the conditional&#10;independence models which are known to be non smooth belong to this class. We&#10;introduce a new marginal log-linear parameterization and show that smoothness&#10;may be restored by restricting one or more independence statements to hold&#10;conditionally to a restricted subset of the configurations of the conditioning&#10;variables. Our results are based on a specific reconstruction algorithm from&#10;log-linear parameters to probabilities and fixed point theory. Several examples&#10;are examined and a general rule for determining the implied conditional&#10;independence restrictions is outlined." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="180" source="Gérard Biau" target="Frédéric Cérou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6461v2" />
          <attvalue for="2" value="New Insights Into Approximate Bayesian Computation" />
          <attvalue for="3" value="Approximate Bayesian Computation (ABC for short) is a family of computational&#10;techniques which offer an almost automated solution in situations where&#10;evaluation of the posterior likelihood is computationally prohibitive, or&#10;whenever suitable likelihoods are not available. In the present paper, we&#10;analyze the procedure from the point of view of k-nearest neighbor theory and&#10;explore the statistical properties of its outputs. We discuss in particular&#10;some asymptotic features of the genuine conditional density estimate associated&#10;with ABC, which is an interesting hybrid between a k-nearest neighbor and a&#10;kernel method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="181" source="Gérard Biau" target="Arnaud Guyader">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6461v2" />
          <attvalue for="2" value="New Insights Into Approximate Bayesian Computation" />
          <attvalue for="3" value="Approximate Bayesian Computation (ABC for short) is a family of computational&#10;techniques which offer an almost automated solution in situations where&#10;evaluation of the posterior likelihood is computationally prohibitive, or&#10;whenever suitable likelihoods are not available. In the present paper, we&#10;analyze the procedure from the point of view of k-nearest neighbor theory and&#10;explore the statistical properties of its outputs. We discuss in particular&#10;some asymptotic features of the genuine conditional density estimate associated&#10;with ABC, which is an interesting hybrid between a k-nearest neighbor and a&#10;kernel method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="182" source="Gérard Biau" target="Luc Devroye">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="183" source="Gérard Biau" target="Vida Dujmovic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="184" source="Gérard Biau" target="Adam Krzyzak">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="185" source="Frédéric Cérou" target="Arnaud Guyader">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6461v2" />
          <attvalue for="2" value="New Insights Into Approximate Bayesian Computation" />
          <attvalue for="3" value="Approximate Bayesian Computation (ABC for short) is a family of computational&#10;techniques which offer an almost automated solution in situations where&#10;evaluation of the posterior likelihood is computationally prohibitive, or&#10;whenever suitable likelihoods are not available. In the present paper, we&#10;analyze the procedure from the point of view of k-nearest neighbor theory and&#10;explore the statistical properties of its outputs. We discuss in particular&#10;some asymptotic features of the genuine conditional density estimate associated&#10;with ABC, which is an interesting hybrid between a k-nearest neighbor and a&#10;kernel method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="186" source="Arnaud Guyader" target="Nicolas Jégou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="187" source="Arnaud Guyader" target="Alexander B. Németh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="188" source="Arnaud Guyader" target="Sándor Z. Németh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="189" source="Pengsheng Ji" target="Michael Nussbaum">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.8162v1" />
          <attvalue for="2" value="Sharp adaptive nonparametric testing for Sobolev ellipsoids" />
          <attvalue for="3" value="We consider testing for presence of a signal in Gaussian white noise with&#10;intensity 1/sqrt(n), when the alternatives are given by smoothness ellipsoids&#10;with an L2-ball of (squared) radius rho removed. It is known that, for a fixed&#10;Sobolev type ellipsoid of smoothness beta and size M, a rho which is of order n&#10;to the power -4 beta/(4 beta+1)} is the critical separation rate, in the sense&#10;that the minimax error of second kind over alpha-tests stays asymptotically&#10;between 0 and 1 strictly (Ingster, 1982). In addition, Ermakov (1990) found the&#10;sharp asymptotics of the minimax error of second kind at the separation rate.&#10;For adaptation over both beta and M in that context, it is known that a&#10;loglog-penalty over the separation rate for rho is necessary for a nonzero&#10;asymptotic power. Here, following an example in nonparametric estimation&#10;related to the Pinsker constant, we investigate the adaptation problem over the&#10;ellipsoid size M only, for fixed smoothness degree beta. It is established that&#10;the sharp risk asymptotics can be replicated in that adaptive setting, if rho&#10;tends to zero slower than the separation rate. The penalty for adaptation here&#10;turns out to be a sequence tending to infinity arbitrarily slowly." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="190" source="Hervé Cardot" target="Peggy Cénac">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3213v1" />
          <attvalue for="2" value="Recursive estimation of the conditional geometric median in Hilbert&#10;  spaces" />
          <attvalue for="3" value="A recursive estimator of the conditional geometric median in Hilbert spaces&#10;is studied. It is based on a stochastic gradient algorithm whose aim is to&#10;minimize a weighted L1 criterion and is consequently well adapted for robust&#10;online estimation. The weights are controlled by a kernel function and an&#10;associated bandwidth. Almost sure convergence and L2 rates of convergence are&#10;proved under general conditions on the conditional distribution as well as the&#10;sequence of descent steps of the algorithm and the sequence of bandwidths.&#10;Asymptotic normality is also proved for the averaged version of the algorithm&#10;with an optimal rate of convergence. A simulation study confirms the interest&#10;of this new and fast algorithm when the sample sizes are large. Finally, the&#10;ability of these recursive algorithms to deal with very high-dimensional data&#10;is illustrated on the robust estimation of television audience profiles&#10;conditional on the total time spent watching television over a period of 24&#10;hours." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="191" source="Hervé Cardot" target="Pierre-André Zitt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3213v1" />
          <attvalue for="2" value="Recursive estimation of the conditional geometric median in Hilbert&#10;  spaces" />
          <attvalue for="3" value="A recursive estimator of the conditional geometric median in Hilbert spaces&#10;is studied. It is based on a stochastic gradient algorithm whose aim is to&#10;minimize a weighted L1 criterion and is consequently well adapted for robust&#10;online estimation. The weights are controlled by a kernel function and an&#10;associated bandwidth. Almost sure convergence and L2 rates of convergence are&#10;proved under general conditions on the conditional distribution as well as the&#10;sequence of descent steps of the algorithm and the sequence of bandwidths.&#10;Asymptotic normality is also proved for the averaged version of the algorithm&#10;with an optimal rate of convergence. A simulation study confirms the interest&#10;of this new and fast algorithm when the sample sizes are large. Finally, the&#10;ability of these recursive algorithms to deal with very high-dimensional data&#10;is illustrated on the robust estimation of television audience profiles&#10;conditional on the total time spent watching television over a period of 24&#10;hours." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="192" source="Hervé Cardot" target="Camelia Goga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.6382v3" />
          <attvalue for="2" value="Uniform convergence and asymptotic confidence bands for model-assisted&#10;  estimators of the mean of sampled functional data" />
          <attvalue for="3" value="When the study variable is functional and storage capacities are limited or&#10;transmission costs are high, selecting with survey sampling techniques a small&#10;fraction of the observations is an interesting alternative to signal&#10;compression techniques, particularly when the goal is the estimation of simple&#10;quantities such as means or totals. We extend, in this functional framework,&#10;model-assisted estimators with linear regression models that can take account&#10;of auxiliary variables whose totals over the population are known. We first&#10;show, under weak hypotheses on the sampling design and the regularity of the&#10;trajectories, that the estimator of the mean function as well as its variance&#10;estimator are uniformly consistent. Then, under additional assumptions, we&#10;prove a functional central limit theorem and we assess rigorously a fast&#10;technique based on simulations of Gaussian processes which is employed to build&#10;asymptotic confidence bands. The accuracy of the variance function estimator is&#10;evaluated on a real dataset of sampled electricity consumption curves measured&#10;every half an hour over a period of one week." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="193" source="Hervé Cardot" target="Pauline Lardin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.6382v3" />
          <attvalue for="2" value="Uniform convergence and asymptotic confidence bands for model-assisted&#10;  estimators of the mean of sampled functional data" />
          <attvalue for="3" value="When the study variable is functional and storage capacities are limited or&#10;transmission costs are high, selecting with survey sampling techniques a small&#10;fraction of the observations is an interesting alternative to signal&#10;compression techniques, particularly when the goal is the estimation of simple&#10;quantities such as means or totals. We extend, in this functional framework,&#10;model-assisted estimators with linear regression models that can take account&#10;of auxiliary variables whose totals over the population are known. We first&#10;show, under weak hypotheses on the sampling design and the regularity of the&#10;trajectories, that the estimator of the mean function as well as its variance&#10;estimator are uniformly consistent. Then, under additional assumptions, we&#10;prove a functional central limit theorem and we assess rigorously a fast&#10;technique based on simulations of Gaussian processes which is employed to build&#10;asymptotic confidence bands. The accuracy of the variance function estimator is&#10;evaluated on a real dataset of sampled electricity consumption curves measured&#10;every half an hour over a period of one week." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="194" source="Peggy Cénac" target="Pierre-André Zitt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3213v1" />
          <attvalue for="2" value="Recursive estimation of the conditional geometric median in Hilbert&#10;  spaces" />
          <attvalue for="3" value="A recursive estimator of the conditional geometric median in Hilbert spaces&#10;is studied. It is based on a stochastic gradient algorithm whose aim is to&#10;minimize a weighted L1 criterion and is consequently well adapted for robust&#10;online estimation. The weights are controlled by a kernel function and an&#10;associated bandwidth. Almost sure convergence and L2 rates of convergence are&#10;proved under general conditions on the conditional distribution as well as the&#10;sequence of descent steps of the algorithm and the sequence of bandwidths.&#10;Asymptotic normality is also proved for the averaged version of the algorithm&#10;with an optimal rate of convergence. A simulation study confirms the interest&#10;of this new and fast algorithm when the sample sizes are large. Finally, the&#10;ability of these recursive algorithms to deal with very high-dimensional data&#10;is illustrated on the robust estimation of television audience profiles&#10;conditional on the total time spent watching television over a period of 24&#10;hours." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="195" source="Liugen Xue" target="Qihua Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5960v1" />
          <attvalue for="2" value="Empirical likelihood for single-index varying-coefficient models" />
          <attvalue for="3" value="In this paper, we develop statistical inference techniques for the unknown&#10;coefficient functions and single-index parameters in single-index&#10;varying-coefficient models. We first estimate the nonparametric component via&#10;the local linear fitting, then construct an estimated empirical likelihood&#10;ratio function and hence obtain a maximum empirical likelihood estimator for&#10;the parametric component. Our estimator for parametric component is&#10;asymptotically efficient, and the estimator of nonparametric component has an&#10;optimal convergence rate. Our results provide ways to construct the confidence&#10;region for the involved unknown parameter. We also develop an adjusted&#10;empirical likelihood ratio for constructing the confidence regions of&#10;parameters of interest. A simulation study is conducted to evaluate the finite&#10;sample behaviors of the proposed methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="196" source="Mathias Drton" target="Aldo Goia">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3360v1" />
          <attvalue for="2" value="Correction on Moments of minors of Wishart matrices" />
          <attvalue for="3" value="Correction on Moments of minors of Wishart matrices by M. Drton and A. Goia&#10;(Ann. Statist. 36 (2008) 2261-2283), arXiv:math/0604488" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="197" source="Mathias Drton" target="Naftali Harris">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0242v1" />
          <attvalue for="2" value="PC algorithm for Gaussian copula graphical models" />
          <attvalue for="3" value="The PC algorithm uses conditional independence tests for model selection in&#10;graphical modeling with acyclic directed graphs. In Gaussian models, tests of&#10;conditional independence are typically based on Pearson correlations, and&#10;high-dimensional consistency results have been obtained for the PC algorithm in&#10;this setting. We prove that high-dimensional consistency carries over to the&#10;broader class of Gaussian copula or \textit{nonparanormal} models when using&#10;rank-based measures of correlation. For graphs with bounded degree, our result&#10;is as strong as prior Gaussian results. In simulations, the `Rank PC' algorithm&#10;works as well as the `Pearson PC' algorithm for normal data and considerably&#10;better for non-normal Gaussian copula data, all the while incurring a&#10;negligible increase of computation time. Simulations with contaminated data&#10;show that rank correlations can also perform better than other robust estimates&#10;considered in previous work when the underlying distribution does not belong to&#10;the nonparanormal family." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="198" source="Vladimir Koltchinskii" target="Pedro Rangel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4819v4" />
          <attvalue for="2" value="Low rank estimation of smooth kernels on graphs" />
          <attvalue for="3" value="Let (V,A) be a weighted graph with a finite vertex set V, with a symmetric&#10;matrix of nonnegative weights A and with Laplacian $\Delta$. Let $S_*:V\times&#10;V\mapsto{\mathbb{R}}$ be a symmetric kernel defined on the vertex set V.&#10;Consider n i.i.d. observations $(X_j,X_j',Y_j),j=1,\ldots,n$, where $X_j,X_j'$&#10;are independent random vertices sampled from the uniform distribution in V and&#10;$Y_j\in{\mathbb{R}}$ is a real valued response variable such that&#10;${\mathbb{E}}(Y_j|X_j,X_j')=S_*(X_j,X_j'),j=1,\ldots,n$. The goal is to&#10;estimate the kernel $S_*$ based on the data&#10;$(X_1,X_1',Y_1),\ldots,(X_n,X_n',Y_n)$ and under the assumption that $S_*$ is&#10;low rank and, at the same time, smooth on the graph (the smoothness being&#10;characterized by discrete Sobolev norms defined in terms of the graph&#10;Laplacian). We obtain several results for such problems including minimax lower&#10;bounds on the $L_2$-error and upper bounds for penalized least squares&#10;estimators both with nonconvex and with convex penalties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="199" source="Vladimir Koltchinskii" target="Ming Yuan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2998v1" />
          <attvalue for="2" value="Sparsity in multiple kernel learning" />
          <attvalue for="3" value="The problem of multiple kernel learning based on penalized empirical risk&#10;minimization is discussed. The complexity penalty is determined jointly by the&#10;empirical $L_2$ norms and the reproducing kernel Hilbert space (RKHS) norms&#10;induced by the kernels with a data-driven choice of regularization parameters.&#10;The main focus is on the case when the total number of kernels is large, but&#10;only a relatively small number of them is needed to represent the target&#10;function, so that the problem is sparse. The goal is to establish oracle&#10;inequalities for the excess risk of the resulting prediction rule showing that&#10;the method is adaptive both to the unknown design distribution and to the&#10;sparsity of the problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="200" source="Claudia Kirch" target="Dimitris N. Politis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.4732v1" />
          <attvalue for="2" value="TFT-bootstrap: Resampling time series in the frequency domain to obtain&#10;  replicates in the time domain" />
          <attvalue for="3" value="A new time series bootstrap scheme, the time frequency toggle&#10;(TFT)-bootstrap, is proposed. Its basic idea is to bootstrap the Fourier&#10;coefficients of the observed time series, and then to back-transform them to&#10;obtain a bootstrap sample in the time domain. Related previous proposals, such&#10;as the &quot;surrogate data&quot; approach, resampled only the phase of the Fourier&#10;coefficients and thus had only limited validity. By contrast, we show that the&#10;appropriate resampling of phase and magnitude, in addition to some smoothing of&#10;Fourier coefficients, yields a bootstrap scheme that mimics the correct&#10;second-order moment structure for a large class of time series processes. As a&#10;main result we obtain a functional limit theorem for the TFT-bootstrap under a&#10;variety of popular ways of frequency domain bootstrapping. Possible&#10;applications of the TFT-bootstrap naturally arise in change-point analysis and&#10;unit-root testing where statistics are frequently based on functionals of&#10;partial sums. Finally, a small simulation study explores the potential of the&#10;TFT-bootstrap for small samples showing that for the discussed tests in&#10;change-point analysis as well as unit-root testing, it yields better results&#10;than the corresponding asymptotic tests if measured by size and power." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="201" source="Dimitris N. Politis" target="Jens-Peter Kreiss">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6211v1" />
          <attvalue for="2" value="On the range of validity of the autoregressive sieve bootstrap" />
          <attvalue for="3" value="We explore the limits of the autoregressive (AR) sieve bootstrap, and show&#10;that its applicability extends well beyond the realm of linear time series as&#10;has been previously thought. In particular, for appropriate statistics, the&#10;AR-sieve bootstrap is valid for stationary processes possessing a general&#10;Wold-type autoregressive representation with respect to a white noise; in&#10;essence, this includes all stationary, purely nondeterministic processes, whose&#10;spectral density is everywhere positive. Our main theorem provides a simple and&#10;effective tool in assessing whether the AR-sieve bootstrap is asymptotically&#10;valid in any given situation. In effect, the large-sample distribution of the&#10;statistic in question must only depend on the first and second order moments of&#10;the process; prominent examples include the sample mean and the spectral&#10;density. As a counterexample, we show how the AR-sieve bootstrap is not always&#10;valid for the sample autocovariance even when the underlying process is linear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="202" source="Dimitris N. Politis" target="Efstathios Paparoditis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6211v1" />
          <attvalue for="2" value="On the range of validity of the autoregressive sieve bootstrap" />
          <attvalue for="3" value="We explore the limits of the autoregressive (AR) sieve bootstrap, and show&#10;that its applicability extends well beyond the realm of linear time series as&#10;has been previously thought. In particular, for appropriate statistics, the&#10;AR-sieve bootstrap is valid for stationary processes possessing a general&#10;Wold-type autoregressive representation with respect to a white noise; in&#10;essence, this includes all stationary, purely nondeterministic processes, whose&#10;spectral density is everywhere positive. Our main theorem provides a simple and&#10;effective tool in assessing whether the AR-sieve bootstrap is asymptotically&#10;valid in any given situation. In effect, the large-sample distribution of the&#10;statistic in question must only depend on the first and second order moments of&#10;the process; prominent examples include the sample mean and the spectral&#10;density. As a counterexample, we show how the AR-sieve bootstrap is not always&#10;valid for the sample autocovariance even when the underlying process is linear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="203" source="Dimitris N. Politis" target="Xiaofeng Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1035v1" />
          <attvalue for="2" value="Fixed-b Subsampling and Block Bootstrap: Improved Confidence Sets Based&#10;  on P-value Calibration" />
          <attvalue for="3" value="Subsampling and block-based bootstrap methods have been used in a wide range&#10;of inference problems for time series. To accommodate the dependence, these&#10;resampling methods involve a bandwidth parameter, such as subsampling window&#10;width and block size in the block-based bootstrap. In empirical work, using&#10;different bandwidth parameters could lead to different inference results, but&#10;the traditional first order asymptotic theory does not capture the choice of&#10;the bandwidth. In this article, we propose to adopt the fixed-b approach, as&#10;advocated by Kiefer and Vogelsang (2005) in the&#10;heteroscedasticity-autocorrelation robust testing context, to account for the&#10;influence of the bandwidth on the inference. Under the fixed-b asymptotic&#10;framework, we derive the asymptotic null distribution of the p-values for&#10;subsampling and the moving block bootstrap, and further propose a calibration&#10;of the traditional small-b based confidence intervals (regions, bands) and&#10;tests. Our treatment is fairly general as it includes both finite dimensional&#10;parameters and infinite dimensional parameters, such as marginal distribution&#10;function and normalized spectral distribution function. Simulation results show&#10;that the fixed-b approach is more accurate than the traditional small-b&#10;approach in terms of approximating the finite sample distribution, and that the&#10;calibrated confidence sets tend to have smaller coverage errors than the&#10;uncalibrated counterparts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="204" source="Laszlo Gyorfi" target="Peter Harremoes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1005v1" />
          <attvalue for="2" value="Some Refinements of Large Deviation Tail Probabilities" />
          <attvalue for="3" value="We study tail probabilities via some Gaussian approximations. Our results&#10;make refinements to large deviation theory. The proof builds on classical&#10;results by Bahadur and Rao. Binomial distributions and their tail probabilities&#10;are discussed in more detail." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="205" source="Laszlo Gyorfi" target="Gabor Tusnady">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1005v1" />
          <attvalue for="2" value="Some Refinements of Large Deviation Tail Probabilities" />
          <attvalue for="3" value="We study tail probabilities via some Gaussian approximations. Our results&#10;make refinements to large deviation theory. The proof builds on classical&#10;results by Bahadur and Rao. Binomial distributions and their tail probabilities&#10;are discussed in more detail." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="206" source="Peter Harremoes" target="Gabor Tusnady">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1005v1" />
          <attvalue for="2" value="Some Refinements of Large Deviation Tail Probabilities" />
          <attvalue for="3" value="We study tail probabilities via some Gaussian approximations. Our results&#10;make refinements to large deviation theory. The proof builds on classical&#10;results by Bahadur and Rao. Binomial distributions and their tail probabilities&#10;are discussed in more detail." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="207" source="José A. Díaz-García" target="Francisco J. Caro-Lopera">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4391v1" />
          <attvalue for="2" value="Asymptotic normality of the optimal solution in multiresponse surface&#10;  methodology" />
          <attvalue for="3" value="In this work is obtained an explicit form for the perturbation effect on the&#10;matrix of regression coefficients on the optimal solution in multiresponse&#10;surface methodology. Then, the sensitivity analysis of the optimal solution is&#10;studied and the critical point characterisation of the convex program,&#10;associated with the optimum of a multiresponse surface, is also analysed.&#10;Finally, the asymptotic normality of the optimal solution is derived by&#10;standard methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="208" source="Francisco J. Caro-Lopera" target="Jose A. Diaz-Garcia">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.5705v1" />
          <attvalue for="2" value="Matrix Kummer-Pearson VII Relation and its Application in Affine Shape" />
          <attvalue for="3" value="A case of the matrix Kummer relation of Herz (1955) based on the Pearson VII&#10;type matrix model is derived in this paper. As a consequence, the polynomial&#10;Pearson VII configuration density is obtained and this set the corresponding&#10;exact inference as a solvable aspect in shape theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="209" source="Sergios Agapiou" target="Andrew M. Stuart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5753v5" />
          <attvalue for="2" value="Posterior Contraction Rates for the Bayesian Approach to Linear&#10;  Ill-Posed Inverse Problems" />
          <attvalue for="3" value="We consider a Bayesian nonparametric approach to a family of linear inverse&#10;problems in a separable Hilbert space setting with Gaussian noise. We assume&#10;Gaussian priors, which are conjugate to the model, and present a method of&#10;identifying the posterior using its precision operator. Working with the&#10;unbounded precision operator enables us to use partial differential equations&#10;(PDE) methodology to obtain rates of contraction of the posterior distribution&#10;to a Dirac measure centered on the true solution. Our methods assume a&#10;relatively weak relation between the prior covariance, noise covariance and&#10;forward operator, allowing for a wide range of applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="210" source="Sergios Agapiou" target="Yuan-Xiang Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1563v3" />
          <attvalue for="2" value="Bayesian Posterior Contraction Rates for Linear Severely Ill-posed&#10;  Inverse Problems" />
          <attvalue for="3" value="We consider a class of linear ill-posed inverse problems arising from&#10;inversion of a compact operator with singular values which decay exponentially&#10;to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown&#10;function. If the observational noise is assumed to be Gaussian then this prior&#10;is conjugate to the likelihood so that the posterior distribution is also&#10;Gaussian. We study Bayesian posterior consistency in the small observational&#10;noise limit. We assume that the forward operator and the prior and noise&#10;covariance operators commute with one another. We show how, for given&#10;smoothness assumptions on the truth, the scale parameter of the prior can be&#10;adjusted to optimize the rate of posterior contraction to the truth, and we&#10;explicitly compute the logarithmic rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="211" source="Sergios Agapiou" target="Stig Larsson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5753v5" />
          <attvalue for="2" value="Posterior Contraction Rates for the Bayesian Approach to Linear&#10;  Ill-Posed Inverse Problems" />
          <attvalue for="3" value="We consider a Bayesian nonparametric approach to a family of linear inverse&#10;problems in a separable Hilbert space setting with Gaussian noise. We assume&#10;Gaussian priors, which are conjugate to the model, and present a method of&#10;identifying the posterior using its precision operator. Working with the&#10;unbounded precision operator enables us to use partial differential equations&#10;(PDE) methodology to obtain rates of contraction of the posterior distribution&#10;to a Dirac measure centered on the true solution. Our methods assume a&#10;relatively weak relation between the prior covariance, noise covariance and&#10;forward operator, allowing for a wide range of applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="212" source="Andrew M. Stuart" target="Yuan-Xiang Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1563v3" />
          <attvalue for="2" value="Bayesian Posterior Contraction Rates for Linear Severely Ill-posed&#10;  Inverse Problems" />
          <attvalue for="3" value="We consider a class of linear ill-posed inverse problems arising from&#10;inversion of a compact operator with singular values which decay exponentially&#10;to zero. We adopt a Bayesian approach, assuming a Gaussian prior on the unknown&#10;function. If the observational noise is assumed to be Gaussian then this prior&#10;is conjugate to the likelihood so that the posterior distribution is also&#10;Gaussian. We study Bayesian posterior consistency in the small observational&#10;noise limit. We assume that the forward operator and the prior and noise&#10;covariance operators commute with one another. We show how, for given&#10;smoothness assumptions on the truth, the scale parameter of the prior can be&#10;adjusted to optimize the rate of posterior contraction to the truth, and we&#10;explicitly compute the logarithmic rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="213" source="Andrew M. Stuart" target="Stig Larsson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5753v5" />
          <attvalue for="2" value="Posterior Contraction Rates for the Bayesian Approach to Linear&#10;  Ill-Posed Inverse Problems" />
          <attvalue for="3" value="We consider a Bayesian nonparametric approach to a family of linear inverse&#10;problems in a separable Hilbert space setting with Gaussian noise. We assume&#10;Gaussian priors, which are conjugate to the model, and present a method of&#10;identifying the posterior using its precision operator. Working with the&#10;unbounded precision operator enables us to use partial differential equations&#10;(PDE) methodology to obtain rates of contraction of the posterior distribution&#10;to a Dirac measure centered on the true solution. Our methods assume a&#10;relatively weak relation between the prior covariance, noise covariance and&#10;forward operator, allowing for a wide range of applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="214" source="Yao Xie" target="David Siegmund">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2386v2" />
          <attvalue for="2" value="Sequential multi-sensor change-point detection" />
          <attvalue for="3" value="We develop a mixture procedure to monitor parallel streams of data for a&#10;change-point that affects only a subset of them, without assuming a spatial&#10;structure relating the data streams to one another. Observations are assumed&#10;initially to be independent standard normal random variables. After a&#10;change-point the observations in a subset of the streams of data have nonzero&#10;mean values. The subset and the post-change means are unknown. The procedure we&#10;study uses stream specific generalized likelihood ratio statistics, which are&#10;combined to form an overall detection statistic in a mixture model that&#10;hypothesizes an assumed fraction $p_0$ of affected data streams. An analytic&#10;expression is obtained for the average run length (ARL) when there is no change&#10;and is shown by simulations to be very accurate. Similarly, an approximation&#10;for the expected detection delay (EDD) after a change-point is also obtained.&#10;Numerical examples are given to compare the suggested procedure to other&#10;procedures for unstructured problems and in one case where the problem is&#10;assumed to have a well-defined geometric structure. Finally we discuss&#10;sensitivity of the procedure to the assumed value of $p_0$ and suggest a&#10;generalization." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="215" source="Xin Gao" target="Grace Y. Yi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.4379v1" />
          <attvalue for="2" value="Simultaneous Model Selection and Estimation for Mean and Association&#10;  Structures with Clustered Binary Data" />
          <attvalue for="3" value="This paper investigates the property of the penalized estimating equations&#10;when both the mean and association structures are modelled. To select variables&#10;for the mean and association structures sequentially, we propose a hierarchical&#10;penalized generalized estimating equations (HPGEE2) approach. The first set of&#10;penalized estimating equations is solved for the selection of significant mean&#10;parameters. Conditional on the selected mean model, the second set of penalized&#10;estimating equations is solved for the selection of significant association&#10;parameters. The hierarchical approach is designed to accommodate possible model&#10;constraints relating the inclusion of covariates into the mean and the&#10;association models. This two-step penalization strategy enjoys a compelling&#10;advantage of easing computational burdens compared to solving the two sets of&#10;penalized equations simultaneously. HPGEE2 with a smoothly clipped absolute&#10;deviation (SCAD) penalty is shown to have the oracle property for the mean and&#10;association models. The asymptotic behavior of the penalized estimator under&#10;this hierarchical approach is established. An efficient two-stage penalized&#10;weighted least square algorithm is developed to implement the proposed method.&#10;The empirical performance of the proposed HPGEE2 is demonstrated through&#10;Monte-Carlo studies and the analysis of a clinical data set." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="216" source="A. Farcomeni" target="F. Pennoni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.5990v1" />
          <attvalue for="2" value="A note on the application of the Oakes' identity to obtain the observed&#10;  information matrix of hidden Markov models" />
          <attvalue for="3" value="We derive the observed information matrix of hidden Markov models by the&#10;application of the Oakes (1999)'s identity. The method only requires the first&#10;derivative of the forward-backward recursions of Baum and Welch (1970), instead&#10;of the second derivative of the forward recursion, which is required within the&#10;approach of Lystig and Hughes (2002). The method is illustrated by an example&#10;based on the analysis of a longitudinal dataset which is well known in&#10;sociology." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="217" source="Camelia Goga" target="Pauline Lardin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.6382v3" />
          <attvalue for="2" value="Uniform convergence and asymptotic confidence bands for model-assisted&#10;  estimators of the mean of sampled functional data" />
          <attvalue for="3" value="When the study variable is functional and storage capacities are limited or&#10;transmission costs are high, selecting with survey sampling techniques a small&#10;fraction of the observations is an interesting alternative to signal&#10;compression techniques, particularly when the goal is the estimation of simple&#10;quantities such as means or totals. We extend, in this functional framework,&#10;model-assisted estimators with linear regression models that can take account&#10;of auxiliary variables whose totals over the population are known. We first&#10;show, under weak hypotheses on the sampling design and the regularity of the&#10;trajectories, that the estimator of the mean function as well as its variance&#10;estimator are uniformly consistent. Then, under additional assumptions, we&#10;prove a functional central limit theorem and we assess rigorously a fast&#10;technique based on simulations of Gaussian processes which is employed to build&#10;asymptotic confidence bands. The accuracy of the variance function estimator is&#10;evaluated on a real dataset of sampled electricity consumption curves measured&#10;every half an hour over a period of one week." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="218" source="Nadia Morsli" target="Jean-François Coeurjolly">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.4402v2" />
          <attvalue for="2" value="Poisson intensity parameter estimation for stationary Gibbs point&#10;  processes of finite interaction range" />
          <attvalue for="3" value="We introduce a semi-parametric estimator of the Poisson intensity parameter&#10;of a spatial stationary Gibbs point process. Under very mild assumptions&#10;satisfied by a large class of Gibbs models, we establish its strong consistency&#10;and asymptotic normality. We also consider its finite-sample properties in a&#10;simulation study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="219" source="Christian Francq" target="Olivier Wintenberger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2060v2" />
          <attvalue for="2" value="GARCH models without positivity constraints: Exponential or Log GARCH?" />
          <attvalue for="3" value="This paper provides a probabilistic and statistical comparison of the&#10;log-GARCH and EGARCH models, which both rely on multiplicative volatility&#10;dynamics without positivity constraints. We compare the main probabilistic&#10;properties (strict stationarity, existence of moments, tails) of the EGARCH&#10;model, which are already known, with those of an asymmetric version of the&#10;log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters&#10;is shown to be strongly consistent and asymptotically normal. Similar&#10;estimation results are only available for particular EGARCH models, and under&#10;much stronger assumptions. The comparison is pursued via simulation experiments&#10;and estimation on real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="220" source="Christian Francq" target="Jean-Michel Zakoïan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2060v2" />
          <attvalue for="2" value="GARCH models without positivity constraints: Exponential or Log GARCH?" />
          <attvalue for="3" value="This paper provides a probabilistic and statistical comparison of the&#10;log-GARCH and EGARCH models, which both rely on multiplicative volatility&#10;dynamics without positivity constraints. We compare the main probabilistic&#10;properties (strict stationarity, existence of moments, tails) of the EGARCH&#10;model, which are already known, with those of an asymmetric version of the&#10;log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters&#10;is shown to be strongly consistent and asymptotically normal. Similar&#10;estimation results are only available for particular EGARCH models, and under&#10;much stronger assumptions. The comparison is pursued via simulation experiments&#10;and estimation on real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="221" source="Olivier Wintenberger" target="Jean-Michel Zakoïan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2060v2" />
          <attvalue for="2" value="GARCH models without positivity constraints: Exponential or Log GARCH?" />
          <attvalue for="3" value="This paper provides a probabilistic and statistical comparison of the&#10;log-GARCH and EGARCH models, which both rely on multiplicative volatility&#10;dynamics without positivity constraints. We compare the main probabilistic&#10;properties (strict stationarity, existence of moments, tails) of the EGARCH&#10;model, which are already known, with those of an asymmetric version of the&#10;log-GARCH. The quasi-maximum likelihood estimation of the log-GARCH parameters&#10;is shown to be strongly consistent and asymptotically normal. Similar&#10;estimation results are only available for particular EGARCH models, and under&#10;much stronger assumptions. The comparison is pursued via simulation experiments&#10;and estimation on real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="222" source="Olivier Wintenberger" target="Pierre Alquier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4283v1" />
          <attvalue for="2" value="Fast rates in learning with dependent observations" />
          <attvalue for="3" value="In this paper we tackle the problem of fast rates in time series forecasting&#10;from a statistical learning perspective. In a serie of papers (e.g. Meir 2000,&#10;Modha and Masry 1998, Alquier and Wintenberger 2012) it is shown that the main&#10;tools used in learning theory with iid observations can be extended to the&#10;prediction of time series. The main message of these papers is that, given a&#10;family of predictors, we are able to build a new predictor that predicts the&#10;series as well as the best predictor in the family, up to a remainder of order&#10;$1/\sqrt{n}$. It is known that this rate cannot be improved in general. In this&#10;paper, we show that in the particular case of the least square loss, and under&#10;a strong assumption on the time series (phi-mixing) the remainder is actually&#10;of order $1/n$. Thus, the optimal rate for iid variables, see e.g. Tsybakov&#10;2003, and individual sequences, see \cite{lugosi} is, for the first time,&#10;achieved for uniformly mixing processes. We also show that our method is&#10;optimal for aggregating sparse linear combinations of predictors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="223" source="Olivier Wintenberger" target="Xiaoyin Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1847v1" />
          <attvalue for="2" value="Prediction of time series by statistical learning: general losses and&#10;  fast rates" />
          <attvalue for="3" value="We establish rates of convergences in time series forecasting using the&#10;statistical learning approach based on oracle inequalities. A series of papers&#10;extends the oracle inequalities obtained for iid observations to time series&#10;under weak dependence conditions. Given a family of predictors and $n$&#10;observations, oracle inequalities state that a predictor forecasts the series&#10;as well as the best predictor in the family up to a remainder term $\Delta_n$.&#10;Using the PAC-Bayesian approach, we establish under weak dependence conditions&#10;oracle inequalities with optimal rates of convergence. We extend previous&#10;results for the absolute loss function to any Lipschitz loss function with&#10;rates $\Delta_n\sim\sqrt{c(\Theta)/ n}$ where $c(\Theta)$ measures the&#10;complexity of the model. We apply the method for quantile loss functions to&#10;forecast the french GDP. Under additional conditions on the loss functions&#10;(satisfied by the quadratic loss function) and on the time series, we refine&#10;the rates of convergence to $\Delta_n \sim c(\Theta)/n$. We achieve for the&#10;first time these fast rates for uniformly mixing processes. These rates are&#10;known to be optimal in the iid case and for individual sequences. In&#10;particular, we generalize the results of Dalalyan and Tsybakov on sparse&#10;regression estimation to the case of autoregression." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="224" source="Victor I. Ivanenko" target="Valery A. Labkovsky">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.4440v6" />
          <attvalue for="2" value="On regularities of mass random phenomena" />
          <attvalue for="3" value="This paper contains an answer to the question of existence of regularities of&#10;the so called \textit{random in a broad sense} mass phenomena, asked by A. N.&#10;Kolmogorov in \cite{Kolmogorov}. It turns out that some family of&#10;finitely-additive probabilities is the statistical regularity of any such&#10;phenomenon. If the mass phenomenon is stochastic, then this family degenerates&#10;into a single probability measure. The paper provides definitions, the&#10;formulation and the proof of the theorem of existence of statistical&#10;regularities, as well as the examples of their application." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="225" source="Jinzhu Jia" target="Karl Rohe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.5584v1" />
          <attvalue for="2" value="Preconditioning to comply with the Irrepresentable Condition" />
          <attvalue for="3" value="Preconditioning is a technique from numerical linear algebra that can&#10;accelerate algorithms to solve systems of equations. In this paper, we&#10;demonstrate how preconditioning can circumvent a stringent assumption for sign&#10;consistency in sparse linear regression. Given $X \in R^{n \times p}$ and $Y&#10;\in R^n$ that satisfy the standard regression equation, this paper demonstrates&#10;that even if the design matrix $X$ does not satisfy the irrepresentable&#10;condition for the Lasso, the design matrix $F X$ often does, where $F \in&#10;R^{n\times n}$ is a preconditioning matrix defined in this paper. By computing&#10;the Lasso on $(F X, F Y)$, instead of on $(X, Y)$, the necessary assumptions on&#10;$X$ become much less stringent.&#10;  Our preconditioner $F$ ensures that the singular values of the design matrix&#10;are either zero or one. When $n\ge p$, the columns of $F X$ are orthogonal and&#10;the preconditioner always circumvents the stringent assumptions. When $p\ge n$,&#10;$F$ projects the design matrix onto the Stiefel manifold; the rows of $F X$ are&#10;orthogonal. We give both theoretical results and simulation results to show&#10;that, in the high dimensional case, the preconditioner helps to circumvent the&#10;stringent assumptions, improving the statistical performance of a broad class&#10;of model selection techniques in linear regression. Simulation results are&#10;particularly promising." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="226" source="Mahendra Mariadassou" target="Catherine Matias">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.7101v3" />
          <attvalue for="2" value="Convergence of the groups posterior distribution in latent or stochastic&#10;  block models" />
          <attvalue for="3" value="We propose a unified framework for studying both latent and stochastic block&#10;models, which are used to cluster simultaneously rows and columns of a data&#10;matrix. In this new framework, we study the behaviour of the groups posterior&#10;distribution, given the data. We characterize whether it is possible to&#10;asymptotically recover the actual groups on the rows and columns of the matrix,&#10;relying on a consistent estimate of the parameter. In other words, we establish&#10;sufficient conditions for the groups posterior distribution to converge (as the&#10;size of the data increases) to a Dirac mass located at the actual (random)&#10;groups configuration. In particular, we highlight some cases where the model&#10;assumes symmetries in the matrix of connection probabilities that prevents&#10;recovering the original groups. We also discuss the validity of these results&#10;when the proportion of non-null entries in the data matrix converges to zero." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="227" source="Catherine Matias" target="Francis Comets">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="228" source="Catherine Matias" target="Mikael Falconnet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="229" source="Catherine Matias" target="Oleg Loukianov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="230" source="Catherine Matias" target="Dasha Loukianova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="231" source="Song Yu-Ping" target="Lin Zheng-Yan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1454v1" />
          <attvalue for="2" value="Local linear estimator for stochastic differential equations driven by&#10;  $α$-stable Lévy motions" />
          <attvalue for="3" value="We study the local linear estimator for the drift coefficient of stochastic&#10;differential equations driven by $\alpha$-stable L\'{e}vy motions observed at&#10;discrete instants letting $T \rightarrow \infty$. Under regular conditions, we&#10;derive the weak consistency and central limit theorem of the estimator. Compare&#10;with Nadaraya-Watson estimator, the local linear estimator has a bias reduction&#10;whether kernel function is symmetric or not under different schemes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="232" source="Marc Hoffmann" target="Richard Nickl">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5145v1" />
          <attvalue for="2" value="On adaptive inference and confidence bands" />
          <attvalue for="3" value="The problem of existence of adaptive confidence bands for an unknown density&#10;$f$ that belongs to a nested scale of H\&quot;{o}lder classes over $\mathbb{R}$ or&#10;$[0,1]$ is considered. Whereas honest adaptive inference in this problem is&#10;impossible already for a pair of H\&quot;{o}lder balls $\Sigma(r),\Sigma(s),r\ne s$,&#10;of fixed radius, a nonparametric distinguishability condition is introduced&#10;under which adaptive confidence bands can be shown to exist. It is further&#10;shown that this condition is necessary and sufficient for the existence of&#10;honest asymptotic confidence bands, and that it is strictly weaker than similar&#10;analytic conditions recently employed in Gin\'{e} and Nickl [Ann. Statist. 38&#10;(2010) 1122--1170]. The exceptional sets for which honest inference is not&#10;possible have vanishingly small probability under natural priors on H\&quot;{o}lder&#10;balls $\Sigma(s)$. If no upper bound for the radius of the H\&quot;{o}lder balls is&#10;known, a price for adaptation has to be paid, and near-optimal adaptation is&#10;possible for standard procedures. The implications of these findings for a&#10;general theory of adaptive inference are discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="233" source="Richard Nickl" target="Ismaël Castillo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3862v4" />
          <attvalue for="2" value="Nonparametric Bernstein-von Mises theorems in Gaussian white noise" />
          <attvalue for="3" value="Bernstein-von Mises theorems for nonparametric Bayes priors in the Gaussian&#10;white noise model are proved. It is demonstrated how such results justify Bayes&#10;methods as efficient frequentist inference procedures in a variety of concrete&#10;nonparametric problems. Particularly Bayesian credible sets are constructed&#10;that have asymptotically exact $1-\alpha$ frequentist coverage level and whose&#10;$L^2$-diameter shrinks at the minimax rate of convergence (within logarithmic&#10;factors) over H\&quot;{o}lder balls. Other applications include general classes of&#10;linear and nonlinear functionals and credible bands for auto-convolutions. The&#10;assumptions cover nonconjugate product priors defined on general orthonormal&#10;bases of $L^2$ satisfying weak conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="234" source="Richard Nickl" target="Evarist Giné">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2043v1" />
          <attvalue for="2" value="Rates of contraction for posterior distributions in&#10;  $\bolds{L^r}$-metrics, $\bolds{1\le r\le\infty}$" />
          <attvalue for="3" value="The frequentist behavior of nonparametric Bayes estimates, more specifically,&#10;rates of contraction of the posterior distributions to shrinking $L^r$-norm&#10;neighborhoods, $1\le r\le\infty$, of the unknown parameter, are studied. A&#10;theorem for nonparametric density estimation is proved under general&#10;approximation-theoretic assumptions on the prior. The result is applied to a&#10;variety of common examples, including Gaussian process, wavelet series, normal&#10;mixture and histogram priors. The rates of contraction are minimax-optimal for&#10;$1\le r\le2$, but deteriorate as $r$ increases beyond 2. In the case of&#10;Gaussian nonparametric regression a Gaussian prior is devised for which the&#10;posterior contracts at the optimal rate in all $L^r$-norms, $1\le r\le\infty$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="235" source="Bing Li" target="Andreas Artemiou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2790v1" />
          <attvalue for="2" value="Principal support vector machines for linear and nonlinear sufficient&#10;  dimension reduction" />
          <attvalue for="3" value="We introduce a principal support vector machine (PSVM) approach that can be&#10;used for both linear and nonlinear sufficient dimension reduction. The basic&#10;idea is to divide the response variables into slices and use a modified form of&#10;support vector machine to find the optimal hyperplanes that separate them.&#10;These optimal hyperplanes are then aligned by the principal components of their&#10;normal vectors. It is proved that the aligned normal vectors provide an&#10;unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the&#10;sufficient dimension reduction space. The method is then generalized to&#10;nonlinear sufficient dimension reduction using the reproducing kernel Hilbert&#10;space. In that context, the aligned normal vectors become functions and it is&#10;proved that they are unbiased in the sense that they are functions of the true&#10;nonlinear sufficient predictors. We compare PSVM with other sufficient&#10;dimension reduction methods by simulation and in real data analysis, and&#10;through both comparisons firmly establish its practical advantages." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="236" source="Bing Li" target="Lexin Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2790v1" />
          <attvalue for="2" value="Principal support vector machines for linear and nonlinear sufficient&#10;  dimension reduction" />
          <attvalue for="3" value="We introduce a principal support vector machine (PSVM) approach that can be&#10;used for both linear and nonlinear sufficient dimension reduction. The basic&#10;idea is to divide the response variables into slices and use a modified form of&#10;support vector machine to find the optimal hyperplanes that separate them.&#10;These optimal hyperplanes are then aligned by the principal components of their&#10;normal vectors. It is proved that the aligned normal vectors provide an&#10;unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the&#10;sufficient dimension reduction space. The method is then generalized to&#10;nonlinear sufficient dimension reduction using the reproducing kernel Hilbert&#10;space. In that context, the aligned normal vectors become functions and it is&#10;proved that they are unbiased in the sense that they are functions of the true&#10;nonlinear sufficient predictors. We compare PSVM with other sufficient&#10;dimension reduction methods by simulation and in real data analysis, and&#10;through both comparisons firmly establish its practical advantages." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="237" source="Bing Li" target="Xiangrong Yin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3313v1" />
          <attvalue for="2" value="Sufficient dimension reduction based on an ensemble of minimum average&#10;  variance estimators" />
          <attvalue for="3" value="We introduce a class of dimension reduction estimators based on an ensemble&#10;of the minimum average variance estimates of functions that characterize the&#10;central subspace, such as the characteristic functions, the Box--Cox&#10;transformations and wavelet basis. The ensemble estimators exhaustively&#10;estimate the central subspace without imposing restrictive conditions on the&#10;predictors, and have the same convergence rate as the minimum average variance&#10;estimates. They are flexible and easy to implement, and allow repeated use of&#10;the available sample, which enhances accuracy. They are applicable to both&#10;univariate and multivariate responses in a unified form. We establish the&#10;consistency and convergence rate of these estimators, and the consistency of a&#10;cross validation criterion for order determination. We compare the ensemble&#10;estimators with other estimators in a wide variety of models, and establish&#10;their competent performance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="238" source="Andreas Artemiou" target="Lexin Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2790v1" />
          <attvalue for="2" value="Principal support vector machines for linear and nonlinear sufficient&#10;  dimension reduction" />
          <attvalue for="3" value="We introduce a principal support vector machine (PSVM) approach that can be&#10;used for both linear and nonlinear sufficient dimension reduction. The basic&#10;idea is to divide the response variables into slices and use a modified form of&#10;support vector machine to find the optimal hyperplanes that separate them.&#10;These optimal hyperplanes are then aligned by the principal components of their&#10;normal vectors. It is proved that the aligned normal vectors provide an&#10;unbiased, $\sqrt{n}$-consistent, and asymptotically normal estimator of the&#10;sufficient dimension reduction space. The method is then generalized to&#10;nonlinear sufficient dimension reduction using the reproducing kernel Hilbert&#10;space. In that context, the aligned normal vectors become functions and it is&#10;proved that they are unbiased in the sense that they are functions of the true&#10;nonlinear sufficient predictors. We compare PSVM with other sufficient&#10;dimension reduction methods by simulation and in real data analysis, and&#10;through both comparisons firmly establish its practical advantages." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="239" source="Romain Azaïs" target="François Dufour">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2211v2" />
          <attvalue for="2" value="Nonparametric estimation of the jump rate for non-homogeneous marked&#10;  renewal processes" />
          <attvalue for="3" value="This paper is devoted to the nonparametric estimation of the jump rate and&#10;the cumulative rate for a general class of non-homogeneous marked renewal&#10;processes, defined on a separable metric space. In our framework, the&#10;estimation needs only one observation of the process within a long time. Our&#10;approach is based on a generalization of the multiplicative intensity model,&#10;introduced by Aalen in the seventies. We provide consistent estimators of these&#10;two functions, under some assumptions related to the ergodicity of an embedded&#10;chain and the characteristics of the process. The paper is illustrated by a&#10;numerical example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="240" source="Romain Azaïs" target="Anne Gégout-Petit">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2211v2" />
          <attvalue for="2" value="Nonparametric estimation of the jump rate for non-homogeneous marked&#10;  renewal processes" />
          <attvalue for="3" value="This paper is devoted to the nonparametric estimation of the jump rate and&#10;the cumulative rate for a general class of non-homogeneous marked renewal&#10;processes, defined on a separable metric space. In our framework, the&#10;estimation needs only one observation of the process within a long time. Our&#10;approach is based on a generalization of the multiplicative intensity model,&#10;introduced by Aalen in the seventies. We provide consistent estimators of these&#10;two functions, under some assumptions related to the ergodicity of an embedded&#10;chain and the characteristics of the process. The paper is illustrated by a&#10;numerical example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="241" source="François Dufour" target="Anne Gégout-Petit">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2211v2" />
          <attvalue for="2" value="Nonparametric estimation of the jump rate for non-homogeneous marked&#10;  renewal processes" />
          <attvalue for="3" value="This paper is devoted to the nonparametric estimation of the jump rate and&#10;the cumulative rate for a general class of non-homogeneous marked renewal&#10;processes, defined on a separable metric space. In our framework, the&#10;estimation needs only one observation of the process within a long time. Our&#10;approach is based on a generalization of the multiplicative intensity model,&#10;introduced by Aalen in the seventies. We provide consistent estimators of these&#10;two functions, under some assumptions related to the ergodicity of an embedded&#10;chain and the characteristics of the process. The paper is illustrated by a&#10;numerical example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="242" source="Dan Shen" target="Haipeng Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2671v6" />
          <attvalue for="2" value="A General Framework For Consistency of Principal Component Analysis" />
          <attvalue for="3" value="A general asymptotic framework is developed for studying consis- tency&#10;properties of principal component analysis (PCA). Our frame- work includes&#10;several previously studied domains of asymptotics as special cases and allows&#10;one to investigate interesting connections and transitions among the various&#10;domains. More importantly, it enables us to investigate asymptotic scenarios&#10;that have not been considered before, and gain new insights into the&#10;consistency, subspace consistency and strong inconsistency regions of PCA and&#10;the boundaries among them. We also establish the corresponding convergence rate&#10;within each region. Under general spike covariance models, the dimension (or&#10;the number of variables) discourages the consistency of PCA, while the sample&#10;size and spike information (the relative size of the population eigenvalues)&#10;encourages PCA consistency. Our framework nicely illustrates the relationship&#10;among these three types of information in terms of dimension, sample size and&#10;spike size, and rigorously characterizes how their relationships affect PCA&#10;consistency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="243" source="Dan Shen" target="J. S. Marron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2671v6" />
          <attvalue for="2" value="A General Framework For Consistency of Principal Component Analysis" />
          <attvalue for="3" value="A general asymptotic framework is developed for studying consis- tency&#10;properties of principal component analysis (PCA). Our frame- work includes&#10;several previously studied domains of asymptotics as special cases and allows&#10;one to investigate interesting connections and transitions among the various&#10;domains. More importantly, it enables us to investigate asymptotic scenarios&#10;that have not been considered before, and gain new insights into the&#10;consistency, subspace consistency and strong inconsistency regions of PCA and&#10;the boundaries among them. We also establish the corresponding convergence rate&#10;within each region. Under general spike covariance models, the dimension (or&#10;the number of variables) discourages the consistency of PCA, while the sample&#10;size and spike information (the relative size of the population eigenvalues)&#10;encourages PCA consistency. Our framework nicely illustrates the relationship&#10;among these three types of information in terms of dimension, sample size and&#10;spike size, and rigorously characterizes how their relationships affect PCA&#10;consistency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="244" source="Haipeng Shen" target="J. S. Marron">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2671v6" />
          <attvalue for="2" value="A General Framework For Consistency of Principal Component Analysis" />
          <attvalue for="3" value="A general asymptotic framework is developed for studying consis- tency&#10;properties of principal component analysis (PCA). Our frame- work includes&#10;several previously studied domains of asymptotics as special cases and allows&#10;one to investigate interesting connections and transitions among the various&#10;domains. More importantly, it enables us to investigate asymptotic scenarios&#10;that have not been considered before, and gain new insights into the&#10;consistency, subspace consistency and strong inconsistency regions of PCA and&#10;the boundaries among them. We also establish the corresponding convergence rate&#10;within each region. Under general spike covariance models, the dimension (or&#10;the number of variables) discourages the consistency of PCA, while the sample&#10;size and spike information (the relative size of the population eigenvalues)&#10;encourages PCA consistency. Our framework nicely illustrates the relationship&#10;among these three types of information in terms of dimension, sample size and&#10;spike size, and rigorously characterizes how their relationships affect PCA&#10;consistency." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="245" source="Weining Shen" target="Subhashis Ghosal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.4238v2" />
          <attvalue for="2" value="MCMC-free adaptive Bayesian procedures using random series prior" />
          <attvalue for="3" value="We consider priors for several nonparametric Bayesian models which use finite&#10;random series with a random number of terms. The prior is constructed through&#10;distributions on the number of basis functions and the associated coefficients.&#10;We derive a general result on the construction of an appropriate sieve and&#10;obtain adaptive posterior contraction rates for all smoothness levels of the&#10;function in the true model. We apply this general result on several statistical&#10;problems such as signal processing, density estimation, nonparametric additive&#10;regression, classification, spectral density estimation, functional regression&#10;etc. The prior can be viewed as an alternative to commonly used Gaussian&#10;process prior, but can be analyzed by relatively simpler techniques and in many&#10;cases allows a simpler approach to computation without using Markov chain&#10;Monte-Carlo (MCMC) methods. A simulation study was conducted to show that the&#10;performance of the random series prior is comparable to that of a Gaussian&#10;process prior." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="246" source="Jean-Baptiste Durand" target="Y. Guédon">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.6545v1" />
          <attvalue for="2" value="Localizing the Latent Structure Canonical Uncertainty: Entropy Profiles&#10;  for Hidden Markov Models" />
          <attvalue for="3" value="This report addresses state inference for hidden Markov models. These models&#10;rely on unobserved states, which often have a meaningful interpretation. This&#10;makes it necessary to develop diagnostic tools for quantification of state&#10;uncertainty. The entropy of the state sequence that explains an observed&#10;sequence for a given hidden Markov chain model can be considered as the&#10;canonical measure of state sequence uncertainty. This canonical measure of&#10;state sequence uncertainty is not reflected by the classic multivariate state&#10;profiles computed by the smoothing algorithm, which summarizes the possible&#10;state sequences. Here, we introduce a new type of profiles which have the&#10;following properties: (i) these profiles of conditional entropies are a&#10;decomposition of the canonical measure of state sequence uncertainty along the&#10;sequence and makes it possible to localize this uncertainty, (ii) these&#10;profiles are univariate and thus remain easily interpretable on tree&#10;structures. We show how to extend the smoothing algorithms for hidden Markov&#10;chain and tree models to compute these entropy profiles efficiently." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="247" source="Fatemeh Azizzadeh" target="Saeid Rezakhah">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0097v1" />
          <attvalue for="2" value="The CUSUM test for detecting structural changes in strong mixing&#10;  processes" />
          <attvalue for="3" value="Strong mixing property holds for a broad class of linear and nonlinear time&#10;series models such as ARMA and GARCH models. In this article we study&#10;correlation structure of strong mixing sequences, and some asymptotic&#10;properties are presented. We also present a new method for detecting change&#10;point in correlation structure of strong mixing sequences, and present a&#10;nonparametric CUSUM test statistic for this. Asymptotic consistency of this&#10;test statistics is shown. This method is applied to simulated data of some&#10;linear and nonlinear models and power of the test is evaluated. For linear&#10;models, it is shown that this method have a better performance in compare to&#10;Berkes et al.(2009)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="248" source="Saeid Rezakhah" target="Akram Kohansal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0094v1" />
          <attvalue for="2" value="Parameter Estimation of Type-II Hybrid Censored Weighted Exponential&#10;  Distribution" />
          <attvalue for="3" value="A hybrid censoring scheme is a mixture of Type-I and Type-II censoring&#10;schemes. We study the estimation of parameters of weighted exponential&#10;distribution based on Type-II hybrid censored data. By applying EM algorithm,&#10;maximum likelihood estimators are evaluated. Also using Fisher infirmation&#10;matrix asymptotic confidence intervals are provided. By applying Markov Chain&#10;Monte Carlo techniques Bayes estimators, and corresponding highest posterior&#10;density confidence intervals of parameters are obtained. Monte Carlo&#10;simulations to compare the performances of the different methods is performed&#10;and one data set is analyzed for illustrative purposes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="249" source="Saeid Rezakhah" target="Navideh Modarresi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2450v1" />
          <attvalue for="2" value="Estimation of Scale and Hurst Parameters of Semi-Selfsimilar Processes" />
          <attvalue for="3" value="The characteristic feature of semi-selfsimilar process is the invariance of&#10;its finite dimensional distributions by certain dilation for specific scaling&#10;factor. Estimating the scale parameter $\lambda$ and the Hurst index of such&#10;processes is one of the fundamental problem in the literature. We present some&#10;iterative method for estimation of the scale and Hurst parameters which is&#10;addressed for semi-selfsimilar processes with stationary increments. This&#10;method is based on some flexible sampling scheme and evaluating sample variance&#10;of increments in each scale intervals $[\lambda^{n-1}, \lambda^n)$, $n\in&#10;\mathbb{N}$. For such iterative method we find the initial estimation for the&#10;scale parameter by evaluating cumulative sum of moving sample variances and&#10;also by evaluating sample variance of preceding and succeeding moving sample&#10;variance of increments. We also present a new efficient method for estimation&#10;of Hurst parameter of selfsimilar processes. As an example we introduce simple&#10;fractional Brownian motion (sfBm) which is semi-selfsimilar with stationary&#10;increments. We present some simulations and numerical evaluation to illustrate&#10;the results and to estimate the scale for sfBm as a semi-selfsimilar process.&#10;We also present another simulation and show the efficiency of our method in&#10;estimation of Hurst parameter by comparing its performance with some previous&#10;methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="250" source="Frédéric Lavancier" target="Jesper Møller">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4818v5" />
          <attvalue for="2" value="Determinantal point process models and statistical inference : Extended&#10;  version" />
          <attvalue for="3" value="Statistical models and methods for determinantal point processes (DPPs) seem&#10;largely unexplored. We demonstrate that DPPs provide useful models for the&#10;description of spatial point pattern datasets where nearby points repel each&#10;other. Such data are usually modelled by Gibbs point processes, where the&#10;likelihood and moment expressions are intractable and simulations are time&#10;consuming. We exploit the appealing probabilistic properties of DPPs to develop&#10;parametric models, where the likelihood and moment expressions can be easily&#10;evaluated and realizations can be quickly simulated. We discuss how statistical&#10;inference is conducted using the likelihood or moment properties of DPP models,&#10;and we provide freely available software for simulation and statistical&#10;inference." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="251" source="Frédéric Lavancier" target="Ege Rubak">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4818v5" />
          <attvalue for="2" value="Determinantal point process models and statistical inference : Extended&#10;  version" />
          <attvalue for="3" value="Statistical models and methods for determinantal point processes (DPPs) seem&#10;largely unexplored. We demonstrate that DPPs provide useful models for the&#10;description of spatial point pattern datasets where nearby points repel each&#10;other. Such data are usually modelled by Gibbs point processes, where the&#10;likelihood and moment expressions are intractable and simulations are time&#10;consuming. We exploit the appealing probabilistic properties of DPPs to develop&#10;parametric models, where the likelihood and moment expressions can be easily&#10;evaluated and realizations can be quickly simulated. We discuss how statistical&#10;inference is conducted using the likelihood or moment properties of DPP models,&#10;and we provide freely available software for simulation and statistical&#10;inference." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="252" source="Frédéric Lavancier" target="David Dereudre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5998v3" />
          <attvalue for="2" value="Estimation of the intensity parameter of the germ-grain&#10;  Quermass-interaction model when the number of germs is not observed" />
          <attvalue for="3" value="The Quermass-interaction model allows to generalise the classical germ-grain&#10;Boolean model in adding a morphological interaction between the grains. It&#10;enables to model random structures with specific morphologies which are&#10;unlikely to be generated from a Boolean model. The Quermass-interaction model&#10;depends in particular on an intensity parameter, which is impossible to&#10;estimate from classical likelihood or pseudo-likelihood approaches because the&#10;number of points is not observable from a germ-grain set. In this paper, we&#10;present a procedure based on the Takacs-Fiksel method which is able to estimate&#10;all parameters of the Quermass-interaction model, including the intensity. An&#10;intensive simulation study is conducted to assess the efficiency of the&#10;procedure and to provide practical recommendations. It also illustrates that&#10;the estimation of the intensity parameter is crucial in order to identify the&#10;model. The Quermass-interaction model is finally fitted by our method to P.&#10;Diggle's heather dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="253" source="Frédéric Lavancier" target="Katerina Helisova Stankova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5998v3" />
          <attvalue for="2" value="Estimation of the intensity parameter of the germ-grain&#10;  Quermass-interaction model when the number of germs is not observed" />
          <attvalue for="3" value="The Quermass-interaction model allows to generalise the classical germ-grain&#10;Boolean model in adding a morphological interaction between the grains. It&#10;enables to model random structures with specific morphologies which are&#10;unlikely to be generated from a Boolean model. The Quermass-interaction model&#10;depends in particular on an intensity parameter, which is impossible to&#10;estimate from classical likelihood or pseudo-likelihood approaches because the&#10;number of points is not observable from a germ-grain set. In this paper, we&#10;present a procedure based on the Takacs-Fiksel method which is able to estimate&#10;all parameters of the Quermass-interaction model, including the intensity. An&#10;intensive simulation study is conducted to assess the efficiency of the&#10;procedure and to provide practical recommendations. It also illustrates that&#10;the estimation of the intensity parameter is crucial in order to identify the&#10;model. The Quermass-interaction model is finally fitted by our method to P.&#10;Diggle's heather dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="254" source="Jesper Møller" target="Ege Rubak">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4818v5" />
          <attvalue for="2" value="Determinantal point process models and statistical inference : Extended&#10;  version" />
          <attvalue for="3" value="Statistical models and methods for determinantal point processes (DPPs) seem&#10;largely unexplored. We demonstrate that DPPs provide useful models for the&#10;description of spatial point pattern datasets where nearby points repel each&#10;other. Such data are usually modelled by Gibbs point processes, where the&#10;likelihood and moment expressions are intractable and simulations are time&#10;consuming. We exploit the appealing probabilistic properties of DPPs to develop&#10;parametric models, where the likelihood and moment expressions can be easily&#10;evaluated and realizations can be quickly simulated. We discuss how statistical&#10;inference is conducted using the likelihood or moment properties of DPP models,&#10;and we provide freely available software for simulation and statistical&#10;inference." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="255" source="Jose A. Diaz-Garcia" target="Ramón Gutierrez-Sanchez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.1993v1" />
          <attvalue for="2" value="Jacobians of singular matrix transformations: Extensions" />
          <attvalue for="3" value="This article presents a unified approach to simultaneously compute the&#10;Jacobians of several singular matrix transformations in the real, complex,&#10;quaternion and octonion cases. Formally, these Jacobians are obtained for real&#10;normed division algebras with respect to the Hausdorff measure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="256" source="Jose A. Diaz-Garcia" target="Jose E. Rodriguez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5587v1" />
          <attvalue for="2" value="Response surface methodology: Asymptotic normality of the optimal&#10;  solution" />
          <attvalue for="3" value="Sensitivity analysis of the optimal solution in response surface methodology&#10;is studied and an explicit form of the effect of perturbation of the regression&#10;coefficients on the optimal solution is obtained. The characterisation of the&#10;critical point of the convex program corresponding to the optimum of a response&#10;surface model is also studied. The asymptotic normality of the optimal solution&#10;follows by standard methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="257" source="Jose A. Diaz-Garcia" target="Rogelio Ramos-Quiroga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5587v1" />
          <attvalue for="2" value="Response surface methodology: Asymptotic normality of the optimal&#10;  solution" />
          <attvalue for="3" value="Sensitivity analysis of the optimal solution in response surface methodology&#10;is studied and an explicit form of the effect of perturbation of the regression&#10;coefficients on the optimal solution is obtained. The characterisation of the&#10;critical point of the convex program corresponding to the optimum of a response&#10;surface model is also studied. The asymptotic normality of the optimal solution&#10;follows by standard methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="258" source="Aharon Birnbaum" target="Iain M. Johnstone">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="259" source="Aharon Birnbaum" target="Boaz Nadler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="260" source="Aharon Birnbaum" target="Debashis Paul">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="261" source="Iain M. Johnstone" target="Boaz Nadler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="262" source="Iain M. Johnstone" target="Debashis Paul">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="263" source="Iain M. Johnstone" target="Gourab Mukherjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2071v4" />
          <attvalue for="2" value="Exact minimax estimation of the predictive density in sparse Gaussian&#10;  models" />
          <attvalue for="3" value="We consider estimating the predictive density under Kullback-Leibler loss in&#10;an $\ell_0$ sparse Gaussian sequence model. Explicit expressions of the first&#10;order minimax risk along with its exact constant, asymptotically least&#10;favorable priors and optimal predictive density estimates are derived. Compared&#10;to the sparse recovery results involving point estimation of the normal mean,&#10;new decision theoretic phenomena are seen. Suboptimal performance of the class&#10;of plug-in density estimates reflects the predictive nature of the problem and&#10;optimal strategies need diversification of the future risk. We find that&#10;minimax optimal strategies lie outside the Gaussian family but can be&#10;constructed with threshold predictive density estimates. Novel minimax&#10;techniques involving simultaneous calibration of the sparsity adjustment and&#10;the risk diversification mechanisms are used to design optimal predictive&#10;density estimates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="264" source="Boaz Nadler" target="Debashis Paul">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0967v1" />
          <attvalue for="2" value="Minimax bounds for sparse PCA with noisy high-dimensional data" />
          <attvalue for="3" value="We study the problem of estimating the leading eigenvectors of a&#10;high-dimensional population covariance matrix based on independent Gaussian&#10;observations. We establish a lower bound on the minimax risk of estimators&#10;under the $l_2$ loss, in the joint limit as dimension and sample size increase&#10;to infinity, under various models of sparsity for the population eigenvectors.&#10;The lower bound on the risk points to the existence of different regimes of&#10;sparsity of the eigenvectors. We also propose a new method for estimating the&#10;eigenvectors by a two-stage coordinate selection scheme." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="265" source="Dong Li" target="Shiqing Ling">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2948v1" />
          <attvalue for="2" value="On moving-average models with feedback" />
          <attvalue for="3" value="Moving average models, linear or nonlinear, are characterized by their short&#10;memory. This paper shows that, in the presence of feedback in the dynamics, the&#10;above characteristic can disappear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="266" source="Dong Li" target="Howell Tong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2948v1" />
          <attvalue for="2" value="On moving-average models with feedback" />
          <attvalue for="3" value="Moving average models, linear or nonlinear, are characterized by their short&#10;memory. This paper shows that, in the presence of feedback in the dynamics, the&#10;above characteristic can disappear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="267" source="Shiqing Ling" target="Howell Tong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2948v1" />
          <attvalue for="2" value="On moving-average models with feedback" />
          <attvalue for="3" value="Moving average models, linear or nonlinear, are characterized by their short&#10;memory. This paper shows that, in the presence of feedback in the dynamics, the&#10;above characteristic can disappear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="268" source="Shiqing Ling" target="Ke Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6216v1" />
          <attvalue for="2" value="Global self-weighted and local quasi-maximum exponential likelihood&#10;  estimators for ARMA--GARCH/IGARCH models" />
          <attvalue for="3" value="This paper investigates the asymptotic theory of the quasi-maximum&#10;exponential likelihood estimators (QMELE) for ARMA--GARCH models. Under only a&#10;fractional moment condition, the strong consistency and the asymptotic&#10;normality of the global self-weighted QMELE are obtained. Based on this&#10;self-weighted QMELE, the local QMELE is showed to be asymptotically normal for&#10;the ARMA model with GARCH (finite variance) and IGARCH errors. A formal&#10;comparison of two estimators is given for some cases. A simulation study is&#10;carried out to assess the performance of these estimators, and a real example&#10;on the world crude oil price is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="269" source="Runze Li" target="Hua Liang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="270" source="Runze Li" target="Xiang Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="271" source="Runze Li" target="Chih-Ling Tsai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="272" source="Juan-Juan Cai" target="John H. J. Einmahl">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5239v1" />
          <attvalue for="2" value="Estimation of extreme risk regions under multivariate regular variation" />
          <attvalue for="3" value="When considering d possibly dependent random variables, one is often&#10;interested in extreme risk regions, with very small probability p. We consider&#10;risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,&#10;where f is the joint density and $\beta$ a small number. Estimation of such an&#10;extreme risk region is difficult since it contains hardly any or no data. Using&#10;extreme value theory, we construct a natural estimator of an extreme risk&#10;region and prove a refined form of consistency, given a random sample of&#10;multivariate regularly varying random vectors. In a detailed simulation and&#10;comparison study, the good performance of the procedure is demonstrated. We&#10;also apply our estimator to financial data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="273" source="Juan-Juan Cai" target="Laurens de Haan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5239v1" />
          <attvalue for="2" value="Estimation of extreme risk regions under multivariate regular variation" />
          <attvalue for="3" value="When considering d possibly dependent random variables, one is often&#10;interested in extreme risk regions, with very small probability p. We consider&#10;risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,&#10;where f is the joint density and $\beta$ a small number. Estimation of such an&#10;extreme risk region is difficult since it contains hardly any or no data. Using&#10;extreme value theory, we construct a natural estimator of an extreme risk&#10;region and prove a refined form of consistency, given a random sample of&#10;multivariate regularly varying random vectors. In a detailed simulation and&#10;comparison study, the good performance of the procedure is demonstrated. We&#10;also apply our estimator to financial data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="274" source="John H. J. Einmahl" target="Laurens de Haan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5239v1" />
          <attvalue for="2" value="Estimation of extreme risk regions under multivariate regular variation" />
          <attvalue for="3" value="When considering d possibly dependent random variables, one is often&#10;interested in extreme risk regions, with very small probability p. We consider&#10;risk regions of the form ${\mathbf{z}\in\mathbb{R}^d:f(\mathbf{z})\leq\beta}$,&#10;where f is the joint density and $\beta$ a small number. Estimation of such an&#10;extreme risk region is difficult since it contains hardly any or no data. Using&#10;extreme value theory, we construct a natural estimator of an extreme risk&#10;region and prove a refined form of consistency, given a random sample of&#10;multivariate regularly varying random vectors. In a detailed simulation and&#10;comparison study, the good performance of the procedure is demonstrated. We&#10;also apply our estimator to financial data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="275" source="Pierre Alquier" target="Cristina Butucea">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="276" source="Pierre Alquier" target="Katia Meziani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="277" source="Pierre Alquier" target="Morimae Tomoyuki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="278" source="Pierre Alquier" target="Xiaoyin Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4294v2" />
          <attvalue for="2" value="Prediction of quantiles by statistical learning and application to GDP&#10;  forecasting" />
          <attvalue for="3" value="In this paper, we tackle the problem of prediction and confidence intervals&#10;for time series using a statistical learning approach and quantile loss&#10;functions. In a first time, we show that the Gibbs estimator (also known as&#10;Exponentially Weighted aggregate) is able to predict as well as the best&#10;predictor in a given family for a wide set of loss functions. In particular,&#10;using the quantile loss function of Koenker and Bassett (1978), this allows to&#10;build confidence intervals. We apply these results to the problem of prediction&#10;and confidence regions for the French Gross Domestic Product (GDP) growth, with&#10;promising results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="279" source="Cristina Butucea" target="Katia Meziani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="280" source="Cristina Butucea" target="Morimae Tomoyuki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="281" source="Katia Meziani" target="Morimae Tomoyuki">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1711v3" />
          <attvalue for="2" value="Rank penalized estimation of a quantum system" />
          <attvalue for="3" value="We introduce a new method to reconstruct the density matrix $\rho$ of a&#10;system of $n$-qubits and estimate its rank $d$ from data obtained by quantum&#10;state tomography measurements repeated $m$ times. The procedure consists in&#10;minimizing the risk of a linear estimator $\hat{\rho}$ of $\rho$ penalized by&#10;given rank (from 1 to $2^n$), where $\hat{\rho}$ is previously obtained by the&#10;moment method. We obtain simultaneously an estimator of the rank and the&#10;resulting density matrix associated to this rank. We establish an upper bound&#10;for the error of penalized estimator, evaluated with the Frobenius norm, which&#10;is of order $dn(4/3)^n /m$ and consistency for the estimator of the rank. The&#10;proposed methodology is computationaly efficient and is illustrated with some&#10;example states and real experimental data sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="282" source="Hua Liang" target="Xiang Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="283" source="Hua Liang" target="Chih-Ling Tsai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="284" source="Xiang Liu" target="Chih-Ling Tsai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3509v1" />
          <attvalue for="2" value="Estimation and testing for partially linear single-index models" />
          <attvalue for="3" value="In partially linear single-index models, we obtain the semiparametrically&#10;efficient profile least-squares estimators of regression coefficients. We also&#10;employ the smoothly clipped absolute deviation penalty (SCAD) approach to&#10;simultaneously select variables and estimate regression coefficients. We show&#10;that the resulting SCAD estimators are consistent and possess the oracle&#10;property. Subsequently, we demonstrate that a proposed tuning parameter&#10;selector, BIC, identifies the true model consistently. Finally, we develop a&#10;linear hypothesis test for the parametric coefficients and a goodness-of-fit&#10;test for the nonparametric component, respectively. Monte Carlo studies are&#10;also presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="285" source="Valentin Konakov" target="Enno Mammen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6307v2" />
          <attvalue for="2" value="Statistical convergence of Markov experiments to diffusion limits" />
          <attvalue for="3" value="Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov&#10;chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency&#10;Markov chain runs in the background on a very fine time grid but that it is&#10;only observed on a coarser grid. This asymptotics reflects a set up occurring&#10;in the high frequency statistical analysis for financial data where diffusion&#10;approximations are used only for coarser time scales. In this paper, we show&#10;that under appropriate conditions the L$_1$-distance between the joint&#10;distribution of the Markov chain and the distribution of the discretized&#10;diffusion limit converges to zero. The result implies that the LeCam deficiency&#10;distance between the statistical Markov experiment and its diffusion limit&#10;converges to zero. This result can be applied to Euler approximations for the&#10;joint distribution of diffusions observed at points&#10;$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by&#10;generating Euler approximations at the points $\Delta k^{-1},2\Delta&#10;k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions&#10;the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="286" source="Valentin Konakov" target="Jeannette Woerner">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6307v2" />
          <attvalue for="2" value="Statistical convergence of Markov experiments to diffusion limits" />
          <attvalue for="3" value="Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov&#10;chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency&#10;Markov chain runs in the background on a very fine time grid but that it is&#10;only observed on a coarser grid. This asymptotics reflects a set up occurring&#10;in the high frequency statistical analysis for financial data where diffusion&#10;approximations are used only for coarser time scales. In this paper, we show&#10;that under appropriate conditions the L$_1$-distance between the joint&#10;distribution of the Markov chain and the distribution of the discretized&#10;diffusion limit converges to zero. The result implies that the LeCam deficiency&#10;distance between the statistical Markov experiment and its diffusion limit&#10;converges to zero. This result can be applied to Euler approximations for the&#10;joint distribution of diffusions observed at points&#10;$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by&#10;generating Euler approximations at the points $\Delta k^{-1},2\Delta&#10;k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions&#10;the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="287" source="Enno Mammen" target="Jeannette Woerner">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6307v2" />
          <attvalue for="2" value="Statistical convergence of Markov experiments to diffusion limits" />
          <attvalue for="3" value="Assume that one observes the $k$th, $2k$th$,\ldots,nk$th value of a Markov&#10;chain $X_{1,h},\ldots,X_{nk,h}$. That means we assume that a high frequency&#10;Markov chain runs in the background on a very fine time grid but that it is&#10;only observed on a coarser grid. This asymptotics reflects a set up occurring&#10;in the high frequency statistical analysis for financial data where diffusion&#10;approximations are used only for coarser time scales. In this paper, we show&#10;that under appropriate conditions the L$_1$-distance between the joint&#10;distribution of the Markov chain and the distribution of the discretized&#10;diffusion limit converges to zero. The result implies that the LeCam deficiency&#10;distance between the statistical Markov experiment and its diffusion limit&#10;converges to zero. This result can be applied to Euler approximations for the&#10;joint distribution of diffusions observed at points&#10;$\Delta,2\Delta,\ldots,n\Delta$. The joint distribution can be approximated by&#10;generating Euler approximations at the points $\Delta k^{-1},2\Delta&#10;k^{-1},\ldots,n\Delta$. Our result implies that under our regularity conditions&#10;the Euler approximation is consistent for $n\to\infty$ if $nk^{-2}\to0$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="288" source="Enno Mammen" target="Young K. Lee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0403v1" />
          <attvalue for="2" value="Projection-type estimation for varying coefficient regression models" />
          <attvalue for="3" value="In this paper we introduce new estimators of the coefficient functions in the&#10;varying coefficient regression model. The proposed estimators are obtained by&#10;projecting the vector of the full-dimensional kernel-weighted local polynomial&#10;estimators of the coefficient functions onto a Hilbert space with a suitable&#10;norm. We provide a backfitting algorithm to compute the estimators. We show&#10;that the algorithm converges at a geometric rate under weak conditions. We&#10;derive the asymptotic distributions of the estimators and show that the&#10;estimators have the oracle properties. This is done for the general order of&#10;local polynomial fitting and for the estimation of the derivatives of the&#10;coefficient functions, as well as the coefficient functions themselves. The&#10;estimators turn out to have several theoretical and numerical advantages over&#10;the marginal integration estimators studied by Yang, Park, Xue and H\&quot;{a}rdle&#10;[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="289" source="Enno Mammen" target="Byeong U. Park">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0403v1" />
          <attvalue for="2" value="Projection-type estimation for varying coefficient regression models" />
          <attvalue for="3" value="In this paper we introduce new estimators of the coefficient functions in the&#10;varying coefficient regression model. The proposed estimators are obtained by&#10;projecting the vector of the full-dimensional kernel-weighted local polynomial&#10;estimators of the coefficient functions onto a Hilbert space with a suitable&#10;norm. We provide a backfitting algorithm to compute the estimators. We show&#10;that the algorithm converges at a geometric rate under weak conditions. We&#10;derive the asymptotic distributions of the estimators and show that the&#10;estimators have the oracle properties. This is done for the general order of&#10;local polynomial fitting and for the estimation of the derivatives of the&#10;coefficient functions, as well as the coefficient functions themselves. The&#10;estimators turn out to have several theoretical and numerical advantages over&#10;the marginal integration estimators studied by Yang, Park, Xue and H\&quot;{a}rdle&#10;[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="290" source="Enno Mammen" target="Christoph Rothe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5594v1" />
          <attvalue for="2" value="Nonparametric regression with nonparametrically generated covariates" />
          <attvalue for="3" value="We analyze the statistical properties of nonparametric regression estimators&#10;using covariates which are not directly observable, but have be estimated from&#10;data in a preliminary step. These so-called generated covariates appear in&#10;numerous applications, including two-stage nonparametric regression, estimation&#10;of simultaneous equation models or censored regression models. Yet so far there&#10;seems to be no general theory for their impact on the final estimator's&#10;statistical properties. Our paper provides such results. We derive a stochastic&#10;expansion that characterizes the influence of the generation step on the final&#10;estimator, and use it to derive rates of consistency and asymptotic&#10;distributions accounting for the presence of generated covariates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="291" source="Enno Mammen" target="Melanie Schienle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5594v1" />
          <attvalue for="2" value="Nonparametric regression with nonparametrically generated covariates" />
          <attvalue for="3" value="We analyze the statistical properties of nonparametric regression estimators&#10;using covariates which are not directly observable, but have be estimated from&#10;data in a preliminary step. These so-called generated covariates appear in&#10;numerous applications, including two-stage nonparametric regression, estimation&#10;of simultaneous equation models or censored regression models. Yet so far there&#10;seems to be no general theory for their impact on the final estimator's&#10;statistical properties. Our paper provides such results. We derive a stochastic&#10;expansion that characterizes the influence of the generation step on the final&#10;estimator, and use it to derive rates of consistency and asymptotic&#10;distributions accounting for the presence of generated covariates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="292" source="Fabien Navarro" target="Christophe Chesneau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.1056v2" />
          <attvalue for="2" value="On adaptive wavelet estimation of a class of weighted densities" />
          <attvalue for="3" value="We investigate the estimation of a weighted density taking the form&#10;$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated&#10;distribution function and $w$ is a known (non-negative) weight. Such a class&#10;encompasses many examples, including those arising in order statistics or when&#10;$g$ is related to the maximum or the minimum of $N$ (random or fixed)&#10;independent and identically distributed (\iid) random variables. We here&#10;construct a new adaptive non-parametric estimator for $g$ based on a plug-in&#10;approach and the wavelets methodology. For a wide class of models, we prove&#10;that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with&#10;$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared&#10;error) over Besov balls. The theoretical findings are illustrated through&#10;several simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="293" source="Fabien Navarro" target="Jalal Fadili">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.1056v2" />
          <attvalue for="2" value="On adaptive wavelet estimation of a class of weighted densities" />
          <attvalue for="3" value="We investigate the estimation of a weighted density taking the form&#10;$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated&#10;distribution function and $w$ is a known (non-negative) weight. Such a class&#10;encompasses many examples, including those arising in order statistics or when&#10;$g$ is related to the maximum or the minimum of $N$ (random or fixed)&#10;independent and identically distributed (\iid) random variables. We here&#10;construct a new adaptive non-parametric estimator for $g$ based on a plug-in&#10;approach and the wavelets methodology. For a wide class of models, we prove&#10;that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with&#10;$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared&#10;error) over Besov balls. The theoretical findings are illustrated through&#10;several simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="294" source="Fabien Navarro" target="Taoufik Sassi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.6316v3" />
          <attvalue for="2" value="Block thresholding for wavelet-based estimation of function derivatives&#10;  from a heteroscedastic multichannel convolution model" />
          <attvalue for="3" value="We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where&#10;for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution&#10;product of an unknown function $f$ and a known blurring function $g_v$&#10;corrupted by Gaussian noise. Under an ordinary smoothness assumption on&#10;$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak&#10;sense) of $f$ from the observations. We propose an adaptive estimator based on&#10;wavelet block thresholding, namely the &quot;BlockJS estimator&quot;. Taking the mean&#10;integrated squared error (MISE), our main theoretical result investigates the&#10;minimax rates over Besov smoothness spaces, and shows that our block estimator&#10;can achieve the optimal minimax rate, or is at least nearly-minimax in the&#10;least favorable situation. We also report a comprehensive suite of numerical&#10;simulations to support our theoretical findings. The practical performance of&#10;our block estimator compares very favorably to existing methods of the&#10;literature on a large set of test functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="295" source="Christophe Chesneau" target="Jalal Fadili">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.1056v2" />
          <attvalue for="2" value="On adaptive wavelet estimation of a class of weighted densities" />
          <attvalue for="3" value="We investigate the estimation of a weighted density taking the form&#10;$g=w(F)f$, where $f$ denotes an unknown density, $F$ the associated&#10;distribution function and $w$ is a known (non-negative) weight. Such a class&#10;encompasses many examples, including those arising in order statistics or when&#10;$g$ is related to the maximum or the minimum of $N$ (random or fixed)&#10;independent and identically distributed (\iid) random variables. We here&#10;construct a new adaptive non-parametric estimator for $g$ based on a plug-in&#10;approach and the wavelets methodology. For a wide class of models, we prove&#10;that it attains fast rates of convergence under the $\mathbb{L}_p$ risk with&#10;$p\ge 1$ (not only for $p = 2$ corresponding to the mean integrated squared&#10;error) over Besov balls. The theoretical findings are illustrated through&#10;several simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="296" source="Christophe Chesneau" target="Taoufik Sassi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.6316v3" />
          <attvalue for="2" value="Block thresholding for wavelet-based estimation of function derivatives&#10;  from a heteroscedastic multichannel convolution model" />
          <attvalue for="3" value="We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where&#10;for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution&#10;product of an unknown function $f$ and a known blurring function $g_v$&#10;corrupted by Gaussian noise. Under an ordinary smoothness assumption on&#10;$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak&#10;sense) of $f$ from the observations. We propose an adaptive estimator based on&#10;wavelet block thresholding, namely the &quot;BlockJS estimator&quot;. Taking the mean&#10;integrated squared error (MISE), our main theoretical result investigates the&#10;minimax rates over Besov smoothness spaces, and shows that our block estimator&#10;can achieve the optimal minimax rate, or is at least nearly-minimax in the&#10;least favorable situation. We also report a comprehensive suite of numerical&#10;simulations to support our theoretical findings. The practical performance of&#10;our block estimator compares very favorably to existing methods of the&#10;literature on a large set of test functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="297" source="Jalal Fadili" target="Taoufik Sassi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.6316v3" />
          <attvalue for="2" value="Block thresholding for wavelet-based estimation of function derivatives&#10;  from a heteroscedastic multichannel convolution model" />
          <attvalue for="3" value="We observe $n$ heteroscedastic stochastic processes $\{Y_v(t)\}_{v}$, where&#10;for any $v\in\{1,\ldots,n\}$ and $t \in [0,1]$, $Y_v(t)$ is the convolution&#10;product of an unknown function $f$ and a known blurring function $g_v$&#10;corrupted by Gaussian noise. Under an ordinary smoothness assumption on&#10;$g_1,\ldots,g_n$, our goal is to estimate the $d$-th derivatives (in weak&#10;sense) of $f$ from the observations. We propose an adaptive estimator based on&#10;wavelet block thresholding, namely the &quot;BlockJS estimator&quot;. Taking the mean&#10;integrated squared error (MISE), our main theoretical result investigates the&#10;minimax rates over Besov smoothness spaces, and shows that our block estimator&#10;can achieve the optimal minimax rate, or is at least nearly-minimax in the&#10;least favorable situation. We also report a comprehensive suite of numerical&#10;simulations to support our theoretical findings. The practical performance of&#10;our block estimator compares very favorably to existing methods of the&#10;literature on a large set of test functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="298" source="Jalal Fadili" target="Samuel Vaiter">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="299" source="Jalal Fadili" target="Charles Deledalle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="300" source="Jalal Fadili" target="Gabriel Peyré">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="301" source="Jalal Fadili" target="Charles Dossal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="302" source="Ming Yuan" target="Richard J. Samworth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0457v1" />
          <attvalue for="2" value="Independent component analysis via nonparametric maximum likelihood&#10;  estimation" />
          <attvalue for="3" value="Independent Component Analysis (ICA) models are very popular semiparametric&#10;models in which we observe independent copies of a random vector $X = AS$,&#10;where $A$ is a non-singular matrix and $S$ has independent components. We&#10;propose a new way of estimating the unmixing matrix $W = A^{-1}$ and the&#10;marginal distributions of the components of $S$ using nonparametric maximum&#10;likelihood. Specifically, we study the projection of the empirical distribution&#10;onto the subset of ICA distributions having log-concave marginals. We show&#10;that, from the point of view of estimating the unmixing matrix, it makes no&#10;difference whether or not the log-concavity is correctly specified. The&#10;approach is further justified by both theoretical results and a simulation&#10;study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="303" source="Ming Yuan" target="T. Tony Cai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2607v1" />
          <attvalue for="2" value="A reproducing kernel Hilbert space approach to functional linear&#10;  regression" />
          <attvalue for="3" value="We study in this paper a smoothness regularization method for functional&#10;linear regression and provide a unified treatment for both the prediction and&#10;estimation problems. By developing a tool on simultaneous diagonalization of&#10;two positive definite kernels, we obtain shaper results on the minimax rates of&#10;convergence and show that smoothness regularized estimators achieve the optimal&#10;rates of convergence for both prediction and estimation under conditions weaker&#10;than those for the functional principal components based methods developed in&#10;the literature. Despite the generality of the method of regularization, we show&#10;that the procedure is easily implementable. Numerical results are obtained to&#10;illustrate the merits of the method and to demonstrate the theoretical&#10;developments." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="304" source="Ming Yuan" target="Marten Wegkamp">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1140v1" />
          <attvalue for="2" value="Support vector machines with a reject option" />
          <attvalue for="3" value="This paper studies $\ell_1$ regularization with high-dimensional features for&#10;support vector machines with a built-in reject option (meaning that the&#10;decision of classifying an observation can be withheld at a cost lower than&#10;that of misclassification). The procedure can be conveniently implemented as a&#10;linear program and computed using standard software. We prove that the&#10;minimizer of the penalized population risk favors sparse solutions and show&#10;that the behavior of the empirical risk minimizer mimics that of the population&#10;risk minimizer. We also introduce a notion of classification complexity and&#10;prove that our minimizers adapt to the unknown complexity. Using a novel oracle&#10;inequality for the excess risk, we identify situations where fast rates of&#10;convergence occur." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="305" source="Sylvain Arlot" target="Alain Celisse">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3878v2" />
          <attvalue for="2" value="A kernel multiple change-point algorithm via model selection" />
          <attvalue for="3" value="We tackle the change-point problem with data belonging to a general set. We&#10;build a penalty for choosing the number of change-points in the kernel-based&#10;method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one&#10;proposed by Lebarbier (2005) for one-dimensional signals. We prove a&#10;non-asymptotic oracle inequality for the proposed method, thanks to a new&#10;concentration result for some function of Hilbert-space valued random&#10;variables. Experiments on synthetic data illustrate the accuracy of our method,&#10;showing that it can detect changes in the whole distribution of data, even when&#10;the mean and variance are constant." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="306" source="Sylvain Arlot" target="Zaid Harchaoui">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3878v2" />
          <attvalue for="2" value="A kernel multiple change-point algorithm via model selection" />
          <attvalue for="3" value="We tackle the change-point problem with data belonging to a general set. We&#10;build a penalty for choosing the number of change-points in the kernel-based&#10;method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one&#10;proposed by Lebarbier (2005) for one-dimensional signals. We prove a&#10;non-asymptotic oracle inequality for the proposed method, thanks to a new&#10;concentration result for some function of Hilbert-space valued random&#10;variables. Experiments on synthetic data illustrate the accuracy of our method,&#10;showing that it can detect changes in the whole distribution of data, even when&#10;the mean and variance are constant." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="307" source="Alain Celisse" target="Zaid Harchaoui">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3878v2" />
          <attvalue for="2" value="A kernel multiple change-point algorithm via model selection" />
          <attvalue for="3" value="We tackle the change-point problem with data belonging to a general set. We&#10;build a penalty for choosing the number of change-points in the kernel-based&#10;method of Harchaoui and Capp{\'e} (2007). This penalty generalizes the one&#10;proposed by Lebarbier (2005) for one-dimensional signals. We prove a&#10;non-asymptotic oracle inequality for the proposed method, thanks to a new&#10;concentration result for some function of Hilbert-space valued random&#10;variables. Experiments on synthetic data illustrate the accuracy of our method,&#10;showing that it can detect changes in the whole distribution of data, even when&#10;the mean and variance are constant." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="308" source="Mohammad Jafari Jozani" target="Eric Marchand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5097v1" />
          <attvalue for="2" value="Estimation of a nonnegative location parameter with unknown scale" />
          <attvalue for="3" value="For normal canonical models, and more generally a vast array of general&#10;spherically symmetric location-scale models with a residual vector, we consider&#10;estimating the (univariate) location parameter when it is lower bounded. We&#10;provide conditions for estimators to dominate the benchmark minimax MRE&#10;estimator, and thus be minimax under scale invariant loss. These minimax&#10;estimators include the generalized Bayes estimator with respect to the&#10;truncation of the common non-informative prior onto the restricted parameter&#10;space for normal models under general convex symmetric loss, as well as&#10;non-normal models under scale invariant $L^p$ loss with $p&gt;0$. We cover many&#10;other situations when the loss is asymmetric, and where other generalized Bayes&#10;estimators, obtained with different powers of the scale parameter in the prior&#10;measure, are proven to be minimax. We rely on various novel representations,&#10;sharp sign change analyses, as well as capitalize on Kubokawa's integral&#10;expression for risk difference technique. Several other analytical properties&#10;are obtained, including a robustness property of the generalized Bayes&#10;estimators above when the loss is either scale invariant $L^p$ or asymmetrized&#10;versions. Applications include inference in two-sample normal model with order&#10;constraints on the means." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="309" source="Mohammad Jafari Jozani" target="William Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5097v1" />
          <attvalue for="2" value="Estimation of a nonnegative location parameter with unknown scale" />
          <attvalue for="3" value="For normal canonical models, and more generally a vast array of general&#10;spherically symmetric location-scale models with a residual vector, we consider&#10;estimating the (univariate) location parameter when it is lower bounded. We&#10;provide conditions for estimators to dominate the benchmark minimax MRE&#10;estimator, and thus be minimax under scale invariant loss. These minimax&#10;estimators include the generalized Bayes estimator with respect to the&#10;truncation of the common non-informative prior onto the restricted parameter&#10;space for normal models under general convex symmetric loss, as well as&#10;non-normal models under scale invariant $L^p$ loss with $p&gt;0$. We cover many&#10;other situations when the loss is asymmetric, and where other generalized Bayes&#10;estimators, obtained with different powers of the scale parameter in the prior&#10;measure, are proven to be minimax. We rely on various novel representations,&#10;sharp sign change analyses, as well as capitalize on Kubokawa's integral&#10;expression for risk difference technique. Several other analytical properties&#10;are obtained, including a robustness property of the generalized Bayes&#10;estimators above when the loss is either scale invariant $L^p$ or asymmetrized&#10;versions. Applications include inference in two-sample normal model with order&#10;constraints on the means." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="310" source="Eric Marchand" target="William Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5097v1" />
          <attvalue for="2" value="Estimation of a nonnegative location parameter with unknown scale" />
          <attvalue for="3" value="For normal canonical models, and more generally a vast array of general&#10;spherically symmetric location-scale models with a residual vector, we consider&#10;estimating the (univariate) location parameter when it is lower bounded. We&#10;provide conditions for estimators to dominate the benchmark minimax MRE&#10;estimator, and thus be minimax under scale invariant loss. These minimax&#10;estimators include the generalized Bayes estimator with respect to the&#10;truncation of the common non-informative prior onto the restricted parameter&#10;space for normal models under general convex symmetric loss, as well as&#10;non-normal models under scale invariant $L^p$ loss with $p&gt;0$. We cover many&#10;other situations when the loss is asymmetric, and where other generalized Bayes&#10;estimators, obtained with different powers of the scale parameter in the prior&#10;measure, are proven to be minimax. We rely on various novel representations,&#10;sharp sign change analyses, as well as capitalize on Kubokawa's integral&#10;expression for risk difference technique. Several other analytical properties&#10;are obtained, including a robustness property of the generalized Bayes&#10;estimators above when the loss is either scale invariant $L^p$ or asymmetrized&#10;versions. Applications include inference in two-sample normal model with order&#10;constraints on the means." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="311" source="Eric Marchand" target="William E. Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.0028v2" />
          <attvalue for="2" value="On Bayesian credible sets in restricted parameter space problems and&#10;  lower bounds for frequentist coverage" />
          <attvalue for="3" value="For estimating a lower bounded parametric function in the framework of&#10;Marchand and Strawderman (2006), we provide through a unified approach a class&#10;of Bayesian confidence intervals with credibility $1-\alpha$ and frequentist&#10;coverage probability bounded below by $\frac{1-\alpha}{1+\alpha}$. In cases&#10;where the underlying pivotal distribution is symmetric, the findings represent&#10;extensions with respect to the specification of the credible set achieved&#10;through the choice of a {\it spending function}, and include Marchand and&#10;Strawderman's HPD procedure result. For non-symmetric cases, the determination&#10;of a such a class of Bayesian credible sets fills a gap in the literature and&#10;includes an &quot;equal-tails&quot; modification of the HPD procedure. Several examples&#10;are presented demonstrating wide applicability." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="312" source="László Varga" target="András Zempléni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.1302v1" />
          <attvalue for="2" value="Weighted bootstrap in GARCH models" />
          <attvalue for="3" value="GARCH models are useful tools in the investigation of phenomena, where&#10;volatility changes are prominent features, like most financial data. The&#10;parameter estimation via quasi maximum likelihood (QMLE) and its properties are&#10;by now well understood. However, there is a gap between practical applications&#10;and the theory, as in reality there are usually not enough observations for the&#10;limit results to be valid approximations. We try to fill this gap by this&#10;paper, where the properties of a recent bootstrap methodology in the context of&#10;GARCH modeling are revealed. The results are promising as it turns out that&#10;this remarkably simple method has essentially the same limit distribution, as&#10;the original estimatorwith the advantage of easy confidence interval&#10;construction, as it is demonstrated in the paper.&#10;  The finite-sample properties of the suggested estimators are investigated&#10;through a simulation study, which ensures that the results are practically&#10;applicable for sample sizes as low as a thousand. On the other hand, the&#10;results are not 100% accurate until sample size reaches 100 thousands - but it&#10;is shown that this property is not a feature of our bootstrap procedure only,&#10;as it is shared by the original QMLE, too." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="313" source="Yacine Aït-Sahalia" target="Jean Jacod">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5219v1" />
          <attvalue for="2" value="Testing whether jumps have finite or infinite activity" />
          <attvalue for="3" value="We propose statistical tests to discriminate between the finite and infinite&#10;activity of jumps in a semimartingale discretely observed at high frequency.&#10;The two statistics allow for a symmetric treatment of the problem: we can&#10;either take the null hypothesis to be finite activity, or infinite activity.&#10;When implemented on high-frequency stock returns, both tests point toward the&#10;presence of infinite-activity jumps in the data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="314" source="Jean Jacod" target="Markus Reiss">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4173v2" />
          <attvalue for="2" value="A remark on the rates of convergence for integrated volatility&#10;  estimation in the presence of jumps" />
          <attvalue for="3" value="The optimal rate of convergence of estimators of the integrated volatility,&#10;for a discontinuous It\^{o} semimartingale sampled at regularly spaced times&#10;and over a fixed time interval, has been a long-standing problem, at least when&#10;the jumps are not summable. In this paper, we study this optimal rate, in the&#10;minimax sense and for appropriate &quot;bounded&quot; nonparametric classes of&#10;semimartingales. We show that, if the $r$th powers of the jumps are summable&#10;for some $r\in[0,2)$, the minimax rate is equal to $\min(\sqrt{n},(n\log&#10;n)^{(2-r)/2})$, where $n$ is the number of observations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="315" source="Jean Jacod" target="Mathieu Rosenbaum">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.3757v3" />
          <attvalue for="2" value="Quarticity and other functionals of volatility: Efficient estimation" />
          <attvalue for="3" value="We consider a multidimensional Ito semimartingale regularly sampled on [0,t]&#10;at high frequency $1/\Delta_n$, with $\Delta_n$ going to zero. The goal of this&#10;paper is to provide an estimator for the integral over [0,t] of a given&#10;function of the volatility matrix. To approximate the integral, we simply use a&#10;Riemann sum based on local estimators of the pointwise volatility. We show that&#10;although the accuracy of the pointwise estimation is at most $\Delta_n^{1/4}$,&#10;this procedure reaches the parametric rate $\Delta_n^{1/2}$, as it is usually&#10;the case in integrated functionals estimation. After a suitable bias&#10;correction, we obtain an unbiased central limit theorem for our estimator and&#10;show that it is asymptotically efficient within some classes of sub models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="316" source="Jean Jacod" target="Mark Podolskij">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5490v1" />
          <attvalue for="2" value="A test for the rank of the volatility process: the random perturbation&#10;  approach" />
          <attvalue for="3" value="In this paper we present a test for the maximal rank of the matrix-valued&#10;volatility process in the continuous Ito semimartingale framework. Our idea is&#10;based upon a random perturbation of the original high frequency observations of&#10;an Ito semimartingale, which opens the way for rank testing. We develop the&#10;complete limit theory for the test statistic and apply it to various null and&#10;alternative hypotheses. Finally, we demonstrate a homoscedasticity test for the&#10;rank process." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="317" source="Filippo Palombi" target="Simona Toti">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.1838v2" />
          <attvalue for="2" value="Numerical reconstruction of the covariance matrix of a spherically&#10;  truncated multinormal distribution" />
          <attvalue for="3" value="In this paper we relate the matrix $S_B$ of the second moments of a&#10;spherically truncated normal multivariate to its full covariance matrix&#10;$\Sigma$ and present an algorithm to invert the relation and reconstruct&#10;$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by&#10;the truncation, its eigenvalues are non-uniformly damped. We show that the&#10;eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts&#10;via a fixed point iteration, whose convergence we prove analytically. The&#10;procedure requires the computation of multidimensional Gaussian integrals over&#10;a Euclidean ball, for which we extend a numerical technique, originally&#10;proposed by Ruben in 1962, based on a series expansion in chi-square&#10;distributions. In order to study the feasibility of our approach, we examine&#10;the convergence rate of some iterative schemes on suitably chosen ensembles of&#10;Wishart matrices. We finally discuss the practical difficulties arising in&#10;sample space and outline a regularization of the problem based on perturbation&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="318" source="Filippo Palombi" target="Romina Filippini">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.1838v2" />
          <attvalue for="2" value="Numerical reconstruction of the covariance matrix of a spherically&#10;  truncated multinormal distribution" />
          <attvalue for="3" value="In this paper we relate the matrix $S_B$ of the second moments of a&#10;spherically truncated normal multivariate to its full covariance matrix&#10;$\Sigma$ and present an algorithm to invert the relation and reconstruct&#10;$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by&#10;the truncation, its eigenvalues are non-uniformly damped. We show that the&#10;eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts&#10;via a fixed point iteration, whose convergence we prove analytically. The&#10;procedure requires the computation of multidimensional Gaussian integrals over&#10;a Euclidean ball, for which we extend a numerical technique, originally&#10;proposed by Ruben in 1962, based on a series expansion in chi-square&#10;distributions. In order to study the feasibility of our approach, we examine&#10;the convergence rate of some iterative schemes on suitably chosen ensembles of&#10;Wishart matrices. We finally discuss the practical difficulties arising in&#10;sample space and outline a regularization of the problem based on perturbation&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="319" source="Simona Toti" target="Romina Filippini">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.1838v2" />
          <attvalue for="2" value="Numerical reconstruction of the covariance matrix of a spherically&#10;  truncated multinormal distribution" />
          <attvalue for="3" value="In this paper we relate the matrix $S_B$ of the second moments of a&#10;spherically truncated normal multivariate to its full covariance matrix&#10;$\Sigma$ and present an algorithm to invert the relation and reconstruct&#10;$\Sigma$ from $S_B$. While the eigenvectors of $\Sigma$ are left invariant by&#10;the truncation, its eigenvalues are non-uniformly damped. We show that the&#10;eigenvalues of $\Sigma$ can be reconstructed from their truncated counterparts&#10;via a fixed point iteration, whose convergence we prove analytically. The&#10;procedure requires the computation of multidimensional Gaussian integrals over&#10;a Euclidean ball, for which we extend a numerical technique, originally&#10;proposed by Ruben in 1962, based on a series expansion in chi-square&#10;distributions. In order to study the feasibility of our approach, we examine&#10;the convergence rate of some iterative schemes on suitably chosen ensembles of&#10;Wishart matrices. We finally discuss the practical difficulties arising in&#10;sample space and outline a regularization of the problem based on perturbation&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="320" source="Ery Arias-Castro" target="Karim Lounici">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.2635v2" />
          <attvalue for="2" value="Variable Selection with Exponential Weights and $l_0$-Penalization" />
          <attvalue for="3" value="In the context of a linear model with a sparse coefficient vector,&#10;exponential weights methods have been shown to be achieve oracle inequalities&#10;for prediction. We show that such methods also succeed at variable selection&#10;and estimation under the necessary identifiability condition on the design&#10;matrix, instead of much stronger assumptions required by other methods such as&#10;the Lasso or the Dantzig Selector. The same analysis yields consistency results&#10;for Bayesian methods and BIC-type variable selection under similar conditions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="321" source="Ery Arias-Castro" target="Sébastien Bubeck">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5536v3" />
          <attvalue for="2" value="Detecting positive correlations in a multivariate sample" />
          <attvalue for="3" value="We consider the problem of testing whether a correlation matrix of a&#10;multivariate normal population is the identity matrix. We focus on sparse&#10;classes of alternatives where only a few entries are nonzero and, in fact,&#10;positive. We derive a general lower bound applicable to various classes and&#10;study the performance of some near-optimal tests. We pay special attention to&#10;computational feasibility and construct near-optimal tests that can be computed&#10;efficiently. Finally, we apply our results to prove new lower bounds for the&#10;clique number of high-dimensional random geometric graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="322" source="Ery Arias-Castro" target="Gábor Lugosi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5536v3" />
          <attvalue for="2" value="Detecting positive correlations in a multivariate sample" />
          <attvalue for="3" value="We consider the problem of testing whether a correlation matrix of a&#10;multivariate normal population is the identity matrix. We focus on sparse&#10;classes of alternatives where only a few entries are nonzero and, in fact,&#10;positive. We derive a general lower bound applicable to various classes and&#10;study the performance of some near-optimal tests. We pay special attention to&#10;computational feasibility and construct near-optimal tests that can be computed&#10;efficiently. Finally, we apply our results to prove new lower bounds for the&#10;clique number of high-dimensional random geometric graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="323" source="Debdeep Pati" target="Anirban Bhattacharya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="324" source="Debdeep Pati" target="Natesh S. Pillai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="325" source="Debdeep Pati" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3627v4" />
          <attvalue for="2" value="Posterior contraction in sparse Bayesian factor models for massive&#10;  covariance matrices" />
          <attvalue for="3" value="Sparse Bayesian factor models are routinely implemented for parsimonious&#10;dependence modeling and dimensionality reduction in high-dimensional&#10;applications. We provide theoretical understanding of such Bayesian procedures&#10;in terms of posterior convergence rates in inferring high-dimensional&#10;covariance matrices where the dimension can be larger than the sample size.&#10;Under relevant sparsity assumptions on the true covariance matrix, we show that&#10;commonly-used point mass mixture priors on the factor loadings lead to&#10;consistent estimation in the operator norm even when $p\gg n$. One of our major&#10;contributions is to develop a new class of continuous shrinkage priors and&#10;provide insights into their concentration around sparse vectors. Using such&#10;priors for the factor loadings, we obtain similar rate of convergence as&#10;obtained with point mass mixture priors. To obtain the convergence rates, we&#10;construct test functions to separate points in the space of high-dimensional&#10;covariance matrices using insights from random matrix theory; the tools&#10;developed may be of independent interest. We also derive minimax rates and show&#10;that the Bayesian posterior rates of convergence coincide with the minimax&#10;rates upto a $\sqrt{\log n}$ term." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="326" source="Debdeep Pati" target="David B. Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="327" source="Anirban Bhattacharya" target="Natesh S. Pillai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="328" source="Anirban Bhattacharya" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3627v4" />
          <attvalue for="2" value="Posterior contraction in sparse Bayesian factor models for massive&#10;  covariance matrices" />
          <attvalue for="3" value="Sparse Bayesian factor models are routinely implemented for parsimonious&#10;dependence modeling and dimensionality reduction in high-dimensional&#10;applications. We provide theoretical understanding of such Bayesian procedures&#10;in terms of posterior convergence rates in inferring high-dimensional&#10;covariance matrices where the dimension can be larger than the sample size.&#10;Under relevant sparsity assumptions on the true covariance matrix, we show that&#10;commonly-used point mass mixture priors on the factor loadings lead to&#10;consistent estimation in the operator norm even when $p\gg n$. One of our major&#10;contributions is to develop a new class of continuous shrinkage priors and&#10;provide insights into their concentration around sparse vectors. Using such&#10;priors for the factor loadings, we obtain similar rate of convergence as&#10;obtained with point mass mixture priors. To obtain the convergence rates, we&#10;construct test functions to separate points in the space of high-dimensional&#10;covariance matrices using insights from random matrix theory; the tools&#10;developed may be of independent interest. We also derive minimax rates and show&#10;that the Bayesian posterior rates of convergence coincide with the minimax&#10;rates upto a $\sqrt{\log n}$ term." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="329" source="Anirban Bhattacharya" target="David B. Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="330" source="Natesh S. Pillai" target="David Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3627v4" />
          <attvalue for="2" value="Posterior contraction in sparse Bayesian factor models for massive&#10;  covariance matrices" />
          <attvalue for="3" value="Sparse Bayesian factor models are routinely implemented for parsimonious&#10;dependence modeling and dimensionality reduction in high-dimensional&#10;applications. We provide theoretical understanding of such Bayesian procedures&#10;in terms of posterior convergence rates in inferring high-dimensional&#10;covariance matrices where the dimension can be larger than the sample size.&#10;Under relevant sparsity assumptions on the true covariance matrix, we show that&#10;commonly-used point mass mixture priors on the factor loadings lead to&#10;consistent estimation in the operator norm even when $p\gg n$. One of our major&#10;contributions is to develop a new class of continuous shrinkage priors and&#10;provide insights into their concentration around sparse vectors. Using such&#10;priors for the factor loadings, we obtain similar rate of convergence as&#10;obtained with point mass mixture priors. To obtain the convergence rates, we&#10;construct test functions to separate points in the space of high-dimensional&#10;covariance matrices using insights from random matrix theory; the tools&#10;developed may be of independent interest. We also derive minimax rates and show&#10;that the Bayesian posterior rates of convergence coincide with the minimax&#10;rates upto a $\sqrt{\log n}$ term." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="331" source="Natesh S. Pillai" target="David B. Dunson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.6088v1" />
          <attvalue for="2" value="Bayesian shrinkage" />
          <attvalue for="3" value="Penalized regression methods, such as $L_1$ regularization, are routinely&#10;used in high-dimensional applications, and there is a rich literature on&#10;optimality properties under sparsity assumptions. In the Bayesian paradigm,&#10;sparsity is routinely induced through two-component mixture priors having a&#10;probability mass at zero, but such priors encounter daunting computational&#10;problems in high dimensions. This has motivated an amazing variety of&#10;continuous shrinkage priors, which can be expressed as global-local scale&#10;mixtures of Gaussians, facilitating computation. In sharp contrast to the&#10;corresponding frequentist literature, very little is known about the properties&#10;of such priors. Focusing on a broad class of shrinkage priors, we provide&#10;precise results on prior and posterior concentration. Interestingly, we&#10;demonstrate that most commonly used shrinkage priors, including the Bayesian&#10;Lasso, are suboptimal in high-dimensional settings. A new class of Dirichlet&#10;Laplace (DL) priors are proposed, which are optimal and lead to efficient&#10;posterior computation exploiting results from normalized random measure theory.&#10;Finite sample performance of Dirichlet Laplace priors relative to alternatives&#10;is assessed in simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="332" source="David Dunson" target="Nate Strawn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="333" source="David Dunson" target="Artin Armagan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="334" source="David Dunson" target="Rayan Saab">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="335" source="David Dunson" target="Lawrence Carin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="336" source="Young K. Lee" target="Byeong U. Park">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0403v1" />
          <attvalue for="2" value="Projection-type estimation for varying coefficient regression models" />
          <attvalue for="3" value="In this paper we introduce new estimators of the coefficient functions in the&#10;varying coefficient regression model. The proposed estimators are obtained by&#10;projecting the vector of the full-dimensional kernel-weighted local polynomial&#10;estimators of the coefficient functions onto a Hilbert space with a suitable&#10;norm. We provide a backfitting algorithm to compute the estimators. We show&#10;that the algorithm converges at a geometric rate under weak conditions. We&#10;derive the asymptotic distributions of the estimators and show that the&#10;estimators have the oracle properties. This is done for the general order of&#10;local polynomial fitting and for the estimation of the derivatives of the&#10;coefficient functions, as well as the coefficient functions themselves. The&#10;estimators turn out to have several theoretical and numerical advantages over&#10;the marginal integration estimators studied by Yang, Park, Xue and H\&quot;{a}rdle&#10;[J. Amer. Statist. Assoc. 101 (2006) 1212--1227]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="337" source="John Kolassa" target="John Robinson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3106v1" />
          <attvalue for="2" value="Saddlepoint approximations for likelihood ratio like statistics with&#10;  applications to permutation tests" />
          <attvalue for="3" value="We obtain two theorems extending the use of a saddlepoint approximation to&#10;multiparameter problems for likelihood ratio-like statistics which allow their&#10;use in permutation and rank tests and could be used in bootstrap&#10;approximations. In the first, we show that in some cases when no density&#10;exists, the integral of the formal saddlepoint density over the set&#10;corresponding to large values of the likelihood ratio-like statistic&#10;approximates the true probability with relative error of order $1/n$. In the&#10;second, we give multivariate generalizations of the Lugannani--Rice and&#10;Barndorff-Nielsen or $r^*$ formulas for the approximations. These theorems are&#10;applied to obtain permutation tests based on the likelihood ratio-like&#10;statistics for the $k$ sample and the multivariate two-sample cases. Numerical&#10;examples are given to illustrate the high degree of accuracy, and these&#10;statistics are compared to the classical statistics in both cases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="338" source="Christophe Giraud" target="Alexandre Tsybakov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0811v1" />
          <attvalue for="2" value="Discussion: Latent variable graphical model selection via convex&#10;  optimization" />
          <attvalue for="3" value="Discussion of &quot;Latent variable graphical model selection via convex&#10;optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky&#10;[arXiv:1008.1290]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="339" source="Alexandre Tsybakov" target="Philippe Rigollet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1210v1" />
          <attvalue for="2" value="Estimation of Covariance Matrices under Sparsity Constraints" />
          <attvalue for="3" value="We prove optimal sparsity oracle inequalities for the estimation of&#10;covariance matrix under the Frobenius norm. In particular we explore various&#10;sparsity structures on the underlying matrix." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="340" source="Alexandre Tsybakov" target="Arnak Dalalyan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.6402v3" />
          <attvalue for="2" value="Statistical inference in compound functional models" />
          <attvalue for="3" value="We consider a general nonparametric regression model called the compound&#10;model. It includes, as special cases, sparse additive regression and&#10;nonparametric (or linear) regression with many covariates but possibly a small&#10;number of relevant covariates. The compound model is characterized by three&#10;main parameters: the structure parameter describing the &quot;macroscopic&quot; form of&#10;the compound function, the &quot;microscopic&quot; sparsity parameter indicating the&#10;maximal number of relevant covariates in each component and the usual&#10;smoothness parameter corresponding to the complexity of the members of the&#10;compound. We find non-asymptotic minimax rate of convergence of estimators in&#10;such a model as a function of these three parameters. We also show that this&#10;rate can be attained in an adaptive way." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="341" source="Alexandre Tsybakov" target="Yuri Ingster">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.6402v3" />
          <attvalue for="2" value="Statistical inference in compound functional models" />
          <attvalue for="3" value="We consider a general nonparametric regression model called the compound&#10;model. It includes, as special cases, sparse additive regression and&#10;nonparametric (or linear) regression with many covariates but possibly a small&#10;number of relevant covariates. The compound model is characterized by three&#10;main parameters: the structure parameter describing the &quot;macroscopic&quot; form of&#10;the compound function, the &quot;microscopic&quot; sparsity parameter indicating the&#10;maximal number of relevant covariates in each component and the usual&#10;smoothness parameter corresponding to the complexity of the members of the&#10;compound. We find non-asymptotic minimax rate of convergence of estimators in&#10;such a model as a function of these three parameters. We also show that this&#10;rate can be attained in an adaptive way." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="342" source="Daniel Commenges" target="Cécile Proust-Lima">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="343" source="Daniel Commenges" target="Cécilia Samieri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="344" source="Daniel Commenges" target="Benoit Liquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="345" source="Cécile Proust-Lima" target="Cécilia Samieri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="346" source="Cécile Proust-Lima" target="Benoit Liquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="347" source="Cécilia Samieri" target="Benoit Liquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1753v1" />
          <attvalue for="2" value="A universal approximate cross-validation criterion and its asymptotic&#10;  distribution" />
          <attvalue for="3" value="A general framework is that the estimators of a distribution are obtained by&#10;minimizing a function (the estimating function) and they are assessed through&#10;another function (the assessment function). The estimating and assessment&#10;functions generally estimate risks. A classical case is that both functions&#10;estimate an information risk (specifically cross entropy); in that case Akaike&#10;information criterion (AIC) is relevant. In more general cases, the assessment&#10;risk can be estimated by leave-one-out crossvalidation. Since leave-one-out&#10;crossvalidation is computationally very demanding, an approximation formula can&#10;be very useful. A universal approximate crossvalidation criterion (UACV) for&#10;the leave-one-out crossvalidation is given. This criterion can be adapted to&#10;different types of estimators, including penalized likelihood and maximum a&#10;posteriori estimators, and of assessment risk functions, including information&#10;risk functions and continuous rank probability score (CRPS). This formula&#10;reduces to Takeuchi information criterion (TIC) when cross entropy is the risk&#10;for both estimation and assessment. The asymptotic distribution of UACV and of&#10;a difference of UACV is given. UACV can be used for comparing estimators of the&#10;distributions of ordered categorical data derived from threshold models and&#10;models based on continuous approximations. A simulation study and an analysis&#10;of real psychometric data are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="348" source="Lee Dicker" target="Xihong Lin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4576v2" />
          <attvalue for="2" value="Parallelism, Uniqueness, and Large-Sample Asymptotics for the Dantzig&#10;  Selector" />
          <attvalue for="3" value="The Dantzig selector (Candes and Tao, 2007) is a popular l1-regularization&#10;method for variable selection and estimation in linear regression. We present a&#10;very weak geometric condition on the observed predictors which is related to&#10;parallelism and, when satisfied, ensures the uniqueness of Dantzig selector&#10;estimators. The condition holds with probability 1, if the predictors are drawn&#10;from a continuous distribution. We discuss the necessity of this condition for&#10;uniqueness and also provide a closely related condition which ensures&#10;uniqueness of lasso estimators (Tibshirani, 1996). Large sample asymptotics for&#10;the Dantzig selector, i.e. almost sure convergence and the asymptotic&#10;distribution, follow directly from our uniqueness results and a continuity&#10;argument. The limiting distribution of the Dantzig selector is generally&#10;non-normal. Though our asymptotic results require that the number of predictors&#10;is fixed (similar to (Knight and Fu, 2000)), our uniqueness results are valid&#10;for an arbitrary number of predictors and observations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="349" source="Joseph S. Koopmeiners" target="Ziding Feng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3014v1" />
          <attvalue for="2" value="Asymptotic properties of the sequential empirical ROC, PPV and NPV&#10;  curves under case-control sampling" />
          <attvalue for="3" value="The receiver operating characteristic (ROC) curve, the positive predictive&#10;value (PPV) curve and the negative predictive value (NPV) curve are three&#10;measures of performance for a continuous diagnostic biomarker. The ROC, PPV and&#10;NPV curves are often estimated empirically to avoid assumptions about the&#10;distributional form of the biomarkers. Recently, there has been a push to&#10;incorporate group sequential methods into the design of diagnostic biomarker&#10;studies. A thorough understanding of the asymptotic properties of the&#10;sequential empirical ROC, PPV and NPV curves will provide more flexibility when&#10;designing group sequential diagnostic biomarker studies. In this paper, we&#10;derive asymptotic theory for the sequential empirical ROC, PPV and NPV curves&#10;under case-control sampling using sequential empirical process theory. We show&#10;that the sequential empirical ROC, PPV and NPV curves converge to the sum of&#10;independent Kiefer processes and show how these results can be used to derive&#10;asymptotic results for summaries of the sequential empirical ROC, PPV and NPV&#10;curves." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="350" source="Anita Behme" target="Makoto Maejima">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="351" source="Anita Behme" target="Muneya Matsui">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="352" source="Anita Behme" target="Noriyoshi Sakuma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="353" source="Makoto Maejima" target="Muneya Matsui">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="354" source="Makoto Maejima" target="Noriyoshi Sakuma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="355" source="Makoto Maejima" target="Víctor Pérez-Abreu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1654v1" />
          <attvalue for="2" value="A class of multivariate infinitely divisible distributions related to&#10;  arcsine density" />
          <attvalue for="3" value="Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures&#10;on $\mathbb{R}^d$ based on the arcsine density are studied and their relation&#10;to general Upsilon transformations is considered. The domains of definition of&#10;$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they&#10;have the same range. The class of infinitely divisible distributions on&#10;$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the&#10;class $A$ and any distribution in the class $A$ is expressed as the law of a&#10;stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect&#10;to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass&#10;the Jurek class of distributions. It is shown that generalized type $G$&#10;distributions are the image of distributions in the class $A$ under a mapping&#10;defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as&#10;an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="356" source="Makoto Maejima" target="Ken-iti Sato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1654v1" />
          <attvalue for="2" value="A class of multivariate infinitely divisible distributions related to&#10;  arcsine density" />
          <attvalue for="3" value="Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures&#10;on $\mathbb{R}^d$ based on the arcsine density are studied and their relation&#10;to general Upsilon transformations is considered. The domains of definition of&#10;$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they&#10;have the same range. The class of infinitely divisible distributions on&#10;$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the&#10;class $A$ and any distribution in the class $A$ is expressed as the law of a&#10;stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect&#10;to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass&#10;the Jurek class of distributions. It is shown that generalized type $G$&#10;distributions are the image of distributions in the class $A$ under a mapping&#10;defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as&#10;an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="357" source="Muneya Matsui" target="Noriyoshi Sakuma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5281v1" />
          <attvalue for="2" value="Distributions of exponential integrals of independent increment&#10;  processes related to generalized gamma convolutions" />
          <attvalue for="3" value="It is known that in many cases distributions of exponential integrals of Levy&#10;processes are infinitely divisible and in some cases they are also&#10;selfdecomposable. In this paper, we give some sufficient conditions under which&#10;distributions of exponential integrals are not only selfdecomposable but&#10;furthermore are generalized gamma convolution. We also study exponential&#10;integrals of more general independent increment processes. Several examples are&#10;given for illustration." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="358" source="Fabienne Comte" target="Charles-André Cuénod">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="359" source="Fabienne Comte" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="360" source="Charles-André Cuénod" target="Yves Rozenholc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2231v1" />
          <attvalue for="2" value="Laplace deconvolution and its application to Dynamic Contrast Enhanced&#10;  imaging" />
          <attvalue for="3" value="In the present paper we consider the problem of Laplace deconvolution with&#10;noisy discrete observations. The study is motivated by Dynamic Contrast&#10;Enhanced imaging using a bolus of contrast agent, a procedure which allows&#10;considerable improvement in {evaluating} the quality of a vascular network and&#10;its permeability and is widely used in medical assessment of brain flows or&#10;cancerous tumors. Although the study is motivated by medical imaging&#10;application, we obtain a solution of a general problem of Laplace deconvolution&#10;based on noisy data which appears in many different contexts. We propose a new&#10;method for Laplace deconvolution which is based on expansions of the&#10;convolution kernel, the unknown function and the observed signal over Laguerre&#10;functions basis. The expansion results in a small system of linear equations&#10;with the matrix of the system being triangular and Toeplitz. The number $m$ of&#10;the terms in the expansion of the estimator is controlled via complexity&#10;penalty. The advantage of this methodology is that it leads to very fast&#10;computations, does not require exact knowledge of the kernel and produces no&#10;boundary effects due to extension at zero and cut-off at $T$. The technique&#10;leads to an estimator with the risk within a logarithmic factor of $m$ of the&#10;oracle risk under no assumptions on the model and within a constant factor of&#10;the oracle risk under mild assumptions. The methodology is illustrated by a&#10;finite sample simulation study which includes an example of the kernel obtained&#10;in the real life DCE experiments. Simulations confirm that the proposed&#10;technique is fast, efficient, accurate, usable from a practical point of view&#10;and competitive." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="361" source="Kshitij Khare" target="James P. Hobert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5205v1" />
          <attvalue for="2" value="A spectral analytic comparison of trace-class data augmentation&#10;  algorithms and their sandwich variants" />
          <attvalue for="3" value="The data augmentation (DA) algorithm is a widely used Markov chain Monte&#10;Carlo algorithm that is easy to implement but often suffers from slow&#10;convergence. The sandwich algorithm is an alternative that can converge much&#10;faster while requiring roughly the same computational effort per iteration.&#10;Theoretically, the sandwich algorithm always converges at least as fast as the&#10;corresponding DA algorithm in the sense that $\Vert {K^*}\Vert \le \Vert&#10;{K}\Vert$, where $K$ and $K^*$ are the Markov operators associated with the DA&#10;and sandwich algorithms, respectively, and $\Vert\cdot\Vert$ denotes operator&#10;norm. In this paper, a substantial refinement of this operator norm inequality&#10;is developed. In particular, under regularity conditions implying that $K$ is a&#10;trace-class operator, it is shown that $K^*$ is also a positive, trace-class&#10;operator, and that the spectrum of $K^*$ dominates that of $K$ in the sense&#10;that the ordered elements of the former are all less than or equal to the&#10;corresponding elements of the latter. Furthermore, if the sandwich algorithm is&#10;constructed using a group action, as described by Liu and Wu [J. Amer. Statist.&#10;Assoc. 94 (1999) 1264--1274] and Hobert and Marchev [Ann. Statist. 36 (2008)&#10;532--554], then there is strict inequality between at least one pair of&#10;eigenvalues. These results are applied to a new DA algorithm for Bayesian&#10;quantile regression introduced by Kozumi and Kobayashi [J. Stat. Comput. Simul.&#10;81 (2011) 1565--1578]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="362" source="James P. Hobert" target="Aixin Tan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.4770v1" />
          <attvalue for="2" value="On the Geometric Ergodicity of Two-Variable Gibbs Samplers" />
          <attvalue for="3" value="A Markov chain is geometrically ergodic if it converges to its in- variant&#10;distribution at a geometric rate in total variation norm. We study geo- metric&#10;ergodicity of deterministic and random scan versions of the two-variable Gibbs&#10;sampler. We give a sufficient condition which simultaneously guarantees both&#10;versions are geometrically ergodic. We also develop a method for simul-&#10;taneously establishing that both versions are subgeometrically ergodic. These&#10;general results allow us to characterize the convergence rate of two-variable&#10;Gibbs samplers in a particular family of discrete bivariate distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="363" source="James P. Hobert" target="Galin L. Jones">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.4770v1" />
          <attvalue for="2" value="On the Geometric Ergodicity of Two-Variable Gibbs Samplers" />
          <attvalue for="3" value="A Markov chain is geometrically ergodic if it converges to its in- variant&#10;distribution at a geometric rate in total variation norm. We study geo- metric&#10;ergodicity of deterministic and random scan versions of the two-variable Gibbs&#10;sampler. We give a sufficient condition which simultaneously guarantees both&#10;versions are geometrically ergodic. We also develop a method for simul-&#10;taneously establishing that both versions are subgeometrically ergodic. These&#10;general results allow us to characterize the convergence rate of two-variable&#10;Gibbs samplers in a particular family of discrete bivariate distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="364" source="Mark S. Kaiser" target="Soumendra N. Lahiri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6086v1" />
          <attvalue for="2" value="Goodness of fit tests for a class of Markov random field models" />
          <attvalue for="3" value="This paper develops goodness of fit statistics that can be used to formally&#10;assess Markov random field models for spatial data, when the model&#10;distributions are discrete or continuous and potentially parametric. Test&#10;statistics are formed from generalized spatial residuals which are collected&#10;over groups of nonneighboring spatial observations, called concliques. Under a&#10;hypothesized Markov model structure, spatial residuals within each conclique&#10;are shown to be independent and identically distributed as uniform variables.&#10;The information from a series of concliques can be then pooled into goodness of&#10;fit statistics. Under some conditions, large sample distributions of these&#10;statistics are explicitly derived for testing both simple and composite&#10;hypotheses, where the latter involves additional parametric estimation steps.&#10;The distributional results are verified through simulation, and a data example&#10;illustrates the method for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="365" source="Mark S. Kaiser" target="Daniel J. Nordman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6086v1" />
          <attvalue for="2" value="Goodness of fit tests for a class of Markov random field models" />
          <attvalue for="3" value="This paper develops goodness of fit statistics that can be used to formally&#10;assess Markov random field models for spatial data, when the model&#10;distributions are discrete or continuous and potentially parametric. Test&#10;statistics are formed from generalized spatial residuals which are collected&#10;over groups of nonneighboring spatial observations, called concliques. Under a&#10;hypothesized Markov model structure, spatial residuals within each conclique&#10;are shown to be independent and identically distributed as uniform variables.&#10;The information from a series of concliques can be then pooled into goodness of&#10;fit statistics. Under some conditions, large sample distributions of these&#10;statistics are explicitly derived for testing both simple and composite&#10;hypotheses, where the latter involves additional parametric estimation steps.&#10;The distributional results are verified through simulation, and a data example&#10;illustrates the method for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="366" source="Soumendra N. Lahiri" target="Daniel J. Nordman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6086v1" />
          <attvalue for="2" value="Goodness of fit tests for a class of Markov random field models" />
          <attvalue for="3" value="This paper develops goodness of fit statistics that can be used to formally&#10;assess Markov random field models for spatial data, when the model&#10;distributions are discrete or continuous and potentially parametric. Test&#10;statistics are formed from generalized spatial residuals which are collected&#10;over groups of nonneighboring spatial observations, called concliques. Under a&#10;hypothesized Markov model structure, spatial residuals within each conclique&#10;are shown to be independent and identically distributed as uniform variables.&#10;The information from a series of concliques can be then pooled into goodness of&#10;fit statistics. Under some conditions, large sample distributions of these&#10;statistics are explicitly derived for testing both simple and composite&#10;hypotheses, where the latter involves additional parametric estimation steps.&#10;The distributional results are verified through simulation, and a data example&#10;illustrates the method for model assessment." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="367" source="Yan Sun" target="Dan Ralescu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2740v2" />
          <attvalue for="2" value="A Normal Hierarchical Model and Minimum Contrast Estimation for Random&#10;  Intervals" />
          <attvalue for="3" value="Many statistical data are imprecise due to factors such as measurement&#10;errors, computation errors, and lack of information. In such cases, data are&#10;better represented by intervals rather than by single numbers. Existing methods&#10;for analyzing interval-valued data include regressions in the metric space of&#10;intervals and symbolic data analysis, the latter being proposed in a more&#10;general setting. However, there has been a lack of literature on the&#10;distribution-based inferences for interval-valued data. In an attempt to fill&#10;this gap, we extend the concept of normality for random sets by Lyashenko&#10;(1983) and propose a normal hierarchical model for random intervals. In&#10;addition, we develop a minimum contrast estimator (MCE) for the model&#10;parameters, which we show is both consistent and asymptotically normal.&#10;Simulation studies support our theoretical findings, and show very promising&#10;results. Finally, we successfully apply our model and MCE to a real dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="368" source="Luai Al Labadi" target="Mahmoud Zarepour">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6658v1" />
          <attvalue for="2" value="On Some Asymptotic Properties and an Almost Sure Approximation of the&#10;  Normalized Inverse-Gaussian Process" />
          <attvalue for="3" value="In this paper, we present some asymptotic properties of the normalized&#10;inverse-Gaussian process. In particular, when the concentration parameter is&#10;large, we establish an analogue of the empirical functional central limit&#10;theorem, the strong law of large numbers and the Glivenko-Cantelli theorem for&#10;the normalized inverse-Gaussian process and its corresponding quantile process.&#10;We also derive a finite sum-representation that converges almost surely to the&#10;Ferguson and Klass representation of the normalized inverse-Gaussian process.&#10;This almost sure approximation can be used to simulate efficiently the&#10;normalized inverse-Gaussian process." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="369" source="Olivier Ledoit" target="Michael Wolf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5322v1" />
          <attvalue for="2" value="Nonlinear shrinkage estimation of large-dimensional covariance matrices" />
          <attvalue for="3" value="Many statistical applications require an estimate of a covariance matrix&#10;and/or its inverse. When the matrix dimension is large compared to the sample&#10;size, which happens frequently, the sample covariance matrix is known to&#10;perform poorly and may suffer from ill-conditioning. There already exists an&#10;extensive literature concerning improved estimators in such situations. In the&#10;absence of further knowledge about the structure of the true covariance matrix,&#10;the most successful approach so far, arguably, has been shrinkage estimation.&#10;Shrinking the sample covariance matrix to a multiple of the identity, by taking&#10;a weighted average of the two, turns out to be equivalent to linearly shrinking&#10;the sample eigenvalues to their grand mean, while retaining the sample&#10;eigenvectors. Our paper extends this approach by considering nonlinear&#10;transformations of the sample eigenvalues. We show how to construct an&#10;estimator that is asymptotically equivalent to an oracle estimator suggested in&#10;previous work. As demonstrated in extensive Monte Carlo simulations, the&#10;resulting bona fide estimator can result in sizeable improvements over the&#10;sample covariance matrix and also over linear shrinkage." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="370" source="Yuan Wu" target="Ying Zhang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5543v1" />
          <attvalue for="2" value="Partially monotone tensor spline estimation of the joint distribution&#10;  function with bivariate current status data" />
          <attvalue for="3" value="The analysis of the joint cumulative distribution function (CDF) with&#10;bivariate event time data is a challenging problem both theoretically and&#10;numerically. This paper develops a tensor spline-based sieve maximum likelihood&#10;estimation method to estimate the joint CDF with bivariate current status data.&#10;The I-splines are used to approximate the joint CDF in order to simplify the&#10;numerical computation of a constrained maximum likelihood estimation problem.&#10;The generalized gradient projection algorithm is used to compute the&#10;constrained optimization problem. Based on the properties of B-spline basis&#10;functions it is shown that the proposed tensor spline-based nonparametric sieve&#10;maximum likelihood estimator is consistent with a rate of convergence&#10;potentially better than $n^{1/3}$ under some mild regularity conditions. The&#10;simulation studies with moderate sample sizes are carried out to demonstrate&#10;that the finite sample performance of the proposed estimator is generally&#10;satisfactory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="371" source="Christoph Rothe" target="Melanie Schienle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5594v1" />
          <attvalue for="2" value="Nonparametric regression with nonparametrically generated covariates" />
          <attvalue for="3" value="We analyze the statistical properties of nonparametric regression estimators&#10;using covariates which are not directly observable, but have be estimated from&#10;data in a preliminary step. These so-called generated covariates appear in&#10;numerous applications, including two-stage nonparametric regression, estimation&#10;of simultaneous equation models or censored regression models. Yet so far there&#10;seems to be no general theory for their impact on the final estimator's&#10;statistical properties. Our paper provides such results. We derive a stochastic&#10;expansion that characterizes the influence of the generation step on the final&#10;estimator, and use it to derive rates of consistency and asymptotic&#10;distributions accounting for the presence of generated covariates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="372" source="Othmane Kortbi" target="Éric Marchand">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.6054v1" />
          <attvalue for="2" value="Estimation of a multivariate normal mean with a bounded signal to noise&#10;  ratio" />
          <attvalue for="3" value="For normal canonical models with $X \sim N_p(\theta, \sigma^{2} I_{p}), \;\;&#10;S^{2} \sim \sigma^{2}\chi^{2}_{k}, \;{independent}$, we consider the problem of&#10;estimating $\theta$ under scale invariant squared error loss $\frac{\|d-\theta&#10;\|^{2}}{\sigma^{2}}$, when it is known that the signal-to-noise ratio&#10;$\frac{\|\theta\|}{\sigma}$ is bounded above by $m$. Risk analysis is achieved&#10;by making use of a conditional risk decomposition and we obtain in particular&#10;sufficient conditions for an estimator to dominate either the unbiased&#10;estimator $\delta_{UB}(X)=X$, or the maximum likelihood estimator&#10;$\delta_{\hbox{mle}}(X,S^2)$, or both of these benchmark procedures. The given&#10;developments bring into play the pivotal role of the boundary Bayes estimator&#10;$\delta_{BU}$ associated with a prior on $(\theta,\sigma)$ such that&#10;$\theta|\sigma$ is uniformly distributed on the (boundary) sphere of radius $m$&#10;and a non-informative $\frac{1}{\sigma}$ prior measure is placed marginally on&#10;$\sigma$. With a series of technical results related to $\delta_{BU}$; which&#10;relate to particular ratios of confluent hypergeometric functions; we show&#10;that, whenever $m \leq \sqrt{p}$ and $p \geq 2$, $\delta_{BU}$ dominates both&#10;$\delta_{UB}$ and $\delta_{\hbox{mle}}$. The finding can be viewed as both a&#10;multivariate extension of $p=1$ result due to Kubokawa (2005) and a unknown&#10;variance extension of a similar dominance finding due to Marchand and Perron&#10;(2001). Various other dominance results are obtained, illustrations are&#10;provided and commented upon. In particular, for $m \leq \sqrt{\frac{p}{2}}$, a&#10;wide class of Bayes estimators, which include priors where $\theta|\sigma$ is&#10;uniformly distributed on the ball of radius $m$, are shown to dominate&#10;$\delta_{UB}$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="373" source="Éric Marchand" target="William E. Strawderman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1964v1" />
          <attvalue for="2" value="A unified minimax result for restricted parameter spaces" />
          <attvalue for="3" value="We provide a development that unifies, simplifies and extends considerably a&#10;number of minimax results in the restricted parameter space literature. Various&#10;applications follow, such as that of estimating location or scale parameters&#10;under a lower (or upper) bound restriction, location parameter vectors&#10;restricted to a polyhedral cone, scale parameters subject to restricted ratios&#10;or products, linear combinations of restricted location parameters, location&#10;parameters bounded to an interval with unknown scale, quantiles for&#10;location-scale families with parametric restrictions and restricted covariance&#10;matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="374" source="Miklos Csorgo" target="Yuliya Martsynyuk">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4089v4" />
          <attvalue for="2" value="Another look at Bootstrapping the Student t-statistic" />
          <attvalue for="3" value="Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E&#10;X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative&#10;random variables (weights), independent of the data sequence&#10;${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $&#10;X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from&#10;re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq&#10;1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define&#10;$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,&#10;$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put&#10;$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,&#10;$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the&#10;bootstrap sample variance. The main aim of this exposition is to study the&#10;asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=&#10;(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=&#10;\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of&#10;conditioning on the weights via assuming that, as $n,m_n\to \infty$,&#10;$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost&#10;surely or in probability on the probability space of the weights. This view of&#10;justifying the validity of the bootstrap is believed to be new. The need for it&#10;arises naturally in practice when exploring the nature of information contained&#10;in a random sample via re-sampling, for example. Conditioning on the data is&#10;also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as&#10;$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval&#10;$(\lambda_1,\lambda_2)$ with 0&lt; \lambda_1 &lt; \lambda_2 &lt; \infty as in Mason and&#10;Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches&#10;to conditioning is established." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="375" source="Miklos Csorgo" target="Masoud Nasari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4089v4" />
          <attvalue for="2" value="Another look at Bootstrapping the Student t-statistic" />
          <attvalue for="3" value="Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E&#10;X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative&#10;random variables (weights), independent of the data sequence&#10;${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $&#10;X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from&#10;re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq&#10;1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define&#10;$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,&#10;$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put&#10;$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,&#10;$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the&#10;bootstrap sample variance. The main aim of this exposition is to study the&#10;asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=&#10;(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=&#10;\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of&#10;conditioning on the weights via assuming that, as $n,m_n\to \infty$,&#10;$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost&#10;surely or in probability on the probability space of the weights. This view of&#10;justifying the validity of the bootstrap is believed to be new. The need for it&#10;arises naturally in practice when exploring the nature of information contained&#10;in a random sample via re-sampling, for example. Conditioning on the data is&#10;also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as&#10;$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval&#10;$(\lambda_1,\lambda_2)$ with 0&lt; \lambda_1 &lt; \lambda_2 &lt; \infty as in Mason and&#10;Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches&#10;to conditioning is established." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="376" source="Miklos Csorgo" target="Masoud M. Nasari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.2757v2" />
          <attvalue for="2" value="Asymptotics of Randomly Weighted u- and v-statistics: Application to&#10;  Bootstrap" />
          <attvalue for="3" value="This paper is mainly concerned with asymptotic studies of weighted bootstrap&#10;for u- and v-statistics. We derive the consistency of the weighted bootstrap u-&#10;and v-statistics, based on i.i.d. and non i.i.d. observations, from some more&#10;general results which we first establish for sums of randomly weighted arrays&#10;of random variables. Some of the results in this paper significantly extend&#10;some well-known results on consistency of u-statistics and also consistency of&#10;sums of arrays of random variables. We also employ a new approach to&#10;conditioning to derive a conditional CLT for weighted bootstrap u- and&#10;v-statistics, assuming the same conditions as the classical central limit&#10;theorems for regular u- and v-statistics." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="377" source="Yuliya Martsynyuk" target="Masoud Nasari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4089v4" />
          <attvalue for="2" value="Another look at Bootstrapping the Student t-statistic" />
          <attvalue for="3" value="Let X, X_1,X_2,... be a sequence of i.i.d. random variables with mean $\mu=E&#10;X$. Let ${v_1^{(n)},...,v_n^{(n)}}_{n=1}^\infty$ be vectors of non-negative&#10;random variables (weights), independent of the data sequence&#10;${X_1,...,X_n}_{n=1}^\infty$, and put $m_n=\sumn v_i^{(n)}$. Consider $&#10;X^{*}_1,..., X^{*}_{m_n}$, $m_n\geq 1$, a bootstrap sample, resulting from&#10;re-sampling or stochastically re-weighing a random sample $X_1,...,X_n$, $n\geq&#10;1$. Put $\bar{X}_n= \sumn X_i/n$, the original sample mean, and define&#10;$\bar{X^*}_{m_n}=\sumn v_i^{(n)} X_i/m_n$, the bootstrap sample mean. Thus,&#10;$\bar{X^*}_{m_n}- \bar{X}_n=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n}) X_i$. Put&#10;$V_n^{2}=\sumn ({v_i^{(n)}}/{m_n}-{1}/{n})^2$ and let $S_n^{2}$,&#10;$S_{m_{n}}^{*^{2}}$ respectively be the the original sample variance and the&#10;bootstrap sample variance. The main aim of this exposition is to study the&#10;asymptotic behavior of the bootstrapped $t$-statistics $T_{m_n}^{*}:=&#10;(\bar{X^*}_{m_n}- \bar{X}_n)/(S_n V_n)$ and $T_{m_n}^{**}:=&#10;\sqrt{m_n}(\bar{X^*}_{m_n}- \bar{X}_n)/ S_{m_{n}}^{*} $ in terms of&#10;conditioning on the weights via assuming that, as $n,m_n\to \infty$,&#10;$\max_{1\leq i \leq n}({v_i^{(n)}}/{m_n}-{1}/{n})^2\big/ V_n^{2}=o(1)$ almost&#10;surely or in probability on the probability space of the weights. This view of&#10;justifying the validity of the bootstrap is believed to be new. The need for it&#10;arises naturally in practice when exploring the nature of information contained&#10;in a random sample via re-sampling, for example. Conditioning on the data is&#10;also revisited for Efron's bootstrap weights under conditions on $n,m_n$ as&#10;$n\to \infty $ that differ from requiring $m_n /n$ to be in the interval&#10;$(\lambda_1,\lambda_2)$ with 0&lt; \lambda_1 &lt; \lambda_2 &lt; \infty as in Mason and&#10;Shao. Also, the validity of the bootstrapped $t$-intervals for both approaches&#10;to conditioning is established." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="378" source="Jan Draisma" target="Sonja Kuhnt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5910v3" />
          <attvalue for="2" value="Groups acting on Gaussian graphical models" />
          <attvalue for="3" value="Gaussian graphical models have become a well-recognized tool for the analysis&#10;of conditional independencies within a set of continuous random variables. From&#10;an inferential point of view, it is important to realize that they are&#10;composite exponential transformation families. We reveal this structure by&#10;explicitly describing, for any undirected graph, the (maximal) matrix group&#10;acting on the space of concentration matrices in the model. The continuous part&#10;of this group is captured by a poset naturally associated to the graph, while&#10;automorphisms of the graph account for the discrete part of the group. We&#10;compute the dimension of the space of orbits of this group on concentration&#10;matrices, in terms of the combinatorics of the graph; and for dimension zero we&#10;recover the characterization by Letac and Massam of models that are&#10;transformation families. Furthermore, we describe the maximal invariant of this&#10;group on the sample space, and we give a sharp lower bound on the sample size&#10;needed for the existence of equivariant estimators of the concentration matrix.&#10;Finally, we address the issue of robustness of these estimators by computing&#10;upper bounds on finite sample breakdown points." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="379" source="Jan Draisma" target="Piotr Zwiernik">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5910v3" />
          <attvalue for="2" value="Groups acting on Gaussian graphical models" />
          <attvalue for="3" value="Gaussian graphical models have become a well-recognized tool for the analysis&#10;of conditional independencies within a set of continuous random variables. From&#10;an inferential point of view, it is important to realize that they are&#10;composite exponential transformation families. We reveal this structure by&#10;explicitly describing, for any undirected graph, the (maximal) matrix group&#10;acting on the space of concentration matrices in the model. The continuous part&#10;of this group is captured by a poset naturally associated to the graph, while&#10;automorphisms of the graph account for the discrete part of the group. We&#10;compute the dimension of the space of orbits of this group on concentration&#10;matrices, in terms of the combinatorics of the graph; and for dimension zero we&#10;recover the characterization by Letac and Massam of models that are&#10;transformation families. Furthermore, we describe the maximal invariant of this&#10;group on the sample space, and we give a sharp lower bound on the sample size&#10;needed for the existence of equivariant estimators of the concentration matrix.&#10;Finally, we address the issue of robustness of these estimators by computing&#10;upper bounds on finite sample breakdown points." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="380" source="Sonja Kuhnt" target="Piotr Zwiernik">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5910v3" />
          <attvalue for="2" value="Groups acting on Gaussian graphical models" />
          <attvalue for="3" value="Gaussian graphical models have become a well-recognized tool for the analysis&#10;of conditional independencies within a set of continuous random variables. From&#10;an inferential point of view, it is important to realize that they are&#10;composite exponential transformation families. We reveal this structure by&#10;explicitly describing, for any undirected graph, the (maximal) matrix group&#10;acting on the space of concentration matrices in the model. The continuous part&#10;of this group is captured by a poset naturally associated to the graph, while&#10;automorphisms of the graph account for the discrete part of the group. We&#10;compute the dimension of the space of orbits of this group on concentration&#10;matrices, in terms of the combinatorics of the graph; and for dimension zero we&#10;recover the characterization by Letac and Massam of models that are&#10;transformation families. Furthermore, we describe the maximal invariant of this&#10;group on the sample space, and we give a sharp lower bound on the sample size&#10;needed for the existence of equivariant estimators of the concentration matrix.&#10;Finally, we address the issue of robustness of these estimators by computing&#10;upper bounds on finite sample breakdown points." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="381" source="Sonia Petrone" target="Judith Rousseau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1470v1" />
          <attvalue for="2" value="Bayes and empirical Bayes: do they merge?" />
          <attvalue for="3" value="Bayesian inference is attractive for its coherence and good frequentist&#10;properties. However, it is a common experience that eliciting a honest prior&#10;may be difficult and, in practice, people often take an {\em empirical Bayes}&#10;approach, plugging empirical estimates of the prior hyperparameters into the&#10;posterior distribution. Even if not rigorously justified, the underlying idea&#10;is that, when the sample size is large, empirical Bayes leads to &quot;similar&quot;&#10;inferential answers. Yet, precise mathematical results seem to be missing. In&#10;this work, we give a more rigorous justification in terms of merging of Bayes&#10;and empirical Bayes posterior distributions. We consider two notions of&#10;merging: Bayesian weak merging and frequentist merging in total variation.&#10;Since weak merging is related to consistency, we provide sufficient conditions&#10;for consistency of empirical Bayes posteriors. Also, we show that, under&#10;regularity conditions, the empirical Bayes procedure asymptotically selects the&#10;value of the hyperparameter for which the prior mostly favors the &quot;truth&quot;.&#10;Examples include empirical Bayes density estimation with Dirichlet process&#10;mixtures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="382" source="Sonia Petrone" target="Catia Scricciolo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1470v1" />
          <attvalue for="2" value="Bayes and empirical Bayes: do they merge?" />
          <attvalue for="3" value="Bayesian inference is attractive for its coherence and good frequentist&#10;properties. However, it is a common experience that eliciting a honest prior&#10;may be difficult and, in practice, people often take an {\em empirical Bayes}&#10;approach, plugging empirical estimates of the prior hyperparameters into the&#10;posterior distribution. Even if not rigorously justified, the underlying idea&#10;is that, when the sample size is large, empirical Bayes leads to &quot;similar&quot;&#10;inferential answers. Yet, precise mathematical results seem to be missing. In&#10;this work, we give a more rigorous justification in terms of merging of Bayes&#10;and empirical Bayes posterior distributions. We consider two notions of&#10;merging: Bayesian weak merging and frequentist merging in total variation.&#10;Since weak merging is related to consistency, we provide sufficient conditions&#10;for consistency of empirical Bayes posteriors. Also, we show that, under&#10;regularity conditions, the empirical Bayes procedure asymptotically selects the&#10;value of the hyperparameter for which the prior mostly favors the &quot;truth&quot;.&#10;Examples include empirical Bayes density estimation with Dirichlet process&#10;mixtures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="383" source="Judith Rousseau" target="Catia Scricciolo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1470v1" />
          <attvalue for="2" value="Bayes and empirical Bayes: do they merge?" />
          <attvalue for="3" value="Bayesian inference is attractive for its coherence and good frequentist&#10;properties. However, it is a common experience that eliciting a honest prior&#10;may be difficult and, in practice, people often take an {\em empirical Bayes}&#10;approach, plugging empirical estimates of the prior hyperparameters into the&#10;posterior distribution. Even if not rigorously justified, the underlying idea&#10;is that, when the sample size is large, empirical Bayes leads to &quot;similar&quot;&#10;inferential answers. Yet, precise mathematical results seem to be missing. In&#10;this work, we give a more rigorous justification in terms of merging of Bayes&#10;and empirical Bayes posterior distributions. We consider two notions of&#10;merging: Bayesian weak merging and frequentist merging in total variation.&#10;Since weak merging is related to consistency, we provide sufficient conditions&#10;for consistency of empirical Bayes posteriors. Also, we show that, under&#10;regularity conditions, the empirical Bayes procedure asymptotically selects the&#10;value of the hyperparameter for which the prior mostly favors the &quot;truth&quot;.&#10;Examples include empirical Bayes density estimation with Dirichlet process&#10;mixtures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="384" source="Judith Rousseau" target="Julyan Arbel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2392v2" />
          <attvalue for="2" value="Bayesian optimal adaptive estimation using a sieve prior" />
          <attvalue for="3" value="We derive rates of contraction of posterior distributions on nonparametric&#10;models resulting from sieve priors. The aim of the paper is to provide general&#10;conditions to get posterior rates when the parameter space has a general&#10;structure, and rate adaptation when the parameter space is, e.g., a Sobolev&#10;class. The conditions employed, although standard in the literature, are&#10;combined in a different way. The results are applied to density, regression,&#10;nonlinear autoregression and Gaussian white noise models. In the latter we have&#10;also considered a loss function which is different from the usual l2 norm,&#10;namely the pointwise loss. In this case it is possible to prove that the&#10;adaptive Bayesian approach for the l2 loss is strongly suboptimal and we&#10;provide a lower bound on the rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="385" source="Judith Rousseau" target="Ghislaine Gayraud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2392v2" />
          <attvalue for="2" value="Bayesian optimal adaptive estimation using a sieve prior" />
          <attvalue for="3" value="We derive rates of contraction of posterior distributions on nonparametric&#10;models resulting from sieve priors. The aim of the paper is to provide general&#10;conditions to get posterior rates when the parameter space has a general&#10;structure, and rate adaptation when the parameter space is, e.g., a Sobolev&#10;class. The conditions employed, although standard in the literature, are&#10;combined in a different way. The results are applied to density, regression,&#10;nonlinear autoregression and Gaussian white noise models. In the latter we have&#10;also considered a loss function which is different from the usual l2 norm,&#10;namely the pointwise loss. In this case it is possible to prove that the&#10;adaptive Bayesian approach for the l2 loss is strongly suboptimal and we&#10;provide a lower bound on the rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="386" source="Judith Rousseau" target="Willem Kruijer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4863v1" />
          <attvalue for="2" value="Bayesian semi-parametric estimation of the long-memory parameter under&#10;  FEXP-priors" />
          <attvalue for="3" value="For a Gaussian time series with long-memory behavior, we use the FEXP-model&#10;for semi-parametric estimation of the long-memory parameter $d$. The true&#10;spectral density $f_o$ is assumed to have long-memory parameter $d_o$ and a&#10;FEXP-expansion of Sobolev-regularity $\be &gt; 1$. We prove that when $k$ follows&#10;a Poisson or geometric prior, or a sieve prior increasing at rate&#10;$n^{\frac{1}{1+2\be}}$, $d$ converges to $d_o$ at a suboptimal rate. When the&#10;sieve prior increases at rate $n^{\frac{1}{2\be}}$ however, the minimax rate is&#10;almost obtained. Our results can be seen as a Bayesian equivalent of the result&#10;which Moulines and Soulier obtained for some frequentist estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="387" source="Silvia L. P. Ferrari" target="Eliane C. Pinheiro">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.3949v3" />
          <attvalue for="2" value="Small-sample likelihood inference in extreme-value regression models" />
          <attvalue for="3" value="We deal with a general class of extreme-value regression models introduced by&#10;Barreto- Souza and Vasconcellos (2011). Our goal is to derive an adjusted&#10;likelihood ratio statistic that is approximately distributed as \c{hi}2 with a&#10;high degree of accuracy. Although the adjusted statistic requires more&#10;computational effort than its unadjusted counterpart, it is shown that the&#10;adjustment term has a simple compact form that can be easily implemented in&#10;standard statistical software. Further, we compare the finite sample&#10;performance of the three classical tests (likelihood ratio, Wald, and score),&#10;the gradient test that has been recently proposed by Terrell (2002), and the&#10;adjusted likelihood ratio test obtained in this paper. Our simulations favor&#10;the latter. Applications of our results are presented. Key words: Extreme-value&#10;regression; Gradient test; Gumbel distribution; Likelihood ratio test;&#10;Nonlinear models; Score test; Small-sample adjustments; Wald test." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="388" source="Silvia L. P. Ferrari" target="Tatiane F. N. Melo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5039v2" />
          <attvalue for="2" value="Modified likelihood ratio tests in heteroskedastic multivariate&#10;  regression models with measurement error" />
          <attvalue for="3" value="In this paper, we develop modified versions of the likelihood ratio test for&#10;multivariate heteroskedastic errors-in-variables regression models. The error&#10;terms are allowed to follow a multivariate distribution in the elliptical class&#10;of distributions, which has the normal distribution as a special case. We&#10;derive the Skovgaard adjusted likelihood ratio statistics, which follow a&#10;chi-squared distribution with a high degree of accuracy. We conduct a&#10;simulation study and show that the proposed tests display superior finite&#10;sample behavior as compared to the standard likelihood ratio test. We&#10;illustrate the usefulness of our results in applied settings using a data set&#10;from the WHO MONICA Projection cardiovascular disease." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="389" source="Silvia L. P. Ferrari" target="Alexandre G. Patriota">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5039v2" />
          <attvalue for="2" value="Modified likelihood ratio tests in heteroskedastic multivariate&#10;  regression models with measurement error" />
          <attvalue for="3" value="In this paper, we develop modified versions of the likelihood ratio test for&#10;multivariate heteroskedastic errors-in-variables regression models. The error&#10;terms are allowed to follow a multivariate distribution in the elliptical class&#10;of distributions, which has the normal distribution as a special case. We&#10;derive the Skovgaard adjusted likelihood ratio statistics, which follow a&#10;chi-squared distribution with a high degree of accuracy. We conduct a&#10;simulation study and show that the proposed tests display superior finite&#10;sample behavior as compared to the standard likelihood ratio test. We&#10;illustrate the usefulness of our results in applied settings using a data set&#10;from the WHO MONICA Projection cardiovascular disease." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="390" source="Silvia L. P. Ferrari" target="Tiago M. Vargas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2206v1" />
          <attvalue for="2" value="Gradient statistic: higher-order asymptotics and Bartlett-type&#10;  correction" />
          <attvalue for="3" value="We obtain an asymptotic expansion for the null distribution function of&#10;thegradient statistic for testing composite null hypotheses in the presence of&#10;nuisance parameters. The expansion is derived using a Bayesian route based on&#10;the shrinkage argument described in Ghosh and Mukerjee (1991). Using this&#10;expansion, we propose a Bartlett-type corrected gradient statistic with&#10;chi-square distribution up to an error of order o(n^{-1}) under the null&#10;hypothesis. Further, we also use the expansion to modify the percentage points&#10;of the large sample reference chi-square distribution. A small Monte Carlo&#10;experiment and various examples are presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="391" source="Silvia L. P. Ferrari" target="Artur J. Lemonte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2206v1" />
          <attvalue for="2" value="Gradient statistic: higher-order asymptotics and Bartlett-type&#10;  correction" />
          <attvalue for="3" value="We obtain an asymptotic expansion for the null distribution function of&#10;thegradient statistic for testing composite null hypotheses in the presence of&#10;nuisance parameters. The expansion is derived using a Bayesian route based on&#10;the shrinkage argument described in Ghosh and Mukerjee (1991). Using this&#10;expansion, we propose a Bartlett-type corrected gradient statistic with&#10;chi-square distribution up to an error of order o(n^{-1}) under the null&#10;hypothesis. Further, we also use the expansion to modify the percentage points&#10;of the large sample reference chi-square distribution. A small Monte Carlo&#10;experiment and various examples are presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="392" source="Fasano María Victoria" target="Ricardo A. Maronna">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0473v1" />
          <attvalue for="2" value="Consistency of M estimates for separable nonlinear regression models" />
          <attvalue for="3" value="Consider a nonlinear regression model : y_{i}=g(x_{i},{\theta})+e_{i},&#10;i=1,...,n, where the x_{i} are random predictors x_{i} and {\theta} is the&#10;unknown parameter vector ranging in a set {\Theta}\subsetR^{p}. All known&#10;results on the consistency of the least squares estimator and in general of M&#10;estimators assume that either {\Theta} is compact or g is bounded, which&#10;excludes frequently employed models such as the Michaelis-Menten, logistic&#10;growth and exponential decay models. In this article we deal with the so-called&#10;separable models, where p=p_{1}+p_{2}, {\theta}=({\alpha},{\beta}) with&#10;{\alpha}\inA\subsetR^{p_{1}}, {\beta}\inB\subsetR^{p_{2},}and g has the form&#10;g(x,{\theta})={\beta}^{T}h(x,{\alpha}) where h is a function with values in&#10;R^{p_{2}}. We prove the strong consistency of M estimators under very general&#10;assumptions, assuming that h is a bounded function of {\alpha}, which includes&#10;the three models mentioned above. Key words and phrases: Nonlinear regression,&#10;separable models, consistency, robust estimation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="393" source="Jelena Bradic" target="Rui Song">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4510v3" />
          <attvalue for="2" value="Structured Estimation in Nonparameteric Cox Model" />
          <attvalue for="3" value="To better understand the interplay of censoring and sparsity we develop&#10;finite sample properties of nonparametric Cox proportional hazard's model. Due&#10;to high impact of sequencing data, carrying genetic information of each&#10;individual, we work with over-parametrized problem and propose general class of&#10;group penalties suitable for sparse structured variable selection and&#10;estimation. Novel non-asymptotic sandwich bounds for the partial likelihood are&#10;developed. We establish how they extend notion of local asymptotic normality&#10;(LAN) of Le Cam's. Such non-asymptotic LAN principles are further extended to&#10;high dimensional spaces where $p \gg n$. Finite sample prediction properties of&#10;penalized estimator in non-parametric Cox proportional hazards model, under&#10;suitable censoring conditions, agree with those of penalized estimator in&#10;linear models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="394" source="Zhao Ren" target="Harrison H. Zhou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0813v1" />
          <attvalue for="2" value="Discussion: Latent variable graphical model selection via convex&#10;  optimization" />
          <attvalue for="3" value="Discussion of &quot;Latent variable graphical model selection via convex&#10;optimization&quot; by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky&#10;[arXiv:1008.1290]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="395" source="Jan Johannes" target="Maik Schwarz">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1226v1" />
          <attvalue for="2" value="Adaptive Gaussian inverse regression with partially unknown operator" />
          <attvalue for="3" value="This work deals with the ill-posed inverse problem of reconstructing a&#10;function $f$ given implicitly as the solution of $g = Af$, where $A$ is a&#10;compact linear operator with unknown singular values and known eigenfunctions.&#10;We observe the function $g$ and the singular values of the operator subject to&#10;Gaussian white noise with respective noise levels $\varepsilon$ and $\sigma$.&#10;  We develop a minimax theory in terms of both noise levels and propose an&#10;orthogonal series estimator attaining the minimax rates. This estimator&#10;requires the optimal choice of a dimension parameter depending on certain&#10;characteristics of $f$ and $A$. This work addresses the fully data-driven&#10;choice of the dimension parameter combining model selection with Lepski's&#10;method. We show that the fully data-driven estimator preserves minimax&#10;optimality over a wide range of classes for $f$ and $A$ and noise levels&#10;$\varepsilon$ and $\sigma$. The results are illustrated considering Sobolev&#10;spaces and mildly and severely ill-posed inverse problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="396" source="Thibault Espinasse" target="Paul Rochet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2763v1" />
          <attvalue for="2" value="A Cramér-Rao inequality for non differentiable models" />
          <attvalue for="3" value="We compute a variance lower bound for unbiased estimators in specified&#10;statistical models. The construction of the bound is related to the original&#10;Cram\'er-Rao bound, although it does not require the differentiability of the&#10;model. Moreover, we show our efficiency bound to be always greater than the&#10;Cram\'er-Rao bound in smooth models, thus providing a sharper result." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="397" source="Loic Le Gratiet" target="Josselin Garnier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.2879v2" />
          <attvalue for="2" value="Regularity dependence of the rate of convergence of the learning curve&#10;  for Gaussian process regression" />
          <attvalue for="3" value="This paper deals with the speed of convergence of the learning curve in a&#10;Gaussian process regression framework. The learning curve describes the average&#10;generalization error of the Gaussian process used for the regression. More&#10;specifically, it is defined in this paper as the integral of the mean squared&#10;error over the input parameter space with respect to the probability measure of&#10;the input parameters. The main result is the proof of a theorem giving the mean&#10;squared error in function of the number of observations for a large class of&#10;kernels and for any dimension when the number of observations is large. From&#10;this result, we can deduce the asymptotic behavior of the generalization error.&#10;The presented proof generalizes previous ones that were limited to more&#10;specific kernels or to small dimensions (one or two). The result can be used to&#10;build an optimal strategy for resources allocation. This strategy is applied&#10;successfully to a nuclear safety problem." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="398" source="Loic Le Gratiet" target="Claire Cannamela">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6187v2" />
          <attvalue for="2" value="Kriging-based sequential design strategies using fast cross-validation&#10;  techniques with extensions to multi-fidelity computer codes" />
          <attvalue for="3" value="Kriging-based surrogate models have become very popular during the last&#10;decades to approximate a computer code output from few simulations. In&#10;practical applications, it is very common to sequentially add new simulations&#10;to obtain more accurate approximations. We propose in this paper a method of&#10;kriging-based sequential design which combines both the error evaluation&#10;providing by the kriging model and the observed errors of a Leave-One-Out&#10;cross-validation procedure. This method is proposed in two versions, the first&#10;one selects points one at-a-time. The second one allows us to parallelize the&#10;simulations and to add several design points at-a-time. Then, we extend these&#10;strategies to multi-fidelity co-kriging models which allow us to surrogate a&#10;complex code using fast approximations of it. The main advantage of these&#10;extensions is that it not only provides the new locations where to perform&#10;simulations but also which versions of code have to be simulated (between the&#10;complex one or one of its fast approximations). A real multi-fidelity&#10;application is used to illustrate the efficiency of the proposed approaches. In&#10;this example, the accurate code is a two-dimensional finite element model and&#10;the less accurate one is a one-dimensional approximation of the system." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="399" source="Bas Kleijn" target="Bartek Knapik">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6204v3" />
          <attvalue for="2" value="Semiparametric posterior limits under local asymptotic exponentiality" />
          <attvalue for="3" value="Consider semiparametric models that display local asymptotic exponentiality&#10;(Ibragimov and Has'minskii (1981)), an asymptotic property of the likelihood&#10;associated with discontinuities of densities. Our interest goes to estimation&#10;of the location of such discontinuities while other aspects of the density form&#10;a nuisance parameter. It is shown that under certain conditions on model and&#10;prior, the posterior distribution displays Bernstein-von Mises-type asymptotic&#10;behaviour, with exponential distributions as the limiting sequence. In contrast&#10;to regular settings, the maximum likelihood estimator is inefficient under this&#10;form of irregularity. However, Bayesian point estimators based on the limiting&#10;posterior distribution attain the minimax risk. Therefore, the limiting&#10;behaviour of the posterior is used to advocate efficiency of Bayesian point&#10;estimation rather than compare it to frequentist estimation procedures based on&#10;the maximum likelihood estimator. Results are applied to semiparametric LAE&#10;location and scaling examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="400" source="Jens-Peter Kreiss" target="Efstathios Paparoditis">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.6211v1" />
          <attvalue for="2" value="On the range of validity of the autoregressive sieve bootstrap" />
          <attvalue for="3" value="We explore the limits of the autoregressive (AR) sieve bootstrap, and show&#10;that its applicability extends well beyond the realm of linear time series as&#10;has been previously thought. In particular, for appropriate statistics, the&#10;AR-sieve bootstrap is valid for stationary processes possessing a general&#10;Wold-type autoregressive representation with respect to a white noise; in&#10;essence, this includes all stationary, purely nondeterministic processes, whose&#10;spectral density is everywhere positive. Our main theorem provides a simple and&#10;effective tool in assessing whether the AR-sieve bootstrap is asymptotically&#10;valid in any given situation. In effect, the large-sample distribution of the&#10;statistic in question must only depend on the first and second order moments of&#10;the process; prominent examples include the sample mean and the spectral&#10;density. As a counterexample, we show how the AR-sieve bootstrap is not always&#10;valid for the sample autocovariance even when the underlying process is linear." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="401" source="Efstathios Paparoditis" target="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.2617v1" />
          <attvalue for="2" value="Short-Term Load Forecasting: The Similar Shape Functional Time Series&#10;  Predictor" />
          <attvalue for="3" value="We introduce a novel functional time series methodology for short-term load&#10;forecasting. The prediction is performed by means of a weighted average of past&#10;daily load segments, the shape of which is similar to the expected shape of the&#10;load segment to be predicted. The past load segments are identified from the&#10;available history of the observed load segments by means of their closeness to&#10;a so-called reference load segment, the later being selected in a manner that&#10;captures the expected qualitative and quantitative characteristics of the load&#10;segment to be predicted. Weak consistency of the suggested functional similar&#10;shape predictor is established. As an illustration, we apply the suggested&#10;functional time series forecasting methodology to historical daily load data in&#10;Cyprus and compare its performance to that of a recently proposed alternative&#10;functional time series methodology for short-term load forecasting." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="402" source="Herold Dehling" target="Daniel Vogel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="403" source="Herold Dehling" target="Martin Wendler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="404" source="Herold Dehling" target="Dominik Wied">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="405" source="Herold Dehling" target="Brice Franke">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="406" source="Herold Dehling" target="Thomas Kott">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="407" source="Herold Dehling" target="Reg Kulperger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="408" source="Daniel Vogel" target="Martin Wendler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="409" source="Daniel Vogel" target="Dominik Wied">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="410" source="Martin Wendler" target="Dominik Wied">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4871v5" />
          <attvalue for="2" value="Testing for Changes in Kendall's Tau" />
          <attvalue for="3" value="For a bivariate time series $((X_i,Y_i))_{i=1,...,n}$ we want to detect&#10;whether the correlation between $X_i$ and $Y_i$ stays constant for all $i =&#10;1,...,n$. We propose a nonparametric change-point test statistic based on&#10;Kendall's tau and derive its asymptotic distribution under the null hypothesis&#10;of no change by means a new U-statistic invariance principle for dependent&#10;processes. The asymptotic distribution depends on the long run variance of&#10;Kendall's tau, for which we propose an estimator and show its consistency.&#10;Furthermore, assuming a single change-point, we show that the location of the&#10;change-point is consistently estimated. Kendall's tau possesses a high&#10;efficiency at the normal distribution, as compared to the normal maximum&#10;likelihood estimator, Pearson's moment correlation coefficient. Contrary to&#10;Pearson's correlation coefficient, it has excellent robustness properties and&#10;shows no loss in efficiency at heavy-tailed distributions. We assume the data&#10;$((X_i,Y_i))_{i=1,...,n}$ to be stationary and P-near epoch dependent on an&#10;absolutely regular process. The P-near epoch dependence condition constitutes a&#10;generalization of the usually considered $L_p$-near epoch dependence, $p \ge&#10;1$, that does not require the existence of any moments. It is therefore very&#10;well suited for our objective to efficiently detect changes in correlation for&#10;arbitrarily heavy-tailed data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="411" source="Hans-Georg Müller" target="Fang Yao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2630v1" />
          <attvalue for="2" value="Empirical dynamics for longitudinal data" />
          <attvalue for="3" value="We demonstrate that the processes underlying on-line auction price bids and&#10;many other longitudinal data can be represented by an empirical first order&#10;stochastic ordinary differential equation with time-varying coefficients and a&#10;smooth drift process. This equation may be empirically obtained from&#10;longitudinal observations for a sample of subjects and does not presuppose&#10;specific knowledge of the underlying processes. For the nonparametric&#10;estimation of the components of the differential equation, it suffices to have&#10;available sparsely observed longitudinal measurements which may be noisy and&#10;are generated by underlying smooth random trajectories for each subject or&#10;experimental unit in the sample. The drift process that drives the equation&#10;determines how closely individual process trajectories follow a deterministic&#10;approximation of the differential equation. We provide estimates for&#10;trajectories and especially the variance function of the drift process. At each&#10;fixed time point, the proposed empirical dynamic model implies a decomposition&#10;of the derivative of the process underlying the longitudinal data into a&#10;component explained by a linear component determined by a varying coefficient&#10;function dynamic equation and an orthogonal complement that corresponds to the&#10;drift process. An enhanced perturbation result enables us to obtain improved&#10;asymptotic convergence rates for eigenfunction derivative estimation and&#10;consistency for the varying coefficient function and the components of the&#10;drift process. We illustrate the differential equation with an application to&#10;the dynamics of on-line auction data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="412" source="Hans-Georg Müller" target="Dong Chen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6040v1" />
          <attvalue for="2" value="Nonlinear manifold representations for functional data" />
          <attvalue for="3" value="For functional data lying on an unknown nonlinear low-dimensional space, we&#10;study manifold learning and introduce the notions of manifold mean, manifold&#10;modes of functional variation and of functional manifold components. These&#10;constitute nonlinear representations of functional data that complement&#10;classical linear representations such as eigenfunctions and functional&#10;principal components. Our manifold learning procedures borrow ideas from&#10;existing nonlinear dimension reduction methods, which we modify to address&#10;functional data settings. In simulations and applications, we study examples of&#10;functional data which lie on a manifold and validate the superior behavior of&#10;manifold mean and functional manifold components over traditional&#10;cross-sectional mean and functional principal components. We also include&#10;consistency proofs for our estimators under certain assumptions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="413" source="Randal Douc" target="Paul Doukhan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.4739v2" />
          <attvalue for="2" value="Ergodicity of observation-driven time series models and consistency of&#10;  the maximum likelihood estimator" />
          <attvalue for="3" value="This paper deals with a general class of observation-driven time series&#10;models with a special focus on time series of counts. We provide conditions&#10;under which there exist strict-sense stationary and ergodic versions of such&#10;processes. The consistency of the maximum likelihood estimators is then derived&#10;for well- specified and misspecified models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="414" source="Randal Douc" target="Eric Moulines">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6898v2" />
          <attvalue for="2" value="Long-term stability of sequential Monte Carlo methods under verifiable&#10;  conditions" />
          <attvalue for="3" value="This paper discusses particle filtering in general hidden Markov models&#10;(HMMs) and presents novel theoretical results on the long-term stability of&#10;bootstrap-type particle filters. More specifically, we establish that the&#10;asymptotic variance of the Monte Carlo estimates produced by the bootstrap&#10;filter is uniformly bounded in time. On the contrary to most previous results&#10;of this type, which in general presuppose that the state space of the hidden&#10;state process is compact (an assumption that is rarely satisfied in practice),&#10;our very mild assumptions are satisfied for a large class of HMMs with possibly&#10;noncompact state space. In addition, we derive a similar time uniform bound on&#10;the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for&#10;misspecified models; that is, we do not at all assume that the data entering&#10;into the particle filter originate from the model governing the dynamics of the&#10;particles or not even from an HMM." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="415" source="Randal Douc" target="Jimmy Olsson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6898v2" />
          <attvalue for="2" value="Long-term stability of sequential Monte Carlo methods under verifiable&#10;  conditions" />
          <attvalue for="3" value="This paper discusses particle filtering in general hidden Markov models&#10;(HMMs) and presents novel theoretical results on the long-term stability of&#10;bootstrap-type particle filters. More specifically, we establish that the&#10;asymptotic variance of the Monte Carlo estimates produced by the bootstrap&#10;filter is uniformly bounded in time. On the contrary to most previous results&#10;of this type, which in general presuppose that the state space of the hidden&#10;state process is compact (an assumption that is rarely satisfied in practice),&#10;our very mild assumptions are satisfied for a large class of HMMs with possibly&#10;noncompact state space. In addition, we derive a similar time uniform bound on&#10;the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for&#10;misspecified models; that is, we do not at all assume that the data entering&#10;into the particle filter originate from the model governing the dynamics of the&#10;particles or not even from an HMM." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="416" source="Paul Doukhan" target="Eric Moulines">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.4739v2" />
          <attvalue for="2" value="Ergodicity of observation-driven time series models and consistency of&#10;  the maximum likelihood estimator" />
          <attvalue for="3" value="This paper deals with a general class of observation-driven time series&#10;models with a special focus on time series of counts. We provide conditions&#10;under which there exist strict-sense stationary and ergodic versions of such&#10;processes. The consistency of the maximum likelihood estimators is then derived&#10;for well- specified and misspecified models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="417" source="Eric Moulines" target="Amandine Schreck">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0662v2" />
          <attvalue for="2" value="Adaptive Equi-Energy Sampler : Convergence and Illustration" />
          <attvalue for="3" value="Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known&#10;up to a multiplicative constant. Classical MCMC samplers are known to have very&#10;poor mixing properties when sampling multimodal distributions. The Equi-Energy&#10;sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006&#10;to sample difficult multimodal distributions. This algorithm runs several&#10;chains at different temperatures in parallel, and allow lower-tempered chains&#10;to jump to a state from a higher-tempered chain having an energy 'close' to&#10;that of the current state. A major drawback of this algorithm is that it&#10;depends on many design parameters and thus, requires a significant effort to&#10;tune these parameters. In this paper, we introduce an Adaptive Equi-Energy&#10;(AEE) sampler which automates the choice of the selection mecanism when jumping&#10;onto a state of the higher-temperature chain. We prove the ergodicity and a&#10;strong law of large numbers for AEE, and for the original Equi-Energy sampler&#10;as well. Finally, we apply our algorithm to motif sampling in DNA sequences." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="418" source="Eric Moulines" target="Gersende Fort">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0662v2" />
          <attvalue for="2" value="Adaptive Equi-Energy Sampler : Convergence and Illustration" />
          <attvalue for="3" value="Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known&#10;up to a multiplicative constant. Classical MCMC samplers are known to have very&#10;poor mixing properties when sampling multimodal distributions. The Equi-Energy&#10;sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006&#10;to sample difficult multimodal distributions. This algorithm runs several&#10;chains at different temperatures in parallel, and allow lower-tempered chains&#10;to jump to a state from a higher-tempered chain having an energy 'close' to&#10;that of the current state. A major drawback of this algorithm is that it&#10;depends on many design parameters and thus, requires a significant effort to&#10;tune these parameters. In this paper, we introduce an Adaptive Equi-Energy&#10;(AEE) sampler which automates the choice of the selection mecanism when jumping&#10;onto a state of the higher-temperature chain. We prove the ergodicity and a&#10;strong law of large numbers for AEE, and for the original Equi-Energy sampler&#10;as well. Finally, we apply our algorithm to motif sampling in DNA sequences." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="419" source="Eric Moulines" target="Jimmy Olsson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6898v2" />
          <attvalue for="2" value="Long-term stability of sequential Monte Carlo methods under verifiable&#10;  conditions" />
          <attvalue for="3" value="This paper discusses particle filtering in general hidden Markov models&#10;(HMMs) and presents novel theoretical results on the long-term stability of&#10;bootstrap-type particle filters. More specifically, we establish that the&#10;asymptotic variance of the Monte Carlo estimates produced by the bootstrap&#10;filter is uniformly bounded in time. On the contrary to most previous results&#10;of this type, which in general presuppose that the state space of the hidden&#10;state process is compact (an assumption that is rarely satisfied in practice),&#10;our very mild assumptions are satisfied for a large class of HMMs with possibly&#10;noncompact state space. In addition, we derive a similar time uniform bound on&#10;the asymptotic $\mathsf{L}^p$ error. Importantly, our results hold for&#10;misspecified models; that is, we do not at all assume that the data entering&#10;into the particle filter originate from the model governing the dynamics of the&#10;particles or not even from an HMM." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="420" source="Mathilde Mougeot" target="Karine Tribouley">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2067v1" />
          <attvalue for="2" value="Grouping Strategies and Thresholding for High Dimensional Linear Models" />
          <attvalue for="3" value="The estimation problem in a high regression model with structured sparsity is&#10;investigated. An algorithm using a two steps block thresholding procedure&#10;called GR-LOL is provided. Convergence rates are produced: they depend on&#10;simple coherence-type indices of the Gram matrix -easily checkable on the data-&#10;as well as sparsity assumptions of the model parameters measured by a&#10;combination of $l_1$ within-blocks with $l_q,q&lt;1$ between-blocks norms. The&#10;simplicity of the coherence indicator suggests ways to optimize the rates of&#10;convergence when the group structure is not naturally given by the problem and&#10;is unknown. In such a case, an auto-driven procedure is provided to determine&#10;the regressors groups (number and contents). An intensive practical study&#10;compares our grouping methods with the standard LOL algorithm. We prove that&#10;the grouping rarely deteriorates the results but can improve them very&#10;significantly. GR-LOL is also compared with group-Lasso procedures and exhibits&#10;a very encouraging behavior. The results are quite impressive, especially when&#10;GR-LOL algorithm is combined with a grouping pre-processing." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="421" source="M. Doostparast" target="N. Balakrishnan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.0638v1" />
          <attvalue for="2" value="Pareto analysis based on records" />
          <attvalue for="3" value="Estimation of the parameters of an exponential distribution based on record&#10;data has been treated by Samaniego and Whitaker (1986) and Doostparast (2009).&#10;Recently, Doostparast and Balakrishnan (2011) obtained optimal confidence&#10;intervals as well as uniformly most powerful tests for one- and two-sided&#10;hypotheses concerning location and scale parameters based on record data from a&#10;two-parameter exponential model. In this paper, we derive optimal statistical&#10;procedures including point and interval estimation as well as most powerful&#10;tests based on record data from a two-parameter Pareto model. For illustrative&#10;purpose, a data set on annual wages of a sample production-line workers in a&#10;large industrial firm is analyzed using the proposed procedures." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="422" source="N. Balakrishnan" target="M. Jafari Jozani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5270v1" />
          <attvalue for="2" value="Some Pitman Closeness Properties Pertinent to Symmetric Populations" />
          <attvalue for="3" value="In this paper, we focus on Pitman closeness probabilities when the estimators&#10;are symmetrically distributed about the unknown parameter $\theta$. We first&#10;consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and&#10;obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman&#10;closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish&#10;some properties in the context of estimation under Pitman closeness criterion.&#10;We define a Pitman closeness probability which measures the frequency with&#10;which an individual order statistic is Pitman closer to $\theta$ than some&#10;symmetric estimator. We show that, for symmetric populations, the sample median&#10;is Pitman closer to the population median than any other symmetrically&#10;distributed estimator of $\theta$. Finally, we discuss the use of Pitman&#10;closeness probabilities in the determination of an optimal ranked set sampling&#10;scheme (denoted by RSS) for the estimation of the population median when the&#10;underlying distribution is symmetric. We show that the best RSS scheme from&#10;symmetric populations in the sense of Pitman closeness is the median and&#10;randomized median RSS for the cases of odd and even sample sizes, respectively." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="423" source="N. Balakrishnan" target="K. F. Davies">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5270v1" />
          <attvalue for="2" value="Some Pitman Closeness Properties Pertinent to Symmetric Populations" />
          <attvalue for="3" value="In this paper, we focus on Pitman closeness probabilities when the estimators&#10;are symmetrically distributed about the unknown parameter $\theta$. We first&#10;consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and&#10;obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman&#10;closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish&#10;some properties in the context of estimation under Pitman closeness criterion.&#10;We define a Pitman closeness probability which measures the frequency with&#10;which an individual order statistic is Pitman closer to $\theta$ than some&#10;symmetric estimator. We show that, for symmetric populations, the sample median&#10;is Pitman closer to the population median than any other symmetrically&#10;distributed estimator of $\theta$. Finally, we discuss the use of Pitman&#10;closeness probabilities in the determination of an optimal ranked set sampling&#10;scheme (denoted by RSS) for the estimation of the population median when the&#10;underlying distribution is symmetric. We show that the best RSS scheme from&#10;symmetric populations in the sense of Pitman closeness is the median and&#10;randomized median RSS for the cases of odd and even sample sizes, respectively." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="424" source="Nelson Antunes" target="Vladas Pipiras">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1076v1" />
          <attvalue for="2" value="Probabilistic sampling of finite renewal processes" />
          <attvalue for="3" value="Consider a finite renewal process in the sense that interrenewal times are&#10;positive i.i.d. variables and the total number of renewals is a random&#10;variable, independent of interrenewal times. A finite point process can be&#10;obtained by probabilistic sampling of the finite renewal process, where each&#10;renewal is sampled with a fixed probability and independently of other&#10;renewals. The problem addressed in this work concerns statistical inference of&#10;the original distributions of the total number of renewals and interrenewal&#10;times from a sample of i.i.d. finite point processes obtained by sampling&#10;finite renewal processes. This problem is motivated by traffic measurements in&#10;the Internet in order to characterize flows of packets (which can be seen as&#10;finite renewal processes) and where the use of packet sampling is becoming&#10;prevalent due to increasing link speeds and limited storage and processing&#10;capacities." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="425" source="Subhajit Dutta" target="Anil K. Ghosh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1171v1" />
          <attvalue for="2" value="Some intriguing properties of Tukey's half-space depth" />
          <attvalue for="3" value="For multivariate data, Tukey's half-space depth is one of the most popular&#10;depth functions available in the literature. It is conceptually simple and&#10;satisfies several desirable properties of depth functions. The Tukey median,&#10;the multivariate median associated with the half-space depth, is also a&#10;well-known measure of center for multivariate data with several interesting&#10;properties. In this article, we derive and investigate some interesting&#10;properties of half-space depth and its associated multivariate median. These&#10;properties, some of which are counterintuitive, have important statistical&#10;consequences in multivariate analysis. We also investigate a natural extension&#10;of Tukey's half-space depth and the related median for probability&#10;distributions on any Banach space (which may be finite- or&#10;infinite-dimensional) and prove some results that demonstrate anomalous&#10;behavior of half-space depth in infinite-dimensional spaces." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="426" source="Subhajit Dutta" target="Probal Chaudhuri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1171v1" />
          <attvalue for="2" value="Some intriguing properties of Tukey's half-space depth" />
          <attvalue for="3" value="For multivariate data, Tukey's half-space depth is one of the most popular&#10;depth functions available in the literature. It is conceptually simple and&#10;satisfies several desirable properties of depth functions. The Tukey median,&#10;the multivariate median associated with the half-space depth, is also a&#10;well-known measure of center for multivariate data with several interesting&#10;properties. In this article, we derive and investigate some interesting&#10;properties of half-space depth and its associated multivariate median. These&#10;properties, some of which are counterintuitive, have important statistical&#10;consequences in multivariate analysis. We also investigate a natural extension&#10;of Tukey's half-space depth and the related median for probability&#10;distributions on any Banach space (which may be finite- or&#10;infinite-dimensional) and prove some results that demonstrate anomalous&#10;behavior of half-space depth in infinite-dimensional spaces." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="427" source="Anil K. Ghosh" target="Probal Chaudhuri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1171v1" />
          <attvalue for="2" value="Some intriguing properties of Tukey's half-space depth" />
          <attvalue for="3" value="For multivariate data, Tukey's half-space depth is one of the most popular&#10;depth functions available in the literature. It is conceptually simple and&#10;satisfies several desirable properties of depth functions. The Tukey median,&#10;the multivariate median associated with the half-space depth, is also a&#10;well-known measure of center for multivariate data with several interesting&#10;properties. In this article, we derive and investigate some interesting&#10;properties of half-space depth and its associated multivariate median. These&#10;properties, some of which are counterintuitive, have important statistical&#10;consequences in multivariate analysis. We also investigate a natural extension&#10;of Tukey's half-space depth and the related median for probability&#10;distributions on any Banach space (which may be finite- or&#10;infinite-dimensional) and prove some results that demonstrate anomalous&#10;behavior of half-space depth in infinite-dimensional spaces." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="428" source="Davy Paindaveine" target="Germain Van Bever">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2996v2" />
          <attvalue for="2" value="Nonparametrically consistent depth-based classifiers" />
          <attvalue for="3" value="We introduce a class of depth-based classification procedures that are of a&#10;nearest-neighbor nature. Depth, after symmetrization, indeed provides the&#10;center-outward ordering that is necessary and sufficient to define nearest&#10;neighbors. Like all their depth-based competitors, the resulting classifiers&#10;are affine-invariant, hence in particular are insensitive to unit changes.&#10;Unlike the former, however, the latter achieve Bayes consistency under&#10;virtually any absolutely continuous distributions - a concept we call&#10;nonparametric consistency, to stress the difference with the stronger universal&#10;consistency of the standard $k$NN classifiers. We investigate the finite-sample&#10;performances of the proposed classifiers through simulations and show that they&#10;outperform affine-invariant nearest-neighbor classifiers obtained through an&#10;obvious standardization construction. We illustrate the practical value of our&#10;classifiers on two real data examples. Finally, we shortly discuss the possible&#10;uses of our depth-based neighbors in other inference problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="429" source="Davy Paindaveine" target="Pauliina Ilmonen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5159v1" />
          <attvalue for="2" value="Semiparametrically efficient inference based on signed ranks in&#10;  symmetric independent component models" />
          <attvalue for="3" value="We consider semiparametric location-scatter models for which the $p$-variate&#10;observation is obtained as $X=\Lambda Z+\mu$, where $\mu$ is a $p$-vector,&#10;$\Lambda$ is a full-rank $p\times p$ matrix and the (unobserved) random&#10;$p$-vector $Z$ has marginals that are centered and mutually independent but are&#10;otherwise unspecified. As in blind source separation and independent component&#10;analysis (ICA), the parameter of interest throughout the paper is $\Lambda$. On&#10;the basis of $n$ i.i.d. copies of $X$, we develop, under a symmetry assumption&#10;on $Z$, signed-rank one-sample testing and estimation procedures for $\Lambda$.&#10;We exploit the uniform local and asymptotic normality (ULAN) of the model to&#10;define signed-rank procedures that are semiparametrically efficient under&#10;correctly specified densities. Yet, as is usual in rank-based inference, the&#10;proposed procedures remain valid (correct asymptotic size under the null, for&#10;hypothesis testing, and root-$n$ consistency, for point estimation) under a&#10;very broad range of densities. We derive the asymptotic properties of the&#10;proposed procedures and investigate their finite-sample behavior through&#10;simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="430" source="Davy Paindaveine" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2117v1" />
          <attvalue for="2" value="Optimal rank-based testing for principal components" />
          <attvalue for="3" value="This paper provides parametric and rank-based optimal tests for eigenvectors&#10;and eigenvalues of covariance or scatter matrices in elliptical families. The&#10;parametric tests extend the Gaussian likelihood ratio tests of Anderson (1963)&#10;and their pseudo-Gaussian robustifications by Davis (1977) and Tyler (1981,&#10;1983). The rank-based tests address a much broader class of problems, where&#10;covariance matrices need not exist and principal components are associated with&#10;more general scatter matrices. The proposed tests are shown to outperform daily&#10;practice both from the point of view of validity as from the point of view of&#10;efficiency. This is achieved by utilizing the Le Cam theory of locally&#10;asymptotically normal experiments, in the nonstandard context, however, of a&#10;curved parametrization. The results we derive for curved experiments are of&#10;independent interest, and likely to apply in other contexts." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="431" source="Ismaël Castillo" target="Aad van der Vaart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1197v1" />
          <attvalue for="2" value="Needles and Straw in a Haystack: Posterior concentration for possibly&#10;  sparse sequences" />
          <attvalue for="3" value="We consider full Bayesian inference in the multivariate normal mean model in&#10;the situation that the mean vector is sparse. The prior distribution on the&#10;vector of means is constructed hierarchically by first choosing a collection of&#10;nonzero means and next a prior on the nonzero values. We consider the posterior&#10;distribution in the frequentist set-up that the observations are generated&#10;according to a fixed mean vector, and are interested in the posterior&#10;distribution of the number of nonzero components and the contraction of the&#10;posterior distribution to the true mean vector. We find various combinations of&#10;priors on the number of nonzero coefficients and on these coefficients that&#10;give desirable performance. We also find priors that give suboptimal&#10;convergence, for instance, Gaussian priors on the nonzero coefficients. We&#10;illustrate the results by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="432" source="Brice Franke" target="Thomas Kott">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="433" source="Brice Franke" target="Reg Kulperger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="434" source="Thomas Kott" target="Reg Kulperger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.0610v2" />
          <attvalue for="2" value="Change Point Testing for the Drift Parameters of a Periodic Mean&#10;  Reversion Process" />
          <attvalue for="3" value="In this paper we investigate the problem of detecting a change in the drift&#10;parameters of a generalized Ornstein-Uhlenbeck process which is defined as the&#10;solution of $dX_t=(L(t)-\alpha X_t) dt + \sigma dB_t$, and which is observed in&#10;continuous time. We derive an explicit representation of the generalized&#10;likelihood ratio test statistic assuming that the mean reversion function&#10;$L(t)$ is a finite linear combination of known basis functions. In the case of&#10;a periodic mean reversion function, we determine the asymptotic distribution of&#10;the test statistic under the null hypothesis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="435" source="Lothar Heinrich" target="Sebastian Lück">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5044v1" />
          <attvalue for="2" value="Non-parametric asymptotic statistics for the Palm mark distribution of&#10;  β-mixing marked point processes" />
          <attvalue for="3" value="We consider spatially homogeneous marked point patterns in an unboundedly&#10;expanding convex sampling window. Our main objective is to identify the&#10;distribution of the typical mark by constructing an asymptotic&#10;\chi^2-goodness-of-fit test. The corresponding test statistic is based on a&#10;natural empirical version of the Palm mark distribution and a smoothed&#10;covariance estimator which turns out to be mean-square consistent. Our approach&#10;does not require independent marks and allows dependences between the mark&#10;field and the point pattern. Instead we impose a suitable \beta-mixing&#10;condition on the underlying stationary marked point process which can be&#10;checked for a number of Poisson-based models and, in particular, in the case of&#10;geostatistical marking. Our method needs a central limit theorem for&#10;\beta-mixing random fields which is proved by extending Bernstein's blocking&#10;technique to non-cubic index sets and seems to be of interest in its own right.&#10;By large-scale model-based simulations the performance of our test is studied&#10;in dependence of the model parameters which determine the range of spatial&#10;correlations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="436" source="Lothar Heinrich" target="Volker Schmidt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5044v1" />
          <attvalue for="2" value="Non-parametric asymptotic statistics for the Palm mark distribution of&#10;  β-mixing marked point processes" />
          <attvalue for="3" value="We consider spatially homogeneous marked point patterns in an unboundedly&#10;expanding convex sampling window. Our main objective is to identify the&#10;distribution of the typical mark by constructing an asymptotic&#10;\chi^2-goodness-of-fit test. The corresponding test statistic is based on a&#10;natural empirical version of the Palm mark distribution and a smoothed&#10;covariance estimator which turns out to be mean-square consistent. Our approach&#10;does not require independent marks and allows dependences between the mark&#10;field and the point pattern. Instead we impose a suitable \beta-mixing&#10;condition on the underlying stationary marked point process which can be&#10;checked for a number of Poisson-based models and, in particular, in the case of&#10;geostatistical marking. Our method needs a central limit theorem for&#10;\beta-mixing random fields which is proved by extending Bernstein's blocking&#10;technique to non-cubic index sets and seems to be of interest in its own right.&#10;By large-scale model-based simulations the performance of our test is studied&#10;in dependence of the model parameters which determine the range of spatial&#10;correlations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="437" source="Sebastian Lück" target="Volker Schmidt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5044v1" />
          <attvalue for="2" value="Non-parametric asymptotic statistics for the Palm mark distribution of&#10;  β-mixing marked point processes" />
          <attvalue for="3" value="We consider spatially homogeneous marked point patterns in an unboundedly&#10;expanding convex sampling window. Our main objective is to identify the&#10;distribution of the typical mark by constructing an asymptotic&#10;\chi^2-goodness-of-fit test. The corresponding test statistic is based on a&#10;natural empirical version of the Palm mark distribution and a smoothed&#10;covariance estimator which turns out to be mean-square consistent. Our approach&#10;does not require independent marks and allows dependences between the mark&#10;field and the point pattern. Instead we impose a suitable \beta-mixing&#10;condition on the underlying stationary marked point process which can be&#10;checked for a number of Poisson-based models and, in particular, in the case of&#10;geostatistical marking. Our method needs a central limit theorem for&#10;\beta-mixing random fields which is proved by extending Bernstein's blocking&#10;technique to non-cubic index sets and seems to be of interest in its own right.&#10;By large-scale model-based simulations the performance of our test is studied&#10;in dependence of the model parameters which determine the range of spatial&#10;correlations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="438" source="Viktor Todorov" target="George Tauchen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5615v1" />
          <attvalue for="2" value="Realized Laplace transforms for pure-jump semimartingales" />
          <attvalue for="3" value="We consider specification and inference for the stochastic scale of&#10;discretely-observed pure-jump semimartingales with locally stable L\'{e}vy&#10;densities in the setting where both the time span of the data set increases,&#10;and the mesh of the observation grid decreases. The estimation is based on&#10;constructing a nonparametric estimate for the empirical Laplace transform of&#10;the stochastic scale over a given interval of time by aggregating&#10;high-frequency increments of the observed process on that time interval into a&#10;statistic we call realized Laplace transform. The realized Laplace transform&#10;depends on the activity of the driving pure-jump martingale, and we consider&#10;both cases when the latter is known or has to be inferred from the data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="439" source="Marta Ferreira" target="Helena Ferreira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.7430v1" />
          <attvalue for="2" value="Extremal behavior of pMAX processes" />
          <attvalue for="3" value="The well-known M4 processes of Smith and Weissman are very flexible models&#10;for asymptotically dependent multivariate data. Extended M4 of Heffernan&#10;\emph{et al.} allows to also account for asymptotic independence. In this paper&#10;we introduce a more general multivariate model comprising asymptotic dependence&#10;and independence, which has the extended M4 class as a particular case. We&#10;study properties of the proposed model. In particular, we compute the&#10;multivariate extremal index, tail dependence and extremal coefficients." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="440" source="Helena Ferreira" target="João Renato Sebastião">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="441" source="Helena Ferreira" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="442" source="Helena Ferreira" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="443" source="Pierre Del Moral" target="Arnaud Doucet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2582v1" />
          <attvalue for="2" value="Sequentially interacting Markov chain Monte Carlo methods" />
          <attvalue for="3" value="Sequential Monte Carlo (SMC) is a methodology for sampling approximately from&#10;a sequence of probability distributions of increasing dimension and estimating&#10;their normalizing constants. We propose here an alternative methodology named&#10;Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work&#10;by generating interacting non-Markovian sequences which behave asymptotically&#10;like independent Metropolis-Hastings (MH) Markov chains with the desired&#10;limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively&#10;improve our estimates in an MCMC-like fashion. We establish convergence results&#10;under realistic verifiable assumptions and demonstrate its performance on&#10;several examples arising in Bayesian time series analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="444" source="Pierre Del Moral" target="Ajay Jasra">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0464v1" />
          <attvalue for="2" value="On adaptive resampling strategies for sequential Monte Carlo methods" />
          <attvalue for="3" value="Sequential Monte Carlo (SMC) methods are a class of techniques to sample&#10;approximately from any sequence of probability distributions using a&#10;combination of importance sampling and resampling steps. This paper is&#10;concerned with the convergence analysis of a class of SMC methods where the&#10;times at which resampling occurs are computed online using criteria such as the&#10;effective sample size. This is a popular approach amongst practitioners but&#10;there are very few convergence results available for these methods. By&#10;combining semigroup techniques with an original coupling argument, we obtain&#10;functional central limit theorems and uniform exponential concentration&#10;estimates for these algorithms." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="445" source="Pierre Del Moral" target="Anthony Brockwell">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2582v1" />
          <attvalue for="2" value="Sequentially interacting Markov chain Monte Carlo methods" />
          <attvalue for="3" value="Sequential Monte Carlo (SMC) is a methodology for sampling approximately from&#10;a sequence of probability distributions of increasing dimension and estimating&#10;their normalizing constants. We propose here an alternative methodology named&#10;Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work&#10;by generating interacting non-Markovian sequences which behave asymptotically&#10;like independent Metropolis-Hastings (MH) Markov chains with the desired&#10;limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively&#10;improve our estimates in an MCMC-like fashion. We establish convergence results&#10;under realistic verifiable assumptions and demonstrate its performance on&#10;several examples arising in Bayesian time series analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="446" source="Arnaud Doucet" target="Ajay Jasra">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0464v1" />
          <attvalue for="2" value="On adaptive resampling strategies for sequential Monte Carlo methods" />
          <attvalue for="3" value="Sequential Monte Carlo (SMC) methods are a class of techniques to sample&#10;approximately from any sequence of probability distributions using a&#10;combination of importance sampling and resampling steps. This paper is&#10;concerned with the convergence analysis of a class of SMC methods where the&#10;times at which resampling occurs are computed online using criteria such as the&#10;effective sample size. This is a popular approach amongst practitioners but&#10;there are very few convergence results available for these methods. By&#10;combining semigroup techniques with an original coupling argument, we obtain&#10;functional central limit theorems and uniform exponential concentration&#10;estimates for these algorithms." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="447" source="Arnaud Doucet" target="Anthony Brockwell">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2582v1" />
          <attvalue for="2" value="Sequentially interacting Markov chain Monte Carlo methods" />
          <attvalue for="3" value="Sequential Monte Carlo (SMC) is a methodology for sampling approximately from&#10;a sequence of probability distributions of increasing dimension and estimating&#10;their normalizing constants. We propose here an alternative methodology named&#10;Sequentially Interacting Markov Chain Monte Carlo (SIMCMC). SIMCMC methods work&#10;by generating interacting non-Markovian sequences which behave asymptotically&#10;like independent Metropolis-Hastings (MH) Markov chains with the desired&#10;limiting distributions. Contrary to SMC, SIMCMC allows us to iteratively&#10;improve our estimates in an MCMC-like fashion. We establish convergence results&#10;under realistic verifiable assumptions and demonstrate its performance on&#10;several examples arising in Bayesian time series analysis." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="448" source="Ole E. Barndorff-Nielsen" target="José Manuel Corcuera">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0868v1" />
          <attvalue for="2" value="Multipower variation for Brownian semistationary processes" />
          <attvalue for="3" value="In this paper we study the asymptotic behaviour of power and multipower&#10;variations of processes $Y$:\[Y_t=\int_{-\in&#10;fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where&#10;$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma &gt;0$ is a random&#10;process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process&#10;in the nature of a drift term. Processes of this type serve, in particular, to&#10;model data of velocity increments of a fluid in a turbulence regime with spot&#10;intermittency $\sigma$. The purpose of this paper is to determine the&#10;probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis&#10;for studying properties of the intermittency process $\sigma$. Notably the&#10;processes $Y$ are in general not of the semimartingale kind and the established&#10;theory of multipower variation for semimartingales does not suffice for&#10;deriving the limit properties. As a key tool for the results, a general central&#10;limit theorem for triangular Gaussian schemes is formulated and proved.&#10;Examples and an application to the realised variance ratio are given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="449" source="Ole E. Barndorff-Nielsen" target="Mark Podolskij">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0868v1" />
          <attvalue for="2" value="Multipower variation for Brownian semistationary processes" />
          <attvalue for="3" value="In this paper we study the asymptotic behaviour of power and multipower&#10;variations of processes $Y$:\[Y_t=\int_{-\in&#10;fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where&#10;$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma &gt;0$ is a random&#10;process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process&#10;in the nature of a drift term. Processes of this type serve, in particular, to&#10;model data of velocity increments of a fluid in a turbulence regime with spot&#10;intermittency $\sigma$. The purpose of this paper is to determine the&#10;probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis&#10;for studying properties of the intermittency process $\sigma$. Notably the&#10;processes $Y$ are in general not of the semimartingale kind and the established&#10;theory of multipower variation for semimartingales does not suffice for&#10;deriving the limit properties. As a key tool for the results, a general central&#10;limit theorem for triangular Gaussian schemes is formulated and proved.&#10;Examples and an application to the realised variance ratio are given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="450" source="José Manuel Corcuera" target="Mark Podolskij">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0868v1" />
          <attvalue for="2" value="Multipower variation for Brownian semistationary processes" />
          <attvalue for="3" value="In this paper we study the asymptotic behaviour of power and multipower&#10;variations of processes $Y$:\[Y_t=\int_{-\in&#10;fty}^tg(t-s)\sigma_sW(\mathrm{d}s)+Z_t,\] where&#10;$g:(0,\infty)\rightarrow\mathbb{R}$ is deterministic, $\sigma &gt;0$ is a random&#10;process, $W$ is the stochastic Wiener measure and $Z$ is a stochastic process&#10;in the nature of a drift term. Processes of this type serve, in particular, to&#10;model data of velocity increments of a fluid in a turbulence regime with spot&#10;intermittency $\sigma$. The purpose of this paper is to determine the&#10;probabilistic limit behaviour of the (multi)power variations of $Y$ as a basis&#10;for studying properties of the intermittency process $\sigma$. Notably the&#10;processes $Y$ are in general not of the semimartingale kind and the established&#10;theory of multipower variation for semimartingales does not suffice for&#10;deriving the limit properties. As a key tool for the results, a general central&#10;limit theorem for triangular Gaussian schemes is formulated and proved.&#10;Examples and an application to the realised variance ratio are given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="451" source="Jérémie Bigot" target="Thierry Klein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.2562v6" />
          <attvalue for="2" value="Characterization of barycenters in the Wasserstein space by averaging&#10;  optimal transport maps" />
          <attvalue for="3" value="This paper is concerned by the study of barycenters for random probability&#10;measures in the Wasserstein space. Using a duality argument, we give a precise&#10;characterization of the population barycenter for various parametric classes of&#10;random probability measures with compact support. In particular, we make a&#10;connection between averaging in the Wasserstein space as introduced in Agueh&#10;and Carlier (2011), and taking the expectation of optimal transport maps with&#10;respect to a fixed reference measure. We also discuss the usefulness of this&#10;approach in statistics for the analysis of deformable models in signal and&#10;image processing. In this setting, the problem of estimating a population&#10;barycenter from n independent and identically distributed random probability&#10;measures is also considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="452" source="Jérémie Bigot" target="Xavier Gendre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.0771v3" />
          <attvalue for="2" value="Minimax properties of Fréchet means of discretely sampled curves" />
          <attvalue for="3" value="We study the problem of estimating a mean pattern from a set of similar&#10;curves in the setting where the variability in the data is due to random&#10;geometric deformations and additive noise. We propose an estimator based on the&#10;notion of Frechet mean that is a generalization of the standard notion of&#10;averaging to non-Euclidean spaces. We derive a minimax rate for this estimation&#10;problem, and we show that our estimator achieves this optimal rate under the&#10;asymptotics where both the number of curves and the number of sampling points&#10;go to infinity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="453" source="Jérémie Bigot" target="Theofanis Sapatinas">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.7640v2" />
          <attvalue for="2" value="Nonparametric adaptive time-dependent multivariate function estimation" />
          <attvalue for="3" value="We consider the nonparametric estimation problem of time-dependent&#10;multivariate functions observed in a presence of additive cylindrical Gaussian&#10;white noise of a small intensity. We derive minimax lower bounds for the&#10;$L^2$-risk in the proposed spatio-temporal model as the intensity goes to zero,&#10;when the underlying unknown response function is assumed to belong to a ball of&#10;appropriately constructed inhomogeneous time-dependent multivariate functions,&#10;motivated by practical applications. Furthermore, we propose both non-adaptive&#10;linear and adaptive non-linear wavelet estimators that are asymptotically&#10;optimal (in the minimax sense) in a wide range of the so-constructed balls of&#10;inhomogeneous time-dependent multivariate functions. The usefulness of the&#10;suggested adaptive nonlinear wavelet estimator is illustrated with the help of&#10;simulated and real-data examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="454" source="Shaul K. Bar-Lev" target="Andreas Löpker">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5902v1" />
          <attvalue for="2" value="On the small-time behavior of subordinators" />
          <attvalue for="3" value="We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain&#10;$(0,\infty)$-valued stochastic processes $(Y_t)_{t&gt;0}$. In particular, we show&#10;for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only&#10;possible weak limit and provide necessary and sufficient conditions for the&#10;convergence. More generally, we also consider the weak convergence of $tL(Y_t)$&#10;as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.&#10;Various examples demonstrating the applicability of the results are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="455" source="Shaul K. Bar-Lev" target="Wolfgang Stadje">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5902v1" />
          <attvalue for="2" value="On the small-time behavior of subordinators" />
          <attvalue for="3" value="We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain&#10;$(0,\infty)$-valued stochastic processes $(Y_t)_{t&gt;0}$. In particular, we show&#10;for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only&#10;possible weak limit and provide necessary and sufficient conditions for the&#10;convergence. More generally, we also consider the weak convergence of $tL(Y_t)$&#10;as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.&#10;Various examples demonstrating the applicability of the results are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="456" source="Andreas Löpker" target="Wolfgang Stadje">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5902v1" />
          <attvalue for="2" value="On the small-time behavior of subordinators" />
          <attvalue for="3" value="We prove several results on the behavior near t=0 of $Y_t^{-t}$ for certain&#10;$(0,\infty)$-valued stochastic processes $(Y_t)_{t&gt;0}$. In particular, we show&#10;for L\'{e}vy subordinators that the Pareto law on $[1,\infty)$ is the only&#10;possible weak limit and provide necessary and sufficient conditions for the&#10;convergence. More generally, we also consider the weak convergence of $tL(Y_t)$&#10;as $t\to0$ for a decreasing function $L$ that is slowly varying at zero.&#10;Various examples demonstrating the applicability of the results are presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="457" source="Amandine Schreck" target="Gersende Fort">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0662v2" />
          <attvalue for="2" value="Adaptive Equi-Energy Sampler : Convergence and Illustration" />
          <attvalue for="3" value="Markov chain Monte Carlo (MCMC) methods allow to sample a distribution known&#10;up to a multiplicative constant. Classical MCMC samplers are known to have very&#10;poor mixing properties when sampling multimodal distributions. The Equi-Energy&#10;sampler is an interacting MCMC sampler proposed by Kou, Zhou and Wong in 2006&#10;to sample difficult multimodal distributions. This algorithm runs several&#10;chains at different temperatures in parallel, and allow lower-tempered chains&#10;to jump to a state from a higher-tempered chain having an energy 'close' to&#10;that of the current state. A major drawback of this algorithm is that it&#10;depends on many design parameters and thus, requires a significant effort to&#10;tune these parameters. In this paper, we introduce an Adaptive Equi-Energy&#10;(AEE) sampler which automates the choice of the selection mecanism when jumping&#10;onto a state of the higher-temperature chain. We prove the ergodicity and a&#10;strong law of large numbers for AEE, and for the original Equi-Energy sampler&#10;as well. Finally, we apply our algorithm to motif sampling in DNA sequences." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="458" source="Elena Di Bernadino" target="Thomas Laloë">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.2035v1" />
          <attvalue for="2" value="Estimating level sets of a distribution function using a plug-in method:&#10;  a multidimensional extension" />
          <attvalue for="3" value="This paper deals with the problem of estimating the level sets $L(c)= \{F(x)&#10;\geq c \}$, with $c \in (0,1)$, of an unknown distribution function $F$ on&#10;\mathbb{R}^d_+$. A plug-in approach is followed. That is, given a consistent&#10;estimator $F_n$ of $F$, we estimate $L(c)$ by $L_n(c)= \{F_n(x) \geq c \}$. We&#10;state consistency results with respect to the Hausdorff distance and the volume&#10;of the symmetric difference. These results can be considered as generalizations&#10;of results previously obtained, in a bivariate framework, in Di Bernardino et&#10;al. (2011). Finally we investigate the effects of scaling data on our&#10;consistency results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="459" source="Xianyang Zhang" target="Xiaofeng Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.4228v2" />
          <attvalue for="2" value="Fixed-smoothing asymptotics for time series" />
          <attvalue for="3" value="In this paper, we derive higher order Edgeworth expansions for the finite&#10;sample distributions of the subsampling-based t-statistic and the Wald&#10;statistic in the Gaussian location model under the so-called fixed-smoothing&#10;paradigm. In particular, we show that the error of asymptotic approximation is&#10;at the order of the reciprocal of the sample size and obtain explicit forms for&#10;the leading error terms in the expansions. The results are used to justify the&#10;second-order correctness of a new bootstrap method, the Gaussian dependent&#10;bootstrap, in the context of Gaussian location model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="460" source="Pongpol Ruankong" target="Tippawan Santiwipanont">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.0405v1" />
          <attvalue for="2" value="Shuffles of copulas and a new measure of dependence" />
          <attvalue for="3" value="Using a characterization of Mutual Complete Dependence copulas, we show that,&#10;with respect to the Sobolev norm, the MCD copulas can be approximated&#10;arbitrarily closed by shuffles of Min. This result is then used to obtain a&#10;characterization of generalized shuffles of copulas introduced by Durante,&#10;Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by&#10;Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the&#10;corresponding shuffles of the two continuous random variables, we define a new&#10;norm which is invariant under shuffling. This norm gives rise to a new measure&#10;of dependence which shares many properties with the maximal correlation&#10;coefficient, the only measure of dependence that satisfies all of R\'enyi's&#10;postulates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="461" source="Pongpol Ruankong" target="Songkiat Sumetkijakan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.0405v1" />
          <attvalue for="2" value="Shuffles of copulas and a new measure of dependence" />
          <attvalue for="3" value="Using a characterization of Mutual Complete Dependence copulas, we show that,&#10;with respect to the Sobolev norm, the MCD copulas can be approximated&#10;arbitrarily closed by shuffles of Min. This result is then used to obtain a&#10;characterization of generalized shuffles of copulas introduced by Durante,&#10;Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by&#10;Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the&#10;corresponding shuffles of the two continuous random variables, we define a new&#10;norm which is invariant under shuffling. This norm gives rise to a new measure&#10;of dependence which shares many properties with the maximal correlation&#10;coefficient, the only measure of dependence that satisfies all of R\'enyi's&#10;postulates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="462" source="Tippawan Santiwipanont" target="Songkiat Sumetkijakan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.0405v1" />
          <attvalue for="2" value="Shuffles of copulas and a new measure of dependence" />
          <attvalue for="3" value="Using a characterization of Mutual Complete Dependence copulas, we show that,&#10;with respect to the Sobolev norm, the MCD copulas can be approximated&#10;arbitrarily closed by shuffles of Min. This result is then used to obtain a&#10;characterization of generalized shuffles of copulas introduced by Durante,&#10;Sarkoci and Sempi in terms of MCD copulas and the $\star$-product discovered by&#10;Darsow, Nguyen and Olsen. Since shuffles of a copula is the copula of the&#10;corresponding shuffles of the two continuous random variables, we define a new&#10;norm which is invariant under shuffling. This norm gives rise to a new measure&#10;of dependence which shares many properties with the maximal correlation&#10;coefficient, the only measure of dependence that satisfies all of R\'enyi's&#10;postulates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="463" source="Ana Karina Fermin" target="Carenne Ludeña">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.4457v2" />
          <attvalue for="2" value="Probability bounds for active learning in the regression problem" />
          <attvalue for="3" value="In this article we consider the problem of choosing an optimal sampling&#10;scheme for the regression problem simultaneously with that of model selection.&#10;We consider a batch type approach and an on-line approach following algorithms&#10;recently developed for the classification problem. Our main tools are&#10;concentration-type inequalities which allow us to bound the supremum of the&#10;deviations of the sampling scheme corrected by an appropriate weight function." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="464" source="Bing-Yi Jing" target="Xin-Bing Kong">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0827v1" />
          <attvalue for="2" value="Modeling high-frequency financial data by pure jump processes" />
          <attvalue for="3" value="It is generally accepted that the asset price processes contain jumps. In&#10;fact, pure jump models have been widely used to model asset prices and/or&#10;stochastic volatilities. The question is: is there any statistical evidence&#10;from the high-frequency financial data to support using pure jump models alone?&#10;The purpose of this paper is to develop such a statistical test against the&#10;necessity of a diffusion component. The test is very simple to use and yet&#10;effective. Asymptotic properties of the proposed test statistic will be&#10;studied. Simulation studies and some real-life examples are included to&#10;illustrate our results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="465" source="Bing-Yi Jing" target="Zhi Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0827v1" />
          <attvalue for="2" value="Modeling high-frequency financial data by pure jump processes" />
          <attvalue for="3" value="It is generally accepted that the asset price processes contain jumps. In&#10;fact, pure jump models have been widely used to model asset prices and/or&#10;stochastic volatilities. The question is: is there any statistical evidence&#10;from the high-frequency financial data to support using pure jump models alone?&#10;The purpose of this paper is to develop such a statistical test against the&#10;necessity of a diffusion component. The test is very simple to use and yet&#10;effective. Asymptotic properties of the proposed test statistic will be&#10;studied. Simulation studies and some real-life examples are included to&#10;illustrate our results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="466" source="Bing-Yi Jing" target="Guangming Pan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="467" source="Bing-Yi Jing" target="Qi-Man Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="468" source="Bing-Yi Jing" target="Wang Zhou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="469" source="Xin-Bing Kong" target="Zhi Liu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0827v1" />
          <attvalue for="2" value="Modeling high-frequency financial data by pure jump processes" />
          <attvalue for="3" value="It is generally accepted that the asset price processes contain jumps. In&#10;fact, pure jump models have been widely used to model asset prices and/or&#10;stochastic volatilities. The question is: is there any statistical evidence&#10;from the high-frequency financial data to support using pure jump models alone?&#10;The purpose of this paper is to develop such a statistical test against the&#10;necessity of a diffusion component. The test is very simple to use and yet&#10;effective. Asymptotic properties of the proposed test statistic will be&#10;studied. Simulation studies and some real-life examples are included to&#10;illustrate our results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="470" source="Jana Jurečková" target="Jan Kalina">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0450v1" />
          <attvalue for="2" value="Nonparametric multivariate rank tests and their unbiasedness" />
          <attvalue for="3" value="Although unbiasedness is a basic property of a good test, many tests on&#10;vector parameters or scalar parameters against two-sided alternatives are not&#10;finite-sample unbiased. This was already noticed by Sugiura [Ann. Inst.&#10;Statist. Math. 17 (1965) 261--263]; he found an alternative against which the&#10;Wilcoxon test is not unbiased. The problem is even more serious in multivariate&#10;models. When testing the hypothesis against an alternative which fits well with&#10;the experiment, it should be verified whether the power of the test under this&#10;alternative cannot be smaller than the significance level. Surprisingly, this&#10;serious problem is not frequently considered in the literature. The present&#10;paper considers the two-sample multivariate testing problem. We construct&#10;several rank tests which are finite-sample unbiased against a broad class of&#10;location/scale alternatives and are finite-sample distribution-free under the&#10;hypothesis and alternatives. Each of them is locally most powerful against a&#10;specific alternative of the Lehmann type. Their powers against some&#10;alternatives are numerically compared with each other and with other rank and&#10;classical tests. The question of affine invariance of two-sample multivariate&#10;tests is also discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="471" source="Nicolas Jégou" target="Alexander B. Németh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="472" source="Nicolas Jégou" target="Sándor Z. Németh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="473" source="Alexander B. Németh" target="Sándor Z. Németh">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3930v2" />
          <attvalue for="2" value="A Geometrical Approach to Iterative Isotone Regression" />
          <attvalue for="3" value="In the present paper, we propose and analyze a novel method for estimating a&#10;univariate regression function of bounded variation. The underpinning idea is&#10;to combine two classical tools in nonparametric statistics, namely isotonic&#10;regression and the estimation of additive models. A geometrical interpretation&#10;enables us to link this iterative method with Von Neumann's algorithm.&#10;Moreover, making a connection with the general property of isotonicity of&#10;projection onto convex cones, we derive another equivalent algorithm and go&#10;further in the analysis. As iterating the algorithm leads to overfitting,&#10;several practical stopping criteria are also presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="474" source="Tatiane F. N. Melo" target="Alexandre G. Patriota">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.5039v2" />
          <attvalue for="2" value="Modified likelihood ratio tests in heteroskedastic multivariate&#10;  regression models with measurement error" />
          <attvalue for="3" value="In this paper, we develop modified versions of the likelihood ratio test for&#10;multivariate heteroskedastic errors-in-variables regression models. The error&#10;terms are allowed to follow a multivariate distribution in the elliptical class&#10;of distributions, which has the normal distribution as a special case. We&#10;derive the Skovgaard adjusted likelihood ratio statistics, which follow a&#10;chi-squared distribution with a high degree of accuracy. We conduct a&#10;simulation study and show that the proposed tests display superior finite&#10;sample behavior as compared to the standard likelihood ratio test. We&#10;illustrate the usefulness of our results in applied settings using a data set&#10;from the WHO MONICA Projection cardiovascular disease." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="475" source="Alexander Jung" target="Sebastian Schmutzhard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6516v2" />
          <attvalue for="2" value="The RKHS Approach to Minimum Variance Estimation Revisited: Variance&#10;  Bounds, Sufficient Statistics, and Exponential Families" />
          <attvalue for="3" value="The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides&#10;powerful tools for minimum variance estimation (MVE) problems. Here, we extend&#10;the classical RKHS based analysis of MVE in several directions. We develop a&#10;geometric formulation of five known lower bounds on the estimator variance&#10;(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya&#10;bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections&#10;onto a subspace of the RKHS associated with a given MVE problem. We show that,&#10;under mild conditions, the Barankin bound (the tightest possible lower bound on&#10;the estimator variance) is a lower semicontinuous function of the parameter&#10;vector. We also show that the RKHS associated with an MVE problem remains&#10;unchanged if the observation is replaced by a sufficient statistic. Finally,&#10;for MVE problems conforming to an exponential family of distributions, we&#10;derive novel closed-form lower bound on the estimator variance and show that a&#10;reduction of the parameter set leaves the minimum achievable variance&#10;unchanged." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="476" source="Alexander Jung" target="Franz Hlawatsch">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6516v2" />
          <attvalue for="2" value="The RKHS Approach to Minimum Variance Estimation Revisited: Variance&#10;  Bounds, Sufficient Statistics, and Exponential Families" />
          <attvalue for="3" value="The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides&#10;powerful tools for minimum variance estimation (MVE) problems. Here, we extend&#10;the classical RKHS based analysis of MVE in several directions. We develop a&#10;geometric formulation of five known lower bounds on the estimator variance&#10;(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya&#10;bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections&#10;onto a subspace of the RKHS associated with a given MVE problem. We show that,&#10;under mild conditions, the Barankin bound (the tightest possible lower bound on&#10;the estimator variance) is a lower semicontinuous function of the parameter&#10;vector. We also show that the RKHS associated with an MVE problem remains&#10;unchanged if the observation is replaced by a sufficient statistic. Finally,&#10;for MVE problems conforming to an exponential family of distributions, we&#10;derive novel closed-form lower bound on the estimator variance and show that a&#10;reduction of the parameter set leaves the minimum achievable variance&#10;unchanged." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="477" source="Sebastian Schmutzhard" target="Franz Hlawatsch">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6516v2" />
          <attvalue for="2" value="The RKHS Approach to Minimum Variance Estimation Revisited: Variance&#10;  Bounds, Sufficient Statistics, and Exponential Families" />
          <attvalue for="3" value="The mathematical theory of reproducing kernel Hilbert spaces (RKHS) provides&#10;powerful tools for minimum variance estimation (MVE) problems. Here, we extend&#10;the classical RKHS based analysis of MVE in several directions. We develop a&#10;geometric formulation of five known lower bounds on the estimator variance&#10;(Barankin bound, Cramer-Rao bound, constrained Cramer-Rao bound, Bhattacharyya&#10;bound, and Hammersley-Chapman-Robbins bound) in terms of orthogonal projections&#10;onto a subspace of the RKHS associated with a given MVE problem. We show that,&#10;under mild conditions, the Barankin bound (the tightest possible lower bound on&#10;the estimator variance) is a lower semicontinuous function of the parameter&#10;vector. We also show that the RKHS associated with an MVE problem remains&#10;unchanged if the observation is replaced by a sufficient statistic. Finally,&#10;for MVE problems conforming to an exponential family of distributions, we&#10;derive novel closed-form lower bound on the estimator variance and show that a&#10;reduction of the parameter set leaves the minimum achievable variance&#10;unchanged." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="478" source="Claire Lacour" target="Thanh Mai Pham Ngoc">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2008v2" />
          <attvalue for="2" value="Goodness-of-fit test for noisy directional data" />
          <attvalue for="3" value="We consider spherical data $X_i$ noised by a random rotation&#10;$\varepsilon_i\in$ SO(3) so that only the sample $Z_i=\varepsilon_iX_i$,&#10;$i=1,\dots, N$ is observed. We define a nonparametric test procedure to&#10;distinguish $H_0:$ ''the density $f$ of $X_i$ is the uniform density $f_0$ on&#10;the sphere'' and $H_1:$ ''$\|f-f_0\|_2^2\geq \C\psi_N$ and $f$ is in a Sobolev&#10;space with smoothness $s$''. For a noise density $f_\varepsilon$ with&#10;smoothness index $\nu$, we show that an adaptive procedure (i.e. $s$ is not&#10;assumed to be known) cannot have a faster rate of separation than&#10;$\psi_N^{ad}(s)=(N/\sqrt{\log\log(N)})^{-2s/(2s+2\nu+1)}$ and we provide a&#10;procedure which reaches this rate. We also deal with the case of super smooth&#10;noise. We illustrate the theory by implementing our test procedure for various&#10;kinds of noise on SO(3) and by comparing it to other procedures. Applications&#10;to real data in astrophysics and paleomagnetism are provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="479" source="Claire Lacour" target="Mélina Bec">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4692v2" />
          <attvalue for="2" value="Adaptive pointwise estimation for pure jump Lévy processes" />
          <attvalue for="3" value="This paper is concerned with adaptive kernel estimation of the L\'evy density&#10;N(x) for bounded-variation pure-jump L\'evy processes. The sample path is&#10;observed at n discrete instants in the &quot;high frequency&quot; context (\Delta =&#10;\Delta(n) tends to zero while n\Delta tends to infinity). We construct a&#10;collection of kernel estimators of the function g(x)=xN(x) and propose a method&#10;of local adaptive selection of the bandwidth. We provide an oracle inequality&#10;and a rate of convergence for the quadratic pointwise risk. This rate is proved&#10;to be the optimal minimax rate. We give examples and simulation results for&#10;processes fitting in our framework. We also consider the case of irregular&#10;sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="480" source="Wen Cao" target="Clifford Hurvich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5372v2" />
          <attvalue for="2" value="Drift in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We study the effect of drift in pure-jump transaction-level models for asset&#10;prices in continuous time, driven by point processes. The drift is as-sumed to&#10;arise from a nonzero mean in the efficient shock series. It follows that the&#10;drift is proportional to the driving point process itself, i.e. the cumulative&#10;number of transactions. This link reveals a mechanism by which properties of&#10;intertrade durations (such as heavy tails and long memory) can have a strong&#10;impact on properties of average returns, thereby poten-tially making it&#10;extremely difficult to determine long-term growth rates or to reliably detect&#10;an equity premium. We focus on a basic univariate model for log price, coupled&#10;with general assumptions on the point process that are satisfied by several&#10;existing flexible models, allowing for both long mem-ory and heavy tails in&#10;durations. Under our pure-jump model, we obtain the limiting distribution for&#10;the suitably normalized log price. This limiting distribution need not be&#10;Gaussian, and may have either finite variance or infinite variance. We show&#10;that the drift can affect not only the limiting dis-tribution for the&#10;normalized log price, but also the rate in the corresponding normalization.&#10;Therefore, the drift (or equivalently, the properties of dura-tions) affects&#10;the rate of convergence of estimators of the growth rate, and can invalidate&#10;standard hypothesis tests for that growth rate. As a rem-edy to these problems,&#10;we propose a new ratio statistic which behaves more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="481" source="Wen Cao" target="Philippe Soulier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5372v2" />
          <attvalue for="2" value="Drift in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We study the effect of drift in pure-jump transaction-level models for asset&#10;prices in continuous time, driven by point processes. The drift is as-sumed to&#10;arise from a nonzero mean in the efficient shock series. It follows that the&#10;drift is proportional to the driving point process itself, i.e. the cumulative&#10;number of transactions. This link reveals a mechanism by which properties of&#10;intertrade durations (such as heavy tails and long memory) can have a strong&#10;impact on properties of average returns, thereby poten-tially making it&#10;extremely difficult to determine long-term growth rates or to reliably detect&#10;an equity premium. We focus on a basic univariate model for log price, coupled&#10;with general assumptions on the point process that are satisfied by several&#10;existing flexible models, allowing for both long mem-ory and heavy tails in&#10;durations. Under our pure-jump model, we obtain the limiting distribution for&#10;the suitably normalized log price. This limiting distribution need not be&#10;Gaussian, and may have either finite variance or infinite variance. We show&#10;that the drift can affect not only the limiting dis-tribution for the&#10;normalized log price, but also the rate in the corresponding normalization.&#10;Therefore, the drift (or equivalently, the properties of dura-tions) affects&#10;the rate of convergence of estimators of the growth rate, and can invalidate&#10;standard hypothesis tests for that growth rate. As a rem-edy to these problems,&#10;we propose a new ratio statistic which behaves more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="482" source="Clifford Hurvich" target="Philippe Soulier">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5372v2" />
          <attvalue for="2" value="Drift in Transaction-Level Asset Price Models" />
          <attvalue for="3" value="We study the effect of drift in pure-jump transaction-level models for asset&#10;prices in continuous time, driven by point processes. The drift is as-sumed to&#10;arise from a nonzero mean in the efficient shock series. It follows that the&#10;drift is proportional to the driving point process itself, i.e. the cumulative&#10;number of transactions. This link reveals a mechanism by which properties of&#10;intertrade durations (such as heavy tails and long memory) can have a strong&#10;impact on properties of average returns, thereby poten-tially making it&#10;extremely difficult to determine long-term growth rates or to reliably detect&#10;an equity premium. We focus on a basic univariate model for log price, coupled&#10;with general assumptions on the point process that are satisfied by several&#10;existing flexible models, allowing for both long mem-ory and heavy tails in&#10;durations. Under our pure-jump model, we obtain the limiting distribution for&#10;the suitably normalized log price. This limiting distribution need not be&#10;Gaussian, and may have either finite variance or infinite variance. We show&#10;that the drift can affect not only the limiting dis-tribution for the&#10;normalized log price, but also the rate in the corresponding normalization.&#10;Therefore, the drift (or equivalently, the properties of dura-tions) affects&#10;the rate of convergence of estimators of the growth rate, and can invalidate&#10;standard hypothesis tests for that growth rate. As a rem-edy to these problems,&#10;we propose a new ratio statistic which behaves more" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="483" source="Isaac Skog" target="Peter Händel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.3516v1" />
          <attvalue for="2" value="Bayesian Estimation with Distance Bounds" />
          <attvalue for="3" value="We consider the problem of estimating a random state vector when there is&#10;information about the maximum distances between its subvectors. The estimation&#10;problem is posed in a Bayesian framework in which the minimum mean square error&#10;(MMSE) estimate of the state is given by the conditional mean. Since finding&#10;the conditional mean requires multidimensional integration, an approximate MMSE&#10;estimator is proposed. The performance of the proposed estimator is evaluated&#10;in a positioning problem. Finally, the application of the estimator in&#10;inequality constrained recursive filtering is illustrated by applying the&#10;estimator to a dead-reckoning problem. The MSE of the estimator is compared&#10;with two related posterior Cram\'er-Rao bounds." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="484" source="Arun Chandrasekhar" target="Victor Chernozhukov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="485" source="Arun Chandrasekhar" target="Francesca Molinari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="486" source="Arun Chandrasekhar" target="Paul Schrimpf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="487" source="Victor Chernozhukov" target="Francesca Molinari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="488" source="Victor Chernozhukov" target="Paul Schrimpf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="489" source="Francesca Molinari" target="Paul Schrimpf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.5627v1" />
          <attvalue for="2" value="Inference for best linear approximations to set identified functions" />
          <attvalue for="3" value="This paper provides inference methods for best linear approximations to&#10;functions which are known to lie within a band. It extends the partial&#10;identification literature by allowing the upper and lower functions defining&#10;the band to be any functions, including ones carrying an index, which can be&#10;estimated parametrically or non-parametrically. The identification region of&#10;the parameters of the best linear approximation is characterized via its&#10;support function, and limit theory is developed for the latter. We prove that&#10;the support function approximately converges to a Gaussian process and&#10;establish validity of the Bayesian bootstrap. The paper nests as special cases&#10;the canonical examples in the literature: mean regression with interval valued&#10;outcome data and interval valued regressor data. Because the bounds may carry&#10;an index, the paper covers problems beyond mean regression; the framework is&#10;extremely versatile. Applications include quantile and distribution regression&#10;with interval valued data, sample selection problems, as well as mean,&#10;quantile, and distribution treatment effects. Moreover, the framework can&#10;account for the availability of instruments. An application is carried out,&#10;studying female labor force participation along the lines of Mulligan and&#10;Rubinstein (2008)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="490" source="Nathan Huntley" target="Matthias C. M. Troffaes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.1154v1" />
          <attvalue for="2" value="Identifying subtree perfectness in decision trees" />
          <attvalue for="3" value="In decision problems, often, utilities and probabilities are hard to&#10;determine. In such cases, one can resort to so-called choice functions. They&#10;provide a means to determine which options in a particular set are optimal, and&#10;allow incomparability among any number of options. Applying choice functions in&#10;sequential decision problems can be highly non-trivial, as the usual properties&#10;of maximising expected utility may no longer be satisfied. In this paper, we&#10;study one of these properties: we revisit and reinterpret Selten's concept of&#10;subgame perfectness in the context of decision trees, leading us to the concept&#10;of subtree perfectness, which basically says that the optimal solution of a&#10;decision tree should not depend on any larger tree it may be embedded in. In&#10;other words, subtree perfectness excludes counterfactual reasoning, and&#10;therefore may be desirable from some philosophical points of view. Subtree&#10;perfectness is also desirable from a practical point of view, because it admits&#10;efficient algorithms for solving decision trees, such as backward induction.&#10;The main contribution of this paper is a very simple non-technical criterion&#10;for determining whether any given choice function will satisfy subtree&#10;perfectness or not. We demonstrate the theorem and illustrate subtree&#10;perfectness, or the lack thereof, through numerous examples, for a wide variety&#10;of choice functions, where incomparability amongst strategies can be caused by&#10;imprecision in either probabilities or utilities. We find that almost no choice&#10;function, except for maximising expected utility, satisfies it in general. We&#10;also find that choice functions other than maximising expected utility can&#10;satisfy it, provided that we restrict either the structure of the tree, or the&#10;structure of the choice function." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="491" source="Sándor Baran" target="Gyula Pap">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4346v1" />
          <attvalue for="2" value="Testing stability in a spatial unilateral autoregressive model" />
          <attvalue for="3" value="Least squares estimator of the stability parameter $\varrho := |\alpha| +&#10;|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha&#10;X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.&#10;Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable&#10;case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1&#10;X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares&#10;estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is&#10;not asymptotically normal in the unstable, i.e., in the unit root case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="492" source="Sándor Baran" target="Kinga Sikolya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4346v1" />
          <attvalue for="2" value="Testing stability in a spatial unilateral autoregressive model" />
          <attvalue for="3" value="Least squares estimator of the stability parameter $\varrho := |\alpha| +&#10;|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha&#10;X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.&#10;Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable&#10;case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1&#10;X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares&#10;estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is&#10;not asymptotically normal in the unstable, i.e., in the unit root case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="493" source="Gyula Pap" target="Kinga Sikolya">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.4346v1" />
          <attvalue for="2" value="Testing stability in a spatial unilateral autoregressive model" />
          <attvalue for="3" value="Least squares estimator of the stability parameter $\varrho := |\alpha| +&#10;|\beta|$ for a spatial unilateral autoregressive process $X_{k,\ell}=\alpha&#10;X_{k-1,\ell}+\beta X_{k,\ell-1}+\varepsilon_{k,\ell}$ is investigated.&#10;Asymptotic normality with a scaling factor $n^{5/4}$ is shown in the unstable&#10;case, i.e., when $\varrho = 1$, in contrast to the AR(p) model $X_k=\alpha_1&#10;X_{k-1}+... +\alpha_p X_{k-p}+ \varepsilon_k$, where the least squares&#10;estimator of the stability parameter $\varrho :=\alpha_1 + ... + \alpha_p$ is&#10;not asymptotically normal in the unstable, i.e., in the unit root case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="494" source="Runlong Tang" target="Moulinath Banerjee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6055v1" />
          <attvalue for="2" value="Likelihood based inference for current status data on a grid: A boundary&#10;  phenomenon and an adaptive inference procedure" />
          <attvalue for="3" value="In this paper, we study the nonparametric maximum likelihood estimator for an&#10;event time distribution function at a point in the current status model with&#10;observation times supported on a grid of potentially unknown sparsity and with&#10;multiple subjects sharing the same observation time. This is of interest since&#10;observation time ties occur frequently with current status data. The grid&#10;resolution is specified as $cn^{-\gamma}$ with $c&gt;0$ being a scaling constant&#10;and $\gamma&gt;0$ regulating the sparsity of the grid relative to $n$, the number&#10;of subjects. The asymptotic behavior falls into three cases depending on&#10;$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma&lt;1/3$,&#10;nonstandard cube-root asymptotics prevail when $\gamma&gt;1/3$ and $\gamma=1/3$&#10;serves as a boundary at which the transition happens. The limit distribution at&#10;the boundary is different from either of the previous cases and converges&#10;weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as&#10;$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to&#10;develop an adaptive procedure to construct confidence intervals for the value&#10;of the event time distribution at a point of interest without needing to know&#10;or estimate $\gamma$, which is of enormous advantage from the perspective of&#10;inference. A simulation study of the adaptive procedure is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="495" source="Runlong Tang" target="Michael R. Kosorok">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6055v1" />
          <attvalue for="2" value="Likelihood based inference for current status data on a grid: A boundary&#10;  phenomenon and an adaptive inference procedure" />
          <attvalue for="3" value="In this paper, we study the nonparametric maximum likelihood estimator for an&#10;event time distribution function at a point in the current status model with&#10;observation times supported on a grid of potentially unknown sparsity and with&#10;multiple subjects sharing the same observation time. This is of interest since&#10;observation time ties occur frequently with current status data. The grid&#10;resolution is specified as $cn^{-\gamma}$ with $c&gt;0$ being a scaling constant&#10;and $\gamma&gt;0$ regulating the sparsity of the grid relative to $n$, the number&#10;of subjects. The asymptotic behavior falls into three cases depending on&#10;$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma&lt;1/3$,&#10;nonstandard cube-root asymptotics prevail when $\gamma&gt;1/3$ and $\gamma=1/3$&#10;serves as a boundary at which the transition happens. The limit distribution at&#10;the boundary is different from either of the previous cases and converges&#10;weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as&#10;$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to&#10;develop an adaptive procedure to construct confidence intervals for the value&#10;of the event time distribution at a point of interest without needing to know&#10;or estimate $\gamma$, which is of enormous advantage from the perspective of&#10;inference. A simulation study of the adaptive procedure is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="496" source="Moulinath Banerjee" target="Michael R. Kosorok">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6055v1" />
          <attvalue for="2" value="Likelihood based inference for current status data on a grid: A boundary&#10;  phenomenon and an adaptive inference procedure" />
          <attvalue for="3" value="In this paper, we study the nonparametric maximum likelihood estimator for an&#10;event time distribution function at a point in the current status model with&#10;observation times supported on a grid of potentially unknown sparsity and with&#10;multiple subjects sharing the same observation time. This is of interest since&#10;observation time ties occur frequently with current status data. The grid&#10;resolution is specified as $cn^{-\gamma}$ with $c&gt;0$ being a scaling constant&#10;and $\gamma&gt;0$ regulating the sparsity of the grid relative to $n$, the number&#10;of subjects. The asymptotic behavior falls into three cases depending on&#10;$\gamma$: regular Gaussian-type asymptotics obtain for $\gamma&lt;1/3$,&#10;nonstandard cube-root asymptotics prevail when $\gamma&gt;1/3$ and $\gamma=1/3$&#10;serves as a boundary at which the transition happens. The limit distribution at&#10;the boundary is different from either of the previous cases and converges&#10;weakly to those obtained with $\gamma\in(0,1/3)$ and $\gamma\in(1/3,\infty)$ as&#10;$c$ goes to $\infty$ and 0, respectively. This weak convergence allows us to&#10;develop an adaptive procedure to construct confidence intervals for the value&#10;of the event time distribution at a point of interest without needing to know&#10;or estimate $\gamma$, which is of enormous advantage from the perspective of&#10;inference. A simulation study of the adaptive procedure is presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="497" source="Michael R. Kosorok" target="Yair Goldberg">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6659v1" />
          <attvalue for="2" value="Q-learning with censored data" />
          <attvalue for="3" value="We develop methodology for a multistage decision problem with flexible number&#10;of stages in which the rewards are survival times that are subject to&#10;censoring. We present a novel Q-learning algorithm that is adjusted for&#10;censored data and allows a flexible number of stages. We provide finite sample&#10;bounds on the generalization error of the policy learned by the algorithm, and&#10;show that when the optimal Q-function belongs to the approximation space, the&#10;expected survival time for policies obtained by the algorithm converges to that&#10;of the optimal policy. We simulate a multistage clinical trial with flexible&#10;number of stages and apply the proposed censored-Q-learning algorithm to find&#10;individualized treatment regimens. The methodology presented in this paper has&#10;implications in the design of personalized medicine trials in cancer and in&#10;other life-threatening diseases." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="498" source="J. Gao" target="Y. Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="499" source="J. Gao" target="M. Guo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="500" source="Y. Yang" target="M. Guo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6607v1" />
          <attvalue for="2" value="Independence Test for High Dimensional Random Vectors" />
          <attvalue for="3" value="This paper proposes a new mutual independence test for a large number of high&#10;dimensional random vectors. The test statistic is based on the characteristic&#10;function of the empirical spectral distribution of the sample covariance&#10;matrix. The asymptotic distributions of the test statistic under the null and&#10;local alternative hypotheses are established as dimensionality and the sample&#10;size of the data are comparable. We apply this test to examine multiple MA(1)&#10;and AR(1) models, panel data models with some spatial cross-sectional&#10;structures. In addition, in a flexible applied fashion, the proposed test can&#10;capture some dependent but uncorrelated structures, for example, nonlinear&#10;MA(1) models, multiple ARCH(1) models and vandermonde matrices.&#10;  Simulation results are provided for detecting these dependent structures. An&#10;empirical study of dependence between closed stock prices of several companies&#10;from New York Stock Exchange (NYSE) demonstrates that the feature of&#10;cross--sectional dependence is popular in stock markets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="501" source="Julyan Arbel" target="Ghislaine Gayraud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2392v2" />
          <attvalue for="2" value="Bayesian optimal adaptive estimation using a sieve prior" />
          <attvalue for="3" value="We derive rates of contraction of posterior distributions on nonparametric&#10;models resulting from sieve priors. The aim of the paper is to provide general&#10;conditions to get posterior rates when the parameter space has a general&#10;structure, and rate adaptation when the parameter space is, e.g., a Sobolev&#10;class. The conditions employed, although standard in the literature, are&#10;combined in a different way. The results are applied to density, regression,&#10;nonlinear autoregression and Gaussian white noise models. In the latter we have&#10;also considered a loss function which is different from the usual l2 norm,&#10;namely the pointwise loss. In this case it is possible to prove that the&#10;adaptive Bayesian approach for the l2 loss is strongly suboptimal and we&#10;provide a lower bound on the rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="502" source="Hiroki Hashiguchi" target="Yasuhide Numata">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="503" source="Hiroki Hashiguchi" target="Nobuki Takayama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="504" source="Hiroki Hashiguchi" target="Akimichi Takemura">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="505" source="Yasuhide Numata" target="Nobuki Takayama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="506" source="Yasuhide Numata" target="Akimichi Takemura">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="507" source="Nobuki Takayama" target="Akimichi Takemura">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0472v4" />
          <attvalue for="2" value="Holonomic gradient method for the distribution function of the largest&#10;  root of a Wishart matrix" />
          <attvalue for="3" value="We apply the holonomic gradient method introduced by Nakayama et al.(2011) to&#10;the evaluation of the exact distribution function of the largest root of a&#10;Wishart matrix, which involves a hypergeometric function 1F1 of a matrix&#10;argument. Numerical evaluation of the hypergeometric function has been one of&#10;the longstanding problems in multivariate distribution theory. The holonomic&#10;gradient method offers a totally new approach, which is complementary to the&#10;infinite series expansion around the origin in terms of zonal polynomials. It&#10;allows us to move away from the origin by the use of partial differential&#10;equations satisfied by the hypergeometric function. From numerical viewpoint we&#10;show that the method works well up to dimension 10. From theoretical viewpoint&#10;the method offers many challenging problems both to statistics and D-module&#10;theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="508" source="Nobuki Takayama" target="Tamio Koyama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="509" source="Nobuki Takayama" target="Hiromasa Nakayama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="510" source="Nobuki Takayama" target="Kenta Nishiyama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="511" source="Y. Ritov" target="P. J. Bickel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="512" source="Y. Ritov" target="A. C. Gamst">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="513" source="Y. Ritov" target="B. J. K. Kleijn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="514" source="P. J. Bickel" target="A. C. Gamst">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="515" source="P. J. Bickel" target="B. J. K. Kleijn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="516" source="A. C. Gamst" target="B. J. K. Kleijn">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5471v4" />
          <attvalue for="2" value="The Bayesian Analysis of Complex, High-Dimensional Models: Can It Be&#10;  CODA?" />
          <attvalue for="3" value="We consider the Bayesian analysis of a few complex, high-dimensional models&#10;and show that intuitive priors, which are not tailored to the fine details of&#10;the model and the estimated parameters, produce estimators which perform poorly&#10;in situations in which good, simple frequentist estimators exist. The models we&#10;consider are: stratified sampling, the partial linear model, linear and&#10;quadratic functionals of white noise and estimation with stopping times. We&#10;present a strong version of Doob's consistency theorem which demonstrates that&#10;the existence of a uniformly $\sqrt{n}$-consistent estimator ensures that the&#10;Bayes posterior is $\sqrt{n}$-consistent for values of the parameter in subsets&#10;of prior probability 1. We also demonstrate that it is, at least, in principle,&#10;possible to construct Bayes priors giving both global and local minimax rates,&#10;using a suitable combination of loss functions. We argue that there is no&#10;contradiction in these apparently conflicting findings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="517" source="David Källberg" target="Oleg Seleznjev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.2544v4" />
          <attvalue for="2" value="Estimation of entropy-type integral functionals" />
          <attvalue for="3" value="Entropy-type integral functionals of densities are widely used in&#10;mathematical statistics, information theory, and computer science. Examples&#10;include measures of closeness between distributions (e.g., density power&#10;divergence) and uncertainty characteristics for a random variable (e.g.,&#10;R\'enyi entropy). In this paper, we study U-statistic estimators for a class of&#10;such functionals. The estimators are based on epsilon-close vector observations&#10;in the corresponding independent and identically distributed samples. We prove&#10;asymptotic properties of the estimators (consistency and asymptotic normality)&#10;under mild integrability and smoothness conditions for the densities. The&#10;results can be applied in diverse problems in mathematical statistics and&#10;computer science (e.g., distribution identification problems, approximate&#10;matching for random databases, two-sample problems)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="518" source="T. Tony Cai" target="Zongming Ma">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4219v2" />
          <attvalue for="2" value="Optimal hypothesis testing for high dimensional covariance matrices" />
          <attvalue for="3" value="This paper considers testing a covariance matrix $\Sigma$ in the high&#10;dimensional setting where the dimension $p$ can be comparable or much larger&#10;than the sample size $n$. The problem of testing the hypothesis&#10;$H_0:\Sigma=\Sigma_0$ for a given covariance matrix $\Sigma_0$ is studied from&#10;a minimax point of view. We first characterize the boundary that separates the&#10;testable region from the non-testable region by the Frobenius norm when the&#10;ratio between the dimension $p$ over the sample size $n$ is bounded. A test&#10;based on a $U$-statistic is introduced and is shown to be rate optimal over&#10;this asymptotic regime. Furthermore, it is shown that the power of this test&#10;uniformly dominates that of the corrected likelihood ratio test (CLRT) over the&#10;entire asymptotic regime under which the CLRT is applicable. The power of the&#10;$U$-statistic based test is also analyzed when $p/n$ is unbounded." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="519" source="T. Tony Cai" target="Yihong Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1309v4" />
          <attvalue for="2" value="Sparse PCA: Optimal rates and adaptive estimation" />
          <attvalue for="3" value="Principal component analysis (PCA) is one of the most commonly used&#10;statistical procedures with a wide range of applications. This paper considers&#10;both minimax and adaptive estimation of the principal subspace in the high&#10;dimensional setting. Under mild technical conditions, we first establish the&#10;optimal rates of convergence for estimating the principal subspace which are&#10;sharp with respect to all the parameters, thus providing a complete&#10;characterization of the difficulty of the estimation problem in term of the&#10;convergence rate. The lower bound is obtained by calculating the local metric&#10;entropy and an application of Fano's lemma. The rate optimal estimator is&#10;constructed using aggregation, which, however, might not be computationally&#10;feasible. We then introduce an adaptive procedure for estimating the principal&#10;subspace which is fully data driven and can be computed efficiently. It is&#10;shown that the estimator attains the optimal rates of convergence&#10;simultaneously over a large collection of the parameter spaces. A key idea in&#10;our construction is a reduction scheme which reduces the sparse PCA problem to&#10;a high-dimensional multivariate regression problem. This method is potentially&#10;also useful for other related problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="520" source="Zongming Ma" target="Yihong Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1309v4" />
          <attvalue for="2" value="Sparse PCA: Optimal rates and adaptive estimation" />
          <attvalue for="3" value="Principal component analysis (PCA) is one of the most commonly used&#10;statistical procedures with a wide range of applications. This paper considers&#10;both minimax and adaptive estimation of the principal subspace in the high&#10;dimensional setting. Under mild technical conditions, we first establish the&#10;optimal rates of convergence for estimating the principal subspace which are&#10;sharp with respect to all the parameters, thus providing a complete&#10;characterization of the difficulty of the estimation problem in term of the&#10;convergence rate. The lower bound is obtained by calculating the local metric&#10;entropy and an application of Fano's lemma. The rate optimal estimator is&#10;constructed using aggregation, which, however, might not be computationally&#10;feasible. We then introduce an adaptive procedure for estimating the principal&#10;subspace which is fully data driven and can be computed efficiently. It is&#10;shown that the estimator attains the optimal rates of convergence&#10;simultaneously over a large collection of the parameter spaces. A key idea in&#10;our construction is a reduction scheme which reduces the sparse PCA problem to&#10;a high-dimensional multivariate regression problem. This method is potentially&#10;also useful for other related problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="521" source="Guillaume Lecué" target="Shahar Mendelson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0871v1" />
          <attvalue for="2" value="General nonexact oracle inequalities for classes with a subexponential&#10;  envelope" />
          <attvalue for="3" value="We show that empirical risk minimization procedures and regularized empirical&#10;risk minimization procedures satisfy nonexact oracle inequalities in an&#10;unbounded framework, under the assumption that the class has a subexponential&#10;envelope function. The main novelty, in addition to the boundedness assumption&#10;free setup, is that those inequalities can yield fast rates even in situations&#10;in which exact oracle inequalities only hold with slower rates. We apply these&#10;results to show that procedures based on $\ell_1$ and nuclear norms&#10;regularization functions satisfy oracle inequalities with a residual term that&#10;decreases like $1/n$ for every $L_q$-loss functions ($q\geq2$), while only&#10;assuming that the tail behavior of the input and output variables are well&#10;behaved. In particular, no RIP type of assumption or &quot;incoherence condition&quot;&#10;are needed to obtain fast residual terms in those setups. We also apply these&#10;results to the problems of convex aggregation and model selection." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="522" source="Chris Lloyd" target="Paul Kabaila">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3594v1" />
          <attvalue for="2" value="Letter to the Editor: Some comments on: On construction of the smallest&#10;  one-sided confidence interval for the difference of two proportions" />
          <attvalue for="3" value="Letter to the Editor: Some comments on &quot;On construction of the smallest&#10;one-sided confidence interval for the difference of two proportions&quot; by Weizhen&#10;Wang [arXiv:1002.4945]." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="523" source="Paul Kabaila" target="Davide Farchione">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.1093v2" />
          <attvalue for="2" value="Confidence intervals in regression centred on the SCAD estimator" />
          <attvalue for="3" value="Consider a linear regression model. Fan and Li (2001) describe the smoothly&#10;clipped absolute deviation (SCAD) point estimator of the regression parameter&#10;vector. To gain insight into the properties of this estimator, they consider an&#10;orthonormal design matrix and focus on the estimation of a specified component&#10;of this vector. They show that the SCAD point estimator has three attractive&#10;properties. We answer the question: To what extent can an interval estimator,&#10;centred on the SCAD estimator, have similar attractive properties?" />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="524" source="Abhishek Bhattacharya" target="Arup Bose">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1166v1" />
          <attvalue for="2" value="Resampling in Time Series Models" />
          <attvalue for="3" value="This project revolves around studying estimators for parameters in different&#10;Time Series models and studying their assymptotic properties. We introduce&#10;various bootstrap techniques for the estimators obtained. Our special emphasis&#10;is on Weighted Bootstrap. We establish the consistency of this scheme in a AR&#10;model and its variations. Numerical calculations lend further support to our&#10;consistency results. Next we analyze ARCH models, and study various estimators&#10;used for different error distributions. We also present resampling techniques&#10;for estimating the distribution of the estimators. Finally by simulating data,&#10;we analyze the numerical properties of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="525" source="G. Fort" target="E. Moulines">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3036v1" />
          <attvalue for="2" value="Convergence of adaptive and interacting Markov chain Monte Carlo&#10;  algorithms" />
          <attvalue for="3" value="Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been&#10;recently introduced in the literature. These novel simulation algorithms are&#10;designed to increase the simulation efficiency to sample complex distributions.&#10;Motivated by some recently introduced algorithms (such as the adaptive&#10;Metropolis algorithm and the interacting tempering algorithm), we develop a&#10;general methodological and theoretical framework to establish both the&#10;convergence of the marginal distribution and a strong law of large numbers.&#10;This framework weakens the conditions introduced in the pioneering paper by&#10;Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the&#10;case when the target distribution $\pi$ is sampled by using Markov transition&#10;kernels with a stationary distribution that differs from $\pi$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="526" source="G. Fort" target="P. Priouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3036v1" />
          <attvalue for="2" value="Convergence of adaptive and interacting Markov chain Monte Carlo&#10;  algorithms" />
          <attvalue for="3" value="Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been&#10;recently introduced in the literature. These novel simulation algorithms are&#10;designed to increase the simulation efficiency to sample complex distributions.&#10;Motivated by some recently introduced algorithms (such as the adaptive&#10;Metropolis algorithm and the interacting tempering algorithm), we develop a&#10;general methodological and theoretical framework to establish both the&#10;convergence of the marginal distribution and a strong law of large numbers.&#10;This framework weakens the conditions introduced in the pioneering paper by&#10;Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the&#10;case when the target distribution $\pi$ is sampled by using Markov transition&#10;kernels with a stationary distribution that differs from $\pi$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="527" source="E. Moulines" target="P. Priouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3036v1" />
          <attvalue for="2" value="Convergence of adaptive and interacting Markov chain Monte Carlo&#10;  algorithms" />
          <attvalue for="3" value="Adaptive and interacting Markov chain Monte Carlo algorithms (MCMC) have been&#10;recently introduced in the literature. These novel simulation algorithms are&#10;designed to increase the simulation efficiency to sample complex distributions.&#10;Motivated by some recently introduced algorithms (such as the adaptive&#10;Metropolis algorithm and the interacting tempering algorithm), we develop a&#10;general methodological and theoretical framework to establish both the&#10;convergence of the marginal distribution and a strong law of large numbers.&#10;This framework weakens the conditions introduced in the pioneering paper by&#10;Roberts and Rosenthal [J. Appl. Probab. 44 (2007) 458--475]. It also covers the&#10;case when the target distribution $\pi$ is sampled by using Markov transition&#10;kernels with a stationary distribution that differs from $\pi$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="528" source="Aboubacar Amiri" target="Christophe Crambes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2780v2" />
          <attvalue for="2" value="Recursive estimation of nonparametric regression with functional&#10;  covariate" />
          <attvalue for="3" value="The main purpose is to estimate the regression function of a real random&#10;variable with functional explanatory variable by using a recursive&#10;nonparametric kernel approach. The mean square error and the almost sure&#10;convergence of a family of recursive kernel estimates of the regression&#10;function are derived. These results are established with rates and precise&#10;evaluation of the constant terms. Also, a central limit theorem for this class&#10;of estimators is established. The method is evaluated on simulations and real&#10;data set studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="529" source="Aboubacar Amiri" target="Baba Thiam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2780v2" />
          <attvalue for="2" value="Recursive estimation of nonparametric regression with functional&#10;  covariate" />
          <attvalue for="3" value="The main purpose is to estimate the regression function of a real random&#10;variable with functional explanatory variable by using a recursive&#10;nonparametric kernel approach. The mean square error and the almost sure&#10;convergence of a family of recursive kernel estimates of the regression&#10;function are derived. These results are established with rates and precise&#10;evaluation of the constant terms. Also, a central limit theorem for this class&#10;of estimators is established. The method is evaluated on simulations and real&#10;data set studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="530" source="Christophe Crambes" target="Baba Thiam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2780v2" />
          <attvalue for="2" value="Recursive estimation of nonparametric regression with functional&#10;  covariate" />
          <attvalue for="3" value="The main purpose is to estimate the regression function of a real random&#10;variable with functional explanatory variable by using a recursive&#10;nonparametric kernel approach. The mean square error and the almost sure&#10;convergence of a family of recursive kernel estimates of the regression&#10;function are derived. These results are established with rates and precise&#10;evaluation of the constant terms. Also, a central limit theorem for this class&#10;of estimators is established. The method is evaluated on simulations and real&#10;data set studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="531" source="Y. Maleki" target="S. Rezakhah">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2831v1" />
          <attvalue for="2" value="The Scale Invariant Wigner Spectrum Estimation of Gaussian Locally&#10;  Self-Similar Processes" />
          <attvalue for="3" value="We study locally self-similar processes (LSSPs) in Silverman's sense. By&#10;deriving the minimum mean-square optimal kernel within Cohen's class&#10;counterpart of time-frequency representations, we obtain an optimal estimation&#10;for the scale invariant Wigner spectrum (SIWS) of Gaussian LSSPs. The class of&#10;estimators is completely characterized in terms of kernels, so the optimal&#10;kernel minimizes the mean-square error of the estimation. We obtain the SIWS&#10;estimation for two cases: global and local, where in the local case, the kernel&#10;is allowed to vary with time and frequency. We also introduce two&#10;generalizations of LSSPs: the locally self-similar chrip process and the&#10;multicomponent locally self-similar process, and obtain their optimal kernels.&#10;Finally, the performance and accuracy of the estimation is studied via&#10;simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="532" source="S. Rezakhah" target="A. Kohansal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.3238v2" />
          <attvalue for="2" value="Two New Entropy Estimators for Testing Exponentiality with Type-II&#10;  Censored Data" />
          <attvalue for="3" value="This paper proposes two estimators of the joint entropy of the Type-II&#10;censored data. Consistency of both estimators is proved. Simulation results&#10;show that the second one shows less bias and root of mean square error (RMSE)&#10;than leading estimator. Also, two goodness of fit test statistics based on the&#10;Kullback-Leibler information with the Type-II censored data are established and&#10;their performances with the leading test statistics are compared. We provide a&#10;Monte Carlo simulation study which shows that the test statistics&#10;$T^{(1)}_{m,n,r}$ and $T^{(2)}_{m,n,r}$ show better powers than leading test&#10;statistics against the alternatives with monotone decreasing and monotone&#10;increasing hazard functions, respectively." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="533" source="Caroline Uhler" target="Garvesh Raskutti">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="534" source="Caroline Uhler" target="Bin Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="535" source="Garvesh Raskutti" target="Bin Yu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.0547v3" />
          <attvalue for="2" value="Geometry of the faithfulness assumption in causal inference" />
          <attvalue for="3" value="Many algorithms for inferring causality rely heavily on the faithfulness&#10;assumption. The main justification for imposing this assumption is that the set&#10;of unfaithful distributions has Lebesgue measure zero, since it can be seen as&#10;a collection of hypersurfaces in a hypercube. However, due to sampling error&#10;the faithfulness condition alone is not sufficient for statistical estimation,&#10;and strong-faithfulness has been proposed and assumed to achieve uniform or&#10;high-dimensional consistency. In contrast to the plain faithfulness assumption,&#10;the set of distributions that is not strong-faithful has nonzero Lebesgue&#10;measure and in fact, can be surprisingly large as we show in this paper. We&#10;study the strong-faithfulness condition from a geometric and combinatorial&#10;point of view and give upper and lower bounds on the Lebesgue measure of&#10;strong-faithful distributions for various classes of directed acyclic graphs.&#10;Our results imply fundamental limitations for the PC-algorithm and potentially&#10;also for other algorithms based on partial correlation testing in the Gaussian&#10;case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="536" source="Xinjia Chen" target="Zhengjia Chen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1912v3" />
          <attvalue for="2" value="Exact Sample Size Methods for Estimating Parameters of Discrete&#10;  Distributions" />
          <attvalue for="3" value="In this paper, we develop an approach for the exact determination of the&#10;minimum sample size for estimating the parameter of an integer-valued random&#10;variable, which is parameterized by its expectation. Under some continuity and&#10;unimodal property assumptions, the exact computation is accomplished by&#10;reducing infinite many evaluations of coverage probability to finite many&#10;evaluations. Such a reduction is based on our discovery that the minimum of&#10;coverage probability with respect to the parameter bounded in an interval is&#10;attained at a discrete set of finite many values." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="537" source="István Berkes" target="Lajos Horváth">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1124v1" />
          <attvalue for="2" value="Asymptotics of trimmed CUSUM statistics" />
          <attvalue for="3" value="There is a wide literature on change point tests, but the case of variables&#10;with infinite variances is essentially unexplored. In this paper we address&#10;this problem by studying the asymptotic behavior of trimmed CUSUM statistics.&#10;We show that in a location model with i.i.d. errors in the domain of attraction&#10;of a stable law of parameter $0&lt;\alpha &lt;2$, the appropriately trimmed CUSUM&#10;process converges weakly to a Brownian bridge. Thus, after moderate trimming,&#10;the classical method for detecting change points remains valid also for&#10;populations with infinite variance. We note that according to the classical&#10;theory, the partial sums of trimmed variables are generally not asymptotically&#10;normal and using random centering in the test statistics is crucial in the&#10;infinite variance case. We also show that the partial sums of truncated and&#10;trimmed random variables have different asymptotic behavior. Finally, we&#10;discuss resampling procedures which enable one to determine critical values in&#10;the case of small and moderate sample sizes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="538" source="István Berkes" target="Johannes Schauer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1124v1" />
          <attvalue for="2" value="Asymptotics of trimmed CUSUM statistics" />
          <attvalue for="3" value="There is a wide literature on change point tests, but the case of variables&#10;with infinite variances is essentially unexplored. In this paper we address&#10;this problem by studying the asymptotic behavior of trimmed CUSUM statistics.&#10;We show that in a location model with i.i.d. errors in the domain of attraction&#10;of a stable law of parameter $0&lt;\alpha &lt;2$, the appropriately trimmed CUSUM&#10;process converges weakly to a Brownian bridge. Thus, after moderate trimming,&#10;the classical method for detecting change points remains valid also for&#10;populations with infinite variance. We note that according to the classical&#10;theory, the partial sums of trimmed variables are generally not asymptotically&#10;normal and using random centering in the test statistics is crucial in the&#10;infinite variance case. We also show that the partial sums of truncated and&#10;trimmed random variables have different asymptotic behavior. Finally, we&#10;discuss resampling procedures which enable one to determine critical values in&#10;the case of small and moderate sample sizes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="539" source="Lajos Horváth" target="Johannes Schauer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1124v1" />
          <attvalue for="2" value="Asymptotics of trimmed CUSUM statistics" />
          <attvalue for="3" value="There is a wide literature on change point tests, but the case of variables&#10;with infinite variances is essentially unexplored. In this paper we address&#10;this problem by studying the asymptotic behavior of trimmed CUSUM statistics.&#10;We show that in a location model with i.i.d. errors in the domain of attraction&#10;of a stable law of parameter $0&lt;\alpha &lt;2$, the appropriately trimmed CUSUM&#10;process converges weakly to a Brownian bridge. Thus, after moderate trimming,&#10;the classical method for detecting change points remains valid also for&#10;populations with infinite variance. We note that according to the classical&#10;theory, the partial sums of trimmed variables are generally not asymptotically&#10;normal and using random centering in the test statistics is crucial in the&#10;infinite variance case. We also show that the partial sums of truncated and&#10;trimmed random variables have different asymptotic behavior. Finally, we&#10;discuss resampling procedures which enable one to determine critical values in&#10;the case of small and moderate sample sizes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="540" source="Angela Blanco-Fernández" target="Marta García-Bárzana">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="541" source="Angela Blanco-Fernández" target="Ana Colubi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="542" source="Angela Blanco-Fernández" target="Erricos J. Kontoghiorghes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="543" source="Marta García-Bárzana" target="Ana Colubi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="544" source="Marta García-Bárzana" target="Erricos J. Kontoghiorghes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="545" source="Ana Colubi" target="Erricos J. Kontoghiorghes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.5881v1" />
          <attvalue for="2" value="Extensions of linear regression models based on set arithmetic for&#10;  interval data" />
          <attvalue for="3" value="Extensions of previous linear regression models for interval data are&#10;presented. A more flexible simple linear model is formalized. The new model may&#10;express cross-relationships between mid-points and spreads of the interval data&#10;in a unique equation based on the interval arithmetic. Moreover, extensions to&#10;the multiple case are addressed. The associated least-squares estimation&#10;problem are solved. Empirical results and a real-life application are presented&#10;in order to show the applicability and the differences among the proposed&#10;models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="546" source="Valentin Patilea" target="Cesar Sanchez-Sellero">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.2085v2" />
          <attvalue for="2" value="Nonparametric testing for no-effect with functional responses and&#10;  functional covariates" />
          <attvalue for="3" value="This paper examines the problem of nonparametric testing for the no-effect of&#10;a random covariate (or predictor) on a functional response. This means testing&#10;whether the conditional expectation of the response given the covariate is&#10;almost surely zero or not, without imposing any model relating response and&#10;covariate. The covariate could be univariate, multivariate or functional. Our&#10;test statistic is a quadratic form involving univariate nearest neighbor&#10;smoothing and the asymptotic critical values are given by the standard normal&#10;law. When the covariate is multidimensional or functional, a preliminary&#10;dimension reduction device is used which allows the effect of the covariate to&#10;be summarized into a univariate random quantity. The test is able to detect not&#10;only linear but nonparametric alternatives. The responses could have&#10;conditional variance of unknown form and the law of the covariate does not need&#10;to be known. An empirical study with simulated and real data shows that the&#10;test performs well in applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="547" source="Valentin Patilea" target="Matthieu Saumard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.2085v2" />
          <attvalue for="2" value="Nonparametric testing for no-effect with functional responses and&#10;  functional covariates" />
          <attvalue for="3" value="This paper examines the problem of nonparametric testing for the no-effect of&#10;a random covariate (or predictor) on a functional response. This means testing&#10;whether the conditional expectation of the response given the covariate is&#10;almost surely zero or not, without imposing any model relating response and&#10;covariate. The covariate could be univariate, multivariate or functional. Our&#10;test statistic is a quadratic form involving univariate nearest neighbor&#10;smoothing and the asymptotic critical values are given by the standard normal&#10;law. When the covariate is multidimensional or functional, a preliminary&#10;dimension reduction device is used which allows the effect of the covariate to&#10;be summarized into a univariate random quantity. The test is able to detect not&#10;only linear but nonparametric alternatives. The responses could have&#10;conditional variance of unknown form and the law of the covariate does not need&#10;to be known. An empirical study with simulated and real data shows that the&#10;test performs well in applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="548" source="Valentin Patilea" target="Loïc Hervé">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2947v1" />
          <attvalue for="2" value="A uniform Berry--Esseen theorem on $M$-estimators for geometrically&#10;  ergodic Markov chains" />
          <attvalue for="3" value="Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some&#10;real-valued functional $F$, define&#10;$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,&#10;$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator&#10;$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying&#10;$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with&#10;$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some&#10;standard regularity and moment assumptions, close to those of the i.i.d. case,&#10;the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with&#10;respect to the underlying probability distribution of the Markov chain." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="549" source="Valentin Patilea" target="James Ledoux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2947v1" />
          <attvalue for="2" value="A uniform Berry--Esseen theorem on $M$-estimators for geometrically&#10;  ergodic Markov chains" />
          <attvalue for="3" value="Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some&#10;real-valued functional $F$, define&#10;$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,&#10;$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator&#10;$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying&#10;$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with&#10;$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some&#10;standard regularity and moment assumptions, close to those of the i.i.d. case,&#10;the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with&#10;respect to the underlying probability distribution of the Markov chain." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="550" source="Cesar Sanchez-Sellero" target="Matthieu Saumard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.2085v2" />
          <attvalue for="2" value="Nonparametric testing for no-effect with functional responses and&#10;  functional covariates" />
          <attvalue for="3" value="This paper examines the problem of nonparametric testing for the no-effect of&#10;a random covariate (or predictor) on a functional response. This means testing&#10;whether the conditional expectation of the response given the covariate is&#10;almost surely zero or not, without imposing any model relating response and&#10;covariate. The covariate could be univariate, multivariate or functional. Our&#10;test statistic is a quadratic form involving univariate nearest neighbor&#10;smoothing and the asymptotic critical values are given by the standard normal&#10;law. When the covariate is multidimensional or functional, a preliminary&#10;dimension reduction device is used which allows the effect of the covariate to&#10;be summarized into a univariate random quantity. The test is able to detect not&#10;only linear but nonparametric alternatives. The responses could have&#10;conditional variance of unknown form and the law of the covariate does not need&#10;to be known. An empirical study with simulated and real data shows that the&#10;test performs well in applications." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="551" source="Alois Kneip" target="Pascal Sarda">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5151v1" />
          <attvalue for="2" value="Factor models and variable selection in high-dimensional regression&#10;  analysis" />
          <attvalue for="3" value="The paper considers linear regression problems where the number of predictor&#10;variables is possibly larger than the sample size. The basic motivation of the&#10;study is to combine the points of view of model selection and functional&#10;regression by using a factor approach: it is assumed that the predictor vector&#10;can be decomposed into a sum of two uncorrelated random components reflecting&#10;common factors and specific variabilities of the explanatory variables. It is&#10;shown that the traditional assumption of a sparse vector of parameters is&#10;restrictive in this context. Common factors may possess a significant influence&#10;on the response variable which cannot be captured by the specific effects of a&#10;small number of individual variables. We therefore propose to include principal&#10;components as additional explanatory variables in an augmented regression&#10;model. We give finite sample inequalities for estimates of these components. It&#10;is then shown that model selection procedures can be used to estimate the&#10;parameters of the augmented model, and we derive theoretical properties of the&#10;estimators. Finite sample performance is illustrated by a simulation study." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="552" source="Axel Bücher" target="Martin Ruppert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1675v1" />
          <attvalue for="2" value="Consistent testing for a constant copula under strong mixing based on&#10;  the tapered block multiplier technique" />
          <attvalue for="3" value="Considering multivariate strongly mixing time series, nonparametric tests for&#10;a constant copula with specified or unspecified change point (candidate) are&#10;derived; the tests are consistent against general alternatives. A tapered block&#10;multiplier technique based on serially dependent multiplier random variables is&#10;provided to estimate p-values of the test statistics. Size and power of the&#10;tests in finite samples are evaluated with Monte Carlo simulations. The block&#10;multiplier technique might have several other applications for statistical&#10;inference on copulas of serially dependent data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="553" source="Liliana Forzani" target="Adam J. Rothman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6556v1" />
          <attvalue for="2" value="Estimating sufficient reductions of the predictors in abundant&#10;  high-dimensional regressions" />
          <attvalue for="3" value="We study the asymptotic behavior of a class of methods for sufficient&#10;dimension reduction in high-dimension regressions, as the sample size and&#10;number of predictors grow in various alignments. It is demonstrated that these&#10;methods are consistent in a variety of settings, particularly in abundant&#10;regressions where most predictors contribute some information on the response,&#10;and oracle rates are possible. Simulation results are presented to support the&#10;theoretical conclusion." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="554" source="Pedro C. Álvarez-Esteban" target="Eustasio del Barrio">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="555" source="Pedro C. Álvarez-Esteban" target="Juan A. Cuesta-Albertos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="556" source="Pedro C. Álvarez-Esteban" target="Carlos Matrán">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="557" source="Eustasio del Barrio" target="Juan A. Cuesta-Albertos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="558" source="Eustasio del Barrio" target="Carlos Matrán">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="559" source="Juan A. Cuesta-Albertos" target="Carlos Matrán">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1950v1" />
          <attvalue for="2" value="Similarity of samples and trimming" />
          <attvalue for="3" value="We say that two probabilities are similar at level $\alpha$ if they are&#10;contaminated versions (up to an $\alpha$ fraction) of the same common&#10;probability. We show how this model is related to minimal distances between&#10;sets of trimmed probabilities. Empirical versions turn out to present an&#10;overfitting effect in the sense that trimming beyond the similarity level&#10;results in trimmed samples that are closer than expected to each other. We show&#10;how this can be combined with a bootstrap approach to assess similarity from&#10;two data samples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="560" source="Davit Varron" target="Ingrid Van Keilegom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.5507v1" />
          <attvalue for="2" value="Uniform in bandwidth exact rates for a class of kernel estimators" />
          <attvalue for="3" value="Given an i.i.d sample $(Y_i,Z_i)$, taking values in $\RRR^{d'}\times \RRR^d$,&#10;we consider a collection Nadarya-Watson kernel estimators of the conditional&#10;expectations $\EEE(&lt;c_g(z),g(Y)&gt;+d_g(z)\mid Z=z)$, where $z$ belongs to a&#10;compact set $H\subset \RRR^d$, $g$ a Borel function on $\RRR^{d'}$ and&#10;$c_g(\cdot),d_g(\cdot)$ are continuous functions on $\RRR^d$. Given two&#10;bandwidth sequences $h_n&lt;\wth_n$ fulfilling mild conditions, we obtain an exact&#10;and explicit almost sure limit bounds for the deviations of these estimators&#10;around their expectations, uniformly in $g\in\GG,\;z\in H$ and $h_n\le h\le&#10;\wth_n$ under mild conditions on the density $f_Z$, the class $\GG$, the kernel&#10;$K$ and the functions $c_g(\cdot),d_g(\cdot)$. We apply this result to prove&#10;that smoothed empirical likelihood can be used to build confidence intervals&#10;for conditional probabilities $\PPP(Y\in C\mid Z=z)$, that hold uniformly in&#10;$z\in H,\; C\in \CC,\; h\in [h_n,\wth_n]$. Here $\CC$ is a Vapnik-Chervonenkis&#10;class of sets." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="561" source="Davit Varron" target="Myriam Maumy">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.5528v1" />
          <attvalue for="2" value="Non standard functional limit laws for the increments of the compound&#10;  empirical distribution function" />
          <attvalue for="3" value="Let $(Y_i,Z_i)_{i\geq 1}$ be a sequence of independent, identically&#10;distributed (i.i.d.) random vectors taking values in $\RRR^k\times\RRR^d$, for&#10;some integers $k$ and $d$. Given $z\in \RRR^d$, we provide a nonstandard&#10;functional limit law for the sequence of functional increments of the compound&#10;empirical process, namely $$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot):=&#10;\frac{1}{nh_n}\sliin 1_{[0,\cdot)}\poo \frac{Z_i-z}{{h_n}^{1/d}}\pff Y_i.$$&#10;Provided that $nh_n\sim c\log n $ as $\nif$, we obtain, under some natural&#10;conditions on the conditional exponential moments of $Y\mid Z=z$, that&#10;$$\mathbf{\Delta}_{n,\cc}(h_n,z,\cdot)\leadsto \Gam\text{almost surely},$$&#10;where $\leadsto$ denotes the clustering process under the sup norm on $\Idd$.&#10;Here, $\Gam$ is a compact set that is related to the large deviations of&#10;certain compound Poisson processes." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="562" source="Magalie Fromont" target="Béatrice Laurent">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3572v2" />
          <attvalue for="2" value="The two-sample problem for Poisson processes: adaptive tests with a&#10;  non-asymptotic wild bootstrap approach" />
          <attvalue for="3" value="Considering two independent Poisson processes, we address the question of&#10;testing equality of their respective intensities. We first propose single tests&#10;whose test statistics are U-statistics based on general kernel functions. The&#10;corresponding critical values are constructed from a non-asymptotic wild&#10;bootstrap approach, leading to level \alpha tests. Various choices for the&#10;kernel functions are possible, including projection, approximation or&#10;reproducing kernels. In this last case, we obtain a parametric rate of testing&#10;for a weak metric defined in the RKHS associated with the considered&#10;reproducing kernel. Then we introduce, in the other cases, an aggregation&#10;procedure, which allows us to import ideas coming from model selection,&#10;thresholding and/or approximation kernels adaptive estimation. The resulting&#10;multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic&#10;oracle type conditions for the classical L2-norm. From these conditions, we&#10;deduce that they are adaptive in the minimax sense over a large variety of&#10;classes of alternatives based on classical and weak Besov bodies in the&#10;univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the&#10;multivariate case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="563" source="Magalie Fromont" target="Patricia Reynaud-Bouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3572v2" />
          <attvalue for="2" value="The two-sample problem for Poisson processes: adaptive tests with a&#10;  non-asymptotic wild bootstrap approach" />
          <attvalue for="3" value="Considering two independent Poisson processes, we address the question of&#10;testing equality of their respective intensities. We first propose single tests&#10;whose test statistics are U-statistics based on general kernel functions. The&#10;corresponding critical values are constructed from a non-asymptotic wild&#10;bootstrap approach, leading to level \alpha tests. Various choices for the&#10;kernel functions are possible, including projection, approximation or&#10;reproducing kernels. In this last case, we obtain a parametric rate of testing&#10;for a weak metric defined in the RKHS associated with the considered&#10;reproducing kernel. Then we introduce, in the other cases, an aggregation&#10;procedure, which allows us to import ideas coming from model selection,&#10;thresholding and/or approximation kernels adaptive estimation. The resulting&#10;multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic&#10;oracle type conditions for the classical L2-norm. From these conditions, we&#10;deduce that they are adaptive in the minimax sense over a large variety of&#10;classes of alternatives based on classical and weak Besov bodies in the&#10;univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the&#10;multivariate case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="564" source="Béatrice Laurent" target="Patricia Reynaud-Bouret">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3572v2" />
          <attvalue for="2" value="The two-sample problem for Poisson processes: adaptive tests with a&#10;  non-asymptotic wild bootstrap approach" />
          <attvalue for="3" value="Considering two independent Poisson processes, we address the question of&#10;testing equality of their respective intensities. We first propose single tests&#10;whose test statistics are U-statistics based on general kernel functions. The&#10;corresponding critical values are constructed from a non-asymptotic wild&#10;bootstrap approach, leading to level \alpha tests. Various choices for the&#10;kernel functions are possible, including projection, approximation or&#10;reproducing kernels. In this last case, we obtain a parametric rate of testing&#10;for a weak metric defined in the RKHS associated with the considered&#10;reproducing kernel. Then we introduce, in the other cases, an aggregation&#10;procedure, which allows us to import ideas coming from model selection,&#10;thresholding and/or approximation kernels adaptive estimation. The resulting&#10;multiple tests are proved to be of level \alpha, and to satisfy non-asymptotic&#10;oracle type conditions for the classical L2-norm. From these conditions, we&#10;deduce that they are adaptive in the minimax sense over a large variety of&#10;classes of alternatives based on classical and weak Besov bodies in the&#10;univariate case, but also Sobolev and anisotropic Nikol'skii-Besov balls in the&#10;multivariate case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="565" source="Patricia Reynaud-Bouret" target="Niels Richard Hansen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.0570v2" />
          <attvalue for="2" value="Lasso and probabilistic inequalities for multivariate point processes" />
          <attvalue for="3" value="Due to its low computational cost, Lasso is an attractive regularization&#10;method for high-dimensional statistical settings. In this paper, we consider&#10;multivariate counting processes depending on an unknown function parameter to&#10;be estimated by linear combinations of a fixed dictionary. To select&#10;coefficients, we propose an adaptive $\ell_1$-penalization methodology, where&#10;data-driven weights of the penalty are derived from new Bernstein type&#10;inequalities for martingales. Oracle inequalities are established under&#10;assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic&#10;results for multivariate Hawkes processes are proven, which allows us to check&#10;these assumptions by considering general dictionaries based on histograms,&#10;Fourier or wavelet bases. Motivated by problems of neuronal activity inference,&#10;we finally carry out a simulation study for multivariate Hawkes processes and&#10;compare our methodology with the adaptive Lasso procedure proposed by Zou in&#10;(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent&#10;behavior of our procedure. We rely on theoretical aspects for the essential&#10;question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.&#10;Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with&#10;respect to all the parameters of the problem, revealing its potential for&#10;concrete purposes, in particular in neuroscience." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="566" source="Patricia Reynaud-Bouret" target="Vincent Rivoirard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.0570v2" />
          <attvalue for="2" value="Lasso and probabilistic inequalities for multivariate point processes" />
          <attvalue for="3" value="Due to its low computational cost, Lasso is an attractive regularization&#10;method for high-dimensional statistical settings. In this paper, we consider&#10;multivariate counting processes depending on an unknown function parameter to&#10;be estimated by linear combinations of a fixed dictionary. To select&#10;coefficients, we propose an adaptive $\ell_1$-penalization methodology, where&#10;data-driven weights of the penalty are derived from new Bernstein type&#10;inequalities for martingales. Oracle inequalities are established under&#10;assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic&#10;results for multivariate Hawkes processes are proven, which allows us to check&#10;these assumptions by considering general dictionaries based on histograms,&#10;Fourier or wavelet bases. Motivated by problems of neuronal activity inference,&#10;we finally carry out a simulation study for multivariate Hawkes processes and&#10;compare our methodology with the adaptive Lasso procedure proposed by Zou in&#10;(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent&#10;behavior of our procedure. We rely on theoretical aspects for the essential&#10;question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.&#10;Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with&#10;respect to all the parameters of the problem, revealing its potential for&#10;concrete purposes, in particular in neuroscience." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="567" source="Xiao Wang" target="Jinglai Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.0023v1" />
          <attvalue for="2" value="Uniform Convergence and Rate Adaptive Estimation of a Convex Function" />
          <attvalue for="3" value="This paper addresses the problem of estimating a convex regression function&#10;under both the sup-norm risk and the pointwise risk using B-splines. The&#10;presence of the convex constraint complicates various issues in asymptotic&#10;analysis, particularly uniform convergence analysis. To overcome this&#10;difficulty, we establish the uniform Lipschitz property of optimal spline&#10;coefficients in the $\ell_\infty$-norm by exploiting piecewise linear and&#10;polyhedral theory. Based upon this property, it is shown that this estimator&#10;attains optimal rates of convergence on the entire interval of interest over&#10;the H\&quot;older class under both the risks. In addition, adaptive estimates are&#10;constructed under both the sup-norm risk and the pointwise risk when the&#10;exponent of the H\&quot;older class is between one and two. These estimates achieve&#10;a maximal risk within a constant factor of the minimax risk over the H\&quot;older&#10;class." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="568" source="Seunggeun Lee" target="Fei Zou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2970v1" />
          <attvalue for="2" value="Convergence and prediction of principal component scores in&#10;  high-dimensional settings" />
          <attvalue for="3" value="A number of settings arise in which it is of interest to predict Principal&#10;Component (PC) scores for new observations using data from an initial sample.&#10;In this paper, we demonstrate that naive approaches to PC score prediction can&#10;be substantially biased toward 0 in the analysis of large matrices. This&#10;phenomenon is largely related to known inconsistency results for sample&#10;eigenvalues and eigenvectors as both dimensions of the matrix increase. For the&#10;spiked eigenvalue model for random matrices, we expand the generality of these&#10;results, and propose bias-adjusted PC score prediction. In addition, we compute&#10;the asymptotic correlation coefficient between PC scores from sample and&#10;population eigenvectors. Simulation and real data examples from the genetics&#10;literature show the improved bias and numerical properties of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="569" source="Seunggeun Lee" target="Fred A. Wright">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2970v1" />
          <attvalue for="2" value="Convergence and prediction of principal component scores in&#10;  high-dimensional settings" />
          <attvalue for="3" value="A number of settings arise in which it is of interest to predict Principal&#10;Component (PC) scores for new observations using data from an initial sample.&#10;In this paper, we demonstrate that naive approaches to PC score prediction can&#10;be substantially biased toward 0 in the analysis of large matrices. This&#10;phenomenon is largely related to known inconsistency results for sample&#10;eigenvalues and eigenvectors as both dimensions of the matrix increase. For the&#10;spiked eigenvalue model for random matrices, we expand the generality of these&#10;results, and propose bias-adjusted PC score prediction. In addition, we compute&#10;the asymptotic correlation coefficient between PC scores from sample and&#10;population eigenvectors. Simulation and real data examples from the genetics&#10;literature show the improved bias and numerical properties of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="570" source="Fei Zou" target="Fred A. Wright">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2970v1" />
          <attvalue for="2" value="Convergence and prediction of principal component scores in&#10;  high-dimensional settings" />
          <attvalue for="3" value="A number of settings arise in which it is of interest to predict Principal&#10;Component (PC) scores for new observations using data from an initial sample.&#10;In this paper, we demonstrate that naive approaches to PC score prediction can&#10;be substantially biased toward 0 in the analysis of large matrices. This&#10;phenomenon is largely related to known inconsistency results for sample&#10;eigenvalues and eigenvectors as both dimensions of the matrix increase. For the&#10;spiked eigenvalue model for random matrices, we expand the generality of these&#10;results, and propose bias-adjusted PC score prediction. In addition, we compute&#10;the asymptotic correlation coefficient between PC scores from sample and&#10;population eigenvectors. Simulation and real data examples from the genetics&#10;literature show the improved bias and numerical properties of our estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="571" source="Jelle J. Goeman" target="Aldo Solari">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3313v1" />
          <attvalue for="2" value="The sequential rejection principle of familywise error control" />
          <attvalue for="3" value="Closed testing and partitioning are recognized as fundamental principles of&#10;familywise error control. In this paper, we argue that sequential rejection can&#10;be considered equally fundamental as a general principle of multiple testing.&#10;We present a general sequentially rejective multiple testing procedure and show&#10;that many well-known familywise error controlling methods can be constructed as&#10;special cases of this procedure, among which are the procedures of Holm,&#10;Shaffer and Hochberg, parallel and serial gatekeeping procedures, modern&#10;procedures for multiple testing in graphs, resampling-based multiple testing&#10;procedures and even the closed testing and partitioning procedures themselves.&#10;We also give a general proof that sequentially rejective multiple testing&#10;procedures strongly control the familywise error if they fulfill simple&#10;criteria of monotonicity of the critical values and a limited form of weak&#10;familywise error control in each single step. The sequential rejection&#10;principle gives a novel theoretical perspective on many well-known multiple&#10;testing procedures, emphasizing the sequential aspect. Its main practical&#10;usefulness is for the development of multiple testing procedures for null&#10;hypotheses, possibly logically related, that are structured in a graph. We&#10;illustrate this by presenting a uniform improvement of a recently published&#10;procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="572" source="Masoud Asgharian" target="Marco Carone">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6275v1" />
          <attvalue for="2" value="Large-sample study of the kernel density estimators under multiplicative&#10;  censoring" />
          <attvalue for="3" value="The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)&#10;751--761] is an incomplete data problem whereby two independent samples from&#10;the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and&#10;$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.&#10;Specifically, sample $\mathcal{X}_m$ is fully observed while&#10;$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where&#10;$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard&#10;uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this&#10;model unifies several important statistical problems, such as the deconvolution&#10;of an exponential random variable, estimation under a decreasing density&#10;constraint and an estimation problem in renewal processes. In this paper, we&#10;establish the large-sample properties of kernel density estimators under the&#10;multiplicative censoring model. We first construct a strong approximation for&#10;the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the&#10;nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and&#10;$k=m+n$ is the total sample size. Using this strong approximation and a result&#10;on the global modulus of continuity, we establish conditions for the strong&#10;uniform consistency of kernel density estimators. We also make use of this&#10;strong approximation to study the weak convergence and integrated squared error&#10;properties of these estimators. We conclude by extending our results to the&#10;setting of length-biased sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="573" source="Masoud Asgharian" target="Vahid Fakoor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6275v1" />
          <attvalue for="2" value="Large-sample study of the kernel density estimators under multiplicative&#10;  censoring" />
          <attvalue for="3" value="The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)&#10;751--761] is an incomplete data problem whereby two independent samples from&#10;the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and&#10;$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.&#10;Specifically, sample $\mathcal{X}_m$ is fully observed while&#10;$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where&#10;$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard&#10;uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this&#10;model unifies several important statistical problems, such as the deconvolution&#10;of an exponential random variable, estimation under a decreasing density&#10;constraint and an estimation problem in renewal processes. In this paper, we&#10;establish the large-sample properties of kernel density estimators under the&#10;multiplicative censoring model. We first construct a strong approximation for&#10;the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the&#10;nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and&#10;$k=m+n$ is the total sample size. Using this strong approximation and a result&#10;on the global modulus of continuity, we establish conditions for the strong&#10;uniform consistency of kernel density estimators. We also make use of this&#10;strong approximation to study the weak convergence and integrated squared error&#10;properties of these estimators. We conclude by extending our results to the&#10;setting of length-biased sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="574" source="Marco Carone" target="Vahid Fakoor">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6275v1" />
          <attvalue for="2" value="Large-sample study of the kernel density estimators under multiplicative&#10;  censoring" />
          <attvalue for="3" value="The multiplicative censoring model introduced in Vardi [Biometrika 76 (1989)&#10;751--761] is an incomplete data problem whereby two independent samples from&#10;the lifetime distribution $G$, $\mathcal{X}_m=(X_1,...,X_m)$ and&#10;$\mathcal{Z}_n=(Z_1,...,Z_n)$, are observed subject to a form of coarsening.&#10;Specifically, sample $\mathcal{X}_m$ is fully observed while&#10;$\mathcal{Y}_n=(Y_1,...,Y_n)$ is observed instead of $\mathcal{Z}_n$, where&#10;$Y_i=U_iZ_i$ and $(U_1,...,U_n)$ is an independent sample from the standard&#10;uniform distribution. Vardi [Biometrika 76 (1989) 751--761] showed that this&#10;model unifies several important statistical problems, such as the deconvolution&#10;of an exponential random variable, estimation under a decreasing density&#10;constraint and an estimation problem in renewal processes. In this paper, we&#10;establish the large-sample properties of kernel density estimators under the&#10;multiplicative censoring model. We first construct a strong approximation for&#10;the process $\sqrt{k}(\hat{G}-G)$, where $\hat{G}$ is a solution of the&#10;nonparametric score equation based on $(\mathcal{X}_m,\mathcal{Y}_n)$, and&#10;$k=m+n$ is the total sample size. Using this strong approximation and a result&#10;on the global modulus of continuity, we establish conditions for the strong&#10;uniform consistency of kernel density estimators. We also make use of this&#10;strong approximation to study the weak convergence and integrated squared error&#10;properties of these estimators. We conclude by extending our results to the&#10;setting of length-biased sampling." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="575" source="Florent Autin" target="Christophe Pouet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2679v2" />
          <attvalue for="2" value="Testing means from sampling populations with undefined labels" />
          <attvalue for="3" value="We consider the problem of testing means from samples of two populations for&#10;which the labels are not defined with certainty. We show that this problem is&#10;connected to another one that is testing expected values of components of&#10;mixture-models from two data samples. The underlying mixture-model is&#10;associated with known varying mixing-weights. We provide a testing procedure&#10;that performs well. Then we point out the loss of performance of our method due&#10;to the mixing-effect by comparing its numerical performances to the Welch's&#10;t-test on means which would have been done if true labels were available." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="576" source="Armin Schwartzman" target="Yulia Gavrilov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3063v1" />
          <attvalue for="2" value="Multiple testing of local maxima for detection of peaks in 1D" />
          <attvalue for="3" value="A topological multiple testing scheme for one-dimensional domains is proposed&#10;where, rather than testing every spatial or temporal location for the presence&#10;of a signal, tests are performed only at the local maxima of the smoothed&#10;observed sequence. Assuming unimodal true peaks with finite support and&#10;Gaussian stationary ergodic noise, it is shown that the algorithm with&#10;Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control&#10;of the family wise error rate and false discovery rate, and is power&#10;consistent, as the search space and the signal strength get large, where the&#10;search space may grow exponentially faster than the signal strength.&#10;Simulations show that error levels are maintained for nonasymptotic conditions,&#10;and that power is maximized when the smoothing kernel is close in shape and&#10;bandwidth to the signal peaks, akin to the matched filter theorem in signal&#10;processing. The methods are illustrated in an analysis of electrical recordings&#10;of neuronal cell activity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="577" source="Armin Schwartzman" target="Robert J. Adler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3063v1" />
          <attvalue for="2" value="Multiple testing of local maxima for detection of peaks in 1D" />
          <attvalue for="3" value="A topological multiple testing scheme for one-dimensional domains is proposed&#10;where, rather than testing every spatial or temporal location for the presence&#10;of a signal, tests are performed only at the local maxima of the smoothed&#10;observed sequence. Assuming unimodal true peaks with finite support and&#10;Gaussian stationary ergodic noise, it is shown that the algorithm with&#10;Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control&#10;of the family wise error rate and false discovery rate, and is power&#10;consistent, as the search space and the signal strength get large, where the&#10;search space may grow exponentially faster than the signal strength.&#10;Simulations show that error levels are maintained for nonasymptotic conditions,&#10;and that power is maximized when the smoothing kernel is close in shape and&#10;bandwidth to the signal peaks, akin to the matched filter theorem in signal&#10;processing. The methods are illustrated in an analysis of electrical recordings&#10;of neuronal cell activity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="578" source="Yulia Gavrilov" target="Robert J. Adler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3063v1" />
          <attvalue for="2" value="Multiple testing of local maxima for detection of peaks in 1D" />
          <attvalue for="3" value="A topological multiple testing scheme for one-dimensional domains is proposed&#10;where, rather than testing every spatial or temporal location for the presence&#10;of a signal, tests are performed only at the local maxima of the smoothed&#10;observed sequence. Assuming unimodal true peaks with finite support and&#10;Gaussian stationary ergodic noise, it is shown that the algorithm with&#10;Bonferroni or Benjamini--Hochberg correction provides asymptotic strong control&#10;of the family wise error rate and false discovery rate, and is power&#10;consistent, as the search space and the signal strength get large, where the&#10;search space may grow exponentially faster than the signal strength.&#10;Simulations show that error levels are maintained for nonasymptotic conditions,&#10;and that power is maximized when the smoothing kernel is close in shape and&#10;bandwidth to the signal peaks, akin to the matched filter theorem in signal&#10;processing. The methods are illustrated in an analysis of electrical recordings&#10;of neuronal cell activity." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="579" source="Loïc Hervé" target="James Ledoux">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2947v1" />
          <attvalue for="2" value="A uniform Berry--Esseen theorem on $M$-estimators for geometrically&#10;  ergodic Markov chains" />
          <attvalue for="3" value="Let $\{X_n\}_{n\ge0}$ be a $V$-geometrically ergodic Markov chain. Given some&#10;real-valued functional $F$, define&#10;$M_n(\alpha):=n^{-1}\sum_{k=1}^nF(\alpha,X_{k-1},X_k)$,&#10;$\alpha\in\mathcal{A}\subset \mathbb {R}$. Consider an $M$ estimator&#10;$\hat{\alpha}_n$, that is, a measurable function of the observations satisfying&#10;$M_n(\hat{\alpha}_n)\leq \min_{\alpha\in\mathcal{A}}M_n(\alpha)+c_n$ with&#10;$\{c_n\}_{n\geq1}$ some sequence of real numbers going to zero. Under some&#10;standard regularity and moment assumptions, close to those of the i.i.d. case,&#10;the estimator $\hat{\alpha}_n$ satisfies a Berry--Esseen theorem uniformly with&#10;respect to the underlying probability distribution of the Markov chain." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="580" source="Yuejie Chi" target="Robert Calderbank">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.6267v1" />
          <attvalue for="2" value="Coherence-Based Performance Guarantees of Orthogonal Matching Pursuit" />
          <attvalue for="3" value="In this paper, we present coherence-based performance guarantees of&#10;Orthogonal Matching Pursuit (OMP) for both support recovery and signal&#10;reconstruction of sparse signals when the measurements are corrupted by noise.&#10;In particular, two variants of OMP either with known sparsity level or with a&#10;stopping rule are analyzed. It is shown that if the measurement matrix&#10;$X\in\mathbb{C}^{n\times p}$ satisfies the strong coherence property, then with&#10;$n\gtrsim\mathcal{O}(k\log p)$, OMP will recover a $k$-sparse signal with high&#10;probability. In particular, the performance guarantees obtained here separate&#10;the properties required of the measurement matrix from the properties required&#10;of the signal, which depends critically on the minimum signal to noise ratio&#10;rather than the power profiles of the signal. We also provide performance&#10;guarantees for partial support recovery. Comparisons are given with other&#10;performance guarantees for OMP using worst-case analysis and the sorted one&#10;step thresholding algorithm." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="581" source="Ahmed Bensalma" target="Mohamed Bentarzi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.1031v3" />
          <attvalue for="2" value="Testing the Fractional Integration Parameter Revisited: a Fractional&#10;  Dickey-Fuller Test" />
          <attvalue for="3" value="In this paper, in the first step, we show that the fractional Dickey-Fuller&#10;test proposed by Dolado et al [10] is useless in practice. In the second step,&#10;we propose a new testing procedure for the degree of fractional integration of&#10;a time series inspired on the unit root test of Dickey-Fuller [7]. Through a&#10;simulation study, we show the good performance of the test in terms of size and&#10;power. Finally, in order to show how to use the new testing procedure, the test&#10;is applied to the well-known Nelson and Plosser data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="582" source="Tamio Koyama" target="Hiromasa Nakayama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="583" source="Tamio Koyama" target="Kenta Nishiyama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="584" source="Hiromasa Nakayama" target="Kenta Nishiyama">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3239v5" />
          <attvalue for="2" value="Holonomic Gradient Descent for the Fisher-Bingham Distribution on the&#10;  $d$-dimensional Sphere" />
          <attvalue for="3" value="We propose an accelerated version of the holonomic gradient descent and apply&#10;it to calculating the maximum likelihood estimate (MLE) of the Fisher-Bingham&#10;distribution on a $d$-dimensional sphere. We derive a Pfaffian system (an&#10;integrable connection) and a series expansion associated with the normalizing&#10;constant with an error estimation. These enable us to solve some MLE problems&#10;up to dimension $d=7$ with a specified accuracy." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="585" source="John Ehrlinger" target="Hemant Ishwaran">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5367v1" />
          <attvalue for="2" value="Characterizing $L_2$Boosting" />
          <attvalue for="3" value="We consider $L_2$Boosting, a special case of Friedman's generic boosting&#10;algorithm applied to linear regression under $L_2$-loss. We study $L_2$Boosting&#10;for an arbitrary regularization parameter and derive an exact closed form&#10;expression for the number of steps taken along a fixed coordinate direction.&#10;This relationship is used to describe $L_2$Boosting's solution path, to&#10;describe new tools for studying its path, and to characterize some of the&#10;algorithm's unique properties, including active set cycling, a property where&#10;the algorithm spends lengthy periods of time cycling between the same&#10;coordinates when the regularization parameter is arbitrarily small. Our fixed&#10;descent analysis also reveals a repressible condition that limits the&#10;effectiveness of $L_2$Boosting in correlated problems by preventing desirable&#10;variables from entering the solution path. As a simple remedy, a data&#10;augmentation method similar to that used for the elastic net is used to&#10;introduce $L_2$-penalization and is shown, in combination with decorrelation,&#10;to reverse the repressible condition and circumvents $L_2$Boosting's&#10;deficiencies in correlated problems. In itself, this presents a new explanation&#10;for why the elastic net is successful in correlated problems and why methods&#10;like LAR and lasso can perform poorly in such settings." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="586" source="Anthony J Webster" target="Richard Kemp">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1150v3" />
          <attvalue for="2" value="Estimating Omissions from Searches" />
          <attvalue for="3" value="The mark-recapture method was devised by Petersen in 1896 to estimate the&#10;number of fish migrating into the Limfjord, and independently by Lincoln in&#10;1930 to estimate waterfowl abundance. The technique applies to any search for a&#10;finite number of items by two or more people or agents, allowing the number of&#10;searched-for items to be estimated. This ubiquitous problem appears in fields&#10;from ecology and epidemiology, through to mathematics, social sciences, and&#10;computing. Here we exactly calculate the moments of the hypergeometric&#10;distribution associated with this long-standing problem, confirming that widely&#10;used estimates conjectured in 1951 are often too small. Our Bayesian approach&#10;highlights how different search strategies will modify the estimates. As an&#10;example, we assess the accuracy of a systematic literature review, an&#10;application we recommend." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="587" source="Vladimir Vovk" target="Ruodu Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.4966v4" />
          <attvalue for="2" value="Combining p-values via averaging" />
          <attvalue for="3" value="This paper proposes general methods for the problem of multiple testing of a&#10;single hypothesis, with a standard goal of combining a number of p-values&#10;without making any assumptions about their dependence structure. An old result&#10;by R\&quot;uschendorf and, independently, Meng implies that the p-values can be&#10;combined by scaling up their arithmetic mean by a factor of 2 (and no smaller&#10;factor is sufficient in general). Based on more recent developments in&#10;mathematical finance, specifically, robust risk aggregation techniques, we show&#10;that $K$ p-values can be combined by scaling up their geometric mean by a&#10;factor of $e$ (for all $K$) and by scaling up their harmonic mean by a factor&#10;of $\ln K$ (asymptotically as $K\to\infty$). These and other results lead to a&#10;generalized version of the Bonferroni-Holm method. A simulation study compares&#10;the performance of various averaging methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="588" source="Piotr Pokarowski" target="Jan Mielniczuk">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4146v1" />
          <attvalue for="2" value="Linear regression model selection using p-values when the model&#10;  dimension grows" />
          <attvalue for="3" value="We consider a new criterion-based approach to model selection in linear&#10;regression. Properties of selection criteria based on p-values of a likelihood&#10;ratio statistic are studied for families of linear regression models. We prove&#10;that such procedures are consistent i.e. the minimal true model is chosen with&#10;probability tending to 1 even when the number of models under consideration&#10;slowly increases with a sample size. The simulation study indicates that&#10;introduced methods perform promisingly when compared with Akaike and Bayesian&#10;Information Criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="589" source="Piotr Pokarowski" target="Paweł Teisseyre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4146v1" />
          <attvalue for="2" value="Linear regression model selection using p-values when the model&#10;  dimension grows" />
          <attvalue for="3" value="We consider a new criterion-based approach to model selection in linear&#10;regression. Properties of selection criteria based on p-values of a likelihood&#10;ratio statistic are studied for families of linear regression models. We prove&#10;that such procedures are consistent i.e. the minimal true model is chosen with&#10;probability tending to 1 even when the number of models under consideration&#10;slowly increases with a sample size. The simulation study indicates that&#10;introduced methods perform promisingly when compared with Akaike and Bayesian&#10;Information Criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="590" source="Jan Mielniczuk" target="Paweł Teisseyre">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4146v1" />
          <attvalue for="2" value="Linear regression model selection using p-values when the model&#10;  dimension grows" />
          <attvalue for="3" value="We consider a new criterion-based approach to model selection in linear&#10;regression. Properties of selection criteria based on p-values of a likelihood&#10;ratio statistic are studied for families of linear regression models. We prove&#10;that such procedures are consistent i.e. the minimal true model is chosen with&#10;probability tending to 1 even when the number of models under consideration&#10;slowly increases with a sample size. The simulation study indicates that&#10;introduced methods perform promisingly when compared with Akaike and Bayesian&#10;Information Criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="591" source="Eric J. Tchetgen Tchetgen" target="Ilya Shpitser">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.4654v1" />
          <attvalue for="2" value="Semiparametric theory for causal mediation analysis: Efficiency bounds,&#10;  multiple robustness and sensitivity analysis" />
          <attvalue for="3" value="While estimation of the marginal (total) causal effect of a point exposure on&#10;an outcome is arguably the most common objective of experimental and&#10;observational studies in the health and social sciences, in recent years,&#10;investigators have also become increasingly interested in mediation analysis.&#10;Specifically, upon evaluating the total effect of the exposure, investigators&#10;routinely wish to make inferences about the direct or indirect pathways of the&#10;effect of the exposure, through a mediator variable or not, that occurs&#10;subsequently to the exposure and prior to the outcome. Although powerful&#10;semiparametric methodologies have been developed to analyze observational&#10;studies that produce double robust and highly efficient estimates of the&#10;marginal total causal effect, similar methods for mediation analysis are&#10;currently lacking. Thus, this paper develops a general semiparametric framework&#10;for obtaining inferences about so-called marginal natural direct and indirect&#10;causal effects, while appropriately accounting for a large number of&#10;pre-exposure confounding factors for the exposure and the mediator variables.&#10;Our analytic framework is particularly appealing, because it gives new insights&#10;on issues of efficiency and robustness in the context of mediation analysis. In&#10;particular, we propose new multiply robust locally efficient estimators of the&#10;marginal natural indirect and direct causal effects, and develop a novel double&#10;robust sensitivity analysis framework for the assumption of ignorability of the&#10;mediator variable." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="592" source="Yanqing Hu" target="Feifang Hu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.4666v1" />
          <attvalue for="2" value="Asymptotic properties of covariate-adaptive randomization" />
          <attvalue for="3" value="Balancing treatment allocation for influential covariates is critical in&#10;clinical trials. This has become increasingly important as more and more&#10;biomarkers are found to be associated with different diseases in translational&#10;research (genomics, proteomics and metabolomics). Stratified permuted block&#10;randomization and minimization methods [Pocock and Simon Biometrics 31 (1975)&#10;103-115, etc.] are the two most popular approaches in practice. However,&#10;stratified permuted block randomization fails to achieve good overall balance&#10;when the number of strata is large, whereas traditional minimization methods&#10;also suffer from the potential drawback of large within-stratum imbalances.&#10;Moreover, the theoretical bases of minimization methods remain largely elusive.&#10;In this paper, we propose a new covariate-adaptive design that is able to&#10;control various types of imbalances. We show that the joint process of&#10;within-stratum imbalances is a positive recurrent Markov chain under certain&#10;conditions. Therefore, this new procedure yields more balanced allocation. The&#10;advantages of the proposed procedure are also demonstrated by extensive&#10;simulation studies. Our work provides a theoretical tool for future research in&#10;this area." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="593" source="Yvik Swan" target="Thomas Verdebout">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4259v2" />
          <attvalue for="2" value="Efficient ANOVA for directional data" />
          <attvalue for="3" value="In this paper we tackle the ANOVA problem for directional data (with&#10;particular emphasis on geological data) by having recourse to the Le Cam&#10;methodology usually reserved for linear multivariate analysis. We construct&#10;locally and asymptotically most stringent parametric tests for ANOVA for&#10;directional data within the class of rotationally symmetric distributions. We&#10;turn these parametric tests into semi-parametric ones by (i) using a&#10;studentization argument (which leads to what we call pseudo-FvML tests) and by&#10;(ii) resorting to the invariance principle (which leads to efficient rank-based&#10;tests). Within each construction the semi-parametric tests inherit optimality&#10;under a given distribution (the FvML distribution in the first case, any&#10;rotationally symmetric distribution in the second) from their parametric&#10;antecedents and also improve on the latter by being valid under the whole class&#10;of rotationally symmetric distributions. Asymptotic relative efficiencies are&#10;calculated and the finite-sample behavior of the proposed tests is investigated&#10;by means of a Monte Carlo simulation. We conclude by applying our findings on a&#10;real-data example involving geological data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="594" source="Chun-Yang Wang" target="Wen-Xian Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.4892v1" />
          <attvalue for="2" value="Critical Properties of $S^{4}$ System Restudied via Generalized&#10;  Migdal-Kadanoff Bond-moving Renormalization" />
          <attvalue for="3" value="We study the critical properties of the spin-continuous $S^{4}$ system on the&#10;typical translational invariant triangular lattices by combining the&#10;recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures&#10;with the cumulative expansion technique. In three different cases of&#10;nearest-neighbor, next nearest neighbor and external field we obtain the&#10;critical points and further calculate the critical exponents according to the&#10;scaling theory. In all case it is found that there exists three fixed points.&#10;The correlation length critical exponents obtained near the Wilson-Fisher fixed&#10;points are found getting smaller and smaller with the increasing of the system&#10;complexity. Others are found similar to the results of the classical Gaussian&#10;model and different from those of the Ising system." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="595" source="Chun-Yang Wang" target="Hong Du">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.4892v1" />
          <attvalue for="2" value="Critical Properties of $S^{4}$ System Restudied via Generalized&#10;  Migdal-Kadanoff Bond-moving Renormalization" />
          <attvalue for="3" value="We study the critical properties of the spin-continuous $S^{4}$ system on the&#10;typical translational invariant triangular lattices by combining the&#10;recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures&#10;with the cumulative expansion technique. In three different cases of&#10;nearest-neighbor, next nearest neighbor and external field we obtain the&#10;critical points and further calculate the critical exponents according to the&#10;scaling theory. In all case it is found that there exists three fixed points.&#10;The correlation length critical exponents obtained near the Wilson-Fisher fixed&#10;points are found getting smaller and smaller with the increasing of the system&#10;complexity. Others are found similar to the results of the classical Gaussian&#10;model and different from those of the Ising system." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="596" source="Wen-Xian Yang" target="Hong Du">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.4892v1" />
          <attvalue for="2" value="Critical Properties of $S^{4}$ System Restudied via Generalized&#10;  Migdal-Kadanoff Bond-moving Renormalization" />
          <attvalue for="3" value="We study the critical properties of the spin-continuous $S^{4}$ system on the&#10;typical translational invariant triangular lattices by combining the&#10;recently-developed generalized Migdal-Kadanoff bond-moving recursion procedures&#10;with the cumulative expansion technique. In three different cases of&#10;nearest-neighbor, next nearest neighbor and external field we obtain the&#10;critical points and further calculate the critical exponents according to the&#10;scaling theory. In all case it is found that there exists three fixed points.&#10;The correlation length critical exponents obtained near the Wilson-Fisher fixed&#10;points are found getting smaller and smaller with the increasing of the system&#10;complexity. Others are found similar to the results of the classical Gaussian&#10;model and different from those of the Ising system." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="597" source="Ben Haaland" target="Peter Z. G. Qian">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2433v1" />
          <attvalue for="2" value="Accurate emulators for large-scale computer experiments" />
          <attvalue for="3" value="Large-scale computer experiments are becoming increasingly important in&#10;science. A multi-step procedure is introduced to statisticians for modeling&#10;such experiments, which builds an accurate interpolator in multiple steps. In&#10;practice, the procedure shows substantial improvements in overall accuracy, but&#10;its theoretical properties are not well established. We introduce the terms&#10;nominal and numeric error and decompose the overall error of an interpolator&#10;into nominal and numeric portions. Bounds on the numeric and nominal error are&#10;developed to show theoretically that substantial gains in overall accuracy can&#10;be attained with the multi-step approach." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="598" source="Natalia A. Bochkina" target="Peter J. Green">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3434v5" />
          <attvalue for="2" value="The Bernstein-von Mises theorem and nonregular models" />
          <attvalue for="3" value="We study the asymptotic behaviour of the posterior distribution in a broad&#10;class of statistical models where the &quot;true&quot; solution occurs on the boundary of&#10;the parameter space. We show that in this case Bayesian inference is&#10;consistent, and that the posterior distribution has not only Gaussian&#10;components as in the case of regular models (the Bernstein-von Mises theorem)&#10;but also has Gamma distribution components whose form depends on the behaviour&#10;of the prior distribution near the boundary and have a faster rate of&#10;convergence. We also demonstrate a remarkable property of Bayesian inference,&#10;that for some models, there appears to be no bound on efficiency of estimating&#10;the unknown parameter if it is on the boundary of the parameter space. We&#10;illustrate the results on a problem from emission tomography." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="599" source="Yindeng Jiang" target="Michael D. Perlman">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.4765v1" />
          <attvalue for="2" value="Reverse Exchangeability and Extreme Order Statistics" />
          <attvalue for="3" value="For a bivariate random vector (X,Y), symmetry conditions are presented that&#10;yield stochastic orderings among |X|, |Y|, |max(X,Y)|, and | min(X, Y)|.&#10;Partial extensions of these results for multivariate random vectors (X1,...,Xn)&#10;are also given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="600" source="Shuyuan He" target="Wei Liang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="601" source="Shuyuan He" target="Junshan Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="602" source="Shuyuan He" target="Grace Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="603" source="Wei Liang" target="Junshan Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="604" source="Wei Liang" target="Grace Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="605" source="Junshan Shen" target="Grace Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5955v1" />
          <attvalue for="2" value="Empirical Likelihood for Right Censored Lifetime Data" />
          <attvalue for="3" value="This paper considers the empirical likelihood (EL) construction of confidence&#10;intervals for a linear functional based on right censored lifetime data. Many&#10;of the results in literature show that log EL has a limiting scaled chi-square&#10;distribution, where the scale parameter is a function of the unknown asymptotic&#10;variance. The scale parameter has to be estimated for the construction.&#10;Additional estimation would reduce the coverage accuracy for the parameter.&#10;This diminishes a main advantage of the EL method for censored data. By&#10;utilizing certain influence functions in an estimating equation, it is shown&#10;that under very general conditions, log EL converges weakly to a standard&#10;chi-square distribution and thereby eliminates the need for estimating the&#10;scale parameter. Moreover, a special way of employing influence functions eases&#10;the otherwise very demanding computations of the EL method. Our approach yields&#10;smaller asymptotic variance of the influence function than those comparable&#10;ones considered by Wang and Jing (2001) and Qin and Zhao (2007). Thus it is not&#10;surprising that confidence intervals using influence functions give a better&#10;coverage accuracy as demonstrated by simulations." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="606" source="M. J. Bayarri" target="J. O. Berger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="607" source="M. J. Bayarri" target="A. Forte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="608" source="M. J. Bayarri" target="G. García-Donato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="609" source="J. O. Berger" target="A. Forte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="610" source="J. O. Berger" target="G. García-Donato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="611" source="A. Forte" target="G. García-Donato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.5240v1" />
          <attvalue for="2" value="Criteria for Bayesian model choice with application to variable&#10;  selection" />
          <attvalue for="3" value="In objective Bayesian model selection, no single criterion has emerged as&#10;dominant in defining objective prior distributions. Indeed, many criteria have&#10;been separately proposed and utilized to propose differing prior choices. We&#10;first formalize the most general and compelling of the various criteria that&#10;have been suggested, together with a new criterion. We then illustrate the&#10;potential of these criteria in determining objective model selection priors by&#10;considering their application to the problem of variable selection in normal&#10;linear models. This results in a new model selection objective prior with a&#10;number of compelling properties." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="612" source="Stefano Favaro" target="Alessandra Guglielmi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6228v1" />
          <attvalue for="2" value="A class of measure-valued Markov chains and Bayesian nonparametrics" />
          <attvalue for="3" value="Measure-valued Markov chains have raised interest in Bayesian nonparametrics&#10;since the seminal paper by (Math. Proc. Cambridge Philos. Soc. 105 (1989)&#10;579--585) where a Markov chain having the law of the Dirichlet process as&#10;unique invariant measure has been introduced. In the present paper, we propose&#10;and investigate a new class of measure-valued Markov chains defined via&#10;exchangeable sequences of random variables. Asymptotic properties for this new&#10;class are derived and applications related to Bayesian nonparametric mixture&#10;modeling, and to a generalization of the Markov chain proposed by (Math. Proc.&#10;Cambridge Philos. Soc. 105 (1989) 579--585), are discussed. These results and&#10;their applications highlight once again the interplay between Bayesian&#10;nonparametrics and the theory of measure-valued Markov chains." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="613" source="Stefano Favaro" target="Antonio Lijoi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5422v1" />
          <attvalue for="2" value="Asymptotics for a Bayesian nonparametric estimator of species variety" />
          <attvalue for="3" value="In Bayesian nonparametric inference, random discrete probability measures are&#10;commonly used as priors within hierarchical mixture models for density&#10;estimation and for inference on the clustering of the data. Recently, it has&#10;been shown that they can also be exploited in species sampling problems: indeed&#10;they are natural tools for modeling the random proportions of species within a&#10;population thus allowing for inference on various quantities of statistical&#10;interest. For applications that involve large samples, the exact evaluation of&#10;the corresponding estimators becomes impracticable and, therefore, asymptotic&#10;approximations are sought. In the present paper, we study the limiting&#10;behaviour of the number of new species to be observed from further sampling,&#10;conditional on observed data, assuming the observations are exchangeable and&#10;directed by a normalized generalized gamma process prior. Such an asymptotic&#10;study highlights a connection between the normalized generalized gamma process&#10;and the two-parameter Poisson-Dirichlet process that was previously known only&#10;in the unconditional case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="614" source="Stefano Favaro" target="Igor Prünster">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5422v1" />
          <attvalue for="2" value="Asymptotics for a Bayesian nonparametric estimator of species variety" />
          <attvalue for="3" value="In Bayesian nonparametric inference, random discrete probability measures are&#10;commonly used as priors within hierarchical mixture models for density&#10;estimation and for inference on the clustering of the data. Recently, it has&#10;been shown that they can also be exploited in species sampling problems: indeed&#10;they are natural tools for modeling the random proportions of species within a&#10;population thus allowing for inference on various quantities of statistical&#10;interest. For applications that involve large samples, the exact evaluation of&#10;the corresponding estimators becomes impracticable and, therefore, asymptotic&#10;approximations are sought. In the present paper, we study the limiting&#10;behaviour of the number of new species to be observed from further sampling,&#10;conditional on observed data, assuming the observations are exchangeable and&#10;directed by a normalized generalized gamma process prior. Such an asymptotic&#10;study highlights a connection between the normalized generalized gamma process&#10;and the two-parameter Poisson-Dirichlet process that was previously known only&#10;in the unconditional case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="615" source="Yonggang Hu" target="Yong Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1146v1" />
          <attvalue for="2" value="Tensor-based projection depth" />
          <attvalue for="3" value="The conventional definition of a depth function is vector-based. In this&#10;paper, a novel projection depth (PD) technique directly based on tensors, such&#10;as matrices, is instead proposed. Tensor projection depth (TPD) is still an&#10;ideal depth function and its computation can be achieved through the iteration&#10;of PD. Furthermore, we also discuss the cases for sparse samples and higher&#10;order tensors. Experimental results in data classification with the two&#10;projection depths show that TPD performs much better than PD for data with a&#10;natural tensor form, and even when the data have a natural vector form, TPD&#10;appears to perform no worse than PD." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="616" source="Yonggang Hu" target="Yi Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1146v1" />
          <attvalue for="2" value="Tensor-based projection depth" />
          <attvalue for="3" value="The conventional definition of a depth function is vector-based. In this&#10;paper, a novel projection depth (PD) technique directly based on tensors, such&#10;as matrices, is instead proposed. Tensor projection depth (TPD) is still an&#10;ideal depth function and its computation can be achieved through the iteration&#10;of PD. Furthermore, we also discuss the cases for sparse samples and higher&#10;order tensors. Experimental results in data classification with the two&#10;projection depths show that TPD performs much better than PD for data with a&#10;natural tensor form, and even when the data have a natural vector form, TPD&#10;appears to perform no worse than PD." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="617" source="Yong Wang" target="Yi Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1146v1" />
          <attvalue for="2" value="Tensor-based projection depth" />
          <attvalue for="3" value="The conventional definition of a depth function is vector-based. In this&#10;paper, a novel projection depth (PD) technique directly based on tensors, such&#10;as matrices, is instead proposed. Tensor projection depth (TPD) is still an&#10;ideal depth function and its computation can be achieved through the iteration&#10;of PD. Furthermore, we also discuss the cases for sparse samples and higher&#10;order tensors. Experimental results in data classification with the two&#10;projection depths show that TPD performs much better than PD for data with a&#10;natural tensor form, and even when the data have a natural vector form, TPD&#10;appears to perform no worse than PD." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="618" source="A. Goldenshluger" target="O. Lepski">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1715v2" />
          <attvalue for="2" value="On adaptive minimax density estimation on $R^d$" />
          <attvalue for="3" value="We address the problem of adaptive minimax density estimation on $\bR^d$ with&#10;$\bL_p$--loss on the anisotropic Nikol'skii classes. We fully characterize&#10;behavior of the minimax risk for different relationships between regularity&#10;parameters and norm indexes in definitions of the functional class and of the&#10;risk. In particular, we show that there are four different regimes with respect&#10;to the behavior of the minimax risk. We develop a single estimator which is&#10;(nearly) optimal in orderover the complete scale of the anisotropic Nikol'skii&#10;classes. Our estimation procedure is based on a data-driven selection of an&#10;estimator from a fixed family of kernel estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="619" source="Xia Cui" target="Wolfgang Karl Härdle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5220v1" />
          <attvalue for="2" value="The EFM approach for single-index models" />
          <attvalue for="3" value="Single-index models are natural extensions of linear models and circumvent&#10;the so-called curse of dimensionality. They are becoming increasingly popular&#10;in many scientific fields including biostatistics, medicine, economics and&#10;financial econometrics. Estimating and testing the model index coefficients&#10;$\bolds{\beta}$ is one of the most important objectives in the statistical&#10;analysis. However, the commonly used assumption on the index coefficients,&#10;$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on&#10;the boundary of the unit ball. In this paper we introduce the EFM approach, a&#10;method of estimating functions, to study the single-index model. The procedure&#10;is to first relax the equality constraint to one with (d-1) components of&#10;$\bolds{\beta}$ lying in an open unit ball, and then to construct the&#10;associated (d-1) estimating functions by projecting the score function to the&#10;linear space spanned by the residuals with the unknown link being estimated by&#10;kernel estimating functions. The root-n consistency and asymptotic normality&#10;for the estimator obtained from solving the resulting estimating equations are&#10;achieved, and a Wilks type theorem for testing the index is demonstrated. A&#10;noticeable result we obtain is that our estimator for $\bolds{\beta}$ has&#10;smaller or equal limiting variance than the estimator of Carroll et al. [J.&#10;Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for&#10;computing this estimator is proposed. This algorithm only involves&#10;one-dimensional nonparametric smoothers, thereby avoiding the data sparsity&#10;problem caused by high model dimensionality. Numerical studies based on&#10;simulation and on applications suggest that this new estimating system is quite&#10;powerful and easy to implement." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="620" source="Xia Cui" target="Lixing Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5220v1" />
          <attvalue for="2" value="The EFM approach for single-index models" />
          <attvalue for="3" value="Single-index models are natural extensions of linear models and circumvent&#10;the so-called curse of dimensionality. They are becoming increasingly popular&#10;in many scientific fields including biostatistics, medicine, economics and&#10;financial econometrics. Estimating and testing the model index coefficients&#10;$\bolds{\beta}$ is one of the most important objectives in the statistical&#10;analysis. However, the commonly used assumption on the index coefficients,&#10;$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on&#10;the boundary of the unit ball. In this paper we introduce the EFM approach, a&#10;method of estimating functions, to study the single-index model. The procedure&#10;is to first relax the equality constraint to one with (d-1) components of&#10;$\bolds{\beta}$ lying in an open unit ball, and then to construct the&#10;associated (d-1) estimating functions by projecting the score function to the&#10;linear space spanned by the residuals with the unknown link being estimated by&#10;kernel estimating functions. The root-n consistency and asymptotic normality&#10;for the estimator obtained from solving the resulting estimating equations are&#10;achieved, and a Wilks type theorem for testing the index is demonstrated. A&#10;noticeable result we obtain is that our estimator for $\bolds{\beta}$ has&#10;smaller or equal limiting variance than the estimator of Carroll et al. [J.&#10;Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for&#10;computing this estimator is proposed. This algorithm only involves&#10;one-dimensional nonparametric smoothers, thereby avoiding the data sparsity&#10;problem caused by high model dimensionality. Numerical studies based on&#10;simulation and on applications suggest that this new estimating system is quite&#10;powerful and easy to implement." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="621" source="Wolfgang Karl Härdle" target="Lixing Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5220v1" />
          <attvalue for="2" value="The EFM approach for single-index models" />
          <attvalue for="3" value="Single-index models are natural extensions of linear models and circumvent&#10;the so-called curse of dimensionality. They are becoming increasingly popular&#10;in many scientific fields including biostatistics, medicine, economics and&#10;financial econometrics. Estimating and testing the model index coefficients&#10;$\bolds{\beta}$ is one of the most important objectives in the statistical&#10;analysis. However, the commonly used assumption on the index coefficients,&#10;$\|\bolds{\beta}\|=1$, represents a nonregular problem: the true index is on&#10;the boundary of the unit ball. In this paper we introduce the EFM approach, a&#10;method of estimating functions, to study the single-index model. The procedure&#10;is to first relax the equality constraint to one with (d-1) components of&#10;$\bolds{\beta}$ lying in an open unit ball, and then to construct the&#10;associated (d-1) estimating functions by projecting the score function to the&#10;linear space spanned by the residuals with the unknown link being estimated by&#10;kernel estimating functions. The root-n consistency and asymptotic normality&#10;for the estimator obtained from solving the resulting estimating equations are&#10;achieved, and a Wilks type theorem for testing the index is demonstrated. A&#10;noticeable result we obtain is that our estimator for $\bolds{\beta}$ has&#10;smaller or equal limiting variance than the estimator of Carroll et al. [J.&#10;Amer. Statist. Assoc. 92 (1997) 447-489]. A fixed-point iterative scheme for&#10;computing this estimator is proposed. This algorithm only involves&#10;one-dimensional nonparametric smoothers, thereby avoiding the data sparsity&#10;problem caused by high model dimensionality. Numerical studies based on&#10;simulation and on applications suggest that this new estimating system is quite&#10;powerful and easy to implement." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="622" source="Wolfgang Karl Härdle" target="Vladimir Spokoiny">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.5384v2" />
          <attvalue for="2" value="Local Quantile Regression" />
          <attvalue for="3" value="Quantile regression is a technique to estimate conditional quantile curves.&#10;It provides a comprehensive picture of a response contingent on explanatory&#10;variables. In a flexible modeling framework, a specific form of the conditional&#10;quantile curve is not a priori fixed. % Indeed, the majority of applications do&#10;not per se require specific functional forms. This motivates a local parametric&#10;rather than a global fixed model fitting approach. A nonparametric smoothing&#10;estimator of the conditional quantile curve requires to balance between local&#10;curvature and stochastic variability. In this paper, we suggest a local model&#10;selection technique that provides an adaptive estimator of the conditional&#10;quantile regression curve at each design point. Theoretical results claim that&#10;the proposed adaptive procedure performs as good as an oracle which would&#10;minimize the local estimation risk for the problem at hand. We illustrate the&#10;performance of the procedure by an extensive simulation study and consider a&#10;couple of applications: to tail dependence analysis for the Hong Kong stock&#10;market and to analysis of the distributions of the risk factors of temperature&#10;dynamics." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="623" source="Wolfgang Karl Härdle" target="Weining Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.5384v2" />
          <attvalue for="2" value="Local Quantile Regression" />
          <attvalue for="3" value="Quantile regression is a technique to estimate conditional quantile curves.&#10;It provides a comprehensive picture of a response contingent on explanatory&#10;variables. In a flexible modeling framework, a specific form of the conditional&#10;quantile curve is not a priori fixed. % Indeed, the majority of applications do&#10;not per se require specific functional forms. This motivates a local parametric&#10;rather than a global fixed model fitting approach. A nonparametric smoothing&#10;estimator of the conditional quantile curve requires to balance between local&#10;curvature and stochastic variability. In this paper, we suggest a local model&#10;selection technique that provides an adaptive estimator of the conditional&#10;quantile regression curve at each design point. Theoretical results claim that&#10;the proposed adaptive procedure performs as good as an oracle which would&#10;minimize the local estimation risk for the problem at hand. We illustrate the&#10;performance of the procedure by an extensive simulation study and consider a&#10;couple of applications: to tail dependence analysis for the Hong Kong stock&#10;market and to analysis of the distributions of the risk factors of temperature&#10;dynamics." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="624" source="Stephane Girard" target="Armelle Guillou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.3111v2" />
          <attvalue for="2" value="Uniform strong consistency of a frontier estimator using kernel&#10;  regression on high order moments" />
          <attvalue for="3" value="We consider the high order moments estimator of the frontier of a random pair&#10;introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier&#10;estimation with kernel regression on high order moments}. In the present paper,&#10;we show that this estimator is strongly uniformly consistent on compact sets&#10;and its rate of convergence is given when the conditional cumulative&#10;distribution function belongs to the Hall class of distribution functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="625" source="Stephane Girard" target="Gilles Stupfler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.3111v2" />
          <attvalue for="2" value="Uniform strong consistency of a frontier estimator using kernel&#10;  regression on high order moments" />
          <attvalue for="3" value="We consider the high order moments estimator of the frontier of a random pair&#10;introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier&#10;estimation with kernel regression on high order moments}. In the present paper,&#10;we show that this estimator is strongly uniformly consistent on compact sets&#10;and its rate of convergence is given when the conditional cumulative&#10;distribution function belongs to the Hall class of distribution functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="626" source="Armelle Guillou" target="Gilles Stupfler">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1212.3111v2" />
          <attvalue for="2" value="Uniform strong consistency of a frontier estimator using kernel&#10;  regression on high order moments" />
          <attvalue for="3" value="We consider the high order moments estimator of the frontier of a random pair&#10;introduced by Girard, S., Guillou, A., Stupfler, G. (2012). {\it Frontier&#10;estimation with kernel regression on high order moments}. In the present paper,&#10;we show that this estimator is strongly uniformly consistent on compact sets&#10;and its rate of convergence is given when the conditional cumulative&#10;distribution function belongs to the Hall class of distribution functions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="627" source="Kung-Sik Chan" target="Henghsiu Tsai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5513v1" />
          <attvalue for="2" value="Inference of seasonal long-memory aggregate time series" />
          <attvalue for="3" value="Time-series data with regular and/or seasonal long-memory are often&#10;aggregated before analysis. Often, the aggregation scale is large enough to&#10;remove any short-memory components of the underlying process but too short to&#10;eliminate seasonal patterns of much longer periods. In this paper, we&#10;investigate the limiting correlation structure of aggregate time series within&#10;an intermediate asymptotic framework that attempts to capture the&#10;aforementioned sampling scheme. In particular, we study the autocorrelation&#10;structure and the spectral density function of aggregates from a discrete-time&#10;process. The underlying discrete-time process is assumed to be a stationary&#10;Seasonal AutoRegressive Fractionally Integrated Moving-Average (SARFIMA)&#10;process, after suitable number of differencing if necessary, and the seasonal&#10;periods of the underlying process are multiples of the aggregation size. We&#10;derive the limit of the normalized spectral density function of the aggregates,&#10;with increasing aggregation. The limiting aggregate (seasonal) long-memory&#10;model may then be useful for analyzing aggregate time-series data, which can be&#10;estimated by maximizing the Whittle likelihood. We prove that the maximum&#10;Whittle likelihood estimator (spectral maximum likelihood estimator) is&#10;consistent and asymptotically normal, and study its finite-sample properties&#10;through simulation. The efficacy of the proposed approach is illustrated by a&#10;real-life internet traffic example." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="628" source="João Renato Sebastião" target="Ana Paula Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="629" source="João Renato Sebastião" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="630" source="Ana Paula Martins" target="Luísa Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.1905v1" />
          <attvalue for="2" value="Estimating the Upcrossings Index" />
          <attvalue for="3" value="For stationary sequences, under general local and asymptotic dependence&#10;restrictions, any limiting point process for time normalized upcrossings of&#10;high levels is a compound Poisson process, i.e., there is a clustering of high&#10;upcrossings, where the underlying Poisson points represent cluster positions,&#10;and the multiplicities correspond to cluster sizes. For such classes of&#10;stationary sequences there exists the upcrossings index $\eta,$ $0\leq \eta\leq&#10;1,$ which is directly related to the extremal index $\theta,$ $0\leq \theta\leq&#10;1,$ for suitable high levels. In this paper we consider the problem of&#10;estimating the upcrossings index $\eta$ for a class of stationary sequences&#10;satisfying a mild oscillation restriction. For the proposed estimator,&#10;properties such as consistency and asymptotic normality are studied. Finally,&#10;the performance of the estimator is assessed through simulation studies for&#10;autoregressive processes and case studies in the fields of environment and&#10;finance." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="631" source="Javier Hualde" target="Peter M. Robinson">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.2750v1" />
          <attvalue for="2" value="Gaussian pseudo-maximum likelihood estimation of fractional time series&#10;  models" />
          <attvalue for="3" value="We consider the estimation of parametric fractional time series models in&#10;which not only is the memory parameter unknown, but one may not know whether it&#10;lies in the stationary/invertible region or the nonstationary or noninvertible&#10;regions. In these circumstances, a proof of consistency (which is a&#10;prerequisite for proving asymptotic normality) can be difficult owing to&#10;nonuniform convergence of the objective function over a large admissible&#10;parameter space. In particular, this is the case for the conditional sum of&#10;squares estimate, which can be expected to be asymptotically efficient under&#10;Gaussianity. Without the latter assumption, we establish consistency and&#10;asymptotic normality for this estimate in case of a quite general univariate&#10;model. For a multivariate model, we establish asymptotic normality of a&#10;one-step estimate based on an initial $\sqrt{n}$-consistent estimate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="632" source="L. R. Haff" target="P. T. Kim">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="633" source="L. R. Haff" target="J. -Y. Koo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="634" source="L. R. Haff" target="D. St. P. Richards">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="635" source="P. T. Kim" target="J. -Y. Koo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="636" source="P. T. Kim" target="D. St. P. Richards">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="637" source="J. -Y. Koo" target="D. St. P. Richards">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.3342v1" />
          <attvalue for="2" value="Minimax estimation for mixtures of Wishart distributions" />
          <attvalue for="3" value="The space of positive definite symmetric matrices has been studied&#10;extensively as a means of understanding dependence in multivariate data along&#10;with the accompanying problems in statistical inference. Many books and papers&#10;have been written on this subject, and more recently there has been&#10;considerable interest in high-dimensional random matrices with particular&#10;emphasis on the distribution of certain eigenvalues. With the availability of&#10;modern data acquisition capabilities, smoothing or nonparametric techniques are&#10;required that go beyond those applicable only to data arising in Euclidean&#10;spaces. Accordingly, we present a Fourier method of minimax Wishart mixture&#10;density estimation on the space of positive definite symmetric matrices." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="638" source="Francis Comets" target="Mikael Falconnet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="639" source="Francis Comets" target="Oleg Loukianov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="640" source="Francis Comets" target="Dasha Loukianova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="641" source="Mikael Falconnet" target="Oleg Loukianov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="642" source="Mikael Falconnet" target="Dasha Loukianova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="643" source="Oleg Loukianov" target="Dasha Loukianova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.6328v2" />
          <attvalue for="2" value="Maximum likelihood estimator consistency for ballistic random walk in a&#10;  parametric random environment" />
          <attvalue for="3" value="We consider a one dimensional ballistic random walk evolving in an i.i.d.&#10;parametric random environment. We provide a maximum likelihood estimation&#10;procedure of the environment parameters based on a single observation of the&#10;path till the time it reaches a distant site, and prove that this estimator is&#10;consistent as the distant site tends to infinity. We also explore the numerical&#10;performances of our estimation procedure." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="644" source="Darren Homrighausen" target="Daniel J. McDonald">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6128v2" />
          <attvalue for="2" value="Leave-one-out cross-validation is risk consistent for lasso" />
          <attvalue for="3" value="The lasso procedure is ubiquitous in the statistical and signal processing&#10;literature, and as such, is the target of substantial theoretical and applied&#10;research. While much of this research focuses on the desirable properties that&#10;lasso possesses---predictive risk consistency, sign consistency, correct model&#10;selection---all of it has assumes that the tuning parameter is chosen in an&#10;oracle fashion. Yet, this is impossible in practice. Instead, data analysts&#10;must use the data twice, once to choose the tuning parameter and again to&#10;estimate the model. But only heuristics have ever justified such a procedure.&#10;To this end, we give the first definitive answer about the risk consistency of&#10;lasso when the smoothing parameter is chosen via cross-validation. We show that&#10;under some restrictions on the design matrix, the lasso estimator is still risk&#10;consistent with an empirically chosen tuning parameter." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="645" source="Cécile Durot" target="Piet Groeneboom">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.2118v2" />
          <attvalue for="2" value="Testing equality of functions under monotonicity constraints" />
          <attvalue for="3" value="We consider the problem of testing equality of functions $f_j:[0,1]\to&#10;\mathbb{R}$ for $j=1,2,...,J$ the basis of $J$ independent samples from&#10;possibly different distributions under the assumption that the functions are&#10;monotone. We provide a uniform approach that covers testing equality of&#10;monotone regression curves, equality of monotone densities and equality of&#10;monotone hazards in the random censorship model. Two test statistics are&#10;proposed based on $L_1$-distances. We show that both statistics are&#10;asymptotically normal and we provide bootstrap implementations, which are shown&#10;to have critical regions with asymptotic level $\alpha$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="646" source="Lyudmila Grigoryeva" target="Juan-Pablo Ortega">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4188v1" />
          <attvalue for="2" value="Finite sample forecasting with estimated temporally aggregated linear&#10;  processes" />
          <attvalue for="3" value="We propose a finite sample based predictor for estimated linear one&#10;dimensional time series models and compute the associated total forecasting&#10;error. The expression for the error that we present takes into account the&#10;estimation error. Unlike existing solutions in the literature, our formulas&#10;require neither assumptions on the second order stationarity of the sample nor&#10;Monte Carlo simulations for their evaluation. This result is used to prove the&#10;pertinence of a new hybrid scheme that we put forward for the forecast of&#10;linear temporal aggregates. This novel strategy consists of carrying out the&#10;parameter estimation based on disaggregated data and the prediction based on&#10;the corresponding aggregated model and data. We show that in some instances&#10;this scheme has a better performance than the &quot;all-disaggregated&quot; approach&#10;presented as optimal in the literature." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="647" source="Bernard Bercu" target="Thi Mong Ngoc Nguyen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5432v1" />
          <attvalue for="2" value="On the asymptotic behavior of the Nadaraya-Watson estimator associated&#10;  with the recursive SIR method" />
          <attvalue for="3" value="We investigate the asymptotic behavior of the Nadaraya-Watson estimator for&#10;the estimation of the regression function in a semiparametric regression model.&#10;On the one hand, we make use of the recursive version of the sliced inverse&#10;regression method for the estimation of the unknown parameter of the model. On&#10;the other hand, we implement a recursive Nadaraya-Watson procedure for the&#10;estimation of the regression function which takes into account the previous&#10;estimation of the parameter of the semiparametric regression model. We&#10;establish the almost sure convergence as well as the asymptotic normality for&#10;our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation&#10;procedure on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="648" source="Bernard Bercu" target="Jerome Saracco">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5432v1" />
          <attvalue for="2" value="On the asymptotic behavior of the Nadaraya-Watson estimator associated&#10;  with the recursive SIR method" />
          <attvalue for="3" value="We investigate the asymptotic behavior of the Nadaraya-Watson estimator for&#10;the estimation of the regression function in a semiparametric regression model.&#10;On the one hand, we make use of the recursive version of the sliced inverse&#10;regression method for the estimation of the unknown parameter of the model. On&#10;the other hand, we implement a recursive Nadaraya-Watson procedure for the&#10;estimation of the regression function which takes into account the previous&#10;estimation of the parameter of the semiparametric regression model. We&#10;establish the almost sure convergence as well as the asymptotic normality for&#10;our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation&#10;procedure on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="649" source="Thi Mong Ngoc Nguyen" target="Jerome Saracco">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5432v1" />
          <attvalue for="2" value="On the asymptotic behavior of the Nadaraya-Watson estimator associated&#10;  with the recursive SIR method" />
          <attvalue for="3" value="We investigate the asymptotic behavior of the Nadaraya-Watson estimator for&#10;the estimation of the regression function in a semiparametric regression model.&#10;On the one hand, we make use of the recursive version of the sliced inverse&#10;regression method for the estimation of the unknown parameter of the model. On&#10;the other hand, we implement a recursive Nadaraya-Watson procedure for the&#10;estimation of the regression function which takes into account the previous&#10;estimation of the parameter of the semiparametric regression model. We&#10;establish the almost sure convergence as well as the asymptotic normality for&#10;our Nadaraya-Watson estimator. We also illustrate our semiparametric estimation&#10;procedure on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="650" source="Yuichi Hirose" target="Alan Lee">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1920v1" />
          <attvalue for="2" value="Reparametrization of the least favorable submodel in semi-parametric&#10;  multisample models" />
          <attvalue for="3" value="The method of estimation in Scott and Wild (Biometrika 84 (1997) 57--71 and&#10;J. Statist. Plann. Inference 96 (2001) 3--27) uses a reparametrization of the&#10;profile likelihood that often reduces the computation times dramatically.&#10;Showing the efficiency of estimators for this method has been a challenging&#10;problem. In this paper, we try to solve the problem by investigating conditions&#10;under which the efficient score function and the efficient information matrix&#10;can be expressed in terms of the parameters in the reparametrized model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="651" source="Paulo C. Marques F." target="Carlos A. de B. Pereira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4947v2" />
          <attvalue for="2" value="Bayesian Analysis of Simple Random Densities" />
          <attvalue for="3" value="A tractable nonparametric prior over densities is introduced which is closed&#10;under sampling and exhibits proper posterior asymptotics." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="652" source="Arnak Dalalyan" target="Yuri Ingster">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.6402v3" />
          <attvalue for="2" value="Statistical inference in compound functional models" />
          <attvalue for="3" value="We consider a general nonparametric regression model called the compound&#10;model. It includes, as special cases, sparse additive regression and&#10;nonparametric (or linear) regression with many covariates but possibly a small&#10;number of relevant covariates. The compound model is characterized by three&#10;main parameters: the structure parameter describing the &quot;macroscopic&quot; form of&#10;the compound function, the &quot;microscopic&quot; sparsity parameter indicating the&#10;maximal number of relevant covariates in each component and the usual&#10;smoothness parameter corresponding to the complexity of the members of the&#10;compound. We find non-asymptotic minimax rate of convergence of estimators in&#10;such a model as a function of these three parameters. We also show that this&#10;rate can be attained in an adaptive way." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="653" source="Arnak Dalalyan" target="Laëtitia Comminges">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.1823v3" />
          <attvalue for="2" value="Minimax testing of a composite null hypothesis defined via a quadratic&#10;  functional in the model of regression" />
          <attvalue for="3" value="We consider the problem of testing a particular type of composite null&#10;hypothesis under a nonparametric multivariate regression model. For a given&#10;quadratic functional $Q$, the null hypothesis states that the regression&#10;function $f$ satisfies the constraint $Q[f]=0$, while the alternative&#10;corresponds to the functions for which $Q[f]$ is bounded away from zero. On the&#10;one hand, we provide minimax rates of testing and the exact separation&#10;constants, along with a sharp-optimal testing procedure, for diagonal and&#10;nonnegative quadratic functionals. We consider smoothness classes of&#10;ellipsoidal form and check that our conditions are fulfilled in the particular&#10;case of ellipsoids corresponding to anisotropic Sobolev classes. In this case,&#10;we present a closed form of the minimax rate and the separation constant. On&#10;the other hand, minimax rates for quadratic functionals which are neither&#10;positive nor negative makes appear two different regimes: &quot;regular&quot; and&#10;&quot;irregular&quot;. In the &quot;regular&quot; case, the minimax rate is equal to $n^{-1/4}$&#10;while in the &quot;irregular&quot; case, the rate depends on the smoothness class and is&#10;slower than in the &quot;regular&quot; case. We apply this to the issue of testing the&#10;equality of norms of two functions observed in noisy environments." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="654" source="Min Yang" target="John Stufken">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1058v1" />
          <attvalue for="2" value="Identifying locally optimal designs for nonlinear models: A simple&#10;  extension with profound consequences" />
          <attvalue for="3" value="We extend the approach in [Ann. Statist. 38 (2010) 2499-2524] for identifying&#10;locally optimal designs for nonlinear models. Conceptually the extension is&#10;relatively simple, but the consequences in terms of applications are profound.&#10;As we will demonstrate, we can obtain results for locally optimal designs under&#10;many optimality criteria and for a larger class of models than has been done&#10;hitherto. In many cases the results lead to optimal designs with the minimal&#10;number of support points." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="655" source="David Dereudre" target="Katerina Helisova Stankova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5998v3" />
          <attvalue for="2" value="Estimation of the intensity parameter of the germ-grain&#10;  Quermass-interaction model when the number of germs is not observed" />
          <attvalue for="3" value="The Quermass-interaction model allows to generalise the classical germ-grain&#10;Boolean model in adding a morphological interaction between the grains. It&#10;enables to model random structures with specific morphologies which are&#10;unlikely to be generated from a Boolean model. The Quermass-interaction model&#10;depends in particular on an intensity parameter, which is impossible to&#10;estimate from classical likelihood or pseudo-likelihood approaches because the&#10;number of points is not observable from a germ-grain set. In this paper, we&#10;present a procedure based on the Takacs-Fiksel method which is able to estimate&#10;all parameters of the Quermass-interaction model, including the intensity. An&#10;intensive simulation study is conducted to assess the efficiency of the&#10;procedure and to provide practical recommendations. It also illustrates that&#10;the estimation of the intensity parameter is crucial in order to identify the&#10;model. The Quermass-interaction model is finally fitted by our method to P.&#10;Diggle's heather dataset." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="656" source="Nate Strawn" target="Artin Armagan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="657" source="Nate Strawn" target="Rayan Saab">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="658" source="Nate Strawn" target="Lawrence Carin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="659" source="Artin Armagan" target="Rayan Saab">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="660" source="Artin Armagan" target="Lawrence Carin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="661" source="Rayan Saab" target="Lawrence Carin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4854v3" />
          <attvalue for="2" value="Finite sample posterior concentration in high-dimensional regression" />
          <attvalue for="3" value="We study the behavior of the posterior distribution in high-dimensional&#10;Bayesian Gaussian linear regression models having $p\gg n$, with $p$ the number&#10;of predictors and $n$ the sample size. Our focus is on obtaining quantitative&#10;finite sample bounds ensuring sufficient posterior probability assigned in&#10;neighborhoods of the true regression coefficient vector, $\beta^0$, with high&#10;probability. We assume that $\beta^0$ is approximately $S$-sparse and obtain&#10;universal bounds, which provide insight into the role of the prior in&#10;controlling concentration of the posterior. Based on these finite sample&#10;bounds, we examine the implied asymptotic contraction rates for several&#10;examples showing that sparsely-structured and heavy-tail shrinkage priors&#10;exhibit rapid contraction rates. We also demonstrate that a stronger result&#10;holds for the Uniform-Gaussian\footnote[2]{A binary vector of indicators&#10;($\gamma$) is drawn from the uniform distribution on the set of binary&#10;sequences with exactly $S$ ones, and then each $\beta_i\sim\mathcal{N}(0,V^2)$&#10;if $\gamma_i=1$ and $\beta_i=0$ if $\gamma_i=0$.} prior. These types of finite&#10;sample bounds provide guidelines for designing and evaluating priors for&#10;high-dimensional problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="662" source="Daniel Berend" target="Aryeh Kontorovich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6711v2" />
          <attvalue for="2" value="On the Convergence of the Empirical Distribution" />
          <attvalue for="3" value="We develop a general technique for bounding the tail of the total variation&#10;distance between the empirical and the true distributions over countable sets.&#10;Our methods sharpen a deviation bound of Devroye (1983) for distributions over&#10;finite sets, and also hold for the broader class of distributions with&#10;countable support. We also provide some lower bounds of possible independent&#10;interest." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="663" source="Yu Tang" target="Hongquan Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0897v1" />
          <attvalue for="2" value="Uniform fractional factorial designs" />
          <attvalue for="3" value="The minimum aberration criterion has been frequently used in the selection of&#10;fractional factorial designs with nominal factors. For designs with&#10;quantitative factors, however, level permutation of factors could alter their&#10;geometrical structures and statistical properties. In this paper uniformity is&#10;used to further distinguish fractional factorial designs, besides the minimum&#10;aberration criterion. We show that minimum aberration designs have low&#10;discrepancies on average. An efficient method for constructing uniform minimum&#10;aberration designs is proposed and optimal designs with 27 and 81 runs are&#10;obtained for practical use. These designs have good uniformity and are&#10;effective for studying quantitative factors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="664" source="Yu Tang" target="Dennis K. J. Lin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0897v1" />
          <attvalue for="2" value="Uniform fractional factorial designs" />
          <attvalue for="3" value="The minimum aberration criterion has been frequently used in the selection of&#10;fractional factorial designs with nominal factors. For designs with&#10;quantitative factors, however, level permutation of factors could alter their&#10;geometrical structures and statistical properties. In this paper uniformity is&#10;used to further distinguish fractional factorial designs, besides the minimum&#10;aberration criterion. We show that minimum aberration designs have low&#10;discrepancies on average. An efficient method for constructing uniform minimum&#10;aberration designs is proposed and optimal designs with 27 and 81 runs are&#10;obtained for practical use. These designs have good uniformity and are&#10;effective for studying quantitative factors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="665" source="Hongquan Xu" target="Dennis K. J. Lin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0897v1" />
          <attvalue for="2" value="Uniform fractional factorial designs" />
          <attvalue for="3" value="The minimum aberration criterion has been frequently used in the selection of&#10;fractional factorial designs with nominal factors. For designs with&#10;quantitative factors, however, level permutation of factors could alter their&#10;geometrical structures and statistical properties. In this paper uniformity is&#10;used to further distinguish fractional factorial designs, besides the minimum&#10;aberration criterion. We show that minimum aberration designs have low&#10;discrepancies on average. An efficient method for constructing uniform minimum&#10;aberration designs is proposed and optimal designs with 27 and 81 runs are&#10;obtained for practical use. These designs have good uniformity and are&#10;effective for studying quantitative factors." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="666" source="Samuel Vaiter" target="Charles Deledalle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="667" source="Samuel Vaiter" target="Gabriel Peyré">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="668" source="Samuel Vaiter" target="Charles Dossal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="669" source="Charles Deledalle" target="Gabriel Peyré">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="670" source="Charles Deledalle" target="Charles Dossal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="671" source="Gabriel Peyré" target="Charles Dossal">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1481v1" />
          <attvalue for="2" value="The Degrees of Freedom of the Group Lasso" />
          <attvalue for="3" value="This paper studies the sensitivity to the observations of the block/group&#10;Lasso solution to an overdetermined linear regression model. Such a&#10;regularization is known to promote sparsity patterns structured as&#10;nonoverlapping groups of coefficients. Our main contribution provides a local&#10;parameterization of the solution with respect to the observations. As a&#10;byproduct, we give an unbiased estimate of the degrees of freedom of the group&#10;Lasso. Among other applications of such results, one can choose in a principled&#10;and objective way the regularization parameter of the Lasso through model&#10;selection criteria." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="672" source="Eric Beutner" target="Henryk Zähle">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5899v1" />
          <attvalue for="2" value="Deriving the asymptotic distribution of U- and V-statistics of dependent&#10;  data using weighted empirical processes" />
          <attvalue for="3" value="It is commonly acknowledged that V-functionals with an unbounded kernel are&#10;not Hadamard differentiable and that therefore the asymptotic distribution of&#10;U- and V-statistics with an unbounded kernel cannot be derived by the&#10;Functional Delta Method (FDM). However, in this article we show that&#10;V-functionals are quasi-Hadamard differentiable and that therefore a modified&#10;version of the FDM (introduced recently in (J. Multivariate Anal. 101 (2010)&#10;2452--2463)) can be applied to this problem. The modified FDM requires weak&#10;convergence of a weighted version of the underlying empirical process. The&#10;latter is not problematic since there exist several results on weighted&#10;empirical processes in the literature; see, for example, (J. Econometrics 130&#10;(2006) 307--335, Ann. Probab. 24 (1996) 2098--2127, Empirical Processes with&#10;Applications to Statistics (1986) Wiley, Statist. Sinica 18 (2008) 313--333).&#10;The modified FDM approach has the advantage that it is very flexible w.r.t.&#10;both the underlying data and the estimator of the unknown distribution&#10;function. Both will be demonstrated by various examples. In particular, we will&#10;show that our FDM approach covers mainly all the results known in literature&#10;for the asymptotic distribution of U- and V-statistics based on dependent data&#10;-- and our assumptions are by tendency even weaker. Moreover, using our FDM&#10;approach we extend these results to dependence concepts that are not covered by&#10;the existing literature." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="673" source="Ismael Castillo" target="Gerard Kerkyacharian">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0459v1" />
          <attvalue for="2" value="Thomas Bayes' walk on manifolds" />
          <attvalue for="3" value="Convergence of the Bayes posterior measure is considered in canonical&#10;statistical settings where observations sit on a geometrical object such as a&#10;compact manifold, or more generally on a compact metric space verifying some&#10;conditions. A natural geometric prior based on randomly rescaled solutions of&#10;the heat equation is considered. Upper and lower bound posterior contraction&#10;rates are derived." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="674" source="Xiang Zhang" target="Yanbing Zheng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6363v1" />
          <attvalue for="2" value="A Note on Spatial-Temporal Lattice Modeling and Maximum Likelihood&#10;  Estimation" />
          <attvalue for="3" value="Spatial-temporal linear model and the corresponding likelihood-based&#10;statistical inference are important tools for the analysis of spatial-temporal&#10;lattice data. In this paper, we study the asymptotic properties of maximum&#10;likelihood estimates under a general asymptotic framework for spatial-temporal&#10;linear models. We propose mild regularity conditions on the spatial-temporal&#10;weight matrices and derive the asymptotic properties (consistency and&#10;asymptotic normality) of maximum likelihood estimates. A simulation study is&#10;conducted to examine the finite-sample properties of the maximum likelihood&#10;estimates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="675" source="Tze Leung Lai" target="Shulamith T. Gross">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5140v1" />
          <attvalue for="2" value="Evaluating probability forecasts" />
          <attvalue for="3" value="Probability forecasts of events are routinely used in climate predictions, in&#10;forecasting default probabilities on bank loans or in estimating the&#10;probability of a patient's positive response to treatment. Scoring rules have&#10;long been used to assess the efficacy of the forecast probabilities after&#10;observing the occurrence, or nonoccurrence, of the predicted events. We develop&#10;herein a statistical theory for scoring rules and propose an alternative&#10;approach to the evaluation of probability forecasts. This approach uses loss&#10;functions relating the predicted to the actual probabilities of the events and&#10;applies martingale theory to exploit the temporal structure between the&#10;forecast and the subsequent occurrence or nonoccurrence of the event." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="676" source="Tze Leung Lai" target="David Bo Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5140v1" />
          <attvalue for="2" value="Evaluating probability forecasts" />
          <attvalue for="3" value="Probability forecasts of events are routinely used in climate predictions, in&#10;forecasting default probabilities on bank loans or in estimating the&#10;probability of a patient's positive response to treatment. Scoring rules have&#10;long been used to assess the efficacy of the forecast probabilities after&#10;observing the occurrence, or nonoccurrence, of the predicted events. We develop&#10;herein a statistical theory for scoring rules and propose an alternative&#10;approach to the evaluation of probability forecasts. This approach uses loss&#10;functions relating the predicted to the actual probabilities of the events and&#10;applies martingale theory to exploit the temporal structure between the&#10;forecast and the subsequent occurrence or nonoccurrence of the event." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="677" source="Shulamith T. Gross" target="David Bo Shen">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5140v1" />
          <attvalue for="2" value="Evaluating probability forecasts" />
          <attvalue for="3" value="Probability forecasts of events are routinely used in climate predictions, in&#10;forecasting default probabilities on bank loans or in estimating the&#10;probability of a patient's positive response to treatment. Scoring rules have&#10;long been used to assess the efficacy of the forecast probabilities after&#10;observing the occurrence, or nonoccurrence, of the predicted events. We develop&#10;herein a statistical theory for scoring rules and propose an alternative&#10;approach to the evaluation of probability forecasts. This approach uses loss&#10;functions relating the predicted to the actual probabilities of the events and&#10;applies martingale theory to exploit the temporal structure between the&#10;forecast and the subsequent occurrence or nonoccurrence of the event." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="678" source="Niels Richard Hansen" target="Vincent Rivoirard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.0570v2" />
          <attvalue for="2" value="Lasso and probabilistic inequalities for multivariate point processes" />
          <attvalue for="3" value="Due to its low computational cost, Lasso is an attractive regularization&#10;method for high-dimensional statistical settings. In this paper, we consider&#10;multivariate counting processes depending on an unknown function parameter to&#10;be estimated by linear combinations of a fixed dictionary. To select&#10;coefficients, we propose an adaptive $\ell_1$-penalization methodology, where&#10;data-driven weights of the penalty are derived from new Bernstein type&#10;inequalities for martingales. Oracle inequalities are established under&#10;assumptions on the Gram matrix of the dictionary. Nonasymptotic probabilistic&#10;results for multivariate Hawkes processes are proven, which allows us to check&#10;these assumptions by considering general dictionaries based on histograms,&#10;Fourier or wavelet bases. Motivated by problems of neuronal activity inference,&#10;we finally carry out a simulation study for multivariate Hawkes processes and&#10;compare our methodology with the adaptive Lasso procedure proposed by Zou in&#10;(J. Amer. Statist. Assoc. 101 (2006) 1418-1429). We observe an excellent&#10;behavior of our procedure. We rely on theoretical aspects for the essential&#10;question of tuning our methodology. Unlike adaptive Lasso of (J. Amer. Statist.&#10;Assoc. 101 (2006) 1418-1429), our tuning procedure is proven to be robust with&#10;respect to all the parameters of the problem, revealing its potential for&#10;concrete purposes, in particular in neuroscience." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="679" source="Jean-David Fermanian" target="Dragan Radulovic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.1243v3" />
          <attvalue for="2" value="An asymptotic total variation test for copulas" />
          <attvalue for="3" value="We propose a new goodness-of-fit test for copulas, based on empirical copula&#10;processes and their nonparametric bootstrap counterparts. The standard&#10;Kolmogorov-Smirnov type test for copulas that takes the supremum of the&#10;empirical copula process indexed by half spaces is extended by test statistics&#10;based on the supremum of the empirical copula process indexed by partitions of&#10;Ln rectangles with Ln slowly tending to infinity. Although the underlying&#10;empirical process does not converge, it is proved that the p-values of our new&#10;test statistic can be consistently estimated by the bootstrap. Simulations&#10;confirm that the power of the new procedure is higher than the power of the&#10;standard Kolmogorov-Smirnov test for copulas." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="680" source="Jean-David Fermanian" target="Marten Wegkamp">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.1243v3" />
          <attvalue for="2" value="An asymptotic total variation test for copulas" />
          <attvalue for="3" value="We propose a new goodness-of-fit test for copulas, based on empirical copula&#10;processes and their nonparametric bootstrap counterparts. The standard&#10;Kolmogorov-Smirnov type test for copulas that takes the supremum of the&#10;empirical copula process indexed by half spaces is extended by test statistics&#10;based on the supremum of the empirical copula process indexed by partitions of&#10;Ln rectangles with Ln slowly tending to infinity. Although the underlying&#10;empirical process does not converge, it is proved that the p-values of our new&#10;test statistic can be consistently estimated by the bootstrap. Simulations&#10;confirm that the power of the new procedure is higher than the power of the&#10;standard Kolmogorov-Smirnov test for copulas." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="681" source="Dragan Radulovic" target="Marten Wegkamp">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.1243v3" />
          <attvalue for="2" value="An asymptotic total variation test for copulas" />
          <attvalue for="3" value="We propose a new goodness-of-fit test for copulas, based on empirical copula&#10;processes and their nonparametric bootstrap counterparts. The standard&#10;Kolmogorov-Smirnov type test for copulas that takes the supremum of the&#10;empirical copula process indexed by half spaces is extended by test statistics&#10;based on the supremum of the empirical copula process indexed by partitions of&#10;Ln rectangles with Ln slowly tending to infinity. Although the underlying&#10;empirical process does not converge, it is proved that the p-values of our new&#10;test statistic can be consistently estimated by the bootstrap. Simulations&#10;confirm that the power of the new procedure is higher than the power of the&#10;standard Kolmogorov-Smirnov test for copulas." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="682" source="Guangming Pan" target="Qi-Man Shao">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="683" source="Guangming Pan" target="Wang Zhou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="684" source="Qi-Man Shao" target="Wang Zhou">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.3230v1" />
          <attvalue for="2" value="Nonparametric estimate of spectral density functions of sample&#10;  covariance matrices: A first step" />
          <attvalue for="3" value="The density function of the limiting spectral distribution of general sample&#10;covariance matrices is usually unknown. We propose to use kernel estimators&#10;which are proved to be consistent. A simulation study is also conducted to show&#10;the performance of the estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="685" source="M. Z. Anis" target="Kinjal Basu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.6763v1" />
          <attvalue for="2" value="The exact null distribution of the generalized Hollander-Proschan type&#10;  test for NBUE alternatives" />
          <attvalue for="3" value="In this note we derive the exact null distribution for the test statistic&#10;proposed by Anis and Mitra (2011) for testing exponentiality against NBUE&#10;alternatives. As a special case, we obtain the exact null distribution for the&#10;test statistic proposed by Hollander and Proschan (1975). Selected critical&#10;values for different size are tabulated for these two statistics. Some remarks&#10;concerning the benefits of using the exact distribution are made." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="686" source="Sokbae Lee" target="Myung Hwan Seo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4875v4" />
          <attvalue for="2" value="The Lasso for High-Dimensional Regression with a Possible Change-Point" />
          <attvalue for="3" value="We consider a high-dimensional regression model with a possible change-point&#10;due to a covariate threshold and develop the Lasso estimator of regression&#10;coefficients as well as the threshold parameter. Our Lasso estimator not only&#10;selects covariates but also selects a model between linear and threshold&#10;regression models. Under a sparsity assumption, we derive non-asymptotic oracle&#10;inequalities for both the prediction risk and the $\ell_1$ estimation loss for&#10;regression coefficients. Since the Lasso estimator selects variables&#10;simultaneously, we show that oracle inequalities can be established without&#10;pretesting the existence of the threshold effect. Furthermore, we establish&#10;conditions under which the estimation error of the unknown threshold parameter&#10;can be bounded by a nearly $n^{-1}$ factor even when the number of regressors&#10;can be much larger than the sample size ($n$). We illustrate the usefulness of&#10;our proposed estimation method via Monte Carlo simulations and an application&#10;to real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="687" source="Sokbae Lee" target="Youngki Shin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4875v4" />
          <attvalue for="2" value="The Lasso for High-Dimensional Regression with a Possible Change-Point" />
          <attvalue for="3" value="We consider a high-dimensional regression model with a possible change-point&#10;due to a covariate threshold and develop the Lasso estimator of regression&#10;coefficients as well as the threshold parameter. Our Lasso estimator not only&#10;selects covariates but also selects a model between linear and threshold&#10;regression models. Under a sparsity assumption, we derive non-asymptotic oracle&#10;inequalities for both the prediction risk and the $\ell_1$ estimation loss for&#10;regression coefficients. Since the Lasso estimator selects variables&#10;simultaneously, we show that oracle inequalities can be established without&#10;pretesting the existence of the threshold effect. Furthermore, we establish&#10;conditions under which the estimation error of the unknown threshold parameter&#10;can be bounded by a nearly $n^{-1}$ factor even when the number of regressors&#10;can be much larger than the sample size ($n$). We illustrate the usefulness of&#10;our proposed estimation method via Monte Carlo simulations and an application&#10;to real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="688" source="Myung Hwan Seo" target="Youngki Shin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.4875v4" />
          <attvalue for="2" value="The Lasso for High-Dimensional Regression with a Possible Change-Point" />
          <attvalue for="3" value="We consider a high-dimensional regression model with a possible change-point&#10;due to a covariate threshold and develop the Lasso estimator of regression&#10;coefficients as well as the threshold parameter. Our Lasso estimator not only&#10;selects covariates but also selects a model between linear and threshold&#10;regression models. Under a sparsity assumption, we derive non-asymptotic oracle&#10;inequalities for both the prediction risk and the $\ell_1$ estimation loss for&#10;regression coefficients. Since the Lasso estimator selects variables&#10;simultaneously, we show that oracle inequalities can be established without&#10;pretesting the existence of the threshold effect. Furthermore, we establish&#10;conditions under which the estimation error of the unknown threshold parameter&#10;can be bounded by a nearly $n^{-1}$ factor even when the number of regressors&#10;can be much larger than the sample size ($n$). We illustrate the usefulness of&#10;our proposed estimation method via Monte Carlo simulations and an application&#10;to real data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="689" source="Roberto Fontana" target="Fabio Rapallo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3911v2" />
          <attvalue for="2" value="Saturated fractions of two-factor designs" />
          <attvalue for="3" value="In this paper we study saturated fractions of a two-factor design under the&#10;simple effect model. In particular, we define a criterion to check whether a&#10;given fraction is saturated or not, and we compute the number of saturated&#10;fractions. All proofs are constructive and can be used as actual methods to&#10;build saturated fractions. Moreover, we show how the theory of Markov bases for&#10;contingency tables can be applied to two-factor designs for moving between the&#10;designs with given margins." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="690" source="Roberto Fontana" target="Maria Piera Rogantin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3911v2" />
          <attvalue for="2" value="Saturated fractions of two-factor designs" />
          <attvalue for="3" value="In this paper we study saturated fractions of a two-factor design under the&#10;simple effect model. In particular, we define a criterion to check whether a&#10;given fraction is saturated or not, and we compute the number of saturated&#10;fractions. All proofs are constructive and can be used as actual methods to&#10;build saturated fractions. Moreover, we show how the theory of Markov bases for&#10;contingency tables can be applied to two-factor designs for moving between the&#10;designs with given margins." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="691" source="Fabio Rapallo" target="Maria Piera Rogantin">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3911v2" />
          <attvalue for="2" value="Saturated fractions of two-factor designs" />
          <attvalue for="3" value="In this paper we study saturated fractions of a two-factor design under the&#10;simple effect model. In particular, we define a criterion to check whether a&#10;given fraction is saturated or not, and we compute the number of saturated&#10;fractions. All proofs are constructive and can be used as actual methods to&#10;build saturated fractions. Moreover, we show how the theory of Markov bases for&#10;contingency tables can be applied to two-factor designs for moving between the&#10;designs with given margins." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="692" source="Alexandra Carpentier" target="Rémi Munos">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.4094v1" />
          <attvalue for="2" value="Bandit Theory meets Compressed Sensing for high dimensional Stochastic&#10;  Linear Bandit" />
          <attvalue for="3" value="We consider a linear stochastic bandit problem where the dimension $K$ of the&#10;unknown parameter $\theta$ is larger than the sampling budget $n$. In such&#10;cases, it is in general impossible to derive sub-linear regret bounds since&#10;usual linear bandit algorithms have a regret in $O(K\sqrt{n})$. In this paper&#10;we assume that $\theta$ is $S-$sparse, i.e. has at most $S-$non-zero&#10;components, and that the space of arms is the unit ball for the $||.||_2$ norm.&#10;We combine ideas from Compressed Sensing and Bandit Theory and derive&#10;algorithms with regret bounds in $O(S\sqrt{n})$." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="693" source="Yuqiang Li" target="Hongshuai Dai">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0872v1" />
          <attvalue for="2" value="Approximations of fractional Brownian motion" />
          <attvalue for="3" value="Approximations of fractional Brownian motion using Poisson processes whose&#10;parameter sets have the same dimensions as the approximated processes have been&#10;studied in the literature. In this paper, a special approximation to the&#10;one-parameter fractional Brownian motion is constructed using a two-parameter&#10;Poisson process. The proof involves the tightness and identification of&#10;finite-dimensional distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="694" source="Eugenia Buta" target="Hani Doss">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5160v1" />
          <attvalue for="2" value="Computational approaches for empirical Bayes methods and Bayesian&#10;  sensitivity analysis" />
          <attvalue for="3" value="We consider situations in Bayesian analysis where we have a family of priors&#10;$\nu_h$ on the parameter $\theta$, where $h$ varies continuously over a space&#10;$\mathcal{H}$, and we deal with two related problems. The first involves&#10;sensitivity analysis and is stated as follows. Suppose we fix a function $f$ of&#10;$\theta$. How do we efficiently estimate the posterior expectation of&#10;$f(\theta)$ simultaneously for all $h$ in $\mathcal{H}$? The second problem is&#10;how do we identify subsets of $\mathcal{H}$ which give rise to reasonable&#10;choices of $\nu_h$? We assume that we are able to generate Markov chain samples&#10;from the posterior for a finite number of the priors, and we develop a&#10;methodology, based on a combination of importance sampling and the use of&#10;control variates, for dealing with these two problems. The methodology applies&#10;very generally, and we show how it applies in particular to a commonly used&#10;model for variable selection in Bayesian linear regression, and give an&#10;illustration on the US crime data of Vandaele." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="695" source="Álvaro Calvache" target="Viswanathan Arunachalam">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.6641v1" />
          <attvalue for="2" value="Theory of two-parameter Markov chain with an application in warranty&#10;  study" />
          <attvalue for="3" value="In this paper we present the classical results of Kolmogorov's backward and&#10;forward equations to the case of a two-parameter Markov process. These&#10;equations relates the infinitesimal transition matrix of the two-parameter&#10;Markov process. However, solving these equations is not possible and we require&#10;a numerical procedure. In this paper, we give an alternative method by use of&#10;double Laplace transform of the transition probability matrix and of the&#10;infinitesimal transition matrix of the process. An illustrative example is&#10;presented for the method proposed. In this example, we consider a two-parameter&#10;warranty model, in which a system can be any of these states: working, failure.&#10;We calculate the transition density matrix of these states and also the cost of&#10;the warranty for the proposed model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="696" source="Dominik Janzing" target="David Balduzzi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="697" source="Dominik Janzing" target="Moritz Grosse-Wentrup">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="698" source="Dominik Janzing" target="Bernhard Schölkopf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="699" source="David Balduzzi" target="Moritz Grosse-Wentrup">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="700" source="David Balduzzi" target="Bernhard Schölkopf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="701" source="Moritz Grosse-Wentrup" target="Bernhard Schölkopf">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.6502v2" />
          <attvalue for="2" value="Quantifying causal influences" />
          <attvalue for="3" value="Many methods for causal inference generate directed acyclic graphs (DAGs)&#10;that formalize causal relations between $n$ variables. Given the joint&#10;distribution on all these variables, the DAG contains all information about how&#10;intervening on one variable changes the distribution of the other $n-1$&#10;variables. However, quantifying the causal influence of one variable on another&#10;one remains a nontrivial question. Here we propose a set of natural, intuitive&#10;postulates that a measure of causal strength should satisfy. We then introduce&#10;a communication scenario, where edges in a DAG play the role of channels that&#10;can be locally corrupted by interventions. Causal strength is then the relative&#10;entropy distance between the old and the new distribution. Many other measures&#10;of causal strength have been proposed, including average causal effect,&#10;transfer entropy, directed information, and information flow. We explain how&#10;they fail to satisfy the postulates on simple DAGs of $\leq3$ nodes. Finally,&#10;we investigate the behavior of our measure on time-series, supporting our&#10;claims with experiments on simulated data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="702" source="Romain Guy" target="Catherine Laredo">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0916v2" />
          <attvalue for="2" value="Parametric inference for discretely observed multidimensional diffusions&#10;  with small diffusion coefficient" />
          <attvalue for="3" value="We consider a multidimensional diffusion X with drift coefficient&#10;b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).&#10;The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a&#10;fixed interval [0,T]. We study minimum contrast estimators derived from the&#10;Gaussian process approximating X for small {\epsilon}. We obtain consistent and&#10;asymptotically normal estimators of {\alpha} for fixed {\Delta} and&#10;{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and&#10;{\epsilon}\rightarrow0. We compare the estimators obtained with various methods&#10;and for various magnitudes of {\Delta} and {\epsilon} based on simulation&#10;studies. Finally, we investigate the interest of using such methods in an&#10;epidemiological framework." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="703" source="Romain Guy" target="Elisabeta Vergu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0916v2" />
          <attvalue for="2" value="Parametric inference for discretely observed multidimensional diffusions&#10;  with small diffusion coefficient" />
          <attvalue for="3" value="We consider a multidimensional diffusion X with drift coefficient&#10;b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).&#10;The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a&#10;fixed interval [0,T]. We study minimum contrast estimators derived from the&#10;Gaussian process approximating X for small {\epsilon}. We obtain consistent and&#10;asymptotically normal estimators of {\alpha} for fixed {\Delta} and&#10;{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and&#10;{\epsilon}\rightarrow0. We compare the estimators obtained with various methods&#10;and for various magnitudes of {\Delta} and {\epsilon} based on simulation&#10;studies. Finally, we investigate the interest of using such methods in an&#10;epidemiological framework." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="704" source="Catherine Laredo" target="Elisabeta Vergu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0916v2" />
          <attvalue for="2" value="Parametric inference for discretely observed multidimensional diffusions&#10;  with small diffusion coefficient" />
          <attvalue for="3" value="We consider a multidimensional diffusion X with drift coefficient&#10;b({\alpha},X(t)) and diffusion coefficient {\epsilon}{\sigma}({\beta},X(t)).&#10;The diffusion is discretely observed at times t_k=k{\Delta} for k=1..n on a&#10;fixed interval [0,T]. We study minimum contrast estimators derived from the&#10;Gaussian process approximating X for small {\epsilon}. We obtain consistent and&#10;asymptotically normal estimators of {\alpha} for fixed {\Delta} and&#10;{\epsilon}\rightarrow0 and of ({\alpha},{\beta}) for {\Delta}\rightarrow0 and&#10;{\epsilon}\rightarrow0. We compare the estimators obtained with various methods&#10;and for various magnitudes of {\Delta} and {\epsilon} based on simulation&#10;studies. Finally, we investigate the interest of using such methods in an&#10;epidemiological framework." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="705" source="Sébastien Loustau" target="Clément Marteau">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.3283v3" />
          <attvalue for="2" value="Minimax fast rates for discriminant analysis with errors in variables" />
          <attvalue for="3" value="The effect of measurement errors in discriminant analysis is investigated.&#10;Given observations $Z=X+\epsilon$, where $\epsilon$ denotes a random noise, the&#10;goal is to predict the density of $X$ among two possible candidates $f$ and&#10;$g$. We suppose that we have at our disposal two learning samples. The aim is&#10;to approach the best possible decision rule $G^\star$ defined as a minimizer of&#10;the Bayes risk. In the free-noise case $(\epsilon=0)$, minimax fast rates of&#10;convergence are well-known under the margin assumption in discriminant analysis&#10;(see \cite{mammen}) or in the more general classification framework (see&#10;\cite{tsybakov2004,AT}). In this paper we intend to establish similar results&#10;in the noisy case, i.e. when dealing with errors in variables. We prove minimax&#10;lower bounds for this problem and explain how can these rates be attained,&#10;using in particular an Empirical Risk Minimizer (ERM) method based on&#10;deconvolution kernel estimators." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="706" source="S. Delattre" target="M. Hoffmann">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="707" source="S. Delattre" target="D. Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="708" source="S. Delattre" target="T. Vareschi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="709" source="M. Hoffmann" target="D. Picard">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="710" source="M. Hoffmann" target="T. Vareschi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="711" source="D. Picard" target="T. Vareschi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2964v1" />
          <attvalue for="2" value="Blockwise SVD with error in the operator and application to blind&#10;  deconvolution" />
          <attvalue for="3" value="We consider linear inverse problems in a nonparametric statistical framework.&#10;Both the signal and the operator are unknown and subject to error measurements.&#10;We establish minimax rates of convergence under squared error loss when the&#10;operator admits a blockwise singular value decomposition (blockwise SVD) and&#10;the smoothness of the signal is measured in a Sobolev sense. We construct a&#10;nonlinear procedure adapting simultaneously to the unknown smoothness of both&#10;the signal and the operator and achieving the optimal rate of convergence to&#10;within logarithmic terms. When the noise level in the operator is dominant, by&#10;taking full advantage of the blockwise SVD property, we demonstrate that the&#10;block SVD procedure overperforms classical methods based on Galerkin projection&#10;or nonlinear wavelet thresholding. We subsequently apply our abstract framework&#10;to the specific case of blind deconvolution on the torus and on the sphere." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="712" source="B. T. Knapik" target="B. T. Szabó">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="713" source="B. T. Knapik" target="A. W. van der Vaart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="714" source="B. T. Knapik" target="J. H. van Zanten">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="715" source="B. T. Szabó" target="A. W. van der Vaart">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="716" source="B. T. Szabó" target="J. H. van Zanten">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="717" source="A. W. van der Vaart" target="J. H. van Zanten">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.3628v2" />
          <attvalue for="2" value="Bayes procedures for adaptive inference in inverse problems for the&#10;  white noise model" />
          <attvalue for="3" value="We study empirical and hierarchical Bayes approaches to the problem of&#10;estimating an infinite-dimensional parameter in mildly ill-posed inverse&#10;problems. We consider a class of prior distributions indexed by a&#10;hyperparameter that quantifies regularity. We prove that both methods we&#10;consider succeed in automatically selecting this parameter optimally, resulting&#10;in optimal convergence rates for truths with Sobolev or analytic &quot;smoothness&quot;,&#10;without using knowledge about this regularity. Both methods are illustrated by&#10;simulation examples." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="718" source="J. H. van Zanten" target="R. de Jonge">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.2121v1" />
          <attvalue for="2" value="Adaptive nonparametric Bayesian inference using location-scale mixture&#10;  priors" />
          <attvalue for="3" value="We study location-scale mixture priors for nonparametric statistical&#10;problems, including multivariate regression, density estimation and&#10;classification. We show that a rate-adaptive procedure can be obtained if the&#10;prior is properly constructed. In particular, we show that adaptation is&#10;achieved if a kernel mixture prior on a regression function is constructed&#10;using a Gaussian kernel, an inverse gamma bandwidth, and Gaussian mixing&#10;weights." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="719" source="Víctor Pérez-Abreu" target="Ken-iti Sato">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.1654v1" />
          <attvalue for="2" value="A class of multivariate infinitely divisible distributions related to&#10;  arcsine density" />
          <attvalue for="3" value="Two transformations $\mathcal{A}_1$ and $\mathcal{A}_2$ of L\'{e}vy measures&#10;on $\mathbb{R}^d$ based on the arcsine density are studied and their relation&#10;to general Upsilon transformations is considered. The domains of definition of&#10;$\mathcal{A}_1$ and $\mathcal{A}_2$ are determined and it is shown that they&#10;have the same range. The class of infinitely divisible distributions on&#10;$\mathbb{R}^d$ with L\'{e}vy measures being in the common range is called the&#10;class $A$ and any distribution in the class $A$ is expressed as the law of a&#10;stochastic integral $\int_0^1\cos(2^{-1}\uppi t)\,\mathrm{d}X_t$ with respect&#10;to a L\'{e}vy process $\{X_t\}$. This new class includes as a proper subclass&#10;the Jurek class of distributions. It is shown that generalized type $G$&#10;distributions are the image of distributions in the class $A$ under a mapping&#10;defined by an appropriate stochastic integral. $\mathcal{A}_2$ is identified as&#10;an Upsilon transformation, while $\mathcal{A}_1$ is shown not to be." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="720" source="Arnold Janssen" target="Martin Tietje">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.4611v1" />
          <attvalue for="2" value="Applications of the Likelihood Theory in Finance: Modelling and Pricing" />
          <attvalue for="3" value="This paper discusses the connection between mathematical finance and&#10;statistical modelling which turns out to be more than a formal mathematical&#10;correspondence. We like to figure out how common results and notions in&#10;statistics and their meaning can be translated to the world of mathematical&#10;finance and vice versa. A lot of similarities can be expressed in terms of&#10;LeCam's theory for statistical experiments which is the theory of the behaviour&#10;of likelihood processes. For positive prices the arbitrage free financial&#10;assets fit into filtered experiments. It is shown that they are given by&#10;filtered likelihood ratio processes. From the statistical point of view,&#10;martingale measures, completeness and pricing formulas are revisited. The&#10;pricing formulas for various options are connected with the power functions of&#10;tests. For instance the Black-Scholes price of a European option has an&#10;interpretation as Bayes risk of a Neyman Pearson test. Under contiguity the&#10;convergence of financial experiments and option prices are obtained. In&#10;particular, the approximation of Ito type price processes by discrete models&#10;and the convergence of associated option prices is studied. The result relies&#10;on the central limit theorem for statistical experiments, which is well known&#10;in statistics in connection with local asymptotic normal (LAN) families. As&#10;application certain continuous time option prices can be approximated by&#10;related discrete time pricing formulas." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="721" source="Antar Bandyopadhyay" target="Sanjay Chaudhuri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3570v2" />
          <attvalue for="2" value="Variance Estimation for Tree Order Restricted Models" />
          <attvalue for="3" value="In this article we discuss estimation of the common variance of several&#10;normal populations with tree order restricted means. We discuss the asymptotic&#10;properties of the maximum likelihood estimator of the variance as the number of&#10;populations tends to infinity. We consider several cases of various orders of&#10;the sample sizes and show that the maximum likelihood estimator of the variance&#10;may or may not be consistent or be asymptotically normal." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="722" source="Paul Lemaître" target="Ekatarina Sergienko">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="723" source="Paul Lemaître" target="Aurélie Arnaud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="724" source="Paul Lemaître" target="Nicolas Bousquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="725" source="Ekatarina Sergienko" target="Aurélie Arnaud">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="726" source="Ekatarina Sergienko" target="Nicolas Bousquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="727" source="Aurélie Arnaud" target="Nicolas Bousquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1210.1074v3" />
          <attvalue for="2" value="Density modification based reliability sensitivity analysis" />
          <attvalue for="3" value="Sensitivity analysis of a numerical model, for instance simulating physical&#10;phenomena, is useful to quantify the influence of the inputs on the model&#10;responses. This paper proposes a new sensitivity index, based upon the&#10;modification of the probability density function (pdf) of the random inputs,&#10;when the quantity of interest is a failure probability (probability that a&#10;model output exceeds a given threshold). An input is considered influential if&#10;the input pdf modification leads to a broad change in the failure probability.&#10;These sensitivity indices can be computed using the sole set of simulations&#10;that has already been used to estimate the failure probability, thus limiting&#10;the number of calls to the numerical model. In the case of a Monte Carlo&#10;sample, asymptotical properties of the indices are derived. Based on&#10;Kullback-Leibler divergence, several types of input perturbations are&#10;introduced. The relevance of this new sensitivity analysis method is analysed&#10;through three case studies." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="728" source="Antonio Lijoi" target="Igor Prünster">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5422v1" />
          <attvalue for="2" value="Asymptotics for a Bayesian nonparametric estimator of species variety" />
          <attvalue for="3" value="In Bayesian nonparametric inference, random discrete probability measures are&#10;commonly used as priors within hierarchical mixture models for density&#10;estimation and for inference on the clustering of the data. Recently, it has&#10;been shown that they can also be exploited in species sampling problems: indeed&#10;they are natural tools for modeling the random proportions of species within a&#10;population thus allowing for inference on various quantities of statistical&#10;interest. For applications that involve large samples, the exact evaluation of&#10;the corresponding estimators becomes impracticable and, therefore, asymptotic&#10;approximations are sought. In the present paper, we study the limiting&#10;behaviour of the number of new species to be observed from further sampling,&#10;conditional on observed data, assuming the observations are exchangeable and&#10;directed by a normalized generalized gamma process prior. Such an asymptotic&#10;study highlights a connection between the normalized generalized gamma process&#10;and the two-parameter Poisson-Dirichlet process that was previously known only&#10;in the unconditional case." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="729" source="A. N. Varaksin" target="V. G. Panov">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.2446v1" />
          <attvalue for="2" value="On relationship between regression models and interpretation of multiple&#10;  regression coefficients" />
          <attvalue for="3" value="In this paper, we consider the problem of treating linear regression equation&#10;coefficients in the case of correlated predictors. It is shown that in general&#10;there are no natural ways of interpreting these coefficients similar to the&#10;case of single predictor. Nevertheless we suggest linear transformations of&#10;predictors, reducing multiple regression to a simple one and retaining the&#10;coefficient at variable of interest. The new variable can be treated as the&#10;part of the old variable that has no linear statistical dependence on other&#10;presented variables." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="730" source="C. Fonseca" target="H. Ferreira">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="731" source="C. Fonseca" target="L. Pereira A. P.">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="732" source="C. Fonseca" target="Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="733" source="H. Ferreira" target="L. Pereira A. P.">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="734" source="H. Ferreira" target="Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="735" source="L. Pereira A. P." target="Martins">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.1228v1" />
          <attvalue for="2" value="Stability and contagion measures for spatial extreme value analyses" />
          <attvalue for="3" value="As part of global climate change an accelerated hydrologic cycle (including&#10;an increase in heavy precipitation) is anticipated. So, it is of great&#10;importance to be able to quantify high-impact hydrologic relationships, for&#10;example, the impact that an extreme precipitation (or temperature) in a&#10;location has on a surrounding region. Building on the Multivariate Extreme&#10;Value Theory we propose a contagion index and a stability index. The contagion&#10;index makes it possible to quantify the effect that an exceedance above a high&#10;threshold can have on a region. The stability index reflects the expected&#10;number of crossings of a high threshold in a region associated to a specific&#10;location i, given the occurrence of at least one crossing at that location. We&#10;will find some relations with well-known extremal dependence measures found in&#10;the literature, which will provide immediate estimators. For these estimators&#10;an application to the annual maxima precipitation in Portuguese regions is&#10;presented." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="736" source="Stéphane Girard" target="Pierre Jacob">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.6378v1" />
          <attvalue for="2" value="A note on extreme values and kernel estimators of sample boundaries" />
          <attvalue for="3" value="In a previous paper, we studied a kernel estimate of the upper edge of a&#10;two-dimensional bounded set, based upon the extreme values of a Poisson point&#10;process. The initial paper &quot;Geffroy J. (1964) Sur un probl\`eme d'estimation&#10;g\'eom\'etrique.Publications de l'Institut de Statistique de l'Universit\'e de&#10;Paris, XIII, 191-200&quot; on the subject treats the frontier as the boundary of the&#10;support set for a density and the points as a random sample. We claimed&#10;in&quot;Girard, S. and Jacob, P. (2004) Extreme values and kernel estimates of point&#10;processes boundaries.ESAIM: Probability and Statistics, 8, 150-168&quot; that we are&#10;able to deduce the random sample case fr om the point process case. The present&#10;note gives some essential indications to this end, including a method which can&#10;be of general interest." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="737" source="Alessandro Soranzo" target="Emanuela Epure">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.6403v1" />
          <attvalue for="2" value="Practical Explicitly Invertible Approximation to 4 Decimals of Normal&#10;  Cumulative Distribution Function Modifying Winitzki's Approximation of erf" />
          <attvalue for="3" value="We give a new explicitly invertible approximation of the normal cumulative&#10;distribution function: $\Phi(x) \simeq 1/2 + 1/2&#10;\sqrt{1-{e}^{-x^2\frac{17+{x}^{2}}{26.694+2x^2}}}$, $\forall x \ge 0$, with&#10;absolute error $&lt;4.00\cdot 10^{-5}$, absolute value of the relative error&#10;$&lt;4.53\cdot 10^{-5}$, which, beeing designed essentially for practical use, is&#10;much simpler than a previously published formula and, though less precise,&#10;still reaches 4 decimals of precision, and has a complexity essentially&#10;comparable with that of the approximation of the normal cumulative distribution&#10;function $\Phi(x)$ immediatly derived from Winitzki's approximation of&#10;erf$(x)$, reducing about 36% the absolute error and about 28% the relative&#10;error with respect to that, overcoming the threshold of 4 decimals of&#10;precision." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="738" source="Elena Chernousova" target="Yuri Golubev">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.4207v1" />
          <attvalue for="2" value="Ordered Smoothers With Exponential Weighting" />
          <attvalue for="3" value="The main goal in this paper is to propose a new method for deriving oracle&#10;inequalities related to the exponential weighting method. For the sake of&#10;simplicity we focus on recovering an unknown vector from noisy data with the&#10;help of a family of ordered smoothers. The estimators withing this family are&#10;aggregated using the exponential weighting and the aim is to control the risk&#10;of the aggregated estimate. Based on simple probabilistic properties of the&#10;unbiased risk estimate, we derive new oracle inequalities and show that the&#10;exponential weighting permits to improve Kneip's oracle inequality." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="739" source="Elena Chernousova" target="Katerina Krymova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.4207v1" />
          <attvalue for="2" value="Ordered Smoothers With Exponential Weighting" />
          <attvalue for="3" value="The main goal in this paper is to propose a new method for deriving oracle&#10;inequalities related to the exponential weighting method. For the sake of&#10;simplicity we focus on recovering an unknown vector from noisy data with the&#10;help of a family of ordered smoothers. The estimators withing this family are&#10;aggregated using the exponential weighting and the aim is to control the risk&#10;of the aggregated estimate. Based on simple probabilistic properties of the&#10;unbiased risk estimate, we derive new oracle inequalities and show that the&#10;exponential weighting permits to improve Kneip's oracle inequality." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="740" source="Yuri Golubev" target="Katerina Krymova">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.4207v1" />
          <attvalue for="2" value="Ordered Smoothers With Exponential Weighting" />
          <attvalue for="3" value="The main goal in this paper is to propose a new method for deriving oracle&#10;inequalities related to the exponential weighting method. For the sake of&#10;simplicity we focus on recovering an unknown vector from noisy data with the&#10;help of a family of ordered smoothers. The estimators withing this family are&#10;aggregated using the exponential weighting and the aim is to control the risk&#10;of the aggregated estimate. Based on simple probabilistic properties of the&#10;unbiased risk estimate, we derive new oracle inequalities and show that the&#10;exponential weighting permits to improve Kneip's oracle inequality." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="741" source="Ting Yan" target="Xu Jinfeng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.1058v3" />
          <attvalue for="2" value="Approximating the inverse of a balanced symmetric matrix with positive&#10;  elements" />
          <attvalue for="3" value="For an $n\times n$ balanced symmetric matrix $T=(t_{i,j})$ with positive&#10;elements satisfying $t_{i,i}= \sum_{j\neq i} t_{i,j}$ and certain bounding&#10;conditions, we propose to use the matrix $S=(s_{i,j})$ to approximate its&#10;inverse, where $s_{i,j}=\delta_{i,j}/t_{i,i}-1/t_{..}$, $\delta_{i,j}$ is the&#10;Kronecker delta function, and $t_{..}=\sum_{i,j=1 }^{n}(1-\delta_{i,j})&#10;t_{i,j}$. An explicit bound on the approximation error is obtained, showing&#10;that the inverse is well approximated to order $1/(n-1)^2$ uniformly." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="742" source="Ting Yan" target="Jinfeng Xu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.3307v3" />
          <attvalue for="2" value="A central limit theorem in the $β$-model for undirected random&#10;  graphs with a diverging number of vertices" />
          <attvalue for="3" value="Chatterjee, Diaconis and Sly (2011) recently established the consistency of&#10;the maximum likelihood estimate in the $\beta$-model when the number of&#10;vertices goes to infinity. By approximating the inverse of the Fisher&#10;information matrix, we obtain its asymptotic normality under mild conditions.&#10;Simulation studies and a data example illustrate the theoretical results." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="743" source="Siegfried Hörmann" target="Łukasz Kidziński">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.2895v3" />
          <attvalue for="2" value="A note on estimation in Hilbertian linear models" />
          <attvalue for="3" value="We study estimation and prediction in linear models where the response and&#10;the regressor variable both take values in some Hilbert space. Our main&#10;objective is to obtain consistency of a principal components based estimator&#10;for the regression operator under minimal assumptions. In particular, we avoid&#10;some inconvenient technical restrictions that have been used throughout the&#10;literature. We develop our theory in a time dependent setup which comprises as&#10;important special case the autoregressive Hilbertian model." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="744" source="E. Rufeil Fiori" target="A. Plastino">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.4507v2" />
          <attvalue for="2" value="A Shannon-Tsallis transformation" />
          <attvalue for="3" value="We determine a general link between two different solutions of the MaxEnt&#10;variational problem, namely, the ones that correspond to using either Shannon's&#10;or Tsallis' entropies in the concomitant variational problem. It is shown that&#10;the two variations lead to equivalent solutions that take different appearances&#10;but contain the same information. These solutions are linked by our&#10;transformation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="745" source="Michaël Chichignoud" target="Johannes Lederer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.4447v4" />
          <attvalue for="2" value="A robust, adaptive M-estimator for pointwise estimation in&#10;  heteroscedastic regression" />
          <attvalue for="3" value="We introduce a robust and fully adaptive method for pointwise estimation in&#10;heteroscedastic regression. We allow for noise and design distributions that&#10;are unknown and fulfill very weak assumptions only. In particular, we do not&#10;impose moment conditions on the noise distribution. Moreover, we do not require&#10;a positive density for the design distribution. In a first step, we study the&#10;consistency of locally polynomial M-estimators that consist of a contrast and a&#10;kernel. Afterwards, minimax results are established over unidimensional&#10;H\&quot;older spaces for degenerate design. We then choose the contrast and the&#10;kernel that minimize an empirical variance term and demonstrate that the&#10;corresponding M-estimator is adaptive with respect to the noise and design&#10;distributions and adaptive (Huber) minimax for contamination models. In a&#10;second step, we additionally choose a data-driven bandwidth via Lepski's&#10;method. This leads to an M-estimator that is adaptive with respect to the noise&#10;and design distributions and, additionally, adaptive with respect to the&#10;smoothness of an isotropic, multivariate, locally polynomial target function.&#10;These results are also extended to anisotropic, locally constant target&#10;functions. Our data-driven approach provides, in particular, a level of&#10;robustness that adapts to the noise, contamination, and outliers." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="746" source="Michal Demetrian" target="Martin Nehez">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.6581v1" />
          <attvalue for="2" value="On computation of clustering coefficient in a class of random networks" />
          <attvalue for="3" value="The random networks enriched with additional structures as metric and&#10;group-symmetry in background metric space are investigated. The important&#10;quantities like he clustering coefficient as well as the mean degree of&#10;separation in such networks are effectively computed with help of additional&#10;structures. Representative models are discussed in details." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="747" source="Pierre-Yves Massé" target="William Meiniel">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.3975v3" />
          <attvalue for="2" value="Adaptive confidence bands in the nonparametric fixed design regression&#10;  model" />
          <attvalue for="3" value="In this note, we consider the problem of existence of adaptive confidence&#10;bands in the fixed design regression model, adapting ideas in Hoffmann and&#10;Nickl (2011) to the present case. In the course of the proof, we show that&#10;sup-norm adaptive estimators exist as well in regression." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="748" source="Sébastien Bubeck" target="Gábor Lugosi">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.5536v3" />
          <attvalue for="2" value="Detecting positive correlations in a multivariate sample" />
          <attvalue for="3" value="We consider the problem of testing whether a correlation matrix of a&#10;multivariate normal population is the identity matrix. We focus on sparse&#10;classes of alternatives where only a few entries are nonzero and, in fact,&#10;positive. We derive a general lower bound applicable to various classes and&#10;study the performance of some near-optimal tests. We pay special attention to&#10;computational feasibility and construct near-optimal tests that can be computed&#10;efficiently. Finally, we apply our results to prove new lower bounds for the&#10;clique number of high-dimensional random geometric graphs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="749" source="Jose E. Rodriguez" target="Rogelio Ramos-Quiroga">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.5587v1" />
          <attvalue for="2" value="Response surface methodology: Asymptotic normality of the optimal&#10;  solution" />
          <attvalue for="3" value="Sensitivity analysis of the optimal solution in response surface methodology&#10;is studied and an explicit form of the effect of perturbation of the regression&#10;coefficients on the optimal solution is obtained. The characterisation of the&#10;critical point of the convex program corresponding to the optimum of a response&#10;surface model is also studied. The asymptotic normality of the optimal solution&#10;follows by standard methods." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="750" source="Eckhard Schlemm" target="Robert Stelzer">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0131v1" />
          <attvalue for="2" value="Multivariate CARMA processes, continuous-time state space models and&#10;  complete regularity of the innovations of the sampled processes" />
          <attvalue for="3" value="The class of multivariate L\'{e}vy-driven autoregressive moving average&#10;(MCARMA) processes, the continuous-time analogs of the classical vector ARMA&#10;processes, is shown to be equivalent to the class of continuous-time state&#10;space models. The linear innovations of the weak ARMA process arising from&#10;sampling an MCARMA process at an equidistant grid are proved to be&#10;exponentially completely regular ($\beta$-mixing) under a mild continuity&#10;assumption on the driving L\'{e}vy process. It is verified that this continuity&#10;assumption is satisfied in most practically relevant situations, including the&#10;case where the driving L\'{e}vy process has a non-singular Gaussian component,&#10;is compound Poisson with an absolutely continuous jump size distribution or has&#10;an infinite L\'{e}vy measure admitting a density around zero." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="751" source="Luo Xiao" target="Yingxing Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="752" source="Luo Xiao" target="Tatiyana V. Apanasovich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="753" source="Luo Xiao" target="David Ruppert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="754" source="Yingxing Li" target="Tatiyana V. Apanasovich">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="755" source="Yingxing Li" target="David Ruppert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="756" source="Tatiyana V. Apanasovich" target="David Ruppert">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0708v3" />
          <attvalue for="2" value="Local Asymptotics of P-splines" />
          <attvalue for="3" value="This report studies local asymptotics of P-splines with $p$th degree&#10;B-splines and a $m$th order difference penalty. Earlier work with $p$ and $m$&#10;restricted is extended to the general case. Asymptotically, penalized splines&#10;are kernel estimators with equivalent kernels depending on $m$, but not on $p$.&#10;A central limit theorem provides simple expressions for the asymptotic mean and&#10;variance. Provided it is fast enough, the divergence rate of the number of&#10;knots does not affect the asymptotic distribution. The optimal convergence rate&#10;of the penalty parameter is given." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="757" source="Clément Dombry" target="Mathieu Ribatet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4737v2" />
          <attvalue for="2" value="Conditional simulation of extremal Gaussian processes" />
          <attvalue for="3" value="Recently the regular conditional distributions of max-infinitely divisible&#10;processes were derived by \citet{Dombry2011} and although these conditional&#10;distributions have complicated closed forms, \citet{Dombry2011b} introduce an&#10;algorithm to get conditional realizations of Brown-Resnick processes. In this&#10;paper we derive the regular conditional distributions of the max-stable process&#10;introduced by \citet{Schlather2002} and adapt the framework of&#10;\citet{Dombry2011b} to this specific process. We test the methods on simulated&#10;data and give an application to extreme temperatures in Switzerland. Results&#10;show that the proposed sampling scheme provide accurate conditional simulations&#10;and can handle real-sized problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="758" source="Bodhisattva Sen" target="Michael Woodroofe">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5420v1" />
          <attvalue for="2" value="Bootstrap confidence intervals for isotonic estimators in a&#10;  stereological problem" />
          <attvalue for="3" value="Let $\mathbf{X}=(X_1,X_2,X_3)$ be a spherically symmetric random vector of&#10;which only $(X_1,X_2)$ can be observed. We focus attention on estimating F, the&#10;distribution function of the squared radius $Z:=X_1^2+X_2^2+X_3^2$, from a&#10;random sample of $(X_1,X_2)$. Such a problem arises in astronomy where&#10;$(X_1,X_2,X_3)$ denotes the three dimensional position of a star in a galaxy&#10;but we can only observe the projected stellar positions $(X_1,X_2)$. We&#10;consider isotonic estimators of F and derive their limit distributions. The&#10;results are nonstandard with a rate of convergence $\sqrt{n/{\log n}}$. The&#10;isotonized estimators of F have exactly half the limiting variance when&#10;compared to naive estimators, which do not incorporate the shape constraint. We&#10;consider the problem of constructing point-wise confidence intervals for F,&#10;state sufficient conditions for the consistency of a bootstrap procedure, and&#10;show that the conditions are met by the conventional bootstrap method&#10;(generating samples from the empirical distribution function)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="759" source="Daniel Bonnéry" target="F. Jay Breidt">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5468v1" />
          <attvalue for="2" value="Uniform convergence of the empirical cumulative distribution function&#10;  under informative selection from a finite population" />
          <attvalue for="3" value="Consider informative selection of a sample from a finite population.&#10;Responses are realized as independent and identically distributed (i.i.d.)&#10;random variables with a probability density function (p.d.f.) f, referred to as&#10;the superpopulation model. The selection is informative in the sense that the&#10;sample responses, given that they were selected, are not i.i.d. f. In general,&#10;the informative selection mechanism may induce dependence among the selected&#10;observations. The impact of such dependence on the empirical cumulative&#10;distribution function (c.d.f.) is studied. An asymptotic framework and weak&#10;conditions on the informative selection mechanism are developed under which the&#10;(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,&#10;to a weighted version of the superpopulation c.d.f. This yields an analogue of&#10;the Glivenko-Cantelli theorem. A series of examples, motivated by real problems&#10;in surveys and other observational studies, shows that the conditions are&#10;verifiable for specified designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="760" source="Daniel Bonnéry" target="François Coquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5468v1" />
          <attvalue for="2" value="Uniform convergence of the empirical cumulative distribution function&#10;  under informative selection from a finite population" />
          <attvalue for="3" value="Consider informative selection of a sample from a finite population.&#10;Responses are realized as independent and identically distributed (i.i.d.)&#10;random variables with a probability density function (p.d.f.) f, referred to as&#10;the superpopulation model. The selection is informative in the sense that the&#10;sample responses, given that they were selected, are not i.i.d. f. In general,&#10;the informative selection mechanism may induce dependence among the selected&#10;observations. The impact of such dependence on the empirical cumulative&#10;distribution function (c.d.f.) is studied. An asymptotic framework and weak&#10;conditions on the informative selection mechanism are developed under which the&#10;(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,&#10;to a weighted version of the superpopulation c.d.f. This yields an analogue of&#10;the Glivenko-Cantelli theorem. A series of examples, motivated by real problems&#10;in surveys and other observational studies, shows that the conditions are&#10;verifiable for specified designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="761" source="F. Jay Breidt" target="François Coquet">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.5468v1" />
          <attvalue for="2" value="Uniform convergence of the empirical cumulative distribution function&#10;  under informative selection from a finite population" />
          <attvalue for="3" value="Consider informative selection of a sample from a finite population.&#10;Responses are realized as independent and identically distributed (i.i.d.)&#10;random variables with a probability density function (p.d.f.) f, referred to as&#10;the superpopulation model. The selection is informative in the sense that the&#10;sample responses, given that they were selected, are not i.i.d. f. In general,&#10;the informative selection mechanism may induce dependence among the selected&#10;observations. The impact of such dependence on the empirical cumulative&#10;distribution function (c.d.f.) is studied. An asymptotic framework and weak&#10;conditions on the informative selection mechanism are developed under which the&#10;(unweighted) empirical c.d.f. converges uniformly, in $L_2$ and almost surely,&#10;to a weighted version of the superpopulation c.d.f. This yields an analogue of&#10;the Glivenko-Cantelli theorem. A series of examples, motivated by real problems&#10;in surveys and other observational studies, shows that the conditions are&#10;verifiable for specified designs." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="762" source="Evgueni Haroutunian" target="Parandzem Hakobyan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1204.2975v1" />
          <attvalue for="2" value="Multiple Objects: Error Exponents in Hypotheses Testing and&#10;  Identification" />
          <attvalue for="3" value="We servey a series of investigations of optimal testing of multiple&#10;hypotheses conserning various multiobject models. These studies are a bright&#10;instance of application of methods and technics developed in Shannon&#10;information theory to solution of typical statistical problems." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="763" source="Ahmed A. Quadeer" target="Tareq Y. Al-Naffouri">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.3847v1" />
          <attvalue for="2" value="Structure-Based Bayesian Sparse Reconstruction" />
          <attvalue for="3" value="Sparse signal reconstruction algorithms have attracted research attention due&#10;to their wide applications in various fields. In this paper, we present a&#10;simple Bayesian approach that utilizes the sparsity constraint and a priori&#10;statistical information (Gaussian or otherwise) to obtain near optimal&#10;estimates. In addition, we make use of the rich structure of the sensing matrix&#10;encountered in many signal processing applications to develop a fast sparse&#10;recovery algorithm. The computational complexity of the proposed algorithm is&#10;relatively low compared with the widely used convex relaxation methods as well&#10;as greedy matching pursuit techniques, especially at a low sparsity rate." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="764" source="Luc Devroye" target="Vida Dujmovic">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="765" source="Luc Devroye" target="Adam Krzyzak">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="766" source="Vida Dujmovic" target="Adam Krzyzak">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.0586v2" />
          <attvalue for="2" value="An Affine Invariant $k$-Nearest Neighbor Regression Estimate" />
          <attvalue for="3" value="We design a data-dependent metric in $\mathbb R^d$ and use it to define the&#10;$k$-nearest neighbors of a given point. Our metric is invariant under all&#10;affine transformations. We show that, with this metric, the standard&#10;$k$-nearest neighbor regression estimate is asymptotically consistent under the&#10;usual conditions on $k$, and minimal requirements on the input data." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="767" source="Noureddine Berrahou" target="Lahcen Douge">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.1725v1" />
          <attvalue for="2" value="Bahadur efficiency of nonparametric test for independence based on&#10;  $L_1$-error" />
          <attvalue for="3" value="We introduce new test statistic to test the independence of two&#10;multi-dimensional random variables. Based on the $L_1$-distance and the&#10;historgram density estimation method, the test is compared via Bahadur relative&#10;efficiency to several tests available in the literature. It arises that our&#10;test reaches better performances than a number of usual tests among whom we&#10;cite the Kolmogorov-Smirnov test. Beforehand, large deviation result is stated&#10;for the associated statistic. The local asymptotic optimality relative to the&#10;test is also studied." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="768" source="Ting Zhang" target="Wei Biao Wu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.3552v1" />
          <attvalue for="2" value="Inference of time-varying regression models" />
          <attvalue for="3" value="We consider parameter estimation, hypothesis testing and variable selection&#10;for partially time-varying coefficient models. Our asymptotic theory has the&#10;useful feature that it can allow dependent, nonstationary error and covariate&#10;processes. With a two-stage method, the parametric component can be estimated&#10;with a $n^{1/2}$-convergence rate. A simulation-assisted hypothesis testing&#10;procedure is proposed for testing significance and parameter constancy. We&#10;further propose an information criterion that can consistently select the true&#10;set of significant predictors. Our method is applied to autoregressive models&#10;with time-varying coefficients. Simulation results and a real data application&#10;are provided." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="769" source="Felix Abramovich" target="Vadim Grinshtein">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.3422v2" />
          <attvalue for="2" value="Model selection in regression under structural constraints" />
          <attvalue for="3" value="The paper considers model selection in regression under the additional&#10;structural constraints on admissible models where the number of potential&#10;predictors might be even larger than the available sample size. We develop a&#10;Bayesian formalism as a natural tool for generating a wide class of model&#10;selection criteria based on penalized least squares estimation with various&#10;complexity penalties associated with a prior on a model size. The resulting&#10;criteria are adaptive to structural constraints. We establish the upper bound&#10;for the quadratic risk of the resulting MAP estimator and the corresponding&#10;lower bound for the minimax risk over a set of admissible models of a given&#10;size. We then specify the class of priors (and, therefore, the class of&#10;complexity penalties) where for the &quot;nearly-orthogonal&quot; design the MAP&#10;estimator is asymptotically at least nearly-minimax (up to a log-factor)&#10;simultaneously over an entire range of sparse and dense setups. Moreover, when&#10;the numbers of admissible models are &quot;small&quot; (e.g., ordered variable selection)&#10;or, on the opposite, for the case of complete variable selection, the proposed&#10;estimator achieves the exact minimax rates." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="770" source="Jianhua Hu" target="Guohua Yan">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1155v1" />
          <attvalue for="2" value="Estimation for an additive growth curve model with orthogonal design&#10;  matrices" />
          <attvalue for="3" value="An additive growth curve model with orthogonal design matrices is proposed in&#10;which observations may have different profile forms. The proposed model allows&#10;us to fit data and then estimate parameters in a more parsimonious way than the&#10;traditional growth curve model. Two-stage generalized least-squares estimators&#10;for the regression coefficients are derived where a quadratic estimator for the&#10;covariance of observations is taken as the first-stage estimator. Consistency,&#10;asymptotic normality and asymptotic independence of these estimators are&#10;investigated. Simulation studies and a numerical example are given to&#10;illustrate the efficiency and parsimony of the proposed model for model&#10;specifications in the sense of minimizing Akaike's information criterion (AIC)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="771" source="Jianhua Hu" target="Jinhong You">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1155v1" />
          <attvalue for="2" value="Estimation for an additive growth curve model with orthogonal design&#10;  matrices" />
          <attvalue for="3" value="An additive growth curve model with orthogonal design matrices is proposed in&#10;which observations may have different profile forms. The proposed model allows&#10;us to fit data and then estimate parameters in a more parsimonious way than the&#10;traditional growth curve model. Two-stage generalized least-squares estimators&#10;for the regression coefficients are derived where a quadratic estimator for the&#10;covariance of observations is taken as the first-stage estimator. Consistency,&#10;asymptotic normality and asymptotic independence of these estimators are&#10;investigated. Simulation studies and a numerical example are given to&#10;illustrate the efficiency and parsimony of the proposed model for model&#10;specifications in the sense of minimizing Akaike's information criterion (AIC)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="772" source="Guohua Yan" target="Jinhong You">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1201.1155v1" />
          <attvalue for="2" value="Estimation for an additive growth curve model with orthogonal design&#10;  matrices" />
          <attvalue for="3" value="An additive growth curve model with orthogonal design matrices is proposed in&#10;which observations may have different profile forms. The proposed model allows&#10;us to fit data and then estimate parameters in a more parsimonious way than the&#10;traditional growth curve model. Two-stage generalized least-squares estimators&#10;for the regression coefficients are derived where a quadratic estimator for the&#10;covariance of observations is taken as the first-stage estimator. Consistency,&#10;asymptotic normality and asymptotic independence of these estimators are&#10;investigated. Simulation studies and a numerical example are given to&#10;illustrate the efficiency and parsimony of the proposed model for model&#10;specifications in the sense of minimizing Akaike's information criterion (AIC)." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="773" source="Ping Wu" target="Winfried Stute">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0431v1" />
          <attvalue for="2" value="Efficient estimation of moments in linear mixed models" />
          <attvalue for="3" value="In the linear random effects model, when distributional assumptions such as&#10;normality of the error variables cannot be justified, moments may serve as&#10;alternatives to describe relevant distributions in neighborhoods of their&#10;means. Generally, estimators may be obtained as solutions of estimating&#10;equations. It turns out that there may be several equations, each of them&#10;leading to consistent estimators, in which case finding the efficient estimator&#10;becomes a crucial problem. In this paper, we systematically study estimation of&#10;moments of the errors and random effects in linear mixed models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="774" source="Ping Wu" target="Li-Xing Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0431v1" />
          <attvalue for="2" value="Efficient estimation of moments in linear mixed models" />
          <attvalue for="3" value="In the linear random effects model, when distributional assumptions such as&#10;normality of the error variables cannot be justified, moments may serve as&#10;alternatives to describe relevant distributions in neighborhoods of their&#10;means. Generally, estimators may be obtained as solutions of estimating&#10;equations. It turns out that there may be several equations, each of them&#10;leading to consistent estimators, in which case finding the efficient estimator&#10;becomes a crucial problem. In this paper, we systematically study estimation of&#10;moments of the errors and random effects in linear mixed models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="775" source="Winfried Stute" target="Li-Xing Zhu">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1203.0431v1" />
          <attvalue for="2" value="Efficient estimation of moments in linear mixed models" />
          <attvalue for="3" value="In the linear random effects model, when distributional assumptions such as&#10;normality of the error variables cannot be justified, moments may serve as&#10;alternatives to describe relevant distributions in neighborhoods of their&#10;means. Generally, estimators may be obtained as solutions of estimating&#10;equations. It turns out that there may be several equations, each of them&#10;leading to consistent estimators, in which case finding the efficient estimator&#10;becomes a crucial problem. In this paper, we systematically study estimation of&#10;moments of the errors and random effects in linear mixed models." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="776" source="Tiago M. Vargas" target="Artur J. Lemonte">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.2206v1" />
          <attvalue for="2" value="Gradient statistic: higher-order asymptotics and Bartlett-type&#10;  correction" />
          <attvalue for="3" value="We obtain an asymptotic expansion for the null distribution function of&#10;thegradient statistic for testing composite null hypotheses in the presence of&#10;nuisance parameters. The expansion is derived using a Bayesian route based on&#10;the shrinkage argument described in Ghosh and Mukerjee (1991). Using this&#10;expansion, we propose a Bartlett-type corrected gradient statistic with&#10;chi-square distribution up to an error of order o(n^{-1}) under the null&#10;hypothesis. Further, we also use the expansion to modify the percentage points&#10;of the large sample reference chi-square distribution. A small Monte Carlo&#10;experiment and various examples are presented and discussed." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="777" source="Victoria Plamadeala" target="William F. Rosenberger">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6043v1" />
          <attvalue for="2" value="Sequential monitoring with conditional randomization tests" />
          <attvalue for="3" value="Sequential monitoring in clinical trials is often employed to allow for early&#10;stopping and other interim decisions, while maintaining the type I error rate.&#10;However, sequential monitoring is typically described only in the context of a&#10;population model. We describe a computational method to implement sequential&#10;monitoring in a randomization-based context. In particular, we discuss a new&#10;technique for the computation of approximate conditional tests following&#10;restricted randomization procedures and then apply this technique to&#10;approximate the joint distribution of sequentially computed conditional&#10;randomization tests. We also describe the computation of a randomization-based&#10;analog of the information fraction. We apply these techniques to a restricted&#10;randomization procedure, Efron's [Biometrika 58 (1971) 403--417] biased coin&#10;design. These techniques require derivation of certain conditional&#10;probabilities and conditional covariances of the randomization procedure. We&#10;employ combinatoric techniques to derive these for the biased coin design." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="778" source="Yunwen Yang" target="Xuming He">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5378v1" />
          <attvalue for="2" value="Bayesian empirical likelihood for quantile regression" />
          <attvalue for="3" value="Bayesian inference provides a flexible way of combining data with prior&#10;information. However, quantile regression is not equipped with a parametric&#10;likelihood, and therefore, Bayesian inference for quantile regression demands&#10;careful investigation. This paper considers the Bayesian empirical likelihood&#10;approach to quantile regression. Taking the empirical likelihood into a&#10;Bayesian framework, we show that the resultant posterior from any fixed prior&#10;is asymptotically normal; its mean shrinks toward the true parameter values,&#10;and its variance approaches that of the maximum empirical likelihood estimator.&#10;A more interesting case can be made for the Bayesian empirical likelihood when&#10;informative priors are used to explore commonality across quantiles. Regression&#10;quantiles that are computed separately at each percentile level tend to be&#10;highly variable in the data sparse areas (e.g., high or low percentile levels).&#10;Through empirical likelihood, the proposed method enables us to explore various&#10;forms of commonality across quantiles for efficiency gains. By using an MCMC&#10;algorithm in the computation, we avoid the daunting task of directly maximizing&#10;empirical likelihood. The finite sample performance of the proposed method is&#10;investigated empirically, where substantial efficiency gains are demonstrated&#10;with informative priors on common features across several percentile levels. A&#10;theoretical framework of shrinking priors is used in the paper to better&#10;understand the power of the proposed method." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="779" source="Persi Diaconis" target="Susan Holmes">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6913v2" />
          <attvalue for="2" value="Sampling From A Manifold" />
          <attvalue for="3" value="We develop algorithms for sampling from a probability distribution on a&#10;submanifold embedded in Rn. Applications are given to the evaluation of&#10;algorithms in 'Topological Statistics'; to goodness of fit tests in exponential&#10;families and to Neyman's smooth test. This article is partially expository,&#10;giving an introduction to the tools of geometric measure theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="780" source="Persi Diaconis" target="Mehrdad Shahshahani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6913v2" />
          <attvalue for="2" value="Sampling From A Manifold" />
          <attvalue for="3" value="We develop algorithms for sampling from a probability distribution on a&#10;submanifold embedded in Rn. Applications are given to the evaluation of&#10;algorithms in 'Topological Statistics'; to goodness of fit tests in exponential&#10;families and to Neyman's smooth test. This article is partially expository,&#10;giving an introduction to the tools of geometric measure theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="781" source="Susan Holmes" target="Mehrdad Shahshahani">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.6913v2" />
          <attvalue for="2" value="Sampling From A Manifold" />
          <attvalue for="3" value="We develop algorithms for sampling from a probability distribution on a&#10;submanifold embedded in Rn. Applications are given to the evaluation of&#10;algorithms in 'Topological Statistics'; to goodness of fit tests in exponential&#10;families and to Neyman's smooth test. This article is partially expository,&#10;giving an introduction to the tools of geometric measure theory." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="782" source="Francesco Bartolucci" target="Luisa Scaccia">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4074v1" />
          <attvalue for="2" value="Bayesian inference through encompassing priors and importance sampling&#10;  for a class of marginal models for categorical data" />
          <attvalue for="3" value="We develop a Bayesian approach for selecting the model which is the most&#10;supported by the data within a class of marginal models for categorical&#10;variables formulated through equality and/or inequality constraints on&#10;generalised logits (local, global, continuation or reverse continuation),&#10;generalised log-odds ratios and similar higher-order interactions. For each&#10;constrained model, the prior distribution of the model parameters is formulated&#10;following the encompassing prior approach. Then, model selection is performed&#10;by using Bayes factors which are estimated by an importance sampling method.&#10;The approach is illustrated through three applications involving some datasets,&#10;which also include explanatory variables. In connection with one of these&#10;examples, a sensitivity analysis to the prior specification is also considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="783" source="Francesco Bartolucci" target="Alessio Farcomeni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4074v1" />
          <attvalue for="2" value="Bayesian inference through encompassing priors and importance sampling&#10;  for a class of marginal models for categorical data" />
          <attvalue for="3" value="We develop a Bayesian approach for selecting the model which is the most&#10;supported by the data within a class of marginal models for categorical&#10;variables formulated through equality and/or inequality constraints on&#10;generalised logits (local, global, continuation or reverse continuation),&#10;generalised log-odds ratios and similar higher-order interactions. For each&#10;constrained model, the prior distribution of the model parameters is formulated&#10;following the encompassing prior approach. Then, model selection is performed&#10;by using Bayes factors which are estimated by an importance sampling method.&#10;The approach is illustrated through three applications involving some datasets,&#10;which also include explanatory variables. In connection with one of these&#10;examples, a sensitivity analysis to the prior specification is also considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="784" source="Luisa Scaccia" target="Alessio Farcomeni">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.4074v1" />
          <attvalue for="2" value="Bayesian inference through encompassing priors and importance sampling&#10;  for a class of marginal models for categorical data" />
          <attvalue for="3" value="We develop a Bayesian approach for selecting the model which is the most&#10;supported by the data within a class of marginal models for categorical&#10;variables formulated through equality and/or inequality constraints on&#10;generalised logits (local, global, continuation or reverse continuation),&#10;generalised log-odds ratios and similar higher-order interactions. For each&#10;constrained model, the prior distribution of the model parameters is formulated&#10;following the encompassing prior approach. Then, model selection is performed&#10;by using Bayes factors which are estimated by an importance sampling method.&#10;The approach is illustrated through three applications involving some datasets,&#10;which also include explanatory variables. In connection with one of these&#10;examples, a sensitivity analysis to the prior specification is also considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="785" source="Wei Liu" target="Yuhong Yang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1202.0391v1" />
          <attvalue for="2" value="Parametric or nonparametric? A parametricness index for model selection" />
          <attvalue for="3" value="In model selection literature, two classes of criteria perform well&#10;asymptotically in different situations: Bayesian information criterion (BIC)&#10;(as a representative) is consistent in selection when the true model is finite&#10;dimensional (parametric scenario); Akaike's information criterion (AIC)&#10;performs well in an asymptotic efficiency when the true model is infinite&#10;dimensional (nonparametric scenario). But there is little work that addresses&#10;if it is possible and how to detect the situation that a specific model&#10;selection problem is in. In this work, we differentiate the two scenarios&#10;theoretically under some conditions. We develop a measure, parametricness index&#10;(PI), to assess whether a model selected by a potentially consistent procedure&#10;can be practically treated as the true model, which also hints on AIC or BIC is&#10;better suited for the data for the goal of estimating the regression function.&#10;A consequence is that by switching between AIC and BIC based on the PI, the&#10;resulting regression estimator is simultaneously asymptotically efficient for&#10;both parametric and nonparametric scenarios. In addition, we systematically&#10;investigate the behaviors of PI in simulation and real data and show its&#10;usefulness." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="786" source="M. Jafari Jozani" target="K. F. Davies">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1207.5270v1" />
          <attvalue for="2" value="Some Pitman Closeness Properties Pertinent to Symmetric Populations" />
          <attvalue for="3" value="In this paper, we focus on Pitman closeness probabilities when the estimators&#10;are symmetrically distributed about the unknown parameter $\theta$. We first&#10;consider two symmetric estimators $\hat{\theta}_1$ and $\hat{\theta}_2$ and&#10;obtain necessary and sufficient conditions for $\hat{\theta}_1$ to be Pitman&#10;closer to the common median $\theta$ than $\hat{\theta}_2$. We then establish&#10;some properties in the context of estimation under Pitman closeness criterion.&#10;We define a Pitman closeness probability which measures the frequency with&#10;which an individual order statistic is Pitman closer to $\theta$ than some&#10;symmetric estimator. We show that, for symmetric populations, the sample median&#10;is Pitman closer to the population median than any other symmetrically&#10;distributed estimator of $\theta$. Finally, we discuss the use of Pitman&#10;closeness probabilities in the determination of an optimal ranked set sampling&#10;scheme (denoted by RSS) for the estimation of the population median when the&#10;underlying distribution is symmetric. We show that the best RSS scheme from&#10;symmetric populations in the sense of Pitman closeness is the median and&#10;randomized median RSS for the cases of odd and even sample sizes, respectively." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="787" source="Aixin Tan" target="Galin L. Jones">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.4770v1" />
          <attvalue for="2" value="On the Geometric Ergodicity of Two-Variable Gibbs Samplers" />
          <attvalue for="3" value="A Markov chain is geometrically ergodic if it converges to its in- variant&#10;distribution at a geometric rate in total variation norm. We study geo- metric&#10;ergodicity of deterministic and random scan versions of the two-variable Gibbs&#10;sampler. We give a sufficient condition which simultaneously guarantees both&#10;versions are geometrically ergodic. We also develop a method for simul-&#10;taneously establishing that both versions are subgeometrically ergodic. These&#10;general results allow us to characterize the convergence rate of two-variable&#10;Gibbs samplers in a particular family of discrete bivariate distributions." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="788" source="Jun Shao" target="Xinwei Deng">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1206.0847v1" />
          <attvalue for="2" value="Estimation in high-dimensional linear models with deterministic design&#10;  matrices" />
          <attvalue for="3" value="Because of the advance in technologies, modern statistical studies often&#10;encounter linear models with the number of explanatory variables much larger&#10;than the sample size. Estimation and variable selection in these&#10;high-dimensional problems with deterministic design points is very different&#10;from those in the case of random covariates, due to the identifiability of the&#10;high-dimensional regression parameter vector. We show that a reasonable&#10;approach is to focus on the projection of the regression parameter vector onto&#10;the linear space generated by the design matrix. In this work, we consider the&#10;ridge regression estimator of the projection vector and propose to threshold&#10;the ridge regression estimator when the projection vector is sparse in the&#10;sense that many of its components are small. The proposed estimator has an&#10;explicit form and is easy to use in application. Asymptotic properties such as&#10;the consistency of variable selection and estimation and the convergence rate&#10;of the prediction mean squared error are established under some sparsity&#10;conditions on the projection vector. A simulation study is also conducted to&#10;examine the performance of the proposed estimator." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="789" source="Thierry Dumont" target="Sylvain Le Corff">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1209.0633v4" />
          <attvalue for="2" value="Nonparametric regression on hidden phi-mixing variables: identifiability&#10;  and consistency of a pseudo-likelihood based estimation procedure" />
          <attvalue for="3" value="This paper outlines a new nonparametric estimation procedure for unobserved&#10;phi-mixing processes. It is assumed that the only information on the stationary&#10;hidden states (Xk) is given by the process (Yk), where Yk is a noisy&#10;observation of f(Xk). The paper introduces a maximum pseudo-likelihood&#10;procedure to estimate the function f and the distribution of the hidden states&#10;using blocks of observations of length b. The identifiability of the model is&#10;studied in the particular cases b=1 and b=2. The consistency of the estimators&#10;of f and of the distribution of the hidden states as the number of observations&#10;grows to infinity is established." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="790" source="Jushan Bai" target="Kunpeng Li">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1205.6617v1" />
          <attvalue for="2" value="Statistical analysis of factor models of high dimension" />
          <attvalue for="3" value="This paper considers the maximum likelihood estimation of factor models of&#10;high dimension, where the number of variables (N) is comparable with or even&#10;greater than the number of observations (T). An inferential theory is&#10;developed. We establish not only consistency but also the rate of convergence&#10;and the limiting distributions. Five different sets of identification&#10;conditions are considered. We show that the distributions of the MLE estimators&#10;depend on the identification restrictions. Unlike the principal components&#10;approach, the maximum likelihood estimator explicitly allows&#10;heteroskedasticities, which are jointly estimated with other parameters.&#10;Efficiency of MLE relative to the principal components method is also&#10;considered." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="791" source="Mor Ndongo" target="Abdou Kâ Diongue">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="792" source="Mor Ndongo" target="Aliou Diop">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="793" source="Mor Ndongo" target="Simplice Dossou-Gbété">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="794" source="Abdou Kâ Diongue" target="Aliou Diop">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="795" source="Abdou Kâ Diongue" target="Simplice Dossou-Gbété">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="796" source="Aliou Diop" target="Simplice Dossou-Gbété">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1211.7262v3" />
          <attvalue for="2" value="Estimation for seasonal fractional ARIMA with stable innovations via the&#10;  empirical characteristic function method" />
          <attvalue for="3" value="Maximum likelihood methods, while widely used, may be non-robust due to&#10;disagreement between the assumptions upon which the models are based and the&#10;true density probability distribution of observed data. Because the Empirical&#10;Characteristic Function (ECF) is the Fourier transform of the empirical&#10;distribution function, it retains all the information in the sample but can&#10;overcome difficulties arising from the likelihood. This paper discusses an&#10;estimation method via the ECF for stable seasonal fractional ARIMA processes.&#10;Under some assumptions, we show that the resulting estimators are consistent&#10;and asymptotically normally distributed. For comparison purpose, we consider&#10;also the MCMC Whittle method developed by Ndongo et al. (2010). The performance&#10;of the two methods is discussed using a Monte Carlo simulation." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
      <edge id="797" source="Vladimir Spokoiny" target="Weining Wang">
        <attvalues>
          <attvalue for="1" value="http://arxiv.org/abs/1208.5384v2" />
          <attvalue for="2" value="Local Quantile Regression" />
          <attvalue for="3" value="Quantile regression is a technique to estimate conditional quantile curves.&#10;It provides a comprehensive picture of a response contingent on explanatory&#10;variables. In a flexible modeling framework, a specific form of the conditional&#10;quantile curve is not a priori fixed. % Indeed, the majority of applications do&#10;not per se require specific functional forms. This motivates a local parametric&#10;rather than a global fixed model fitting approach. A nonparametric smoothing&#10;estimator of the conditional quantile curve requires to balance between local&#10;curvature and stochastic variability. In this paper, we suggest a local model&#10;selection technique that provides an adaptive estimator of the conditional&#10;quantile regression curve at each design point. Theoretical results claim that&#10;the proposed adaptive procedure performs as good as an oracle which would&#10;minimize the local estimation risk for the problem at hand. We illustrate the&#10;performance of the procedure by an extensive simulation study and consider a&#10;couple of applications: to tail dependence analysis for the Hong Kong stock&#10;market and to analysis of the distributions of the risk factors of temperature&#10;dynamics." />
          <attvalue for="4" value="3" />
        </attvalues>
      </edge>
    </edges>
  </graph>
</gexf>
