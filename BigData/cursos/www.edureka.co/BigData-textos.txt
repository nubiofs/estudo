(https://www.edureka.co/blog/big-data-tutorial?utm_source=blog&utm_medium=left-menu&utm_campaign=hadoop-tutorial)

* Story of Big Data

In ancient days, people used to travel from one village to another village on a horse driven cart, but as the time passed, villages became towns and people spread out. The distance to travel from one town to the other town also increased. 

1) Ter outro cavalo de reserva (para substituir o anterior)?
2) Ter varios cavalos para levar a mesma carga?

Big Data says, till today, we were okay with storing the data into our servers because the volume of the data was pretty limited, and the amount of time to process this data was also okay.  But now in this current technological world, the data is growing too fast and people are relying on the data a lot of times. Also the speed at which the data is growing, it is becoming impossible to store the data into any server.

+ (Problema do hotel)

+ (Gráfico: Aumento Dados vs Poder Computacional)

+ (Timeline evolução BigData com Google "BigTable x MapReduce x GFS")

+ (Google Trends)

* Big Data Driving Factors

The quantity of data on planet earth is growing exponentially for many reasons.

The major sources of Big Data are social media sites, sensor networks, digital images/videos, cell phones, purchase transaction records, web logs, medical records, archives, military surveillance, eCommerce, complex scientific research and so on.

By 2020, the data volumes will be around 40 Zettabytes which is equivalent to adding every single grain of sand on the planet multiplied by seventy-five.

+ (outros exemplos práticos do volume de dados?)

90 % of the world’s data has been created in last two years.

* What is Big Data?

Big Data is a term used for a collection of data sets that are large and complex, which is difficult to store and process using available database management tools or traditional data processing applications. The challenge includes capturing, curating, storing, searching, sharing, transferring, analyzing and visualization of this data.

+ (Definição Gartner)

* Big Data Characteristics

The five characteristics that define Big Data are: Volume, Velocity, Variety, Veracity and Value.

VOLUME: Volume refers to the ‘amount of data’, which is growing day by day at a very fast pace. The size of data generated by humans, machines and their interactions on social media itself is massive.

+ (Internet das Coisas "poder para nossa infra-estrutura total poder nos falar/comunicar")

VELOCITY: If you are able to handle the velocity, you will be able to generate insights and take decisions based on real-time data.

+ (batch vs Stream)

VARIETY: As there are many sources which are contributing to Big Data, the type of data they are generating is different. It can be structured, semi-structured or unstructured. Earlier, we used to get the data from excel and databases, now the data are coming in the form of images, audios, videos, sensor data etc.

-> Structured: The data that can be stored and processed in a fixed format is called as Structured Data. Data stored in a relational database management system (RDBMS) is one example of  ‘structured’ data. It is easy to process structured data as it has a fixed schema.

-> Semi-Structured: Semi-Structured Data is a type of data which does not have a formal structure of a data model, i.e. a table definition in a relational DBMS, but nevertheless it has some organizational properties like tags and other markers to separate semantic elements that makes it easier to analyze. XML files or JSON documents are examples of semi-structured data.

-> Unstructured: The data which have unknown form and cannot be stored in RDBMS and cannot be analyzed unless it is transformed into a structured format is called as unstructured data. Text Files and multimedia contents like images, audios, videos are example of unstructured data. The unstructured data is growing quicker than others, experts say that 80 percent of the data in an organization are unstructured. 

* Examples of Big Data

    Walmart handles more than 1 million customer transactions every hour.
    Facebook stores, accesses, and analyzes 30+ Petabytes of user generated data.
    230+ millions of tweets are created every day.
    More than 5 billion people are calling, texting, tweeting and browsing on mobile phones worldwide.
    YouTube users upload 48 hours of new video every minute of the day.
    Amazon handles 15 million customer click stream user data per day to recommend products.
    294 billion emails are sent every day. Services analyses this data to find the spams.
    Modern cars have close to 100 sensors which monitors fuel level, tire pressure etc. , each vehicle generates a lot of sensor data.

* Applications of Big Data

benefited by Big Data applications. Almost all the industries today are leveraging Big Data applications in one or the other way.

    Smarter Healthcare: Making use of the petabytes of patient’s data, the organization can extract meaningful information and then build applications that can predict the patient’s deteriorating condition in advance.

    Telecom: Telecom sectors collects information, analyzes it and provide solutions to different problems. By using Big Data applications, telecom companies have been able to significantly reduce data packet loss, which occurs when networks are overloaded, and thus, providing a seamless connection to their customers.

    Retail: Retail has some of the tightest margins, and is one of the greatest beneficiaries of big data. The beauty of using big data in retail is to understand consumer behavior. Amazon’s recommendation engine provides suggestion based on the browsing history of the consumer.

    Traffic control: Traffic congestion is a major challenge for many cities globally. Effective use of data and sensors will be key to managing traffic better as cities become increasingly densely populated.

    Manufacturing: Analyzing big data in the manufacturing industry can reduce component defects, improve product quality, increase efficiency, and save time and money.

    Search Quality: Every time we are extracting information from google, we are simultaneously generating data for it. Google stores this data and uses it to improve its search quality.


* Hadoop to the Rescue

Hadoop is an open source, Java-based programming framework that supports the storage and processing of extremely large data sets in a distributed computing environment.

Hadoop with its distributed processing, handles large volumes of structured and unstructured data more efficiently than the traditional enterprise data warehouse. Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data.


xxxxxxxxxxxxxxxxxxxxxxxxx

(https://www.edureka.co/blog/what-is-hadoop?utm_source=blog&utm_medium=left-menu&utm_campaign=what-is-hadoop)

* Problems with Traditional Approach

The RDBMS focuses mostly on structured data like banking transaction, operational data etc. and Hadoop specializes in semi-structured, unstructured data like text, videos, audios, Facebook posts, logs, etc. RDBMS technology is a proven, highly consistent, matured systems supported by many companies. While on the other hand, Hadoop system technology is developed and is in demand due to Big Data, which mostly consists of unstructured data in different formats.

- So, the first problem is storing the colossal amount of data. Storing this huge data in a traditional system is not possible. The reason is obvious the storage will be limited to one system and the data is increasing in tremendous rate.

- Second problem is storing heterogeneous data. Now we know storing is a problem, but let me tell you it is just one part of the problem. Since we discussed that the data is not only huge, but it is present in various formats as well like: Unstructured, Semi-structured and Structured. So, you need to make sure that you have a system to store these varieties of data, generated from various sources.

- Now, let’s focus on third problem, which is accessing and processing speed. The hard disk capacity is increasing but disk transfer speed or the access speed is not increasing at similar rate. Let me explain you this with an example: If you have only one 100mbps I/O channel and you are processing say 1TB of data, it will take around 2.91 hours. Now, if you have four machines with four I/O channel for the same amount of data, then it will take 43 minutes approx. Thus for me, accessing and processing speed is the bigger problem than storage of Big Data.

+ (colocar figuras de exemplos de acesso ao disco)

* Evolution of Hadoop

Oct 2003 – Google releases papers with GFS (Google File System). In Dec 2004, Google releases papers with MapReduce. In 2005, Nutch used GFS and MapReduce to perform operations. In 2006, Yahoo created Hadoop based on GFS and MapReduce with Doug Cutting and team. You would be surprised if I would tell you that, in 2007 Yahoo started using Hadoop on a 1000 node cluster.

See the History how it was lead to the way it look like.

    2003 – Google launches project Nutch to handle billions of searches and indexing millions of web pages.
    Oct 2003 – Google releases papers with GFS (Google File System)
    Dec 2004 – Google releases papers with MapReduce
    2005 – Nutch used GFS and MapReduce to perform operations
    2006 – Yahoo! created Hadoop based on GFS and MapReduce (with Doug Cutting and team)
    2007 – Yahoo started using Hadoop on a 1000 node cluster
    Jan 2008 – Apache took over Hadoop
    Jul 2008 – Tested a 4000 node cluster with Hadoop successfully
    2009 – Hadoop successfully sorted a petabyte of data in less than 17 hours to handle billions of searches and indexing millions of web pages.
    Dec 2011 – Hadoop releases version 1.0
    Aug 2013 – Version 2.0.6 is available

First Google faced the problem of bigdata and researched and released the white-papers of  Google GFS(google file system) and MapReduce.

Later inspired by GFS and Mapreduce papers Doug Cutting and team lead to a open source Framework “HADOOP” by yahoo.

Now Facebook, Linkedin, ebay, Hortonworks, Cloudera etc have contributed to the Hadoop project.

* What is Hadoop?

Hadoop is a framework that allows you to first store Big Data in a distributed environment so that you can process it parallely. There are basically two components in Hadoop:

The first one is HDFS for storage (Hadoop distributed File System) that allows you to store data of various formats across a cluster. The second one is YARN, it is nothing but a processing unit of Hadoop. It allows parallel processing of data i.e. stored across the HDFS.

HDFS creates an abstraction of resources, let me simplify it for you. Similar as virtualization, you can see HDFS logically as a single unit for storing Big Data, but actually you are storing your data across multiple nodes in a distributed fashion. Here, you have master-slave architecture.

In HDFS, Namenode is a master node and Datanodes are slaves. Namenode contains the metadata about the data stored in Data nodes, like which data block is stored in which data node, where are the replications of the data block kept etc. The actual data is stored in Data Nodes.

I also want to add, we actually replicate the data blocks present in Data Nodes, and by default, the replication factor is 3. Since we are using commodity hardware and we know the failure rate of these hardwares are pretty high, so if one of the DataNodes fails, HDFS will still have the copy of those lost data blocks. That’s the reason we need to replicate the data block. You can configure replication factor based on your requirements.

* Hadoop-as-a-Solution

- The first problem is storing Big data. HDFS solved it, let’s know how.

HDFS provides a distributed way to store Big data. Your Data is stored in blocks in data nodes and you specify the size of each block. Basically, if you have 512MB of data and you have configured HDFS such that it will create 128 MB of data blocks. So HDFS will divide data into 4 blocks as 512/128=4 and store it across different DataNodes, it will also replicate the data blocks on different DataNodes.

It focuses on horizontal scaling instead of vertical scaling. You can always add some extra data nodes to HDFS cluster as and when required, instead of scaling up the resources of your data nodes. Let me summarize it for you basically for storing 1 TB of data I don’t need a 1TB system. You can instead do it on multiple 128GB systems or even less.

- Next problem was storing the variety of data. This problem is also addressed by HDFS.

With HDFS you can store all kinds of data whether it is structured, semi-structured or unstructured. Since in HDFS, there is no pre-dumping schema validation. And it also follows write once and read many model. Due to this, you can just write the data once and you can read it multiple times for finding insights.

- the third challenge was accessing & processing the data faster. Yes, this is one of the major challenges with Big Data. In order to solve it, we move processing to data and not data to processing. What does it mean? Instead of moving data to the master node and then processing it. In YARN, the processing logic is sent to the various slave nodes & then data is processed parallely across different slave nodes. Then the processed results are sent to the master node where the results is merged and the response is sent back to the client.

* When to use Hadoop ? 

Hadoop is used for:

    Search – Yahoo, Amazon, Zvents
    Log processing – Facebook, Yahoo
    Data Warehouse – Facebook, AOL
    Video and Image Analysis – New York Times, Eyealike

* When to not to use Hadoop ?

Following are some of those scenarios :

    Low Latency data access : Quick access to small parts of data
    Multiple data modification : Hadoop is a better fit only if we are primarily concerned about reading data and not writing data.
    Lots of small files : Hadoop is a better fit in scenarios, where we have few but large files.

* Hadoop-CERN Case Study

The Large Hadron Collider in Switzerland is one of the largest and most powerful machines in the world. It is equipped with around 150 million sensors, producing a petabyte of data every second, and the data is growing continuously.

CERN researches said that this data has been scaling up in terms of amount and complexity, and one of the important task is to serve these scalable requirements. So, they setup a Hadoop cluster. By using Hadoop, they limited their cost in hardware and complexity in maintenance.

* They integrated Oracle & Hadoop and they got advantages of integrating. Oracle, optimized their Online Transactional System & Hadoop provided them scalable distributed data processing platform. They designed a hybrid systems, and first they moved data from Oracle to Hadoop. Then, they executed query over Hadoop data from Oracle using Oracle APIs. They also used Hadoop data formats like Avro & Parquet for high performance analytics without need of changing the end-user apps connecting to Oracle.

xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

(https://www.edureka.co/blog/hadoop-tutorial/?utm_source=blog&utm_medium=left-menu&utm_campaign=what-is-hadoop)

*  an interesting story on how Hadoop came into the picture and why is it so popular in the industry nowadays

So, it all started with two people, Mike Cafarella and Doug Cutting, who were in the process of building a search engine system that can index 1 billion pages. After their research, they estimated that such a system will cost around half a million dollars in hardware, with a monthly running cost of $30,000, which is quite expensive. However, they soon realized that their architecture will not be capable enough to work around with billions of pages on the web.

They came across a paper, published in 2003, that described the architecture of Google’s distributed file system, called GFS, which was being used in production at Google. Now, this paper on GFS proved to be something that they were looking for, and soon, they realized that it would solve all their problems of storing very large files that are generated as a part of the web crawl and indexing process. Later in 2004, Google published one more paper that introduced MapReduce to the world. Finally, these two papers led to the foundation of  the framework called “Hadoop“. Doug quoted on Google’s contribution in the development of Hadoop framework:

“Google is living a few years in the future and sending the rest of us messages.”


IoT connects your physical device to the internet and makes it smarter.

* Introduction

Social media is actually one of the most important factors in the evolution of Big Data as it provides information about the people’s behavior. You can look at the figure below and get an idea how much data is getting generated every minute:

* Big Data & Hadoop – Restaurant Analogy

Let us take an analogy of a restaurant to understand the problems associated with Big Data and how Hadoop solved that problem.

Bob is a businessman who has opened a small restaurant. Initially, in his restaurant, he used to receive two orders per hour and he had one chef with one food shelf in his restaurant which was sufficient enough to handle all the orders.

Now let us compare the restaurant example with the traditional scenario where data was getting generated at a steady rate and our traditional systems like RDBMS is capable enough to handle it, just like Bob’s chef. Here, you can relate the data storage with the restaurant’s food shelf and the traditional processing unit with the chef as shown in the figure above.

(Traditional-Restaurant-Analogy-Hadoop-Tutorial-Edureka-768x353.png)

After few months, Bob thought of expanding his business and therefore, he started taking online orders and added few more cuisines to the restaurant’s menu in order to engage a larger audience. Because of this transition, the rate at which they were receiving orders rose to an alarming figure of 10 orders per hour and it became quite difficult for a single cook to cope up with the current situation. Aware of the situation in processing the orders, Bob started thinking about the solution. 

(Traditional-Scenario-Failed-Hadoop-Tutorial-Edureka-768x350.png) 


Similarly, in Big Data scenario, the data started getting generated at an alarming rate because of the introduction of various data growth drivers such as social media, smartphones etc. Now, the traditional system, just like cook in Bob’s restaurant, was not efficient enough to handle this sudden change. Thus, there was a need for a different kind of solutions strategy to cope up with this problem.

After a lot of research, Bob came up with a solution where he hired 4 more chefs to tackle the huge rate of orders being received. Everything was going quite well but, this solution led to one more problem. Since four chefs were sharing the same food shelf, the very food shelf becoming the bottleneck of the whole process. Hence, the solution was not that efficient as Bob had thought. 

"Bringing data to processing generated lot of Network overhead"!!!

Similarly, to tackle the problem of processing huge datasets, multiple processing units were installed so as to process the data parallelly (just like Bob hired 4 chefs). But even in this case bringing multiple processing units was not an effective solution because: the centralized storage unit is the bottleneck. In other words, the performance of the whole system is driven by the performance of the central storage unit. Therefore, the moment our central storage goes down, the whole system gets compromised. Hence, again there was a need to resolve this single point of failure. 


(Distributed-Chef-Hadoop-Tutorial-Edureka-768x371.png)


Bob came up with another efficient solution, he divided all the chefs in two hierarchies, i.e. junior and head chef and assigned each junior chef with a food shelf. Let us assume that the dish is Meat Sauce. Now, according to Bob’s plan, one junior chef will prepare meat and the other junior chef will prepare the sauce. Moving ahead they will transfer both meat and sauce to the head chef, where the head chef will prepare the meat sauce after combining both the ingredients, which then will be delivered as the final order.

(Restaurant-Solution-Hadoop-Tutorial-Edureka-768x362.png)

Hadoop functions in a similar fashion as Bob’s restaurant. As the food shelf is distributed in Bob’s restaurant, similarly, in Hadoop, the data is stored in a distributed fashion with replications, to provide fault tolerance. For parallel processing, first the data is processed by the slaves where it is stored for some intermediate results and then those intermediate results are merged by master node to send the final result.

(Hadoop-as-a-Solution-Restaurant-Analogy-Hadoop-Tutorial-Edureka-768x315.png)

Now, you must have got an idea why Big Data is a problem statement and how Hadoop solves it.

there were three major challenges with Big Data:

    The first problem is storing the colossal amount of data. Storing huge data in a traditional system is not possible. The reason is obvious, the storage will be limited to one system and the data is increasing at a tremendous rate.
    The second problem is storing heterogeneous data. Now we know that storing is a problem, but let me tell you it is just one part of the problem. The data is not only huge, but it is also present in various formats i.e. unstructured, semi-structured and structured. So, you need to make sure that you have a system to store different types of data that is generated from various sources.
    Finally let’s focus on the third problem, which is the processing speed. Now the time taken to process this huge amount of data is quite high as the data to be processed is too large.

- The first problem is storing huge amount of data. 

HDFS provides a distributed way to store Big Data. Your data is stored in blocks in DataNodes and you specify the size of each block. Suppose you have 512MB of data and you have configured HDFS such that it will create 128 MB of data blocks. Now, HDFS will divide data into 4 blocks as 512/128=4 and stores it across different DataNodes. While storing these data blocks into DataNodes, data blocks are replicated on different DataNodes to provide fault tolerance.

Hadoop follows horizontal scaling instead of vertical scaling. In horizontal scaling, you can add new nodes to HDFS cluster on the run as per requirement, instead of increasing the hardware stack present in each node.

- Next problem was storing the variety of data. 

HDFS you can store all kinds of data whether it is structured, semi-structured or unstructured. In HDFS, there is no pre-dumping schema validation. It also follows write once and read many model. Due to this, you can just write any kind of data once and you can read it multiple times for finding insights.

- third challenge was about processing the data faster

In order to solve this, we move processing unit to data instead of moving data to processing unit. So, what does it mean by moving the computation unit to data? It means that instead of moving data from different nodes to a single master node for processing, the processing logic is sent to the nodes where data is stored so as that each node can process a part of data in parallel. Finally, all of the intermediary output produced by each node is merged together and the final response is sent back to the client.

* Hadoop Features

Reliability
Economical
Scalability
Flexibility

* Hadoop Ecosystem

So far you would have figured out that Hadoop is neither a programming language nor a service, it is a platform or framework which solves Big Data problems. You can consider it as a suite which encompasses a number of services for ingesting, storing and analyzing huge data sets along with tools for configuration management.
 
Below are the Hadoop components, that together form a Hadoop ecosystem


    HDFS -> Hadoop Distributed File System
    YARN -> Yet Another Resource Negotiator
    MapReduce -> Data processing using programming
    Spark -> In-memory Data Processing
    PIG, HIVE-> Data Processing Services using Query (SQL-like)
    HBase -> NoSQL Database
    Mahout, Spark MLlib -> Machine Learning
    Apache Drill -> SQL on Hadoop
    Zookeeper -> Managing Cluster
    Oozie -> Job Scheduling
    Flume, Sqoop -> Data Ingesting Services
    Solr & Lucene -> Searching & Indexing 
    Ambari -> Provision, Monitor and Maintain cluster

- HDFS

Hadoop Distributed File System is the core component or you can say, the backbone of Hadoop Ecosystem. 
HDFS is the one, which makes it possible to store different types of large data sets (i.e. structured, unstructured and semi structured data).
HDFS creates a level of abstraction over the resources, from where we can see the whole HDFS as a single unit.
It helps us in storing our data across various nodes and maintaining the log file about the stored data (metadata).
HDFS has two core components, i.e. NameNode and DataNode. 

insane statistics related to HDFS:

    In 2010, Facebook claimed to have one of the largest HDFS cluster storing 21 Petabytes of data.
    In 2012, Facebook declared that they have the largest single HDFS cluster with more than 100 PB of data.
    And Yahoo! has more than 100,000 CPU in over 40,000 servers running Hadoop, with its biggest Hadoop cluster running 4,500 nodes. All told, Yahoo! stores 455 petabytes of data in HDFS.
    In fact, by 2013, most of the big names in the Fortune 50 started using Hadoop.


- YARN

Consider YARN as the brain of your Hadoop Ecosystem. It performs all your processing activities by allocating resources and scheduling tasks.

It has two major components, i.e. ResourceManager and NodeManager.

- MAPREDUCE

It is the core component of processing in a Hadoop Ecosystem as it provides the logic of processing. In other words, MapReduce is a software framework which helps in writing applications that processes large data sets using distributed and parallel algorithms inside Hadoop environment.

----

I would like to draw your attention on three things importantly:

1. Hadoop Ecosystem owes its success to the whole developer community, many big companies like Facebook, Google, Yahoo, University of California (Berkeley) etc. have contributed their part to increase Hadoop’s capabilities.

2. Inside a Hadoop Ecosystem, knowledge about one or two tools (Hadoop components) would not help in building a solution. You need to learn a set of Hadoop components, which works together to build a solution.

3. Based on the use cases, we can choose a set of services from Hadoop Ecosystem and create a tailored solution for an organization.

----

 Distributed & Parallel Computation

(Parallel-Processing-HDFS-Introduction-HDFS-tutorial-Edureka-768x472.png)


Because the data is divided across the machines, it allows us to take advantage of Distributed and Parallel Computation. Let’s understand this concept by the above example. Suppose, it takes 43 minutes to process 1 TB file on a single machine. So, now tell me, how much time will it take to process the same 1 TB file when you have 10 machines in a Hadoop cluster with similar configuration – 43 minutes or 4.3 minutes? 4.3 minutes, Right! What happened here? Each of the nodes is working with a part of the 1 TB file in parallel. Therefore, the work which was taking 43 minutes before, gets finished in just 4.3 minutes now as the work got divided over ten machines.

There are two types of scaling: vertical and horizontal. In vertical scaling (scale up), you increase the hardware capacity of your system. In other words, you procure more RAM or CPU and add it to your existing system to make it more robust and powerful. But there are challenges associated with vertical scaling or scaling up:

    There is always a limit to which you can increase your hardware capacity. So, you can’t keep on increasing the RAM or CPU of the machine.
    In vertical scaling, you stop your machine first. Then you increase the RAM or CPU to make it a more robust hardware stack. After you have increased your hardware capacity, you restart the machine. This down time when you are stopping your system becomes a challenge.

In case of horizontal scaling (scale out), you add more nodes to existing cluster instead of increasing the hardware capacity of individual machines. And most importantly, you can add more machines on the go i.e. Without stopping the system.


High Throughput: Throughput is the amount of work done in a unit time. It talks about how fast you can access the data from the file system. Basically, it gives you an insight about the system performance. As you have seen in the above example where we used ten machines collectively to enhance computation. There we were able to reduce the processing time from 43 minutes to a mere 4.3 minutes as all the machines were working in parallel. Therefore, by processing data in parallel, we decreased the processing time tremendously and thus, achieved high throughput. 



    Data Locality: Data locality talks about moving processing unit to data rather than the data to processing unit. In our traditional system, we used to bring the data to the application layer and then process it. But now, because of the architecture and huge volume of the data, bringing the data to the application layer will reduce the network performance to a noticeable extent. So, in HDFS, we bring the computation part to the data nodes where the data is residing. Hence, you are not moving the data, you are bringing the program or processing part to the data. 


, if we had a block size of let’s say of 4 KB, as in Linux file system, we would be having too many blocks and therefore too much of the metadata. So, managing these no. of blocks and metadata will create huge overhead, which is something, we don’t want.

- Advantages of Rack Awareness:

So, now you will be thinking why do we need a Rack Awareness algorithm? The reasons are:

    To improve the network performance: The communication between nodes residing on different racks is directed via switch. In general, you will find greater network bandwidth between machines in the same rack than the machines residing in different rack. So, the Rack Awareness helps you to have reduce write traffic in between different racks and thus providing a better write performance. Also, you will be gaining increased read performance because you are using the bandwidth of multiple racks.

    To prevent loss of data: We don’t have to worry about the data even if an entire rack fails because of the switch failure or power failure. And if you think about it, it will make sense, as it is said that never put all your eggs in the same basket.

- HDFS Read/ Write Architecture:

Now let’s talk about how the data read/write operations are performed on HDFS. HDFS follows Write Once – Read Many Philosophy. So, you can’t edit files already stored in HDFS. But, you can append new data by re-opening the file.

The client will copy Block A and Block B to the first DataNode simultaneously.
Therefore, in our case, two pipelines will be formed for each of the block and all the process discussed above will happen in parallel in these two pipelines.
The client writes the block into the first DataNode and then the DataNodes will be replicating the block sequentially.

- HDFS Read Architecture:

Now, following steps will be taking place while reading the file:

The client will reach out to NameNode asking for the block metadata for the file “example.txt”.
Once the client gets all the required file blocks, it will combine these blocks to form a file.

While serving read request of the client, HDFS selects the replica which is closest to the client. This reduces the read latency and the bandwidth consumption. 

==============
MapReduce Tutorial: Introduction
=============

Google released a paper on MapReduce technology in December, 2004. This became the genesis of the Hadoop Processing Model. So, MapReduce is a programming model that allows us to perform parallel and distributed processing on huge data sets.

MapReduce is a programming framework that allows us to perform distributed and parallel processing on large data sets in a distributed environment.

MapReduce is based on Divide and Conquer paradigm which helps us to process the data using different machines. As the data is processed by multiple machine instead of a single machine in parallel, the time taken to process the data gets reduced by a tremendous amount

 Data Locality: 

Instead of moving data to the processing unit, we are moving processing unit to the data in the MapReduce Framework.  In the traditional system, we used to bring data to the processing unit and process it. But, as the data grew and became very huge, bringing this huge amount of data to the processing unit posed following issues: 

    Moving huge data to processing is costly and deteriorates the network performance. 
    Processing takes time as the data is processed by a single unit which becomes the bottleneck.
    Master node can get over-burdened and may fail.

the data is distributed among multiple nodes where each node processes the part of the data residing on it. This allows us to have the following advantages:

    It is very cost effective to move processing unit to the data.
    The processing time is reduced as all the nodes are working with their part of the data in parallel.
    Every node gets a part of the data to process and therefore, there is no chance of a node getting overburdened. 

---

IBM states that, every day, almost 2.5 quintillion bytes of data are created, with 90 percent of world’s data created in the last two years! It is a challenging task to store such an expansive amount of data. Hadoop can handle large volumes of structured and unstructured data more efficiently than the traditional enterprise Data Warehouse. It stores these enormous data sets across distributed clusters of computers. Hadoop Streaming uses MapReduce framework which can be used to write applications to process humongous amounts of data.

















---

Hadoop was developed, based on the paper written by Google on MapReduce system and it applies concepts of functional programming.

(The Battle for Next Generation Big Data Analysis Framework)

Apache Flink is a new forth generation Big Data processing tool that is changing the landscape of data processing technologies.

The movement towards the unification of batch and stream processing has been a challenging one. 2nd Generation frameworks such as Scalding or Storm failed to satisfy both requirements at the same time.

Apache Spark was, and continues to be, a great evolution as it introduced lightweight threads instead of heavyweight JVMs, perfected lazy evaluation and introduced SQL and streaming capabilities, while targeting multiple languages. The above, made Apache Spark the 3rd Generation analytics framework.

At the same time and across the industry we were the witnesses of stream processing emerging as a first-class citizen in multiple open source projects including Akka and Kafka. The new technical innovations that bring with them a unique vision and philosophy is now more visible than ever.

Apache Flink is one of the most promising incubating projects and is already considered the 4th Generation of Big Data Analytics Frameworks.

The reason is that since it's conceptual design it was a project born to run everything in a streaming fashion. Even when you get to execute Batch jobs onto Flink, they are executed by the framework as a streaming job, where data stream from the filesystem.
