(https://www.edureka.co/blog/big-data-tutorial?utm_source=blog&utm_medium=left-menu&utm_campaign=hadoop-tutorial)

* Story of Big Data

In ancient days, people used to travel from one village to another village on a horse driven cart, but as the time passed, villages became towns and people spread out. The distance to travel from one town to the other town also increased. 

1) Ter outro cavalo de reserva (para substituir o anterior)?
2) Ter varios cavalos para levar a mesma carga?

Big Data says, till today, we were okay with storing the data into our servers because the volume of the data was pretty limited, and the amount of time to process this data was also okay.  But now in this current technological world, the data is growing too fast and people are relying on the data a lot of times. Also the speed at which the data is growing, it is becoming impossible to store the data into any server.

+ (Problema do hotel)

+ (Gráfico: Aumento Dados vs Poder Computacional)

+ (Timeline evolução BigData com Google "BigTable x MapReduce x GFS")

+ (Google Trends)

* Big Data Driving Factors

The quantity of data on planet earth is growing exponentially for many reasons.

The major sources of Big Data are social media sites, sensor networks, digital images/videos, cell phones, purchase transaction records, web logs, medical records, archives, military surveillance, eCommerce, complex scientific research and so on.

By 2020, the data volumes will be around 40 Zettabytes which is equivalent to adding every single grain of sand on the planet multiplied by seventy-five.

+ (outros exemplos práticos do volume de dados?)

90 % of the world’s data has been created in last two years.

* What is Big Data?

Big Data is a term used for a collection of data sets that are large and complex, which is difficult to store and process using available database management tools or traditional data processing applications. The challenge includes capturing, curating, storing, searching, sharing, transferring, analyzing and visualization of this data.

+ (Definição Gartner)

* Big Data Characteristics

The five characteristics that define Big Data are: Volume, Velocity, Variety, Veracity and Value.

VOLUME: Volume refers to the ‘amount of data’, which is growing day by day at a very fast pace. The size of data generated by humans, machines and their interactions on social media itself is massive.

+ (Internet das Coisas "poder para nossa infra-estrutura total poder nos falar/comunicar")

VELOCITY: If you are able to handle the velocity, you will be able to generate insights and take decisions based on real-time data.

+ (batch vs Stream)

VARIETY: As there are many sources which are contributing to Big Data, the type of data they are generating is different. It can be structured, semi-structured or unstructured. Earlier, we used to get the data from excel and databases, now the data are coming in the form of images, audios, videos, sensor data etc.

-> Structured: The data that can be stored and processed in a fixed format is called as Structured Data. Data stored in a relational database management system (RDBMS) is one example of  ‘structured’ data. It is easy to process structured data as it has a fixed schema.

-> Semi-Structured: Semi-Structured Data is a type of data which does not have a formal structure of a data model, i.e. a table definition in a relational DBMS, but nevertheless it has some organizational properties like tags and other markers to separate semantic elements that makes it easier to analyze. XML files or JSON documents are examples of semi-structured data.

-> Unstructured: The data which have unknown form and cannot be stored in RDBMS and cannot be analyzed unless it is transformed into a structured format is called as unstructured data. Text Files and multimedia contents like images, audios, videos are example of unstructured data. The unstructured data is growing quicker than others, experts say that 80 percent of the data in an organization are unstructured. 

* Examples of Big Data

    Walmart handles more than 1 million customer transactions every hour.
    Facebook stores, accesses, and analyzes 30+ Petabytes of user generated data.
    230+ millions of tweets are created every day.
    More than 5 billion people are calling, texting, tweeting and browsing on mobile phones worldwide.
    YouTube users upload 48 hours of new video every minute of the day.
    Amazon handles 15 million customer click stream user data per day to recommend products.
    294 billion emails are sent every day. Services analyses this data to find the spams.
    Modern cars have close to 100 sensors which monitors fuel level, tire pressure etc. , each vehicle generates a lot of sensor data.

* Applications of Big Data

benefited by Big Data applications. Almost all the industries today are leveraging Big Data applications in one or the other way.

    Smarter Healthcare: Making use of the petabytes of patient’s data, the organization can extract meaningful information and then build applications that can predict the patient’s deteriorating condition in advance.

    Telecom: Telecom sectors collects information, analyzes it and provide solutions to different problems. By using Big Data applications, telecom companies have been able to significantly reduce data packet loss, which occurs when networks are overloaded, and thus, providing a seamless connection to their customers.

    Retail: Retail has some of the tightest margins, and is one of the greatest beneficiaries of big data. The beauty of using big data in retail is to understand consumer behavior. Amazon’s recommendation engine provides suggestion based on the browsing history of the consumer.

    Traffic control: Traffic congestion is a major challenge for many cities globally. Effective use of data and sensors will be key to managing traffic better as cities become increasingly densely populated.

    Manufacturing: Analyzing big data in the manufacturing industry can reduce component defects, improve product quality, increase efficiency, and save time and money.

    Search Quality: Every time we are extracting information from google, we are simultaneously generating data for it. Google stores this data and uses it to improve its search quality.


* Hadoop to the Rescue

Hadoop is an open source, Java-based programming framework that supports the storage and processing of extremely large data sets in a distributed computing environment.

Hadoop with its distributed processing, handles large volumes of structured and unstructured data more efficiently than the traditional enterprise data warehouse. Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data.


xxxxxxxxxxxxxxxxxxxxxxxxx

(https://www.edureka.co/blog/what-is-hadoop?utm_source=blog&utm_medium=left-menu&utm_campaign=what-is-hadoop)

* Problems with Traditional Approach

The RDBMS focuses mostly on structured data like banking transaction, operational data etc. and Hadoop specializes in semi-structured, unstructured data like text, videos, audios, Facebook posts, logs, etc. RDBMS technology is a proven, highly consistent, matured systems supported by many companies. While on the other hand, Hadoop system technology is developed and is in demand due to Big Data, which mostly consists of unstructured data in different formats.

- So, the first problem is storing the colossal amount of data. Storing this huge data in a traditional system is not possible. The reason is obvious the storage will be limited to one system and the data is increasing in tremendous rate.

- Second problem is storing heterogeneous data. Now we know storing is a problem, but let me tell you it is just one part of the problem. Since we discussed that the data is not only huge, but it is present in various formats as well like: Unstructured, Semi-structured and Structured. So, you need to make sure that you have a system to store these varieties of data, generated from various sources.

- Now, let’s focus on third problem, which is accessing and processing speed. The hard disk capacity is increasing but disk transfer speed or the access speed is not increasing at similar rate. Let me explain you this with an example: If you have only one 100mbps I/O channel and you are processing say 1TB of data, it will take around 2.91 hours. Now, if you have four machines with four I/O channel for the same amount of data, then it will take 43 minutes approx. Thus for me, accessing and processing speed is the bigger problem than storage of Big Data.

+ (colocar figuras de exemplos de acesso ao disco)

* Evolution of Hadoop

Oct 2003 – Google releases papers with GFS (Google File System). In Dec 2004, Google releases papers with MapReduce. In 2005, Nutch used GFS and MapReduce to perform operations. In 2006, Yahoo created Hadoop based on GFS and MapReduce with Doug Cutting and team. You would be surprised if I would tell you that, in 2007 Yahoo started using Hadoop on a 1000 node cluster.

See the History how it was lead to the way it look like.

    2003 – Google launches project Nutch to handle billions of searches and indexing millions of web pages.
    Oct 2003 – Google releases papers with GFS (Google File System)
    Dec 2004 – Google releases papers with MapReduce
    2005 – Nutch used GFS and MapReduce to perform operations
    2006 – Yahoo! created Hadoop based on GFS and MapReduce (with Doug Cutting and team)
    2007 – Yahoo started using Hadoop on a 1000 node cluster
    Jan 2008 – Apache took over Hadoop
    Jul 2008 – Tested a 4000 node cluster with Hadoop successfully
    2009 – Hadoop successfully sorted a petabyte of data in less than 17 hours to handle billions of searches and indexing millions of web pages.
    Dec 2011 – Hadoop releases version 1.0
    Aug 2013 – Version 2.0.6 is available

First Google faced the problem of bigdata and researched and released the white-papers of  Google GFS(google file system) and MapReduce.

Later inspired by GFS and Mapreduce papers Doug Cutting and team lead to a open source Framework “HADOOP” by yahoo.

Now Facebook, Linkedin, ebay, Hortonworks, Cloudera etc have contributed to the Hadoop project.
